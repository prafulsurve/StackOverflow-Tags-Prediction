[{"body": "imagine programming a 3 wheel soccer robot. what type of controller would you use for spinning it? p? pid?\n\nthe goal for this controller is that it should make the robot stand in a defined angle ( 0 degree ) and turn back if rotated by hand or other robot. \n\ni use stepper motors for my robot and not servos so i need to implement this in my software!\n\ni have written a sample p type controller already and the movement is fairly good. but i would like to make it better if possible. the code is as follows:\n\n\n\n is a range , in which robot has no movement. \n is a number between -127 and 128 which is returned from the compass.\n is a number between 0 and 255 which is applied to the pwm.\n", "tags": "soccer control", "id": "1", "title": "what is the right approach to write the spin controller for a soccer robot?"}, {"body": "i've got some hobby servos (power hd 1501mgs) and i'd like to be able to control them (via an arduino) so they will either go to the angle i set, or put them in a 'free running' mode, where the load will take them wherever it goes.\n\nis this even possible, or am i just going to end up stripping the gears?\n\nmy first thought is to simply kill the power to the servo, but the force required to move them in that state is more than i'd like.\n\nif it is possible, am i looking at a hardware change, or could i do it in software?\n", "tags": "control rcservo", "id": "2", "title": "how can i modify a low cost hobby servo to run 'freely'?"}, {"body": "http://www.oricomtech.com/projects/leg-time.htm lists three gaits:\n\n\nthe tripod\nwave, and\nripple.\n\n\ncan these be improved, or can their relative pros and cons be altered, and are there other gaits worth considering?\n", "tags": "gait walk", "id": "3", "title": "what useful gaits exist for a six legged robot, and what are their pros and cons?"}, {"body": "i am looking for a starting point for my project, preferably using popular systems (ones there is a lot of support for). i have an arduino uno, a raspberry pi, and a lot of willpower :) anyone here built a project using the systems above?\n\nobservation: i'd like to start with a simple line-following vehicle and build up afterwards.\n", "tags": "microcontroller arduino raspberry-pi", "id": "4", "title": "good microcontrollers/socs for a robotics project"}, {"body": "i'm trying to implement a nearest-neighbor structure for use in an rrt motion planner. in order to do better than a linear brute-force nearest-neighbor search, i'd like to implement something like a kd-tree. however, it seems like the classical implementation of the kd-tree assumes that each dimension of the space can be split into \"left\" and \"right\". this notion doesn't seem to apply to non-euclidean spaces like so(2), for instance.\n\ni'm working with a serial manipulator arm with fully rotational links, meaning that each dimension of the robot's configuration space is so(2), and therefore non-euclidean. can the kd-tree algorithm be modified to handle these kinds of subspaces? if not, is there another nearest-neighbor structure that can handle these non-euclidean subspaces while still being easy to update and query? i also took a look at flann, but it wasn't clear to me from their documentation whether they can handle non-euclidean subspaces.\n", "tags": "motion-planning rrt", "id": "5", "title": "nearest-neighbor data structure for non-euclidean configuration space"}, {"body": "my company will soon be starting a brand new robotics project, and we are still trying to decide whether we should design and code a robotics software platform from scratch, or if there are any good existing ones.\n\nit would be very useful if there was a software platform which was commonly used among both academics and industry so that our robotic system was generally compatible with others, and so that people were already familiar with it.\n\nwe would like the software platform to be able to:\n\n\nintegrate new robotic hardware components easily.\nalready contain a wide array of useful data processing and visualisation tools\nmake efficient use of computing hardware\n\n", "tags": "software platform", "id": "6", "title": "what good robotics software platforms / operating systems are available?"}, {"body": "what is the best software (despite the price) for designing the circuits and pcb boards for robots?\n\ni mean having lots of components, different designing methods, best accuracy, ...\n\ni myself use the altium designer which i think answers my needs, but maybe there are better ones in the market i don't know about!\n", "tags": "software circuit", "id": "11", "title": "what software do you use to design your pcb in the robotics field?"}, {"body": "most often tuning the kalman filter noise matrices is done by trial and error or domain knowledge. are there more principled ways for tuning all the kalman filter parameters?\n", "tags": "odometry localization kalman-filter", "id": "18", "title": "what are good methods for tuning the process noise on kalman filters?"}, {"body": "i'm working with a wild thumper 6 wheel chasis that is designed for use with an rc controller. however i'd like to have a mapping to a keyboard for control as well. can you suggest a set of keys and behaviors that you've used to deal with the continuous value control normally offered by a joystick or pair of joysticks? the standard wasd keys + an accelerate/decelerate pair? i'd also take a pointer to a videogame that you think does this well.\n", "tags": "untagged", "id": "19", "title": "keyboard control map for scalar based movement?"}, {"body": "what is the best option to use for the shooting system of a soccer robot?\n\ni have already implemented a solenoid-based system for shooting and it works perfectly. \nhowever, i'd like some other methods to check if they are better than mine. \n", "tags": "soccer mechanism", "id": "20", "title": "ideas for shooting the ball in a soccer robot"}, {"body": "i am beginning work on a larger scale, 250-350 lbs wheeled robot and looking to use both optical and other means of object avoidance. i am concerned with a robot this large causing issues with running into things, including people as it has a top speed of 15mph and that would cause issues with safety. i am starting out with a remote control but am looking to have the robot become self contained. i have been loosely following the darpa driver-less car project but will not have anywhere near the fiscal or power budget that they do for sensors and computers. am i thinking to far afield with my idea of having a self-contained robot in the 250-300 lbs range that does not break the bank on optical object avoidance? any comments or experiences will be greatly appreciated.\n", "tags": "computer-vision wheeled-robot", "id": "23", "title": "f/oss optical object avoidance"}, {"body": "there are many sites which explain briefly this problem and even propose combinations. i however would like a much more detailed explanation. what is going to give my quad the most agility? do i need bigger motors/props in a heavy quad to achieve the same level of agility than in a lighter quad?\n\nedit:\nhere is what i have understood on the subject:\n\n\na quadcopter doesn't need high revving motors as there are 4 propellers providing thrust and high revving motors require more battery power.\nlarger propellers give more thrust per revolution from the motor.\n\n\nthe question is focused more on the general characteristics of various combinations but some specific questions do spring to mind: \n\n\nfor a given combination what would be the effect of upgrading propeller size in comparison to installing higher revving motors?\nwhat changes would need to be made to lift a heavier quad?\nhow can i achieve more agility in my quad?\n\n", "tags": "quadcopter", "id": "25", "title": "how to choose the right propeller/motor combination for a quadcopter?"}, {"body": "i love computer programming, and if i can interact with the world programming, it's even better. i used to steal them from school, and make little robots that bounce when collided with a wall, but i want to go further, but i don't have the needed pieces, and i don't know any good place to buy pieces , like servos, etc. either.\n\nalso, i'm spanish, so if the page can sell to places in spain, even better.\n\ni would highly appreciate your help,\n\naritzh\n", "tags": "servos", "id": "26", "title": "cheap web to buy robotic pieces from"}, {"body": "i want to localize a mobile robot equipped with a 2d laser scanner in a known indoor environment.  the map is a 2d occupancy grid, but is not perfect.\n\nwhat algorithms are appropriate for mobile robot localization?\n", "tags": "localization mobile-robot", "id": "37", "title": "mobile robot localization in a known map"}, {"body": "as in the title, i'd like to implement gesture recognition on my robot and i'm looking for the pros and cons between the kinect and the xtion - and also if there are any other sensible options available.\n\ni'm thinking of the following, but open to other suggestions:\n\n\naccuracy\nprice\ndriver quality\npower draw\n\n", "tags": "kinect input", "id": "39", "title": "i'd like to use gesture based input for my robot. what are the pros and cons between the xtion live and the kinect?"}, {"body": "i'm looking to potentially build an autonomous robot that will frequently venture off road, and remain autonomous for up to 6 hours at a time. i've found limited information however about the best tyre tread for this purpose, what could be most suitable?\n\ni'm especially looking for a tread pattern that won't need regular cleaning, to save setting this up automatically (a tread that gets \"clogged\" very quickly clearly won't be that effective at tackling tough terrain autonomously.)\n", "tags": "wheel", "id": "42", "title": "what tyre tread would be best suited to an off road robot expected to deal with frequently muddy conditions?"}, {"body": "is there a good, popular and reliable algorithm i can use by taking input from a gyroscope and using this to control two independant wheels to keep such a balanced robot reliably upright? i'm looking for an algorithm that will let me use it to drive a robot around as well as keep it upright when stationary. the ability to deal with inclines and people nudging it would also be a bonus, but not essential.\n", "tags": "control gyroscope balance two-wheeled", "id": "43", "title": "what algorithm should i use for balancing a two wheeled robot using a gyroscope?"}, {"body": "i'm looking to potentially build an underwater glider, a type of submarine that's slow but can operate on extremely low power draw. however, in order for it to work effectively i've found several sources hinting that the dimensions of the components, especially the wings, are critical to its success.\n\nhowever, i've found very sparse information about what these dimensions should be! i'm happy to do a bit of trial and error if it comes down to it, but to save some work does anyone have any information on what the critical dimensions should be?\n", "tags": "design underwater auv", "id": "44", "title": "choosing the right dimensions for an underwater glider"}, {"body": "i'm looking to build an underwater glider robot that will need to remain autonomous for long periods of time, perhaps even months. power draw should be minimal, and i'm thinking of including some form of charging device (such as a solar charger) however i'd also like the battery capacity to be large enough so i don't hugely need to worry about this. large current draw isn't really needed, but the battery does need to hold its charge effectively for long periods of time. considering this is an underwater vehicle, weight and size are also a concern.\n\ncost isn't too much of an issue, as long as it's within reason of a hobbyist project.\n\ni am looking to understand the pros and cons of each technology (lead acid, lipo, nicad, fuel cell?), so i can decide what type of battery would be best suited to my purpose. as such, i'm looking at battery technology rather than looking for a specific shopping recommendation.\n", "tags": "underwater battery auv", "id": "46", "title": "what's the most effective type of rechargeable battery when taking into account size / weight / ah?"}, {"body": "it's common for components on some types of robots to experience large environmental stresses, one key one being vibration. is this something i need to worry about with typical electronics and other sensitive components, or not really? if it is, then how do i secure such components? \n\ni've heard of two main philosophies behind this, the first being that you should use a damping system such as with springs to absorb the shock. the second is that you should keep everything rigidly in place so it can't move, and therefore can't hit against anything else and break.\n\nwhich one should i follow, or if the answer is \"it depends\" what should i use as a guide as to best protect sensitive components?\n", "tags": "electronics protection", "id": "48", "title": "how can i best protect sensitive components against damage through vibration?"}, {"body": "obviously gps is the most obvious and accessible technology for obtaining a locational \"fix\" for a robot at any particular time. however, while it's great sometimes, in other locations and situations it's not as accurate as i'd like, so i'm investigating whether there's a relatively easy way to improve on this accuracy (or not, if that turns out to be the case.)\n\ni've considered the following options, but found limited information online:\n\n\nwould using a much better antenna help, especially for low signal areas? i'm thinking yes to this, but if so how would i construct such an antenna and know that it's an improvement? are there any good guides on how to do this? i could use a ready made antenna if they're not too expensive.\nwould using multiple separate receivers in tandem help, or would they likely all be off by a similar amount, or would i not be able to extract a meaningful average with this approach?\nwhat sort of characteristics should i look for when choosing a good gps receiver to help accuracy?\nis there anything else i should consider which i've missed?\n\n", "tags": "localization gps", "id": "49", "title": "what's the most accurate way to obtain a locational fix using gps?"}, {"body": "ultrasound sensors are incredibly cheap these days which makes them a popular choice for many hobbyist robotic applications, and i'd like to use a bunch of them (say 10) around a robot with an algorithm to build a rough map of an area (as the robot explores it.) i'm not interested in dealing with moving objects at this stage, just pinpointing stationary ones, and i'll be using gps for location. i realise that other components such as a laser scanner would produce much more accurate results, however such devices are also astronomically more expensive.\n\ndoes an algorithm exist for this purpose?\n", "tags": "slam localization gps mapping acoustic-rangefinder", "id": "53", "title": "what algorithm can i use for constructing a map of an explored area using a number of ultrasound sensors?"}, {"body": "hobby servos are generally not sufficient for real robotics for a number of reasons: quality, precision, range of motion, torque, etc.\n\nmeanwhile, industrial servos, such as abb, emerson, ge, etc, are generally both heavy and expensive, and not suitable for small-humanoid scale actuation. similarly, building your own servo from motors, gearboxes, and encoders, is akin to trying to design your own cpu just to control a motor -- too much detail getting in the way of real work.\n\nthere exists an in-between level of servos -- reasonably priced, reasonable performance, and reasonably controllable -- in the form of the competing brands of dynamixel and herculex servos.\n\nthe smallest offerings in those lines generally are not strong enough for real-world interaction, but the next step up hold a lot of promise. for the robotis dynamixel line, this is the rx-24f servo (priced between the cheap ax-12f and the next step up the mx-28r.) asking around, it seems that the specs and interface on that servo is great, but that it shuts down from thermal overload if you actually try to run it at rated load -- something that i'd expect from a hobby servo, but not a robotics servo.\n\nnow, stepping up to the mx-28r doubles the price. thus, if the rx-24f heat flaw could be fixed, it would be positioned at a nice price/performance point.\n\ndoes anyone have experience in providing additional cooling for this servo? anything from thermal-gluing heat sinks to the case, to drilling holes and running cooling fluid tubing to any hot parts on the interior would be reasonable approaches. however, before i spend significant time and effort investigating this, i'd like a second opinion -- is it possible, does anyone have experience doing this, is it worth it?\n", "tags": "servos heat-management cooling", "id": "55", "title": "adding external heat sinking to a dynamixel rx-24f servo?"}, {"body": "what characteristics can i look for which could be reliable early warning signs that a dc motor on my robot, say one used for the drive, could be failing? i'm looking for an answer that deals in terms of sensors rather than manual inspection, so a circuit could be constructed to warn of a potential failure before it happens.\n\ni have a few ideas such as an increase in current draw or decrease in rotation speed / voltage, but i want to guard against false warnings caused by reasonable wear and tear, or just the robot struggling on tough terrain.\n\nobviously such a system will never be foolproof, but are there any points i can look out for?\n", "tags": "sensors failure motor", "id": "57", "title": "how can i detect if a dc motor on a robot is starting to fail?"}, {"body": "with two wheeled robot like this one, i have managed to stabilize it while keeping it stationary. this was done using a digital feedback control system by reading the position of the wheels to determine position, and the natural back electromotive force from the wheel motors was used in the feedback loop to determine velocity. it was kept stable with a pid controller, which was designed using a root locus algorithm to keep it stable and modulate the performance parameters (such as percent overshoot, settling time, etc.). i wanted to attempt to keep it stable while simultaneously propelling it forward, but i couldn't figure out how to go about designing a linear controller that could do that. is it possible to both propel the robot forward and keep it stable using a feedback controller on the wheels, or is a gyroscope necessary?\n", "tags": "two-wheeled stability", "id": "60", "title": "is it possible to both move and stabilize a two wheeled robot with no gyroscopes?"}, {"body": "i'm part of a first robotics team, and we're looking into using mecanum wheels for our robot.\n\nwhat are the advantages and disadvantages of using mecanum wheel versus regular ones? from looking through google, it looks like mecanum wheels give more mobility but don't have as much traction. are there any other advantages or disadvantages?\n\ncompared to regular wheels, are mecanum wheels less efficient or more efficient in any way? and if so, is there a quantifiable way to determine by how much?\n\nare there equations i can use to calculate efficiency (or inefficiency) and/or speed of moving forwards, sideways, or at arbitrary angles?\n\na picture of a robot with mecanum wheels:\n\n\n", "tags": "mobile-robot design movement wheel first-robotics", "id": "65", "title": "calculating the efficiency of mecanum wheels"}, {"body": "i've got this driver: http://www.pololu.com/catalog/product/1182\n\n... a a4988 stepper motor driver carrier\n\n\n\ni'm attempting to control a connection between the reset and sleep pins with logic ( code ) running on my arduino. the motor runs perfectly when these two pins are connected however i'd like to control when the stepper is powered off from my arduino ( and thus not generating extra heat )\n\ni'd like to:\n\n\ndesignate a pin to control the connection between these two pins\nuse a \"digitalwrite\" to the above pin with a high or low to\nswitch power on and off from the stepper\n\n\nnote: the data sheet mentioned that for the driver to be powering the stepper both reset and sleep needed to be in switched on ( high )\n", "tags": "arduino logic-control stepper-motor stepper-driver", "id": "75", "title": "using an arduino to control an on / off connection between two pins"}, {"body": "for a robotic gripper arm we are designing for factory floor use on very small components, we propose to use electrically activated shape memory alloy (sma) wire harnesses for actuation. \n\nthe device being designed is akin to pick &amp; place machines used for circuit assembly, but moves over an aircraft-hanger sized work surface on wheels. it manipulates irregular shaped and porous objects between 0.5 cu.cm and 8 cu.cm each - hence the traditional vacuum p&amp;p mechanism does not appeal. also, individual objects in the assembly line have varying hardness and weights.\n\nour design constraints are: \n\n\nensuring minimal to zero vibration and sound \nusing minimal volume within the mechanism (batteries are at the wheelbase, providing stability, so their weight is not a concern)\nfine variation of gripper pressure\n\n\nwe believe sma meets the first two constraints well, but need some guidance on achieving constraint 3, i.e. different levels of pressure of the gripper controlled electronically.\n\nmy questions:\n\n\ncan pwm of a current above the activation threshold (320 ma for 0.005 inch flexinol ht) provide variable, repeatable actuation force? \nwould we need pressure sensors on each fingertip and a closed loop control for grip, or can the gripper be calibrated periodically and maintain repeatable force?\nis there any well-documented precedent or study we should be referring to?\n\n", "tags": "mobile-robot", "id": "85", "title": "shape memory alloy wire for robot gripper arm actuation: how to vary grip pressure?"}, {"body": "design goal is to have a mobile robot that operates on 3 large casters, essentially 2 to 4 inch diameter steel ball bearings, that are motorized. no other mechanism would touch the surface. the robot should thus be able to move in any xy direction on a flat surface, with steering being achieved by varying the speed and rolling direction of these wheels. the robot has no designated \"front\" side, so it does not need to (and should not have to) bodily turn, in order to move off in any given direction. \n\nconventional wheels or tracks are not the preferred approach. \n\nlooking for suggested mechanical layouts of multiple rubber wheels, pressing down onto the steel ball from within the castor housing, to drive the ball in any direction. a single wheel on a stepper, rotated around the vertical axis using a sail-winch servo, is one approach under consideration. would this be ideal, or are there any serious flaws in this approach?\n\nalternatively, is there any other suggested method of driving such a steel ball in any arbitrary direction under electronic control?\n", "tags": "servos mobile-robot stepper-motor", "id": "88", "title": "mechanical design for motorized spherical caster wheels"}, {"body": "many robots and other mechanical devices produce the signature whirring noise as they move, some produce less. what makes the difference? what restrictions a silence requirement places on a robot?\n", "tags": "motor actuator noise", "id": "91", "title": "what determines the amount of noise an actuator produces?"}, {"body": "i am looking for a servo drive to control a brushless dc motor, with at least 10a, 30v rating. however, i want to know if any exist which take sinusoidal hall sensor signals directly.\n\ni already know there are servo drives taking hall sensor pulses (with 6 different phases), but that is trapezoidal control.\n\nnote: a servo drive includes the driving electronics (no additional transistors required).\n", "tags": "brushless-motor hall-sensor", "id": "93", "title": "what bldc servo drive takes sinusoidal hall sensor signals?"}, {"body": "i would be very interested to ask for a list of repos of free open code, applicable to 8-bit avr-s and having relation to robotics - object avoidance, process controllers, battery management, etc. this would be of huge help for me, preventing me from wasting weeks and months to invent the wheel.\n", "tags": "software microcontroller", "id": "94", "title": "developing for 8-bit avr-s, what are the current, open and free libraries out there?"}, {"body": "i have a number of interrupt service routines on an avr. these include interrupts for usart serial communication, timers, and spi communication.\n\nfor all of these, i use circular queues (using a start and end pointer, without boundary checking).\n\nsome problems start to occur if the avr starts to get overloaded. the circular queues will lose chunks of data. to solve this, i can reduce the load on the avr (for example, by skipping some cycles in the timer). however, this is a manual process, where it is reduced if the avr appears to have problems. this is partly because i do want relatively consistent timer periods.\n\nhowever, even at 70% average processor load, the queues can fill up randomly by chance.\n\nin case of spurious overloading at times, how can i make this more adaptive to avoid queue overflows?\n", "tags": "microcontroller avr interrupts", "id": "99", "title": "how to manage interrupts on an avr?"}, {"body": "if you have used connectors for signal wiring for any length of time, you may find that they are unreliable.\n\nspecifically, i find these to be unreliable when used for a long time, with a number of disconnections and re-connections:\n\n\n\nthis is due to the loss of springy-ness of the crimped metal end on the wire, which causes contact problems.\n\nwhich connectors (with rapid connection time) are reliable for multiple re-connections for simple signal wiring?\n\nthis excludes screw terminals and connectors with screws (eg. d-subminiature connectors), because they are not simple plug-in connectors.\n", "tags": "wiring", "id": "100", "title": "what connectors are most reliable?"}, {"body": "what is a suitable model for two-wheeled robots? that is, what equations of motion describe the dynamics of a two-wheeled robot.\n\nmodel of varying fidelity are welcome. this includes non-linear models, as well as linearized models.\n", "tags": "mobile-robot two-wheeled", "id": "106", "title": "what is a suitable model for two-wheeled robots?"}, {"body": "emergency stops are obviously a good idea on most robots, how should they be wired?  what systems should be killed immediately, and what should stay working?\n", "tags": "mobile-robot errors", "id": "112", "title": "how should emergency stops be wired?"}, {"body": "range sensors (for example sonar, infrared, and lidar) are notoriously noisy.  how can i characterize the noise characteristics to include these in a probabilistic localization sensor model?\n", "tags": "sensors noise", "id": "113", "title": "how to model the noise in a range sensor's return?"}, {"body": "in graph-based planning (say, a*), states are connected to their neighbors.  how should one decide whether to connect to the 4 neighbors or the 8 neighbors?  what are the pros and cons of each approach?\n", "tags": "motion-planning artificial-intelligence planning", "id": "117", "title": "what is the difference between 4-point and 8-point connectivity in graph based planning?"}, {"body": "i am attempting to upload a custom firmware to a lego mindstorms nxt and am having issues.\n\nfirst of all, i'm attempting to use nxtosek, which would let me run c++ programs on it.  the problem is, everytime i put it into firmware update mode, the download doesn't seem to actually occur.\n\nwhat i mean by this is that, according to the output in my terminal (both mac and windows), the download was successful, however when the nxt reboots, i still see the normal logo (not nxtosek).\n\nso, what i'm doing is first holding down the  button for a few seconds, then hitting the orange button, giving me that tic-tic-tic sound.  then i run the firmware update (either using the windows nexttool or mac osx gui nexttool) and attempt the download.  i get a success message, yet the robot is still using the old firmware.\n\nwhat could be the cause of this problem and how can i solve it?\n", "tags": "mindstorms nxt", "id": "118", "title": "properly flashing the firmware on a lego mindstorms nxt"}, {"body": "when designing a standard 4 or 6 wheel robot, is it better to have the weight distributed primarily in the center of the robot, or over the wheels, or is there no difference?\n\nspecifically, which weight distribution will make the robot less likely to tip over?\n", "tags": "mobile-robot design stability wheeled-robot", "id": "119", "title": "is it better to have weight distributed over the wheels or the center of the robot?"}, {"body": "edit: i don't know why, but this question seems to be confusing many people. i am aware of when/where/why/how to use real-time. i am interested in knowing whether people who have a real-time task would actually care enough to implement it in real-time or not.\n\nthere's no need to mention why real-time operations are important for a robot. my question is however, how much is it actually used in robotics?\n\ntake this question for example. only one answer mentions any platform with real-time capabilities, and it is far from the top too. ros apparently, being a very popular platform which is not real-time.\n\nin the real-time world however, rtai1 seems to be the only workable free real-time platform of use. it is however, limited to linux (no problem), badly documented and slowly developed.\n\nso, how much is real-time behavior sought after among robotics developers? the question is, how much are developers inclined to write real-time applications when real-time behavior is actually needed? if not much, why?\n\nfor example, reflexive behavior based on tactile data, cannot go through ros because it would lose its real-time property. but do people really come up with a real-time solution or use ros anyway, ignoring the real-time property?\n\n1 or similarly xenomai\n", "tags": "software platform real-time", "id": "128", "title": "how mature is real-time programming in robotics?"}, {"body": "i have an atmega16 mc which is master on the i2c and a atmega8 mc which is slave on the i2c.\n\ni have connected the two mcs' sda and scl ports to each other alongside a pullup resistor.\n\nnow i want to read a register from the atmega8 using the atmega16. \n\nthe problem is that i don't want to assign all the variables manually. is there any libs or headers that will do this thing for me?\n", "tags": "software microcontroller i2c", "id": "131", "title": "connecting two microcontrollers using i2c"}, {"body": "a beginning graduate student in robotics asked me the areas of mathematics that he should brush up on (prerequisites) to begin a masters research program in robotics. what are some good materials/books that are indispensable for a research student? which ones should we suggest in order that the student develops a solid foundation in robotics?\n", "tags": "research", "id": "138", "title": "mathematical prerequisites for beginning graduate student in robotics"}, {"body": "what kind of sensors and algorithms are the mobile robots of kiva systems equipped with? \n", "tags": "mobile-robot industrial-robot", "id": "142", "title": "technology behind kiva systems mobile robots"}, {"body": "i have a motor which drives a string connected to a load cell. i would like to implement a closed loop controller to control the load applied by the motor to the string.\n\n\n\nhow do i go about determining the required loop frequency in order to create a stable control system? is it something like the nyquist frequency, where the loop speed should be at least twice the highest frequency inherent in the mechanical system?\n", "tags": "control motor force", "id": "143", "title": "how do i calculate the required loop frequency for a servo controller?"}, {"body": "for the 3d printer reprap prusa there are several rails (smooth rods) that guide the printer head on the different axises. the printer head uses several linear bearings to glide along the rails. \n\nthere isn't any specification on what kind of material would be best suited for this purpose with the linear bearings. my first assumption would be for stainless steel because it won't corrode (rust) on the surface, but i'm not sure if this is true for all printers (whether they are 3d printers or not) as a different material may allow the linear bearings to glide more easily. aluminum would have been my second choice but i have the same reservations of which grade would be least resistant. \n\nthis resource has some limited information but does not help with which would be best suited for this particular application.\n\nwhat material is best suited for this purpose?\n", "tags": "reprap 3d-printing linear-bearing", "id": "146", "title": "what rail material is best used for linear bearings?"}, {"body": "i am not sure if this has been tried before but i am trying to use kinect and detect gestures made by the nao robot.\n\ni have made a kinect application, a gesture based picture viewer and it detects humans fine(obviously it does!) what i wanted to try was (lazy as i am), to see if i could use some (say, voice) command to tell the nao to do a swipe right gesture and have my application identify that gesture. the nao can easily identify my command and do some gesture. the problem however is, when i put the nao in front of the kinect sensor, the kinect does not track it. \n\nwhat i want to know is, are there some basics behind kinect's human body motion tracking that essentially fails when a robot is placed in front of it instead of a human?\n\nps: i have kept the nao at the right distance from the sensor. i have also checked if the entire robot is in the field of view of the sensor.\n\nedit: this has been posted on stackoverflow and on msdn by me so as to target a large audience as this problem has not been encountered by anyone in the past.\n", "tags": "kinect", "id": "148", "title": "detect nao robot in kinect"}, {"body": "whenever building an aquatic bot, we always have to take care to prevent leakages, for obvious reasons. now, holes for wires can be made watertight easily--but what about motors? we can easily seal the casing in place (and fill in any holes in the casing), but the part where the axle meets the casing is still left unprotected. \n\n\n\nwater leaking into the motor is still quite bad. i doubt there's any way to seal up this area properly, since any solid seal will not let the axle move, and any liquid seal (or something like grease) will rub off eventually.\n\ni was thinking of putting a second casing around the motor, maybe with a custom rubber orifice for the shaft. something like (forgive the bad drawing, not used to gimp):\n\n\n\nthis would probably stop leakage, but would reduce the torque/rpm significantly via friction.\n\nso, how does one prevent water from leaking into a motor without significantly affecting the motor's performance?\n\n(to clarify, i don't want to buy a special underwater motor, i'd prefer a way to make my own motors watertight)\n", "tags": "motor underwater auv protection", "id": "150", "title": "preventing leaks in motor shafts for underwater bots"}, {"body": "often we use microcontrollers to do things in our robots, but need to make some calculations in decimal. using floating point variables is very slow, because a software floating point library is automatically included (unless you have a high-end microcontroller). therefore, we generally use fixed point arithmetic.\n\nwhenever i do this, i just use an integer, and remember where the decimal place is. however, it does take some care to ensure that everything is consistent, especially when calculations involve variables where the decimal point is in a different place.\n\ni have implemented a fixed point atan2 function, but because i was trying to squeeze every last drop of limited precision (16 bits), i would often change the definition of where the decimal point is, and it would change as i tweaked it. in addition, i would have some constants, as a quasi look-up table, which themselves have an implied decimal point somewhere.\n\ni want to know if there is a better way. is there a library, or set of macros, that can simplify the use of fixed point variables, making multiplication and division between mixed variables easier, and allowing declaration of decimal numbers or constant expressions, but automatically converting to the desired fixed point representation at compile time?\n", "tags": "microcontroller c", "id": "154", "title": "fixed point arithmetic on microcontrollers"}, {"body": "at our lab, we have a several \"kurt\" type robots (about the size of a pioneer, six wheels, differential drive). the built-in gyroscopes are by now really outdated; the main problem is that the gyroscopes have a large drift that increases as the gyro heats up (the error is up to 3\u00b0/s). we mainly use the imu to get initial pose estimates that are later corrected by some localization algorithm, but even so the large initial pose error caused by the imu is often annoying.\n\nwe've temporarily used an android phone (galaxy s2) as a replacement imu, and the results are so much better compared to the old imus. however, i don't like depending on a wifi connection between imu and the control computer (a laptop running ros/ubuntu), so we're looking to buy a new imu.\n\nwhat imu should we choose? what criteria are important to consider for our application? please share your experiences! :-)\n", "tags": "ros imu odometry gyroscope ugv", "id": "155", "title": "how to choose a good imu for a wheeled robot?"}, {"body": "tuning controller gains can be difficult, what general strategies work well to get a stable system that converges to the right solution?\n", "tags": "control pid tuning", "id": "167", "title": "what are good strategies for tuning pid loops?"}, {"body": "using an imu a robot can estimate its current position relative to its starting position, but this incurs error over time. gps is especially useful for providing position information not biased by local error accumulation. but gps cannot be used indoors, and even outdoors it can be spotty.\n\nso what are some methods or sensors that a robot can use to localize (relative to some frame of reference) without using gps? \n", "tags": "localization gps sensors slam", "id": "172", "title": "absolute positioning without gps"}, {"body": "i have a time-series of force data of robots interacting with environment objects with various textures. i would like to develop models of various textures using the time-series data to classify textures into smooth, rough, moderate, etc. categories. for this purpose, will hidden markov models be sufficient or should i use conditional random fields? if i decide to classify into more categories and the distinction between each of are categories are very subtle, in that case what would be a good choice? will force-data be sufficient to capture all the information i need to classify textures into these categories?\n\nthanks for your replies :)\n", "tags": "artificial-intelligence", "id": "178", "title": "hmms vs. crfs to model time-series force data of robots interacting with environment?"}, {"body": "i have a joint actuated by an antagonistic pair of pneumatic muscles.\n\n\n\nthere are two valves per muscle, one to fill and one to empty each muscle. the joint has an angle sensor, and each muscle also contain an air pressure sensor.\n\nwhat is a suitable control algorithm set up?\n\n\na pid controller controlling the valve orifice sizes?\na pid controller controlling the mass flow rate?\na pid controller controlling the pressure using two pid pressure controllers?\na fuzzy logic controller?\na neural network?\n\n", "tags": "control pid air-muscle", "id": "182", "title": "suitable control algorithm for air muscle based joint?"}, {"body": "robotics has always been one of those engineering fields which has promised so much, but is taking a long time to deliver all the things that people imagine.\n\nwhen someone asks: \"how long before we have [x] type of robots?\" are there any resources we can call upon to try to calculate a rough answer. these resources might include:\n\n\nrate of progress of computational power, and some estimate of how much will be needed for various types of ai.\nrate of progress of electrical energy storage density, and some estimate of how much will be needed for various types of robot.\nrate of progress of actuation systems, and some estimate of what would be needed for various types of robot.\nlists of milestones towards various types of robot, and which ones have been achieved and when.\n\n\nare these types of studies performed, and are the results published?\n\n\n\nadded:\n\nin response to jakob's comment, i am not looking for opinions or discussions on this subject. what i am looking for are published studies which might shed light on this question.\n", "tags": "research", "id": "184", "title": "robotics trends"}, {"body": "most of the linear actuators i've seen are nonuniform and/or slow. those using a cam or crankshaft-like mechanism (and nearly anything hydraulic/pneumatic) cannot be moved at a constant speed without some programming. those using a screw-like mechanism are uniform, but slow. \n\naside from a rack and pinion/rope wound around a stick, what other fast, uniform linear actuators exist? by uniform, i mean that the speed is uniform (or the distance moved is linearly dependant on the angle rotated by the motor)\n", "tags": "actuator", "id": "205", "title": "creating a fast, uniform, linear actuator"}, {"body": "in slam frontends which use the iterative closest point (icp) algorithm for identifying the association between two matching point clouds, how can you determine if the algorithm is stuck in a local minimum and returns a wrong result? \n\nthe problem is defined as matching two pointclouds which are both samples of some arbitrary surface structure, and the sampled areas have an overlap of 0-100% which is unknown. i know the trimmed icp variant works by iteratively trying to determine the overlap, but even this one can be stuck in a local minimum. \n\na naive approach would be to look a the mean square error of the identified point pairs. but without some estimate of the sampling this seems a risky thresholding. in the manual for the leica cyclone they suggest manual inspection of the pair error histogram. if it has a gaussian shape the fit is good. if there is a linear fall-off the match is probably bad. this seems plausible for me, but i've never seen it used in an algorithm.\n", "tags": "slam", "id": "209", "title": "how to determine quality of icp matches?"}, {"body": "i have a simple servo system that uses a pid controller implemented in an mcu to perform the feedback. however, the properties of the system change dynamically, and so the pid parameters can never be tuned for all circumstances.\n\nmy robot is a light weight arm with back-drivable electric motors, similar to this one:\n\n\n\nthe arm performs several tasks, including picking up heavy weights, pushing and pulling objects across the desk. each of these tasks requires different pid tuning parameters which i cannot easily predict.\n\nwhat i would really like is for some higher level function which can carefully adjust the parameters in response to the arm's behaviour. for example, if it notices that the arm is oscillating, it might reduce p and increase d. or if it noticed that the arm wasn't reaching its target, it might increase i.\n\ndo such algorithms exist? i would be happy even if the algorithm didn't perfect the parameters immediately. e.g. the arm could oscillate a few times before the parameters were adjusted to their new values. \n", "tags": "control pid automatic tuning", "id": "210", "title": "how can i automatically adjust pid parameters on the fly?"}, {"body": "smart phones these days typically come with a gyroscope, accelerometer, compass, camera, and gps sensor all on board. they also usually have a connection to the internet with wifi and mobile data networks. i've seen many cases of using a phone as a remote control for a robot, but to me, it seems like the phone itself is a perfect lightweight computing and sensing platform for an autonomous robot.\n\nthe main obstacle i see is interfacing with actuators. being able to control motors to steer even a table-top robot, or control servos, for example. connecting and communicating to a microcontroller could be an obstacle as well. \n\nas a robot hobbyist, i'd like to know how i can overcome these and other obstacles to be able to harness the power of my smart phone with my robotics projects.\n", "tags": "actuator", "id": "213", "title": "how can i integrate a smart phone with my robotics project?"}, {"body": "i'd like to start making robots and tinkering with microcontrollers. where do i start, and what do i need?\n\ni'd like to make my own robots. i'm comfortable with programming (assembly and c) so i've got that part covered, but my electronics/circuits knowledge is a little weak. i have no idea what material to start with and which tools i need, nor how to put stuff together.\n\nfor the microcontroller, i'm thinking about going with the pololu orangutan lv-168 or the arduino duemilanove, although i'm leaning more towards the orangutan because of the built-in lcd and pushbuttons (which i would expect to use, especially for debugging and user interaction). am i on the right track? it seems to me like the number of i/o ports is small, but is that the case in practice?\n", "tags": "arduino microcontroller beginner", "id": "215", "title": "starting out advice on making robots and tinkering with microcontrollers"}, {"body": "in my application, my robot has the following physical setup:\n\n\ndifferential drive mechanics with feedback (wheel encoders)\ncommercially available webcam mounted with a known transform to the base of the robot (rgb, no depth)\n\n\nthe robot will be navigating through a structured, indoor type environment (think office, home, or university), and i would like to be able to determine the navigable paths through the environment using my vision sensor.\n\nwhat is the best way to approach the problem of finding safe paths to travel when given a single vision sensor?\n\nedit:  i think that i am more interested in the vision processing techniques than the actual path-planning mechanics.\n", "tags": "computer-vision", "id": "222", "title": "floor segmentation to determine navigable paths"}, {"body": "in ros, i cannot get [error] logs to print in red when i use python. how can i make them appear in red instead of black?\n\n\n\nfor example, the following python:\n\n\n\nproduces this output in black:\n\n\n  [error] [walltime: 135601422.876123] no analog input received.\n\n\n\n\nwhereas the following c++:\n\n\n\nproduces the following output in red.\n\n\n  [error] [135601551.192412]: no analog input received.\n\n", "tags": "ros python", "id": "229", "title": "red [error] output in python in ros"}, {"body": "can ros run on a raspberry pi?\n\nros is resigned to run on a network of machines, with different machines, even different cores on the same machine doing different jobs. can one of those machines be a raspberry pi?\n\ni am considering using an r-pi as the ethercat master on a mobile robot, communicating with the main pc over wifi, using a dongle.\n\n\ncan an r-pi even run ros at all?\nwould an r-pi have enough processing power to do some 1khz servoing?\nwould it be possible to run some servoing on the host through the wifi connecion?\n\n", "tags": "ros raspberry-pi wifi", "id": "230", "title": "can ros run on a raspberry pi?"}, {"body": "with a 600 mm (2 foot) motor-to-motor quadcopter, what frequency does my output-sense-calculate-output update loop need to stay stable?\n\ni'm estimating a total takeoff weight of very roughly 2 pounds ( 0.9 kg ),\nwhich i expect to be mostly motors and batteries.\n", "tags": "stability quadcopter", "id": "231", "title": "what frequency does my quadcopter output-sense-calculate-output update loop need to stay stable?"}, {"body": "i've seen 3 approaches to mounting batteries on a multicopter:\n\n\nall the batteries rigidly mounted near the center of the airframe\nall the batteries in a bag hanging under the center of the airframe\neach rotor has its share of the batteries rigidly mounted near/under it. (for example, a quadcopter with 1/4 of all the batteries mounted underneath each motor).\n\n\nwhich design is the best, and why?\nif there is no one best design, what are the advantages/tradeoffs between the designs?\nis there some other design i'm overlooking that is better in some way?\n\n(this question focuses on multirotor flying machines.\nfor ground vehicles, see \" is it better to have weight distributed over the wheels or the center of the robot? \").\n", "tags": "design stability quadcopter", "id": "235", "title": "is it better to have batteries distributed at the rotors or the center of the multicopter?"}, {"body": "a common scenario is to have a pc that sends commands to a microcontroller via rs232.  my pc program is sending commands (each of which are composed of multiple bytes) as fast as it can to a small robot.  the microcontroller on the robot is a parallax propellor.\n\ni have noticed that if i don't process bytes quickly enough on the microcontroller side of things, it can very quickly overflow the default buffers in the popular serial port drivers that are available for the propellor. (the buffers are generally anywhere from 16 to 256 bytes).  i can arbitrarily increase these buffers or create my own larger circular buffer, but i would like to have a more methodical approach to determining appropriate size requirements and/or the minimal amount of time i can wait before pulling bytes out of the serial port driver buffer.\n\nat 1st glance:\n\n\n115200 == 115.2 bits per millisecond == ~12.8 bytes per millisecond (assuming 1 stop bit)\n\n\n1) is that a valid way to calculate timing for serial transmissions?\n\nalso, given my specific setup:\n\n\npc program &lt;--> bluetooth serial profile driver &lt;--> bluetooth transceiver &lt;-*-> bluesmirf wireless modem &lt;--> parallax propellor program\n\n\n2) what is the maximum amount of data i can send for a given period of time consistently without eventually running in to problems?\n\nmaybe i'm over complicating things, but it seems like there are potentially multiple buffers involved in the transmission chain above.  how do others commonly deal with this? do they throttle the pc sending to a known safe rate? implement flow control?  if implementing flow control, how does that affect bandwidth and response times?\n\n(if it matters, my experiment is to use a joystick on the pc to control multiple servos with instant reaction to the joystick movements. so every small movement of the joystick results in multiple commands being sent to the microcontroller. the commands are not just simple positional commands though, they also involve acceleration/deacceleration of servos over time and this is the reason that the microcontroller spends a significant amount of clock cycles before processing new bytes.)\n", "tags": "microcontroller serial rs232", "id": "237", "title": "how to calculate serial speed and buffer requirements for pc to microcontroller communications?"}, {"body": "i would really like a six-axis force/torque sensor for my robot, but i just can't afford one. i was thinking about making one of my own.\n\ni have experience using strain gauges, but i can't work out how to arrange them so as to create a six-axis force/torque sensor.\n\n\nis this something i could feasibly make myself?\nhow do they work? what is the theory behind them?\n\n\ni'm curious to know how they work, even if it's not feasible to make one myself.\n\n\n\nadded:\n\njust to be clear, i'm talking about force / torque sensors, like this ati nano 17:\n\n\n\ni am not talking about accelerometers or gyros, or mems imus.\n", "tags": "sensors force-sensor", "id": "249", "title": "how does a six-axis force/torque sensor work?"}, {"body": "i have a robot that uses brushed motors in its servo system. these are maxon 3w motors, with 131:1 planetary gearboxes. the motors are controlled by a pic microcontroller, running a 1khz pid controller. the servos are for a low speed high torque application. there is significant backlash between the sensor and the motor.\n\nmaxon offer 12w brushless motors which are the same size. these are better in many ways: double the torque, better heat dissipation, higher efficiency.\n\nthe problem, obviously, is that they require more complex drive electronics. also, i have heard a couple of people mention that brushed motors are better for servo applications, though they never explained why.\n\n\nhas anyone else implemented this kind of system?\nare there any gotchas when using brushed motors for servos?\nis it possible to servo it at low speeds if i only have the 3 integral digital hall sensors, and no encoder? (i would prefer not to add an encoder because of the money and space cost)\nit torque ripple likely to be a problem?\n\n", "tags": "brushless-motor servomotor", "id": "254", "title": "should i switch my servo system from brushed to brushless motors?"}, {"body": "i'm really new to robotics, however i am a programmer familiar with several different languages. i don't have a ton of money to spend and i was wondering what is a really good starter kit. \n\nmy criteria is for the kit to be inexpensive and powerful, in that its functionality is extensible -- something that would allow the builder to be creative and possibly invent new ways to use it, not just a glorified model kit.  \n\nbeing extendable to smartphones is a plus.\n\ni'm not looking for something easy or introductory, just something powerful, flexible, and cost effective.\n", "tags": "uav kit", "id": "255", "title": "what uav kit(s) would be suitable for a beginner roboticist with programming experience?"}, {"body": "i want to use an rf beacon to localize my quadcopter for autolanding, when gps is not precise enough, for example, when my driveway is only 10 feet wide, and the gps is only showing 20-30 ft. accuracy (with a proverbial lake of lava on either side). the quadcopter would use the gps to fly to the rough location until it had a strong enough signal off the beacon, when it would begin to use that signal to come to a landing in a precise location, referenced off said beacon. can someone please explain to me the concepts and theories behind building the beacon and it's accompanying receiver (suitable for connection to an arduino via any digital or analog method) and achieving, say, a 4\" or better horizontal and vertical accuracy within a 50' sphere? minimally, the quad should have range and altitude, i.e. \"i am 10 feet away from the beacon and 2 feet above it\". how much added complexity would it take to make the robot fully position aware about the beacon, i.e. \"x ft. south, y ft. west and z ft. above it\", where the coordinate system is determined by the beacon and not linked to any sort of geographic coordinate system? if the beacon is mounted on a, say, 10 ft pole, are there any changes to be made versus having it on the ground and presuming that all activity takes place above it's x-y plane?\n\nlast note-\nthis thing would prefferably operate in the 72mhz band, please presume that where i'm operating, there are not other devices operating on the same band.\n", "tags": "localization quadcopter gps", "id": "256", "title": "quadcopter localization beacon"}, {"body": "i have seen waveforms for driving a brushless motor.\n\n\n\ni guess this is the waveform used for the simpler block commutation. but if i want to do sinusoidal waveforms, what does the pwm signal look like now? is there a need to carefully synchronise the edges on the three phases?\n", "tags": "brushless-motor pwm", "id": "261", "title": "what do the commutation waveforms look like for a brushless motor?"}, {"body": "do mobile and/or autonomous robots become more or less effective the bigger they get? for example, a bigger robot has bigger batteries, and thus bigger motors, whereas a smaller robot has the exact opposite, making it need less energy, but also have smaller motors. is there any known theorem that models this?\n", "tags": "mobile-robot design dynamics", "id": "262", "title": "effectiveness of a mobile robot in relation to mass"}, {"body": "i've seen many motors having capacitors attached in parallel in bots. apparently, this is for the \"safety\" of the motor. as i understand it, all these will do is smoothen any fluctuations--and i doubt that fluctuations can have any adverse effects on a motor. apparently these protect the motor if the shaft is being slowed/blocked, but i fail to see how.\n\nwhat exactly is the function of such a capacitor? what does it prevent, and how?\n", "tags": "motor protection", "id": "267", "title": "why are capacitors added to motors (in parallel); what is their purpose?"}, {"body": "i have two unmanned aerial vehicles (planes) which work well. they can fly to various waypoints automatically using gps.\n\nnow i would like them to fly together in formation. i would like them to fly side-by-side fairly close. this is too close to reliably use gps to guarantee that they keep the correct relative positions safely, and so i am looking for another way.\n\nsomehow the uavs need to be able to measure their position and orientation in space relative to the other one. how can i do this? is there some kind of sensor which can do this? it would need to have the following properties:\n\n\n6 axes (position and orientation)\nrange 0m - 5m, (from between plane centres, but planes won't actually ever touch wingtips)\nworks in day or night and all weather conditions\nlight weight (this is for 1.5m wingspan rc planes, so max extra weight of about 100g)\nprobably need about 50hz - 100hz refresh rate, but might get away with less, using the imu to fill in the gaps\n\n", "tags": "sensors uav multi-agent", "id": "271", "title": "spatial tracking between two uavs"}, {"body": "some years ago, there where some projects that provided hardware and software to perform modifications on standard hobby servos to convert them to digital servos, with all the advantages that come with it. \n\n\nopenservo is a little outdated, and does not seem to be worked on anymore, and there is no hardware to buy.\nsparkfun has its own version of the openservo, which at least is available for buying.\n\n\ndo you know if there are other mods, or even complete low cost digital servos? i am mostly interested in position feedback, and servo chaining.\n", "tags": "servos i2c", "id": "274", "title": "low-cost servo with digital control interfaces?"}, {"body": "i am designing an unmanned aerial vehicle, which will include several types of sensors: \n\n\n3-axis accelerometer\n3-axis gyroscope \n3-axis magnetometer\nhorizon sensor\ngps \ndownward facing ultrasound.\n\n\na friend of mine told me that i will need to put all of this sensor data through a kalman filter, but i don't understand why. why can't i just put this straight into my micro controller. how does the kalman filter help me about my sensor data?\n", "tags": "kalman-filter uav", "id": "277", "title": "why do i need a kalman filter?"}, {"body": "i wish to build a robotic arm that can lift a useful amount of weight (such as 3-6kg on an arm that can extend to approx 1.25 meters). what actuators are available to accomplish this. the main factors and design points are:\n\n\nnot expensive\n5 to 6 d.o.f.\nto be mounted on a yet to be designed mobile platform\nbattery powered\nstronger than hobby servos (at least for the 'shoulder' and 'elbow' joints)\nnot slow to actuate\n\n", "tags": "mobile-robot robotic-arm actuator", "id": "284", "title": "which type of actuator will be suitable for a very strong robot arm"}, {"body": "as somebody who is spending the majority of his time programming in javascript, what's the best route to get into small-robotics without needing to deviate too much from my current language focus?\n\nare there any project kits or tools that make use of the javascript language that might make the field more approachable for developers like myself? i would even be interested in virtual environments where all code is executed in a simulation.\n", "tags": "software programming-languages", "id": "287", "title": "programming robots with javascript"}, {"body": "the forward kinematics of a robot arm can be solved easily. we can represent each joint using denavit\u2013hartenberg transformation matrices.\n\nfor example, if the $i^{th}$ joint is a linear actuator, it may have the transformation matrix:\n\n$t_i = \\left[\\begin{matrix}\n1&amp;0&amp;0&amp;0\\\\\n0&amp;1&amp;0&amp;0\\\\\n0&amp;0&amp;1&amp;d_i\\\\\n0&amp;0&amp;0&amp;1\n\\end{matrix} \\right]$\nwhere the extension length is defined by $d_i$\n\nwhereas, a rotating link may be:\n\n$t_i = \\left[\\begin{matrix}\n1&amp;0&amp;0&amp;l\\\\\n0&amp;\\cos\\alpha_i&amp;-\\sin\\alpha_i&amp;0\\\\\n0&amp;\\sin\\alpha_i&amp;\\cos\\alpha_i&amp;0\\\\\n0&amp;0&amp;0&amp;1\n\\end{matrix} \\right]$ where $\\alpha$ is the angle, and $l$ is the length of the link.\n\nwe can then find the position and orientation of the end effector by multiplying all the transformation matrices: $\\prod{t_i}$.\n\nthe question is, how do we solve the inverse problem?\n\nmathematically, for a desired end effector position $m$, find the parameters $d_i$, $\\alpha_i$ such that $\\prod{t_i} = m$. what methods exist to solve this equation?\n", "tags": "inverse-kinematics kinematics joint arm", "id": "299", "title": "how can the inverse kinematics problem be solved?"}, {"body": "what's the best kind of spline that can be used for generating trajectory that can be adapted during execution time?\n\nthe use case is having a differential drive which has to move towards a point (x,y,theta) without stopping during the movement (e.g. no, turn toward the goal, straight move to the goal position, turn to the goal orientation). the robot is provided with a laser scanner for detecting dynamic obstacles which have to be avoided.\n\nwhat's the best kind of controller in this case?\n", "tags": "control motion-planning", "id": "301", "title": "which spline function would be best suited for the trajectory of a differential drive"}, {"body": "i've been doing a lot of reading lately about subsumption architecture and there are a few different ways people seem to advocate. \n\nfor instance some people use a global \"flag\" variable to have a task take control. others use the  and allow the arbiter to really choose. and i think this is correct. \n\ni have this small section of robotc code that i'm working on for a line following robot but am not sure i am doing it right as currently the track method will always take over the find method. the correct flow should be that find should guide the robot to the line using a spiral path to find the line. once the line is found track should take over. \n\n\n\ni've just used some comments here rather than the actual code to keep it brief. are my if statements not good enough as conditions because when the robot is off the line,  takes over. is this due to the else statement within track? if so, how to have  perform turns when it looses the line without taking over from forage at the start of the program? \n", "tags": "mobile-robot software two-wheeled robotc", "id": "309", "title": "correct way to use subsumption architecture with robot c"}, {"body": "i have built a few simple x/y/z cnc machines.  i've learned about g-code, motor control, firmware and open loop systems.  i see machines like rovers, big dog and factory arms that seem incredibly complex by comparison, yet they don't seem that magical any more.  \n\nwhat are the important skills to pick up from working with cnc machines?  what's the next logical thing to learn?  what things would cnc machines never teach me?\n", "tags": "control", "id": "317", "title": "how much can working with cnc machines teach you about robotics?"}, {"body": "is anyone able to help me out getting ipc-bridge working on my ubuntu lucid installation (with matlab 2012a)? i'm not being able to finish the last step on here (compiling the messages folders): https://alliance.seas.upenn.edu/~meam620/wiki/index.php?n=roslab.ipcbridge#installation\n\ni'm able to rosmake the ipc_bridge_ros, however when i enter this \"roscd ipc_roslib &amp;&amp; make\", it seems mex does not recognize the commands. here is what i get (screen shot): http://img13.imageshack.us/img13/6031/screenshot20121108at191.png\n\nnote: i'm going to use ipc-bridge so that i can control a pioneer 3dx and implement a fast-slam algorithm in matlab.\n", "tags": "mobile-robot software slam ros", "id": "323", "title": "ipc-bridge problem"}, {"body": "i'm building a 4 legged robot (quadruped) with 3 degrees of freedom per leg.\nthe goal of my project is to make this robot able to learn how to walk.\nwhat learning algorithms will i need to implement for it to work?\n\ni'm using an arduino uno for the microcontroller.\n", "tags": "arduino microcontroller machine-learning walking-robot", "id": "327", "title": "learning algorithms for walking quadruped"}, {"body": "we are using a koro robot for our pc based automation solution. but sometimes the robot is getting the command but refuses to respond. then i get a serial communication timeout error. the error is happening for a random type of commands and it is also not happening all the time making the troubleshooting difficult.\n\ni doubt the driver problem. how do you approach this problem.?\n\nthanks\n", "tags": "logic-control", "id": "331", "title": "robot serial communication error"}, {"body": "i have a platform with two tracks and two motors. each one uses an electronic speed control with \"double tap to reverse\". each esc takes an input pulse train frequency from at 1500 neutral +/-700.\n\ni'm interested in learning if there are algorithms or a list of commands i can use to control how such platform executes maneuvers. for example:\n\n\nlock one thread and have the platform rotate by using the other one\nhave two treads rotate in opposite directions\nexecute a u turn\n\n\ni'm struggling with expressing in code how such maneuvers should be executed. there's a \"dead\" zone around the 1500 pulse train frequency where the esc output is too weak to cause the platform to move. the double tap to reverse also makes it tough to understand for how long each track should be turned off.\n\nthank you for your input\n", "tags": "control motion-planning tracks", "id": "338", "title": "is there a list of maneuvers related to control of a tracked platform?"}, {"body": "i am thinking of developing a tendon driven robot manipulator for an industrial application that requires a high level of reliability. however, i am aware that tendons in a robot are prone to wear and tear, and failure.\n\nhow can i go about selecting a suitable tendon material (steel, kevlar, spectra, etc.) and use it appropriately?\n\nhave any studies been undertaken to examine longevity and failure patterns in robotic tendon materials?\n\nif i were to perform tests on materials myself, how can i perform those tests efficiently, and make best use of the testing time (learn as much as possible about tendon failure in a reasonable length of time).\n", "tags": "failure reliability", "id": "341", "title": "tendon longevity"}, {"body": "is there a taxonomy of errors that are common in robotics? things that come to mind but i don't have names for are:\n\n\ngetting stuck in a stable infinite loop\ngoing into an unstable feedback loop (a balancing robot overcompensating more with each correction)\nan inability to generalize between tasks (pick up a bowl vs pick up a glass)\nan inability to generalize between 'similar' sensory inputs.\ncausing damage to itself or its environment.\n\n\nthese would be things that make a robot look 'stupid' to a non-roboticist. if you're curious i want to have this list so i can then prepare a clear answer ready for people who don't know why these various things are hard.\n", "tags": "errors", "id": "342", "title": "what are some common mistakes that robots make?"}, {"body": "given a six-axis articulated robot arm holding a tool at its end-effector,  if i have a desired tool position and tool orientation,  there will be exactly 1 solution to the inverse kinematics equation for the robot to reach that position.\n(or rather up to 16 different solutions, depending on range of the joints)\n\n\n\nbut if the robot is holding something like a pen, and i want the robot to mark a specific point with that pen on the target, then i do not care how the pen is oriented, as long as it is perpendicular to the marked surface.\n\nso the inverse-kinematics equation will have infinitely many solutions.\n\nhow can i pick among these solutions the joint configuration that is closest to the current configuration: the one that will require the least amount of movement to reach?\n(or the joint configuration that is optimal according to some other similar criterion, such as that all joint angles are furthest from their maximum and minimum?)\n", "tags": "localization motion-planning industrial-robot inverse-kinematics kinematics", "id": "344", "title": "with a 6-axis robot, given end-effector position and range of orientations, how to find optimal joint values"}, {"body": "i am interested in learning more about subsumption architecture. i have read a number of books that talk about the idea but none of them go into great detail. i have also read a fair number of dr. brooks papers on the topic however he hasn't published much on the topic in recent years.\n\nis this still an active area of research? are there are any must read papers on the topic?\n", "tags": "control research", "id": "354", "title": "is subsumption architecture still an active area of research?"}, {"body": "i am looking for a way to restrict a robot's range of motion, using complex constraints such as not tearing of a cable attached to the robot.\n\ntake an articulated 6-axis robot arm as shown below, with attached cable (red), fixed at points x (before axis a4) and y (after axis a6).\n\n\n\nthe cable will limit the range of movement for the robot. it can stretch and bend only to some extend, but something like a full 360\u00b0 turn of axis a4, with all other axes remaining as they are in the picture, will tie the cable around the arm and rip it off.\nif joint a5 is at 0\u00b0, then a4 and a6 can still move the full 360\u00b0, but they cannot diverge too much from each other, as that would twist the cable. if a5 is tilted, the relationship becomes even more complicated.\n\nhow can you express such a constraint?  \n\nit is not a simple joint constraint, where you can independently limit the range of the joints, and it is also not a positional constraint, where you can define a region the robot must not enter. checking a start and a goal posture is not sufficient, since along the path from start to goal posture there may still be a posture that puts too much strain on the cable.\n\nwithout limiting the robot to a small set of pre-tested paths, how can you limit the robot to movements that will not rip off the cable?\n\nwhat are the standard techniques used for this sort of problem?\n", "tags": "motion-planning industrial-robot joint", "id": "358", "title": "restricting range of motion with complex constraints"}, {"body": "currently i have a tricycle style robot that uses an extended kalman filter in order to track 6 state variables. the inputs to the system are a steer encoder, a distance encoder, and a rotating laser that returns bearing only information to known landmarks. currently both encoders are located on the main wheel (the one that steers, and is also powered).\n\nthe 6 variables tracked by the kalman filter are x, y, heading, distance scaling (calibration of the distance encoder), steer calibration (offset of the steer encoder), and finally a bearing calibration of a rotating laser. \n\nwith this kind of system we put together a vehicle give it a known good location with plenty of landmarks, drive it around a bit, and end up with a well calibrated vehicle that can drive extended distances reliably with few landmarks. its simple and it works great. over time if an encoder drifts it will automatically follow the drift and adjust. \n\nwe are now attempting to apply the same principles to a robot with multiple steer and drive wheels. in this case the vehicle will be able to move in any direction, spin in place, etc. . each steer/drive wheel will have its own steer and distance encoder that each need to be calibrated. \n\ncan i expect to get the same kind of reliability and performance out of the more complex system? are there any common pitfalls to look out for when expanding a kalman filter to include more variables? is there a risk of it settling on sub-optimal values?\n", "tags": "localization kalman-filter", "id": "359", "title": "what kind of performance can i expect when using an extended kalman filter for calibration and localization?"}, {"body": "i am considering programming a line following robot using reinforcement learning algorithms. the question i am pondering over is how can i get the algorithm to learn navigating through any arbitrary path?\n\nhaving followed the sutton &amp; barto book for reinforcement learning, i did solve an exercise problem involving a racetrack where in the car agent learnt not to go off the track and regulate its speed. however, that exercise problem got the agent to learn how to navigate the track it trained on.\n\nis it in the scope of reinforcement learning to get a robot to navigate arbitrary paths? does the agent absolutely have to have a map of the race circuit or path? what parameters could i possibly use for my state space?\n", "tags": "machine-learning artificial-intelligence reinforcement-learning line-following", "id": "361", "title": "programming a line following robot with reinforcement learning"}, {"body": "are inverse kinematics and reinforcement learning techniques contending techniques to solve the same problem viz. movement of robotic manipulators or arm?\n\nby a glance through the wikipedia article, it appears that inverse kinematics seems to attempt to achieve a solution  as opposed to reinforcement learning which attempts to optimizes the problem. have i misunderstood anything?\n", "tags": "inverse-kinematics reinforcement-learning machine-learning", "id": "369", "title": "are inverse kinematics and reinforcement learning competitive techniques?"}, {"body": "i want to build a robot arm that'll be approximately 1.25 meter long and will be able to lift up to 2 kilograms. it'll have 6 dof and it is an expensive project. and most importantly, i am the only programmer in this brand new robotics facility of ours. :)\n\nthe robot that i want to build will be led by inverse kinematics, so with all these parameters and matrices, i think that i'll need a tough processor (not so sure).\n\nassuming that my robots control interface will be on an android tablet, i thought that i also could develop my program for android, and send necessary commands to the control chip via rs-232 interface.\n\nso, my question is, are standart 1 ghz android tablets suitable for these tasks? if not, has anybody got an advice for me?\n", "tags": "software inverse-kinematics arm rs232", "id": "380", "title": "processor and command interface preference for a robot arm"}, {"body": "my team and i are setting up an outdoor robot that has encoders, a commercial-grade imu, and gps sensor. the robot has a basic tank drive, so the encoders sufficiently supply ticks from the left and right wheels. the imu gives roll, pitch, yaw, and linear accelerations in x, y, and z. we could later add other imus, which would give redundancy, but could also additionally provide angular rates of roll, pitch, and yaw. the gps publishes global x, y, and z coordinates.\n\nknowing the robot's x y position and heading will useful for the robot to localize and map out it's environment to navigate. the robot's velocity could also be useful for making smooth movement decisions. it's a ground-based robot, so we don't care too much about the z axis. the robot also has a lidar sensor and a camera--so roll and pitch will be useful for transforming the lidar and camera data for better orientation. \n\ni'm trying to figure out how to fuse all these numbers together in a way that optimally takes advantage of all sensors' accuracy. right now we're using a kalman filter to generate an estimate of  with the simple transition matrix:\n\n\n\nthe filter estimates state exclusively based on the accelerations provided by the imu. (the imu isn't the best quality; within about 30 seconds it will show the robot (at rest) drifting a good 20 meters from its initial location.) i want to know out how to use roll, pitch, and yaw from the imu, and potentially roll, pitch, and yaw rates, encoder data from the wheels, and gps data to improve the state estimate. \n\nusing a bit of math, we can use the two encoders to generate x, y, and heading information on the robot, as well as linear and angular velocities. the encoders are very accurate, but they can be susceptible to slippage on an outdoor field. \n\nit seems to me that there are two separate sets of data here, which are difficult to fuse:\n\n\nestimates of x, x-vel, x-accel, y, y-vel, y-accel\nestimates of roll, pitch, yaw, and rates of roll, pitch, and yaw\n\n\neven though there's crossover between these two sets, i'm having trouble reasoning about how to put them together. for example, if the robot is going  at a constant speed, the direction of robot, determined by its x-vel and y-vel, will be the same as its yaw. although, if the robot is at rest, the yaw cannot be accurately determined by the x and y velocities. also, data provided by the encoders, translated to angular velocity, could  be an update to the yaw rate... but how could an update to the yaw rate end up providing better positional estimates?\n\ndoes it make sense to put all 12 numbers into the same filter, or are they normally kept separate? is there already a well-developed way of dealing with this type of problem?\n", "tags": "sensors kalman-filter sensor-fusion", "id": "382", "title": "how to fuse linear and angular data from sensors"}, {"body": "i have been using the cyberglove to control a humanoid robot hand, but found it disappointing as it doesn't measure the posture of the human hand very accurately.\n\n\n\ni have been wondering about the possibility of using inertial measurement units (imus) mounted on the fingers to track position and measure posture. but i'm not sure how feasible it is.\n\n\nwould an imu return enough data to make tracking reliable in all circumstances?\nwould it be possible to fool the system into incorrectly tracking the fingers?\nmight it be possible to get away with using simple 3-axis accelerometers, or would it need 9-axis (accelerometer, gyro, and magnetometer)?\n\n", "tags": "imu sensor-fusion hri", "id": "390", "title": "can i use imus to improve the position/posture measurement of fingers in a \"data glove\"?"}, {"body": "i am interested in robotics. practically i have no idea about robotics. i want to start learning the basics of robotics. but i am confused what to start with. so i need suggestions about what will be the best resources to start robotics with. that may be books, sites, or others. please help me with this.\n", "tags": "books", "id": "397", "title": "resources for learning basics of robotics"}, {"body": "i am aware of the legislation's in nevada, but what is happening with the technology currently. when is it expected to be commercialized ?\n", "tags": "ugv", "id": "403", "title": "what is the current state of the google self driving car project?"}, {"body": "i have never used an accelerometer before, but i am aware that they come with i2c, spi and analog outputs. if i choose to use an i2c or spi, device, will i accumulate errors due to communication time?\n\nis the fast sampling of an analog signal likely to get me a more accurate deduced position than using am i2c?\n\nwill this be true for \n\n\na robot moving in a room \na robot moving in an outdoor terrain and is likely to slip and roll down a slope.\n\n\nalso, i have no sense of gs. i tried to move my hand around fast with my phone running andro-sensor in my fist and saw that the readings saturated at 20m/s2. what g can i expect my robot to experience if it is hit by another fat moving bot or bumped by a fast walking human?\n", "tags": "sensors deduced-reckoning navigation accelerometer", "id": "405", "title": "selecting an accelerometer for deduced reckoning"}, {"body": "i have not bought any parts yet, but i am making my own quadcopter. i have done the research and know all about the parts that i need, but many guides are sponsored and cost thousand(s) of euros/dollars while not explaining things entirely clearly.\n\nfirstly, i have found this flight control board. would i need another microcontroller (such as the arduino nano) for it to work? (if anyone has experience with this board, let me know!).\n\nsecondly, would the above board work with this radio controller. are controllers universal?\n\n(please tell me if i'm not in the right section here, or if this doesn't count as a relevant topic).\n", "tags": "microcontroller quadcopter radio-control", "id": "413", "title": "questions about quadcopter and radio controller"}, {"body": "i apologize if this question may sound a little vague. i am working on a robotics project that will contain 27 servos of various sizes and i am having trouble figuring it how they should be powered.\n\ni was hoping to use several (3-6) 5 w 18650 battery boxes to power them, but the smallest motors would use 2.5 w each, so 1 battery box can only power two. the larger servos, obviously, use even more current, so this plan of using a small number of 18650's becomes infeasible.\n\nthere is not enough room on the robot for a 12 v car battery, and adding one would require recalculating the sizes of the servomotors that would be needed. furthermore, i am not sure how to convert the 12 v it gives down to 5 v for the servomotors.\n\np.s. what about the stall current of the motors? should the power supply be able to supply the stall current of all the motors it supplies (at the same time) or just the working current? should i use a fuse to handle when (if?) the servomotors stall? should i use a fuse or a circuit breaker? do they make 5 v fuses? if so, where can i get one?\n\nsomething like a larger version of the 18650 box would be most preferable.\n", "tags": "design servos power", "id": "416", "title": "what is the best way to power a large number (27) servos at 5 v?"}, {"body": "i'm building a walking robot that will need to know when it moves forward. i'm using on-board intelligence and i plan on using accelerometers, gyros, and magnometers (if needed) to be able to detect if the robot moves forward. the problem is, i dont know how to program an internal navigation system or an imu.  what software algorithms are needed?\n\nto clarify my problem, i need to know how to program the micro controller to read the sensors and be able to tell if the robot has displaced itself forward since a previous measurement.\nalso if i used this sensor board (or similar) could i use it to determine the displacement. \n", "tags": "software imu deduced-reckoning artificial-intelligence", "id": "419", "title": "how do you implement an ins from an accelerometer and (optionally) gyros and a magnetometer?"}, {"body": "i'm planning to write an inverse kinematics controlled 6 dof virtual robot for android. i did some research on packages avaliable and couldn't choose the right one which will satisfy my needs on this project. i've seen a work with eigen in c++, and used it, it was just fine. but since i'm not so experienced in java, i wanted to ask before i start, if someone knows some appropiate packages for these operations.\n\nhere is what i found so far:\n\njama,\nvecmath,\njmathtools,\nejml,\njampack\ni ask this because i really dont want to get stuck in the middle of my project. thanks in advance.\n", "tags": "inverse-kinematics programming-languages", "id": "431", "title": "inverse kinematics in java"}, {"body": "what are the pros/cons of the different visual odometry options? \n\n\nstereo camera\noptical flow\nslam\nother?\n\n\ncriteria:\n\n\nhow well it performs vs other odometry options/sensors (lidar, radar)\nsensor fidelity\ncomputation\naccuracy\nprecision\ndrift\nnative resilience and repeadability in sensor noise or vehicle speed\nease of integrating with imu/gps\netc\n\n\nin general, of course, because there are a lot of different ways the trade-offs go when we get into specifics about applications and hardware.  i'm asking out of curiosity, not for designing anything in particular.\n", "tags": "mobile-robot localization computer-vision odometry", "id": "433", "title": "visual odometry options?"}, {"body": "the answers i received to the question on training a line following robot using reinforcement learning techniques, got me to think on how to train a robot. i believe there are essentially two ways -\n\n\ntrain the physical robot.\nmodel the robot and simulate the training.\ndid i miss something?\n\n\napproach 2 is definitely the better approach. however, a priori knowledge of the motion (response), a certain pwm signal (stimulus) would cause when the robot is in a given state is required. the motion caused by a pwm signal may depend on the (1) current battery voltage, (2) the mass of the robot and the (3) current velocity (did i miss something?).\n\nhow do i model such a robot? and how do i model it quick? if i change the battery or add a few boards and other peripherals and change the mass of the robot, i would have to remodel and retrain the robot. can i do this by providing some random stimulus pwms and measuring the response?\n\nadded: my related question in dsp.se\n\nupdate: a suggested edit to the title by ian worth mentioning - \"how do i model train a robot so that if its dynamics change, it does not need complete re-training?\" i think this is a good question too but different from the one i am asking here. i am okay with re-training for now.\n", "tags": "reinforcement-learning pwm", "id": "436", "title": "how do i model a robot?"}, {"body": "digital compasses (magnetometers) require a hard/soft iron calibration in order to be accurate.  this compensates for the magnetic disturbances caused by nearby metal objects -- the robot's chassis.\n\n\n\n(image from http://diydrones.com)\n\nhowever, digital compasses are also susceptible to the electric fields caused by the relatively high amount of current drawn by motors.  \n\nin order to get an accurate compass reading, what is the best way to measure (and compensate for) the interference caused by changing motor current levels?\n", "tags": "mobile-robot sensors", "id": "442", "title": "how can the dynamic effects of motor current on a digital compass be characterized and compensated for?"}, {"body": "i am trying to use a stereo camera for scene reconstruction, but i can usually only obtain sparse point clouds (i.e. over half the image does not have any proper depth information). \n\ni realize that stereo processing algorithms rely on the presence of texture in the images and have a few parameters that can be tweaked to obtain better results, such as the disparity range or correlation window size. as much as i tune these parameters, though, i am never able to get results that are even remotely close to what can be obtained using an active sensor such as the kinect.\n\nthe reason why i want that is because very often point clouds corresponding to adjacent regions don't have enough overlap for me to obtain a match, so reconstruction is severely impaired.\n\nmy question to the computer vision experts out there is the following: what can i do to obtain denser point clouds in general (without arbitrarily modifying my office environment)?\n", "tags": "slam computer-vision", "id": "445", "title": "how to obtain dense point clouds from stereo cameras?"}, {"body": "i have a handful of 31.2oz-in stepper motors (mouser.com - applied motion: ht17-268d), and i was curious if they would be big enough to run a 3d printing/cutting/etching type (think reprap) of machine. i had in mind to attach them via  a simple gear to a screw-type drive to run the tool head back and forth. \n\n\nmaximum bed size would probably be ~1.5'3. \nheaviest tool head would be something about half the weight of a dremel tool.\nhardest substances i would use it on would probably be hardwoods (with high speed cutter) and copper (for pcb etching).\n\n\nhow do i figure the amount of torque needed to drive the head, and would the motors that i already have be big enough to do the job?\n", "tags": "stepper-motor reprap", "id": "446", "title": "how much torque do i need for a cnc machine?"}, {"body": "in our lab we use lipo batteries to power our quadrotors. lately we have been experiencing stability issues when using certain batteries. the batteries seem to charge and balance normally and our battery monitor indicates they are fine even when putting them under load. however when we attempt to fly the quadrotor with one of these batteries, manually or autonomously, it has a severe tendency to pitch and/or roll. my guess is that the battery is not supplying sufficient power to all the motors which brings me to my question. is this behavior indicative of a lipo going bad? if so what is the best way to test a battery to confirm my suspicions?\n", "tags": "battery troubleshooting", "id": "453", "title": "how can one determine whether a lipo battery is going bad?"}, {"body": "whenever i read a text about control (e.g. pid control) it often mentions 'poles' and 'zeros'. what do they mean by that?  what physical state does a pole or a zero describe?\n", "tags": "control pid", "id": "461", "title": "in pid control, what do the poles and zeros represent?"}, {"body": "we are trying to power this motor with this motor driver , using a 11.1v 2.2ah lithium-ion polymer battery.\n\n(we're in over our heads with this and really need the help) \n\nwe checked with the company (e-flite) and the motor is definitely dc -- we're a bit confused as to the purpose of three wires, and how we should connect them to the motor. \n\nany help would be appreciated.\n", "tags": "brushless-motor driver", "id": "463", "title": "connecting a 6 pole motor to a motor driver?"}, {"body": "i would like to have a better understanding of work in the field of \"navigation among movable obstacles\". i started off with michael stilman's thesis under james kuffner, but that has not yet sated my appetite.\n\ni am currently trying to simulate a scenario where debris (tables and table parts) from a disaster scenario block pathways. the debris forms part of a movable obstacle. the robot which will be used is a bipedal humanoid.\n\nthe thesis describes an approach to define the search space of possible actions leading from the start point to the goal. however, it assumes a mobile robot which works via gliding. \n\ni think the state space definitions would change for a bi-pedal robot. why is why i wonder what other work is being done in this field. perhaps the work of other research groups could give me clues as to how to design and perhaps reduce the search space for a bipedal humanoid robot.\n\nan implementation of navigation among movable obstacles would also aid me in understanding how to reduce the search space of possible actions.\n\nso does anyone know of a working implementation of navigation among movable obstacles? \n\nany supporting information about other professors or research groups working on similar problems would also be very useful.\n\ni hope this edit is sufficient for the problem description.\n", "tags": "navigation", "id": "469", "title": "is there a working implementation of \"navigation among movable obstacles\" for a bi-pedal robot?"}, {"body": "i want to learn robotics and build my first robot. i am looking for a well supported kit that is simple enough and can walk me through, the initial stages of my intellectual pursuit in robotics. i want to be able to do the basic things first and build a solid foundation in robotics. and then i want to be able to use the solid foundation, to gain confidence in my ability to build new and interesting robotic contraptions. in other words, i want to be able to follow the rules off the game to gain a solid foundation and then once i'm comfortable with what i know, i want to break free of the rules and start making my own robots. \n\ni would like help with 2 things,\n\n\ni would like to begin my robotics learning with a good kit that can walk me through my initial stages. i expect that this initial stage might take quite a while. so, any recommendations for how i can start and/or what kit i can buy, to get my feet wet, would be helpful.\ni would like suggestions for \"other\" actions i can take, that will set me on a path to gain confidence in my knowledge of robotics.\n\n\na little bit about myself. i have a bs and ms in it. so i am not new to programming. i like to code in golang and haskell. i do not know if it is possible, but it would be awesome if i can write the software aspect of all my robotic projects in haskell.\n\nthanks\n", "tags": "kit", "id": "472", "title": "humble beginnings"}, {"body": "rs232 is not popular as it used to be and is mainly replaced by usb [wikipedia]. problems such as mentioned in this question doesn't help its reputation either.\n\nin a new system design therefore, one could think of using usb instead of serial port for communication. however, it still seems like rs232 is the serial communication protocol/port of choice.\n\nwhy is that? i understand changing old machinery that work with rs232 is costly, but what prevents new system designers from using usb instead of rs232?\n", "tags": "rs232 usb", "id": "474", "title": "usb instead of rs232"}, {"body": "as an industrial roboticist i spent most of my time working with robots and machines which used brushless dc motors or linear motors, so i have lots of experience tuning pid parameters for those motors.\n\nnow i'm moving to doing hobby robotics using stepper motors (i'm building my first reprap), i wonder what i need to do differently. \n\nobviously without encoder feedback i need to be much more conservative in requests to the motor, making sure that i always keep within the envelope of what is possible, but how do i find out whether my tuning is optimal, sub optimal or (worst case) marginally unstable?\n\nobviously for a given load (in my case the extruder head) i need to generate step pulse trains which cause a demanded acceleration and speed that the motor can cope with, without missing steps.\n\nmy first thought is to do some test sequences, for instance:\n\n\nhome motor precisely on it's home sensor.\nmove $c$ steps away from home slowly.\nmove $m$ steps away from home with a conservative move profile.\nmove $n$ steps with the test acceleration/speed profile.\nmove $n$ steps back to the start of the test move with a conservative move profile.\nmove $m$ steps back to home with a conservative move profile.\nmove $c$ steps back to the home sensor slowly, verifying that the sensor is triggered at the correct position.\nrepeat for a variety of $n$, $m$, acceleration/speed &amp; load profiles.\n\n\nthis should reliably detect missed steps in the test profile move, but it does seem like an awfully large space to test through however, so i wonder what techniques have been developed to optimise stepper motor control parameters.\n", "tags": "control stepper-motor tuning", "id": "478", "title": "how can i optimise control parameters for a stepper motor?"}, {"body": "i understand the basic principle of a particle filter and tried to implement one. however, i got hung up on the resampling part. \n\ntheoretically speaking, it is quite simple: from the old (and weighted) set of particles, draw a new set of particles with replacement. while doing so, favor those particles that have high weights. particles with high weights get drawn more often and particles with low weights less often. perhaps only once or not at all. after resampling, all weights get assigned the same weight.\n\nmy first idea on how to implement this was essentially this:\n\n\nnormalize the weights\nmultiply each weight by the total number of particles\nround those scaled weights to the nearest integer (e.g. with  in python)\n\n\nnow i should know how often to draw each particle, but due to the roundoff errors, i end up having less particles than before the resampling step. \n\nthe question: how do i \"fill up\" the missing particles in order to get to the same number of particles as before the resampling step? or, in case i am completely off track here, how do i resample correctly?\n", "tags": "localization particle-filter", "id": "479", "title": "particle filters: how to do resampling?"}, {"body": "what are some good strategies to follow while designing power supply for electrical systems on mobile robots?\n\nsuch robots typically comprise of systems with\n\n\nmicroprocessor, microcontroller, dsp, etc units and boards along with immediate peripherals\nmotor control \nanalog sensors(proximity, audio, voltage, etc)\ndigital sensors (vision, imu, and other exotica)\nradio comm circuits (wifi, bluetooth, zigbee, etc)\nother things more specific to the purpose of the robot being designed.\n\n\nare there unified approaches/architectural rules to designing power systems which can manage clean power supply to all these various units which may be distributed across boards, without issues of interference, common ground, etc? furthermore, also including aspects of redundancy, failure management, and other such 'power management/monitoring' features?\n\nwell explained examples of some such existing power systems on robots would make for excellent answers.\n", "tags": "mobile-robot electronics", "id": "483", "title": "strategies for managing power on electrical systems for mobile robots"}, {"body": "is it possible to use the matlab's \"system\" function to call ros commands?\n\nfor example, using:\nsystem('rostopic pub /cmd_vel geometry.msgs.twist {....}\nor system('rospack find ipc_bridge)\n\ni'm trying to send some commands to ros without using something like ipc-bridge.\n\nps: i know, however, that i need to use ipc-bridge to subscribe to topics.\n", "tags": "mobile-robot software ros", "id": "488", "title": "matlab 'system' function with ros"}, {"body": "i'm interested to build robot from my imagination, and i was looking to purchase a robotic kit.\n\ni find the lego mindstorm nxt 2.0 really interesting for many reasons : you can plug whatever brick you want, and you can develop in the language you want.\n\ni am a developer, and the use of this kind of robotic would be interaction mostly (not moving, so the servo motors are useless to me, at least now).\n\nbut regarding the spec of the nxt main component, i feel it's a bit low (proc, ram &amp; rom).\n\nthat made me wonder if any of you know something similar (where i can plug whatever i want on it, and most importantly, program the reaction), but with a more powerful hardware ?\n\nprice will also be a limitation : i like the nxt also because i can build what i want under 300 usd. i don't want to spend 10k usd on my first kit, but i would appreciate buying a better piece of robotic if the price isn't too distant from the nxt's.\n\ndo you have some alternatives to check out ?\n\nthanks for your help !  :)\n", "tags": "nxt", "id": "494", "title": "more powerful alternatives to lego mindstorm nxt 2.0?"}, {"body": "i was wondering whether something like this is possible: a block of ice(say) needs to be transferred piece by piece from a source to a destination with the help of 5 robots standing in a straight line between the source and destination. the first robot picks up a piece of the block from the source and checks if the next robot in line is busy. if yes, it waits for it to complete its task and proceeds, otherwise it transfers the piece and goes back to collect another piece. please help me on implementing this if it is possible, as i am thinking to make it a project topic.\n\nto clear out the confusions, here's a smaller prototype of the project i'm thinking,\n\ni have two cars, one wired, another wireless. the wired car is the master here, the wireless, the slave. through a remote, i send a command to the wired car to instead command the wireless car to move forward. the wired car will then check if the wireless slave is already executing some previously given command or no, and accordingly send the command.\n\nconversely, the master may send the command as soon as it receives it, it's on the slave now to complete the task it's doing, and then execute the command it just received.\n", "tags": "mobile-robot multi-agent", "id": "497", "title": "collaborative behavior: implementing a bucket brigade with robot arms"}, {"body": "i dont understand integral part of pid controller. let's assume this pseudocode from wikipedia:\n\n\n\nintegral is set to zero in the beginning. and then in the loop it's integrating the error over the time. when i make a (positive) change in setpoint, the error will become positive and integral will \"eat\" the values over the time (from the beginning). but what i dont understand is, when error stabilizes back to zero, the integral part will still have some value (integrated errors over time) and will still contribute to the output value of controller, but it should not, because if error is zero, output of pid should be zero as well, right?\n\ncan somebody explain me that please?\n", "tags": "control pid", "id": "499", "title": "i don't understand integral part of pid controller"}, {"body": "robots are somewhat videogenic, and the old saying \"show me, don't tell me\" is especially applicable.\n\nbut of course, a video is not a question, so it doesn't fit the stack exchange format.  maybe video links would be more suitable in a codeproject post.  it just seems like this board hits the right cross-section of people, whose projects i would be interested in seeing.\n", "tags": "untagged", "id": "502", "title": "is there a place for posting \"look at what i did\" videos?"}, {"body": "i am making a 2 wheel drive robot.\n\nsuppose i know that my robot is going to weight x kg when finished and i know the diameter of the wheels y (geared motors will be connected directly to the wheels). i can choose from several geared motors and i know the peak torque of each motor and the idling speed.\n\nhow can i calculate the load that a specific motor can take? i.e. will a motor with a given torque be able to move my robot without being too overloaded? what rpm will the motor have when it has load?\n", "tags": "motor", "id": "504", "title": "choosing motors for 2 wheel drive robot"}, {"body": "i recently got an arduino wifi shield known as \"juniper\" (i believe it was by cutedigi). i've tried to find code examples, but when i saw code, it was un-commented and very little explained, i could really use a tutorial or some sample code with a good explanation, can anyone help me find a place to start? i found a piece of code here: http://arduino.cc/forum/index.php?action=printpage;topic=103582.0\nand i just want to connect to a network, maybe send some get requests, or open a socket.\n\nedit:\nafter poking around for a while, i found documentation, but i still can't get it to work.\nmy code:\nhttp://pastie.org/5455603\ni can't seem to get any input at all from the wifi shield.\n", "tags": "arduino electronics wifi", "id": "511", "title": "where can i find a tutorial or sample code for the juniper wifi arduino shield?"}, {"body": "for someone interested in robotics but do not know the abc of robotics or mechanical/electronic engineering .what's a good roadmap for becoming an amateur roboticist . i'm studying theoretical physics so that i have no problems on the physics/math . if the question is too broad and doesn't meet the criteria of posting on this site . please inform me of any helpful advice/study material etc. before the question get closed .\nthanks in advance.\n", "tags": "books", "id": "512", "title": "how can i start learning robotics?"}, {"body": "i'm using an ekf for slam and i'm having some problem with the update step.  i'm getting a warning that k is singular, rcond evaluates to near eps or nan. i think i've traced the problem to the inversion of z.  is there a way to calculate the kalman gain without inverting the last term? \n\ni'm not 100% positive this is the cause of the problem, so i've also put my entire code here https://github.com/jdowns/ekf-slam.  the main entry point is slam2d.\n\n\n\nedits:\nproject(x(r), x(lmk)) should have been project(x(r), x(lmk_idx)) and is now corrected above.  \n\nk goes singular after a little while, but not immediately.  i think it's around 20 seconds or so.  i'll try the changes @josh suggested when i get home tonight and post the results.\n\nupdate 1:\n\nmy simulation first observes 2 landmarks, so k is 7x2.  (p(rl,rl) * e_rl') * inv( z ) results in a 5x2 matrix, so it can't be added to x in the next line.  \n\nk becomes singular after 4.82 seconds, with measurements at 50hz (241 steps).  following the advice here (http://www.mathworks.com/help/matlab/ref/inv.html), i tried k = (p(:, rl) * e_rl')/z which results in 250 steps before a warning about k being singular is produced.  \n\nthis tells me the problem isn't with matrix inversion, but it's somewhere else that's causing the problem.    \n\nupdate 2\n\nmy main loop is (with a robot object to store x,p and landmark pointers):\n\n\n\nat the end of this loop, i save x and p back to the robot, so i believe i'm propagating the covariance through each iteration.  \n\nmore edits\nthe (hopefully) correct eigenvalues are now here: http://pastebin.com/vn4nzkqy\n\nthere are a number of eigenvalues that are negative.  although their magnitude is never very large, 10^-2 at most, it happens on the iteration immediately after the first landmark is observed and added to the map (in the \"new landmarks\" section of the main loop).\n", "tags": "slam kalman-filter", "id": "519", "title": "ekf-slam update step, kalman gain becomes singular"}, {"body": "a lot of awesome optics projects like hacking cameras and projectors become possible with cad lens modelling software1, if we can also easily prototype the lenses we design.\n\nwhat are some materials and additive or subtractive 3d fabrication strategies that can make a clear lens with strong refraction and the ability to be polished?\n\n1 here is a helpful list of 37 different lens design &amp; simulation programs.\n", "tags": "3d-printing manufacturing", "id": "520", "title": "is it practical to 3d print a refractive lens?"}, {"body": "when computing the jacobian matrix for solving an inverse kinematic analytically,i read from many places that i could use this formula to create each of the columns of a joint in the jacobian matrix:\n\n\n\nsuch that $a'$ is the rotation axis in world space, $r'$ is the pivot point in world space, and $e_{pos}$ is the position of end effector in world space.\n\nhowever, i don't understand how this can work when the joints have more than one dofs. take the following as example:\n\n\n\nthe $\\theta$ are the rotational dof, the $e$ is the end effector, the $g$ is the goal of the end effector, the $p_1$, $p_2$ and $p_3$ are the joints.\n\nfirst, if i were to compute the jacobian matrix based on the formula above for the diagram, i will get something like this:\n\n$$j=\\begin{bmatrix}\n((0,0,1)\\times \\vec { e } )_{ x } &amp; ((0,0,1)\\times (\\vec { e } -\\vec { p_{ 1 } } ))_{ x } &amp; ((0,0,1)\\times (\\vec { e } -\\vec { p_{ 2 } } ))_{ x } \\\\ ((0,0,1)\\times \\vec { e } )_{ y } &amp; ((0,0,1)\\times (\\vec { e } -\\vec { p_{ 1 } } ))_{ y } &amp; ((0,0,1)\\times (\\vec { e } -\\vec { p_{ 2 } } ))_{ y } \\\\ ((0,0,1)\\times \\vec { e } )_{ z } &amp; ((0,0,1)\\times (\\vec { e } -\\vec { p_{ 1 } } ))_{ z } &amp; ((0,0,1)\\times (\\vec { e } -\\vec { p_{ 2 } } ))_{ z } \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \n\\end{bmatrix} $$\n\nthis is assumed that all the rotation axes are $(0,0,1)$ and all of them only have one rotational dof. so, i believe each column is for one dof, in this case, the $\\theta_\\#$.\n\nnow, here's the problem: what if all the joints have full 6 dofs? say now, for every joint, i have rotational dofs in all axes, $\\theta_x$, $\\theta_y$ and $\\theta_z$, and also translational dofs in all axes, $t_x$, $t_y$ and $t_z$.\n\nto make my question clearer, suppose if i were to \"forcefully\" apply the formula above to all the dofs of all the joints, then i probably will get a jacobian matrix like this:\n\n\n\n(click for full size)\n\nbut this is incredibly weird because all the 6 columns of the dof for every joint is repeating the same thing.\n\nhow can i use the same formula to build the jacobian matrix with all the dofs? how would the jacobian matrix look like in this case?\n", "tags": "inverse-kinematics kinematics", "id": "521", "title": "computing the jacobian matrix for inverse kinematics"}, {"body": "am trying to find the right esc for the following two motors\n\nhttp://www.e-fliterc.com/products/default.aspx?prodid=eflm30180mdfa#quickfeatures\n\nhttp://www.e-fliterc.com/products/default.aspx?prodid=eflm3032dfa\n\ncan't figure out which of the esc's listed on the site would be best? are there alternative (cheaper or better?) options?\n", "tags": "motor brushless-motor", "id": "524", "title": "compatable esc's with brushless 3 phase motors"}, {"body": "ok, not really robotics, but has anyone been able to upload to a rainboduino v3.0 using the arduino ide? i can't seem to figure it out, and there is virutally no documentation online. i followed this blog entry, but got no connection to the board. \n\nif anyone can give me some suggestions, i would appreciate it! \n", "tags": "software arduino programming-languages", "id": "530", "title": "rainbowduino 3.0 - arduino ide fails to upload"}, {"body": "what's needed to utilize an imu such as the arduimu+ v3 to be used in an ins. is there any other hardware needed? \n", "tags": "arduino slam imu deduced-reckoning", "id": "531", "title": "using an imu to build an ins"}, {"body": "i'm a highschool student studying electronics and for an assessment task on the history of electronics i have decided to focus on the history of robotics. i want to begin with the earliest possible concept of a robot and progress through major developments in robotics to the current day. where should i begin my research?\n", "tags": "electronics research", "id": "533", "title": "what was the earliest concept of a robot?"}, {"body": "what can an arduino board such as the uno really do? of course simple things like controlling a couple servos is very easy for it. however, i don't think an uno board would be able to preform real-time 3d slam from point cloud data gathered from a kinect sensor on a mobile robot, right? if the robot had any speed at all the arduino wouldn't be able to keep up, correct? could it do 2d slam while moving and be able to keep up? what about taking 1/10 of the points from the kinect sensor and processing only those?\n\nbasically, what are some examples of the resource limitations of such an arduino board?\n", "tags": "arduino slam kinect", "id": "535", "title": "how computationally powerful is an arduino uno board?"}, {"body": "i'm building a small robot using some cheap vex robotics tank treads. however, my choice of picking tank treads is almost purely based on the fact that they seem like more fun than wheels. i don't actually know if they really have much of an advantage or disadvantage when compared to wheels.\n\nwhat are the pros and cons of both wheels and continuous tracks?\n", "tags": "mobile-robot design wheeled-robot tracks", "id": "541", "title": "wheels vs continuous tracks (tank treads)"}, {"body": "i've noticed that almost all research being done with helicopter robots is done using quadcopters (four propellers). why is there so little work done using tricopters in comparison? or a different number of propellers? what about four propellers has made quadcopters the most popular choice?\n", "tags": "quadcopter design uav", "id": "543", "title": "why are quadcopters more common in robotics than other configurations?"}, {"body": "lets say i drop a robot into a featureless environment and any magnetic field based sensors (magnetometer/compass) are not allowed.\n\nwhat methods are there of determining where north is?\n\ntracking the sun/stars is an option but not reliable enough when the weather is considered.\ncan you pick up the rotation of the earth using gyros?\n\nare there any more clever solutions?\n", "tags": "localization", "id": "550", "title": "how to determine heading without compass"}, {"body": "i'm trying to find where additional battery capacity becomes worthless in relation to the added weight in terms of a quadcopter. currently with a 5500 mah battery, 11.1v, i can get between 12 minutes and 12:30 flight time out of it. my question, then, is this - within the quads lifting capability of course, is there any way to find out where the added weight of a larger battery (or more batteries) cancels out any flight time improvement? obviously it's not going to be as long as two separate flights, landing and swapping batteries; i'm just trying to maximize my continuous 'in air' time. i'm trying to figure out where the line is (and if i've already crossed it) with tacking bigger batteries onto the quad and seeing diminishing returns. thanks!\n\n(again, for now presume that the quad is strong enough to lift whatever you throw at it. with one 5500mah, ~ 470 grams, my max throttle is about 70%)\n", "tags": "battery quadcopter power", "id": "554", "title": "quadcopter lipo battery weight/capacity trade off"}, {"body": "do i need a complex system (of gyros, accelerometers etc.) to detect if a robot has moved forward or can i simply use an accelerometer. \n\ni'm building a robot that learns to walk and i need to detect displacement for machine learning. can i use an accelerometer or will i need a complicated/expensive internal navigation system?\n", "tags": "slam machine-learning deduced-reckoning gyroscope accelerometer", "id": "556", "title": "is an accelerometer sufficient to detect displacement, or do i need an ins?"}, {"body": "typically mars rovers use wheels and not tracks. i guess spirit would have better chances getting out of that soft soil should it have tracks. in general, mars surface structure is not known in advance, so it seems wiser to be prepaired for difficult terrain and so use tracks.\n\nwhy do mars rovers typically use wheels and not tracks?\n", "tags": "mobile-robot design", "id": "558", "title": "why do mars rovers designers prefer wheels over tracks?"}, {"body": "what are the pros and cons of each? which is better maintained? which allows for more functionality? which utilizes the hardware more efficiently? etc.\n", "tags": "software sensors kinect", "id": "565", "title": "kinect - libfreenect vs openni+sensorkinect"}, {"body": "while experimenting with the opencv machine learning library, i tried to make an example to learn the inverse kinematics of a 2d, 2 link arm using decision trees. the forward kinematics code looks like this:\n\n\n\ni generate a random set of 1000 (xy -> alpha) and (xy -> beta) pairs, and then use that data to train two decision tree models in opencv (one for alpha, one for beta). then i use the models to predict joint angles for a given xy position. \n\nit seems like it sometimes gets the right answer, but is wildly inconsistent. i understand that inverse kinematic problems like this have multiple solutions, but some of the answers i get back are just wrong.\n\nis this a reasonable thing to try to do, or will it never work? are there other learning algorithms that would be better suited to this kind of problem than decision trees?\n", "tags": "inverse-kinematics machine-learning", "id": "566", "title": "decision trees for solving 2d inverse kinematics?"}, {"body": "could you implement a simple neural network on a microprocessor such as the arduino uno to be used in machine learning?\n", "tags": "microcontroller machine-learning", "id": "568", "title": "is it possible to run a neural network on a microcontroller"}, {"body": "we have an optional course in our high-school which is about robotics. we're using the lego mindstorms nxt and program it with the original mindstorms-software.\nhowever, we want to advance and use a major programming-language. we have tried nxc and lejos. plus, i tried out the microsoft robotics development studio, but with all these different possibilities we are a little bit overwhelmed.\n\nbecause of that (now it becomes interesting), i want to ask, what technology is the best for nxt and especially: what is easy to use? i don't want to need 14 steps just to compile a program and get it running on the nxt. also, it would be nice, if it's an extend-able language, like using c#, but are there some better or easier possibilities?\n", "tags": "programming-languages nxt", "id": "578", "title": "which programming language should i use with the nxt?"}, {"body": "i have a manipulator having 4 revolute joints with some movement limitations. so, when i apply inverse kinematics, i'm getting results which are out of limits. please provide me an algorithm that implements inverse kinematics considering joint limitations.\n", "tags": "inverse-kinematics", "id": "583", "title": "inverse kinematics with joint contraints"}, {"body": "i want to know if its currently possible for a robot to speak by it self as  does, or is just someone speaking on his behalf?\n\nyoutube video\n", "tags": "control software", "id": "585", "title": "king robota: does he speak for himself?"}, {"body": "according to wikipedia's article on slam, the original idea came from randal smith and peter cheeseman (on the estimation and representation of spatial uncertainty [pdf]) in 1986, and was refined by hugh f. durrant-whyte and j.j. leonard (simultaneous map building and localization for an autonomous mobile robot) in 1991. \n\nhowever, neither paper uses the term \"slam\".  where (and when) did that term come from?  was there a particular author or whitepaper that popularized it?\n", "tags": "slam", "id": "599", "title": "who coined (or popularized) the term \"slam\"?"}, {"body": "i've got a couple vex 269 motors hooked up to an arduino duemilanove. these motors run a some vex tank treads. i powered the whole setup with an off-brand 9-volt battery. everything seems to run great, except that it is only able to run for about 30 seconds worth of motor movement. then the battery quickly isn't able to pump out the energy needed to move the treads and the whole thing quickly slows to being unusable.\n\nwhat's my problem here? the tank treads seem loose enough that i don't think they're so restricting the motor has to pump out too much energy to move them. there's nothing else being powered except the arduino and the motors. is it because this enercell 9-volt (alkaline) is just a terrible battery choice? should i only expect that long of battery life for this robot on a 9-volt? or is there something else i'm missing? thank you much!\n", "tags": "battery tracks troubleshooting", "id": "602", "title": "vex motors and tank treads drained 9-volt battery more quickly than expected"}, {"body": "i've got a couple vex 269 motors hooked up to an arduino duemilanove. these motors run a some vex tank treads. the two motors are run as servos on the arduino using the servo library. the problem i'm having is that the two tracks don't turn at the same speed when sent the same servo angle. this is clearly due to the fact that the continuous tracks have so many moving parts that having identical friction forces on each track is hard to get.\n\nhow do i get them to move the same speed? should they be moving the same speed given the same servo angle regardless of the friction and the vex 269 motors just aren't strong enough (meaning i should use the vex 369 or some other more powerful motor)? is it best to just doing trial and error long enough to figure out which servo angle results in equal speeds on each? should i tinker with the tracks until they have nearly identical frictions? thank you much!\n", "tags": "mobile-robot motor tracks", "id": "603", "title": "how to get two continuous tracks (tank treads) to move at the same rate?"}, {"body": "i know this is a broad statement, but when you've got support for both tcp as well as a full fledged computer on board (to integrate/run an arduino), does this essentially allow for anything that would run on a linux box (raspberrypi) to run and operate your robot?\n\ni know clock speed as well as the dependency libraries for a given code base (on the pi) would add some complexity here, but what are some of the big issues that i'm overlooking in such a vertically-integrated control system?\n\nincluding a raspberrypi within a robot... does this allow for a \"universal api\"?\n", "tags": "software arduino raspberry-pi", "id": "609", "title": "including a raspberrypi within a robot... does this allow for a \"universal api\"?"}, {"body": "what are the stall and free currents of an electric motor? for example, this vex motor lists its stall and free currents at the bottom of the page.\n\ni think i understand the general idea, but a detailed description would be helpful.\n", "tags": "motor current", "id": "613", "title": "what is stall current and free current of motors?"}, {"body": "i've found that arduino (duemilanove) has a current limit of 40ma per pin. does this include the vin pin? or does the vin pin have some sort of work around in place on the board to allow for higher currents?\n\nif this is the limit on the vin, is there good way of using still using the power supply jack on the board and allowing other sources to draw on that supply without it needing to pass through the chip first?\n\nthank you much.\n\nedit: for the second part, what should i do if i wanted to get up to something like 2 amps?\n", "tags": "arduino current", "id": "616", "title": "arduino vin current limit"}, {"body": "how do you program an esc to have a reverse mode? we're looking to control an esc from a servo board (for a robotics project).\n\nassuming that the input will be between 0 and 255, we're looking for 127 as off, 255 as fully forward and 0 as full reverse, so how do we achieve that?\n", "tags": "control motor", "id": "619", "title": "programming an esc to have reverse mode"}, {"body": "i'm building an open-source bio-research hardware (ask me how you can help!) and i've got this guy here:\n\n\n\nmy big questions are:\n\n\ncan i get away with all the ground being common? (i've got a 12v and 5v needing to be grounded)\ndo i need two sets of capacitors? there are 2 wired up to the 12v regulator and 2 wired to the 5v regulator. (these are shown in blue)\n\n\ni've generally denoted connections which go under the shield as orange, and those above as green.\n\nif anyone happens to see something which might backfire, feel free to point it out. as this is also my first time making anything quite like this!\n\n\ni've verified the regulator positions and they are correct.\nthis is a proto-shield for an arduino r3 uno.\n\n\na larger version of the image can be seen here: http://i.imgur.com/bpxjn.jpg\n", "tags": "arduino electronics", "id": "623", "title": "when taking vcc power from an arduino to a 12v regulator, then to a 5v, do i need two sets of capacitors?"}, {"body": "i have a panda board es. i am not able to get it to boot. i sent it back to svtronics to get it checked and they said that the board is ok; i am the one who is not able to configure it properly.\n\nafter doing a little research and following all the directions on the panda board and ubuntu website, i am still not able to get the board to boot. i think the problem is how i am formatting the sd card. i am using disk utility for mac to format the sd card to \"msdos(fat)\" partition.\n\ni would like to know how to format an \"sd card\" on a macintosh to install ubuntu on it for panda board es.\n", "tags": "electronics operating-systems", "id": "627", "title": "formatting an sd card for panda board es"}, {"body": "for this question assume that the following things are unknown:\n\n\nthe size and shape of the room\nthe location of the robot\nthe presence of any obstacles\n\n\nalso assume that the following things are constant:\n\n\nthe size and shape of the room\nthe number, shape and location of all (if any) obstacles\n\n\nand assume that the robot has the following properties:\n\n\nit can only move forward in increments of absolute units and turn in degrees. also the operation that moves will return true if it succeeded or false if it failed to move due to an obstruction\na reasonably unlimited source of power (let's say it is a solar powered robot placed on a space station that faces the sun at all times with no ceiling)\nevery movement and rotation is carried out with absolute precision every time (don't worry about unreliable data)\n\n\nfinally please consider the following properties of the robot's environment:\n\n\nbeing on a ceiling-less space station the room is a safe but frustratingly close distance to passing comets, so the dust (and ice) are constantly littering the environment.\n\n\ni was asked a much simpler version of this question (room is a rectangle and there are no obstacles, how would you move over it guaranteeing you could over every part at least once) and after i started wondering how you would approach this if you couldn't guarantee the shape or the presence of obstacles. i've started looking at this with dijkstra's algorithm, but i'm fascinated to hear how others approach this (or if there is a well accepted answer to this? (how does roomba do it?)\n", "tags": "mobile-robot artificial-intelligence algorithm coverage theory", "id": "628", "title": "what algorithm should i implement to program a room cleaning robot?"}, {"body": "from what i've seen, lifepo4 batteries seem like one of the top battery choices for robotics applications. however, i've seen people mentioning that you can't use a charger for a different battery to charge these, but i haven't seen why. if i were to build my own setup to charge lifepo4 batteries what would it specifically need to do? what kind of voltages or current rates does it need to supply to charge these?\n\nmore specifically, i was think about setting up a solar charger for these batteries. is there any immediate reason why this is a bad solution? such as, the battery needs to charge with a current above some amount for it to work properly?\n\nif you're ambitious enough to provide an example along with your explanation, i'm specifically thinking of having 4 of these batteries with 2 pairs of 2 in series in parallel. \n", "tags": "battery", "id": "634", "title": "how to charge a lifepo4 battery?"}, {"body": "many people claim that turning an nxt motor by hand will damage it. is this true? does it matter if the motor is idle or set on break? are there any facts to confirm or refute this argument? i know that some projects (e.g. etch-a-sketch) use the built-in rotation sensor to measure how much the motor has turned. does this indicate that hand-turning nxt motors is okay? do they need to be put in a special 'rotation sensor' mode?\n", "tags": "motor nxt mindstorms", "id": "636", "title": "will turning an nxt motor by hand damage it?"}, {"body": "i have built a robot from a wheelchair that has worked very well thus far. it is now time for me to take the next step. i need to implement a permanent power circuit with proper protection. \n\nthe lowest level of protection i can think of is a fuse, but i would like to take a step further (current/voltage/direction/switches/high/low voltages). if some one could give some insight on this project of mine any info will be greatly appreciated.\n\n\n\nmoderator comment:  please see how do we address questions about related subject areas? before answering.  this question is close to the boundary, but is on-topic here.\n", "tags": "mobile-robot wheeled-robot protection circuit", "id": "637", "title": "protecting electronics against voltage/current extremes and bad polarity"}, {"body": "i am working with students (9th &amp; 10th grade) on robotics and wanted to get a good book which covers basic mechanisms.  does anyone have any recommendations.  searching google or amazon yields many results, however, i thought the community might have a standard book to use.  \n", "tags": "design mechanism", "id": "641", "title": "good book on mechanisms"}, {"body": "i recently purchased a 3-axis accelerometer from amazon, and can't seem to find how it works. i've been looking for quite a while now, and haven't found any real clues. the x, y, and z values always seem to return the same values. they change when i tilt or move the accelerometer, but revert to about 120 for each reading. i am currently using this device with the arduino uno, using the following code: \n\n\n\nalso, how would i go about converting this to tilt?\n", "tags": "arduino sensors accelerometer", "id": "642", "title": "mma7361 accelerometer always displays same values"}, {"body": "i am trying to calibrate a mems accelerometer. i was able to calibrate it for the current axis which is parallel to gravity and shows correctly, 1g. but the other two axes which should be 0.00g are showing +-0.02g instead. \nso, e.g., when the accelerometer's x axis is parallel to gravity, it should show (1g, 0g, 0g) and not (1g, 0.02g, -0.01g) like now.\n\nhow could i eliminate those values, e.g. further calibrate accelerometer? \n\nedit: the acelerometer's datasheet says nothing about calibrating except that the ic interface is factory calibrated for sensitivity (so) and zero-g level (off) (page 20).\n", "tags": "design electronics accelerometer calibration", "id": "646", "title": "mems accelerometer calibration"}, {"body": "the optimal sampling-based motion planning algorithm $\\text{rrt}^*$ (described in this paper) has been shown to yield collision-free paths which converge to the optimal path as planning time increases. however, as far as i can see, the optimality proofs and experiments have assumed that the path cost metric is euclidean distance in configuration space. can $\\text{rrt}^*$ also yield optimality properties for other path quality metrics, such as maximizing minimum clearance from obstacles throughout the path?\n\nto define minimum clearance: for simplicity, we can consider a point robot moving about in euclidean space. for any configuration $q$ that is in the collision-free configuration space, define a function $d(q)$ which returns the distance between the robot and the nearest c-obstacle. for a path $\\sigma$, the minimum clearance $\\text{min_clear}(\\sigma)$ is the minimum value of $d(q)$ for all $q \\in \\sigma$. in optimal motion planning, one might wish to maximize minimum clearance from obstacles along a path. this would mean defining some cost metric $c(\\sigma)$ such that $c$ increases as the minimum clearance decreases. one simple function would be $c(\\sigma) = \\exp(-\\text{min_clear}(\\sigma))$.\n\nin the first paper introducing $\\text{rrt}^*$, several assumptions are made about the path cost metric so that the proofs hold; one of the assumptions concerned additivity of the cost metric, which doesn't hold for the above minimum clearance metric. however, in the more recent journal article describing the algorithm, several of the prior assumptions weren't listed, and it seemed that the minimum clearance cost metric might also be optimized by the algorithm.\n\ndoes anyone know if the proofs for the optimality of $\\text{rrt}^*$ can hold for a minimum clearance cost metric (perhaps not the one i gave above, but another which has the same minimum), or if experiments have been performed to support the algorithm's usefulness for such a metric?\n", "tags": "motion-planning algorithm rrt theory", "id": "649", "title": "does rrt* guarantee asymptotic optimality for a minimum clearance cost metric?"}, {"body": "i can control a relay from an android smartphone using arduino and bluetooth as seen here.\n\nhowever, it seems too costly to be using arduino and a bluetooth receiver for driving a switch. as long as bluetooth is a radio frequency, is it possible to make a simple bluetooth receiver which can output 1 or 0 to drive a relay? if yes, how tough that is going to be?\n\nthe main factor here is the cost, which should be \\$1-$5. \n", "tags": "sensors circuit", "id": "650", "title": "can i make a simple bluetooth receiver?"}, {"body": "i'm a long time java developer who is starting to learn on the lego mindstorms nxt 2.0. are there any limitations to using the java api? which language is the most robust on the platform?\n\ni found a post, which programming language should i use with the nxt? which mentions many of the alternatives. the answer is helpful, but doesn't mention the different languages' limitations.\n", "tags": "nxt programming-languages mindstorms", "id": "653", "title": "what are the notable limitations on using java with mindstorms nxt 2.0?"}, {"body": "as i see there is a huge price gap between the two \\$223 vs \\$99 (at amazon).\n\nmy intention is to use one of those from ubuntu linux to perform depth sensing, navigation etc. and naturally i prefer the cheaper. \nhowever i am not sure if i miss some important point while betting on the kinect for xbox version. \n\nas it seems the windows version is overpriced because it has the license for development. here it is stated that there are internal differences but without exact details (the minimum sensing distance seems to be better for windows version.).\n\ncould anyone give a comparison chart?\nit would be good to know about\n\n\nconnectivity: usb, special connector, ... .\nhardware differences: are they the same or do they really differ in weight, energy consumption, speed, sensing range, ...?\ndriver: could i use xbox version under ubuntu?\napi usage: could i develop on xbox version, could i use the same/similar api on both, is the api for xbox mature enough?\nlicense: is it against the license of xbox version to develop for home/hobby/educational use?\n\n\nthanks.\n", "tags": "sensors kinect", "id": "654", "title": "what is the difference between kinect for windows and kinect for xbox?"}, {"body": "i was wondering what options are there in terms of lightweight (&lt; 5 lbs) robotic arms. i see robai cyton gamma 300, and crustcrawler ax18 look like interesting options. what lightweight arms do people use/like? \n", "tags": "mobile-robot arm", "id": "656", "title": "lightweight, commercially available robotic arms"}, {"body": "is there an operating system for the raspberry pi that is specifically made for running robotics applications? or an operating system whose purpose is to optimized just to run a few specific programs?\n\ni've been working with an arduino for a while now. as far as efficiency goes, it makes sense to me to just upload a specific set of commands and have the hardware only need to handle that, and not have to worry about running a full fledged operating system. is something like this possible to do on a raspberry pi?\n", "tags": "raspberry-pi operating-systems", "id": "667", "title": "raspberry pi operating system for robotics"}, {"body": "is there a way to check if a task, function or variable exists in not exactly c?\n\ni know that in php you can use  to check if a variable exists and  to do the same for a function, but is there a way to do that in nxc?\n\ni am specifically interested in checking whether a task exists or it is alive.\n", "tags": "nxt programming-languages mindstorms not-exactly-c", "id": "671", "title": "check if task exists in not exactly c"}, {"body": "i'm currently working on a line-following robot which uses three sensors to follow a black line. the sensors are pretty much in line and right next to each other.\n\nright now, i'm doing a simple line follow: if on the line go forward, otherwise turn left or right to regain the line. this means that the robot is wiggling along the line most of the time.\n\ni'm looking for a better way for this robot to follow the line using three sensors. the program i'm writing is in not exactly c code. i'm trying to get the robot to utilize the power of pid control, but i'm not sure how one would go about writing a three-sensor pid line-follower program in nxc.\n", "tags": "nxt programming-languages mindstorms algorithm not-exactly-c", "id": "672", "title": "pid line follow with three sensors in not exactly c"}, {"body": "mars rovers are typically very slow. curiosity, for example, has average speed of about 30 meters per hour.\n\nwhy is it designed so slow? is it because of some specific power restrictions or for other reasons? what is the top reason why it is so slow?\n", "tags": "mobile-robot design", "id": "679", "title": "why are mars rovers so slow?"}, {"body": "i have an lm2576 circuit plus an adjuster to adjust the output voltage, for controlling motor speed in a line follower robot. the circuit works great when adjusted to give out low voltages, but when i adjust it to higher voltages for my motors to go faster, it works great for 1-2 minutes, then suddenly cuts down the power and motors start to go extremely slow.\n\neven when i decrease or increase the output voltage, it won't respond until i turn off the power and turn it back on again. there is something mentioned in the lm2576 datasheet that if we overload the ic it will cut down the power until the load comes lower, so i think it might be a problem with that.\n\nsince this problem has already caused us to lose the competitions with 5+ teams, i would like to solve it for our next competition, so why does our lm2576 circuit suddenly reduce the power?\n", "tags": "motor electronics power", "id": "684", "title": "why does our lm2576 circuit suddenly cut down the power?"}, {"body": "i want to learn robotics and really interested in making a robot based on kinect sensor.\ni see so many projects like this one : http://www2.macleans.ca/2011/11/03/the-150-robot-revolution/\nand just wondering how it works on top level. i downloaded kinect sdk and did some basic tutorials, but i just don't think that microsoft sdk is the library to use for real robotics projects. any suggestions where to start and what library to use? any good books in particular or online resources? any help is appreciated, thank you.\n", "tags": "kinect", "id": "687", "title": "robotics with kinect"}, {"body": "i'm building a motion control platform with 3 dof: 1 axis of rotation (theta) and 2 cartesian (x,y). in most applications, like wrist actuation, you have an x-y stage with a rotating servo as the stage's payload. this configuration works well since little of the power and data wiring needs to transit to the non-linear moving portion of the platform. \n\nfor my inverted application, the stackup is reversed. the rotating axis comes first (from the mounting plane) with the stage connected as the rotating platform's payload. now nearly all of the wiring (power, command, sensor, and otherwise) must be routed to the non-linearly moving section.\n\ni can see two broad approaches: \n\n\nthe inside track, i route the cabling through the center of rotation.\nthe outside track, i route the cabling around outside the outer diameter of the rotating platform.\n\n\nmathematically, i can see that (1) results in minimum cable length, but maximum torsional loading, while (2) results in maximum cable length, but minimum torsional loading on the wires.\n\nhaving limited experience with cable routing (and the associated carriers, strategies, and products) in non-linear applications, my question is...\n\n...which approach is better in practice?\n\ncost isn't really the issue here. i'm more interested in reliability, ease of construction, availability of commercial components (says something about the popularity of the technique), etc...\n\ne.g. the generic concepts behind why you pick one over the other. \n\n...of course, if you have some part numbers for me i wouldn't be upset &lt;-- i know i'm not supposed ask that here ;-)\n", "tags": "control wiring routing motion", "id": "689", "title": "cable routing in theta, x, y motion control system. better inside or outside?"}, {"body": "when using an ekf for slam, i often see the motion and measurement models being described as having some noise term.  \n\nthis makes sense to me if you're doing a simulation, where you need to add noise to a simulated measurement to make it stochastic.  but what about when using real robot data?  is the noise already in the measurement and thus does not need to be added, or does the noise matrix mean something else?\n\nfor example, in probabilistic robotics (on page 319), there is a measurement model: $z_t^i = h(y,j) + q_t$, where $q_t$ is a noise covariance.  does $q_t$ need to be calculated when working with real data?\n", "tags": "slam kalman-filter", "id": "690", "title": "noise in motion and measurement models"}, {"body": "is there anything different between a irobot roomba and the create?  i want go start building my own turtlebot and playing with ros but with the cost of all the parts i'm going to have to do it piece by piece.  it's pretty easy to find cheap used roombas.  \n", "tags": "ros roomba irobot-create", "id": "693", "title": "can i use ros with a roomba?"}, {"body": "the interesting kilobot project from harvard for investigating multi-robot behavior with masses of small dumb robots has been made open hardware for a year now. \n\nhowever i cannot find so much activity about robot creation and movies about results. is it too hard to create the robots, the programmer, the charger or isn't the project interesting enough?\n", "tags": "multi-agent", "id": "696", "title": "are there working instances of kilobot projects?"}, {"body": "i'm a software engineer who volunteers with a non-profit that introduces young girls to technology. we have recently been talking about methods of introducing these children to the world of robotics, and i am curious what types of low-cost options we have.\n\none very appealing idea would be to have an online simulator, or (more preferable) an off-line standalone-simulator that we can build and program simple robots with. perhaps nothing more than dragging components together, and then programming the interactions between those components.\n\nwhat solution(s) exist that i might be able to make use of in our outreach?\n", "tags": "software simulator children", "id": "697", "title": "standalone (or capable of being) robotics simulator"}, {"body": "i recently asked a question about the juniper wifi shield, and am now working with wifly from spark fun. i've been using an updated version of their experimental library, and have been attempting to set up a webserver. unfortunately, when i attempt to connect through a web browser, i get an error saying that the page sent no data. here's my code:\n\n\n\ni am using arduino uno, and the serial monitor looks like this:\n\n\n\nis there anything obviously wrong with my code?\n\nedit:\ni now have a new shield, but i'm still working with the same problem. is it a malfunction in the hardware? i just can't figure this out!\n", "tags": "arduino software wifi c", "id": "704", "title": "wifly shield not connecting"}, {"body": "i was working on a project to make a bedside night light out of a stuffed butterfly or bird. i was making a mechanism to make the wings flap with a servo motor and some small gears. the servo motor was very loud as it moved. and this was whether or not the servo was moving large amounts, small amounts, fast or slow. \n\ni've worked with small servos before and realized they usually are pretty noisy machines, but i can't really explain why.\n\nwhy are small servo motors noisy when they move? is it backlash in the internal gearing?\n", "tags": "rcservo", "id": "709", "title": "why are servo motors so noisy?"}, {"body": "i'm currently building a robot with four legs (quadruped), 3 dof (degrees of freedom) and its been suggested here that i use a simulator to do the learning on a computer and then upload the algorithms to the robot. i'm using an arduino uno for the robot and what software could i use to simulate the learning and then be able to upload to the arduino board?\n", "tags": "mobile-robot arduino microcontroller machine-learning simulator", "id": "712", "title": "quadruped learning simulator"}, {"body": "often when i need to perform model fitting i find myself looking for a decent c++ library to do this. there is the ransac implementation in mrpt, but i was wondering if there are alternatives available.\n\nto give an example for the type of problems i would like to solve: for a set $a$ of (approx 500) 3d point pairs $(a, b)$ i would like to find the isometry transform $t$, which maps the points onto each other so that $|(a - tb)| &lt; \\epsilon$. i would like to get the largest subset of $a$ for a given $\\epsilon$. alternatively i guess i could have the subset size fixed and ask for the lowest $\\epsilon$.\n", "tags": "c++ ransac", "id": "716", "title": "c++ robust model fitting library"}, {"body": "how do we know that an object is contained inside another object or is just lying on top of it? \n\nlets take an example of a cup-plate-spoon. the cup is lying on top of the plate. but the spoon is inside the cup. how do we distinguish between the 2 situations? what are the criteria to decide whether a is contained inside b or just lying above b?\n\ni am trying to solve it using kinect.\n", "tags": "kinect computer-vision algorithm", "id": "718", "title": "how can computer vision distinguish one object being contained by another vs being on top of it?"}, {"body": "there is a lot of background here, scroll to the bottom for the question\n\ni am trying out the map joining algorithm described in how far is slam from a linear least squares problem; specifically, formula (36).  the code i have written seems to always take the values of the second map for landmark positions.  my question is, am i understanding the text correctly or am i making some sort of error. i'll try to explain the formulas as i understand them and show how my code implements that.  i'm trying to do the simple case of joining just two local maps. \n\nfrom the paper (36) says joining two local maps is finding the a state vector $x_{join,rel}$ that minimizes:\n\n$$\n\\sum_{j=1}^{k}(\\hat{x_j^l} - h_{j,rel}(x_{join,rel}))^t(p_j^l)^{-1}(\\hat{x_j^l} - h_{j,rel}(x_{join,rel}))\n$$\n\nexpanded for two local maps $\\hat{x_1^l}$ and $\\hat{x_2^l}$ i have:\n\n$$\n(\\hat{x_1^l} - h_{j,rel}(x_{join,rel}))^t(p_1^l)^{-1}(\\hat{x_1^l} - h_{j,rel}(x_{join,rel})) + (\\hat{x_2^l} - h_{j,rel}(x_{join,rel}))^t(p_2^l)^{-1}(\\hat{x_2^l} - h_{j,rel}(x_{join,rel}))\n$$\n\nas i understand it, a submap can be viewed as an integrated observation for a global map, so $p^l_j$ is noise associated with the submap (as opposed to being the process noise in the ekf i used to make the submap, which may or may not be different). \n\nthe vector $x_{join,rel}$ is the pose from the first map, the pose from the second map and the union of the landmarks in both maps.\n\nthe function $h_{j,rel}$ is:\n\n$$\n\\begin{bmatrix} x_{r_{je}}^{r_{(j-1)e}}\\\\\n                 \\phi_{r_{je}}^{r_{(j-1)e}}\\\\\n                 r(\\phi_{r_{(j-1)e}}^{r_{m_{j1}e}})\n                        (x^{r_{m_{j1}e}}_{f_{j1}} -\n                         x^{r_{m_{j1}e}}_{r_{(j-1)e}})\\\\.\\\\.\\\\.\\\\\n                 r(\\phi_{r_{(j-1)e}}^{r_{m_{jl}e}})\n                        (x^{r_{m_{jl}e}}_{f_{jl}} -\n                         x^{r_{m_{jl}e}}_{r_{(j-1)e}})\\\\\n                         x_{f_{j(l+1)}}^{r_{j-1e}}\\\\\n                         .\\\\.\\\\.\\\\\n                         x_{f_{jn}}^{r_{j-1e}}\n\\end{bmatrix}\n$$\n\ni'm not convinced that my assessment below is correct:\n\nthe first two elements are the robot's pose in the reference frame of the previous map.  for example, for map 1, the pose will be in initial frame at $t_0$; for map 2, it will be in the frame of map 1.\n\nthe next group of elements are those common to map 1 and map 2, which are transformed into map 1's reference frame.\n\nthe final rows are the features unique to map 2, in the frame of the first map.\n\nmy matlab implementation is as follows:\n\n\n\ni am using the optimization toolbox to find the minimum of the fitness function described above.  the fitness function itself is pretty straightforward i think.  the function h returns the vector h described above.\n\nthe result is:\nwhen i run join_maps on the two vectors\n\n\n\nthe output is:\n\n\n\nthe question:\n\nmy program gives map 2 is the minimum of the map joining function.  it seems like the minimum should be somewhere between map 1 and map 2.  i'm pretty sure the problem is with the matrix h.  what am i doing wrong?\n", "tags": "slam", "id": "725", "title": "least squares map joining"}, {"body": "i am working on a quadrotor.  i know its position -- $a$, where i would like to go -- target position $b$, and from that i calculate a vector $c$ -- a unit vector that will take me to my target:\n\n\n\nsince a quadrotor can move in any direction without rotation, what i have tried to do is \n\n\nrotate $c$ by the robots yaw angle\nsplit it into its $x, y$ components \npass them to the robot as roll and pitch angles.  \n\n\nthe problem is that if the yaw is 0&deg; &plusmn;5 then this works, but if the yaw is near +90 or -90 it fails and steers to wrong directions. my question is am i missing something obvious here?\n", "tags": "quadcopter uav navigation", "id": "726", "title": "guiding a quadrotor towards a target"}, {"body": "have you ever seen one those video games that has headset/goggles you stand in and look around the virtual scene with? i'm building one of those, and i'm trying to design a simple controller. i need the output of the controller to emulate a mouse input. so if you look to the left, it's as if you were moving the mouse to the left. supposing i use optical encoders, the pan and tilt will need to be in separate locations (a couple of inches apart). it seems that many mouse hacks online have the components very close together.\n\ndo you think it's possible to have one of the encoders some distance away from the controller chip? for oem purposes, is there a good mouse controller chip that will output usb protocol mouse movements that i could buy in bulk?\n\nmany thanks for any suggestions. cheers\n", "tags": "microcontroller", "id": "730", "title": "good method for building a pan and tilt controller?"}, {"body": "when you've created a map with a slam implementation and you have some groundtruth data, what is the best way to determine the accuracy of that map?  \n\nmy first thought is to use the euclidean distance between the map and groundtruth. is there some other measure that would be better?  i'm wondering if it's also possible to take into account the covariance of the map estimate in this comparison. \n", "tags": "slam mapping", "id": "734", "title": "comparing maps to groundtruth"}, {"body": "i'm building a hobby 6-dof robotic arm and am wondering what the best way is to communicate between the processors (3-4 avrs, 18 inches max separation). i'd like to have the control loop run on the computer, which sends commands to the microprocessors via an atmega32u4 usb-to-??? bridge.\n\nsome ideas i'm considering:\n\n\nrs485\n\npros: all processors on same wire, differential signal more robust\ncons: requires additional chips, need to write (or find?) protocol to prevent processors from transmitting at the same time\n\nuart loop (ie, tx of one processor is connected to rx of next)\n\npros: simple firmware, processors have uart built in\ncons: last connection has to travel length of robot, each processor has to spend cycles retransmitting messages\n\ncanbus (i know very little about this)\n\n\nmy main considerations are hardware and firmware complexity, performance, and price (i can't buy an expensive out-of-box system).\n", "tags": "microcontroller electronics arm", "id": "736", "title": "inter-processor communication for robotic arm"}, {"body": "i've got a tread-driven robot, with low precision wheel encoders for tracking distance and an electronic compass for determining heading.  the compass has significant (> 1 second) lag when the robot turns quickly, e.g. after reaching a waypoint &mdash; pivoting in place to point to its new heading.  \n\nwhat are ways for dealing with the lag?  i would think one could take a lot of measurements and model the compass response.  however, this seems problematic since it's rate-dependent and i don't know the instantaneous rate.\n\nas a simple-but-slow approach, i have the robot turn until it's very roughly pointed in the right direction, then make very small incremental turns with brief measurement pauses until it's pointed the right way.  are there other ways of dealing with this? \n", "tags": "sensors compass", "id": "738", "title": "what are methods for dealing with compass lag (rate dependent hysteresis)?"}, {"body": "i've seen this question, which asks about determining the process noise for an ekf.  i don't see anything there about pre-recorded data sets.  \n\nmy thought on how to determine the noise parameters, assuming ground truth is available, would be to run the data several times with the ekf and minimize the mean square error, while varying the noise parameters.\n\nis this an acceptable way to determine noise for a pre recorded data set?  are there better (or just other) ways from determining the optimal noise values based just on the data set?\n", "tags": "noise ekf", "id": "741", "title": "how do you determine ekf process noise for pre-recorded data sets?"}, {"body": "i'm trying to power 7-12 servos, and i was under the impression that each one would need about an amp, but in looking around for an appropriate bec to supply them, i notice that most seem to output around 1-3.5 amps.\n\nthey won't all be running at once, but often, say 4 will be drawing enough juice to move.\n\nobviously, i'm missing some link in my understanding.  how do i determine how many amps will be needed from the power supply?\n", "tags": "design power rcservo bec", "id": "748", "title": "how many amps do i want my switching bec to provide?"}, {"body": "i'm trying to program advanced functions in robotc but i'm not too sure i'm doing it right. i want to specify the motor port i'm using, but i assigned names to all the motors. funny thing though, they don't exactly work the same as regular variables.\n\nfor instance, motor[port7]'s alternate name is light_blue.\n\n\n\ni'm not really sure if these are new variables, or just specifications.  anyway, here is the variable's signature:\n\n\n\nmy code plans on doing something similar to this:\n\n\n\nbut with the int/motor hybrid variable/unidentified i'm not sure how well that would work out. or at all.\n", "tags": "robotc", "id": "751", "title": "confused about the variables in robotc?"}, {"body": "i would like a high torque motor (37 oz-in @ 5760 rpm) for souping up a scorbot 3 i bought. i really need it to have an encoder to count the number of revolutions and to allow high start-up torque. so far, i'm having difficulty finding a suitable motor.\n\nthe closest i've found are:\n\n\nrevolver s stubby\n(still not ready for purchase)\nteam novak ballistic 25.5t\n\n\ni've found other rc car motors, but they are usually too big.\n\nsome alternatives i thought about are:\n\n\nadding hall sensors to an existing motor - how hard is this?\nrewinding a motor with more turns to increase torque (decrease kv)\n\n\ndoes anybody know of any motors that fit these requirements or modifications i can make to existing ones?\n\n\n\nupdate: i had almost given up hope, until someone at homebrew robotics suggested using the maxon motor finder.\n\nif you just type in my given torque and speed, it returns 3 motors, but they're all over powered because the search interprets your specs as a continuous operating point, whereas my robot will only need that much power 20% of the time, and maybe for 1 second max.\n\nif i type in 12v, 5000rpm, and 15 oz-in, then it returns 2 brushless motors, of which, the motor ec 45 is the best fit, which has this operating curve:\n\n\n\nhowever, i don't want to pay what maxon is charging, so instead, i've contacted the guy who makes the yet to be released revolver stubby and he has kindly offered to build a custom high torque, low rpm motor for me.\n\ncan anyone comment on why high torque, low rpm motors like the one i want seem so rare? is due to lack of applications (robotics) or is there some intrinsic difficulty in making them?\n", "tags": "motor brushless-motor", "id": "757", "title": "how can i upgrade an existing robot with a higher torque, sensored motor @ ~100 watts?"}, {"body": "i'm familiar with the idea of the uncanny valley theory in human-robot interaction, where robots with almost human appearance are perceived as creepy. i also know that there have been research studies done to support this theory using mri scans. \n\nthe effect is an important consideration when designing robotic systems that can successfully interact with people. in order to avoid the uncanny valley, designers often create robots that are very far from humanlike. for example, many therapeutic robots (paro, keepon) are designed to look like animals or be \"cute\" and non-threatening.\n\nother therapeutic robots, like kaspar, look very humanlike. kaspar is an excellent example of the uncanny valley, since when i look at kaspar it creeps me out. however, people on the autism spectrum may not experience kaspar the same way that i do. and according to shahbaz's comment, children with autism have responded well to kaspar.\n\nin the application of therapeutic robots for people on the autism spectrum, some of the basic principles of human-robot interaction (like the uncanny valley) may not be valid. i can find some anecdotal evidence (with google) that people on the autism spectrum don't experience the uncanny valley, but so far i haven't seen any real studies in that area.\n\ndoes anyone know of active research in human-robot interaction for people on the autism spectrum?  in particular, how does the uncanny valley apply (or doesn't it apply) when people on the autism spectrum interact with a humanlike robot?\n", "tags": "research hri uncanny-valley", "id": "758", "title": "in hri, how is the \"uncanny valley\" experienced by people on the autism spectrum?"}, {"body": "i got the following homework question:\n\n\n  what are the general differences between robots with ackermann steering and standard bicycles or tricycles concerning the kinematics?\n\n\nbut, i don't see what differences there should be, because a car-like robot (with 2 fixed rear wheels and 2 dependent adjustable front wheels) can be seen as a tricycle-like robot (with a single adjustable front wheel in the middle).\n\nthen, if you let the distance between the two rear wheels approach zero, you get the bicycle.\n\nso, i can't see any difference between those three mobile robots. is there something i am missing?\n", "tags": "mobile-robot design kinematics theory", "id": "763", "title": "differences between ackermann steering and standard bi/tricycles concerning kinematics?"}, {"body": "i am most familiar with slam maps that are point clouds, usually in the form of a vector like $&lt;x,y,\\theta,f_{1x},f_{1y},...,f_{nx},f_{ny}&gt;$.  i also understand how to create a map like this using an ekf.\n\ntoday i came across a .graph file format, which as you would expect consists of vertices and edges in the format:\n\n\n\n\n\ni know that there's a connection between matrices and graphs (an adjacency matrix for example).  but it's not clear to me how this graph format of a map is equivalent to a point cloud map that i'm familiar with.  \n\nwhat is the relationship?  are the vertices both poses and landmarks? are they in a global reference frame? how is this created from say velocity information and a range/bearing sensor?  is there a transformation between a graph map and a point cloud?  \n", "tags": "slam mapping", "id": "764", "title": "the relationship between point cloud maps and graph maps"}, {"body": "im using my own code to create a quadcopter robot. the hardware part is done but i need to balance the copter. \n\nthis is the video of its current status: \nhttps://www.dropbox.com/s/53tpf1jzaly6m33/movie%20on%202013-01-10%20at%2006.36.mov\n\ni have tried to play with the speed of each motor to get it balanced. it didnt go. \ni actually have a gyro and accelerometer onboard. but how shall i adjust the motor speed based on these values? what are the rules that i should beware of?\n\nis there any better solution other that try and error? where shall i begin? any tips? \n", "tags": "balance quadcopter", "id": "768", "title": "how to balance a flying quadcopter?"}, {"body": "i would like to design a robotic arm to hold a weight x at length y (in my case i want to hold x=2.5 lbs at y = 4 inches). starting out simply, i would like try building an arm with a gripper plus one servo joint. \n\n[servo joint] ----- y ------ [gripper]  \n\nwhen designing an arm, would i want to say that the gripper has to have enough torque to hold the desired weight (e.g. 2.5 lbs) at a minimal distance (however long the fingers are) then design the servo joint to bear the weight of the gripper + the load?\n\ni would like to be able to hold the object at full extension\n", "tags": "design servos arm", "id": "775", "title": "getting started with robotic arm design"}, {"body": "i am trying to build a semi-analog timer. something like those old egg timers that you rotate the face of. i want a knob that i can turn that can be read by a microcontroller, and i also want the microcontroller to be able to position the knob. i'd like to implement \"stops\" by letting the microcontroller push the knob towards certain positions. as it runs down, the knob should turn. this is my first project of this kind; i've built small robots in the past, but it's been many years.\n\ni've considered hacking a servo motor to read its position, but the small hobby servos i've tried are too hard to turn, very noisy, and pick up too much momentum when turned. they don't act like a good knob.\n\ni'm now considering a rotary encoder connected to a motor, but after hunting at several sites (sparkfun, servocity, digikey, trossen, and some others), i haven't been able to find anything that seemed appropriate. i'm not certain how to find a motor that's going to have the right kind of low torque.\n\nthis seems like it shouldn't be a really uncommon problem. is there a fairly normal approach to creating a knob that can be adjusted both by the user and a microcontroller?\n", "tags": "motor servos", "id": "777", "title": "building a controllable \"knob\""}, {"body": "the fact is that the more i search the less i find autonomous (real) robots in use. the companion robots are all toys with limited useless functionality. whenever there is a natural disaster you don\u2019t see operational search and rescue robots in the news. even military robots in service are all remotely controlled machines. they are not intelligent machines. industrial robotic arms are deterministic machines. the only robots with some levels of autonomous functionality are cleaning bots, warehouse operations bots and farming robots.\n\non the other hand, today:\n\n\nthe artificial intelligence algorithms are very good in making decisions \nthe sensing technologies are very sophisticated\nthe communication technologies are very fast\nwe can manufacture cheap parts\npeople are extremely gadget savvy\n\n\nso, why there is no real robot in our day to day life? no investment in the domain? no market yet? not enough knowledge in the domain? a missing technology? any idea?\n", "tags": "mobile-robot", "id": "780", "title": "what are the reasons for not having autonomous robots in our daily activities?"}, {"body": "i am looking for a good embedded pc to run ros on. i recently came across a couple of little machines using new very multi-core processors, such as the epiphany and the xmos.\n\nsince the one thing that ros really seems to want is cores, would ros be able to take advantage of all of these cores? or are they all just too feeble with too little ram to be of any use?\n\nwould it make more sense to focus on machines with fewer, more powerful cores?\n", "tags": "ros", "id": "783", "title": "would ros benefit from a multicore processor like epiphany or xmos?"}, {"body": "i was wondering, do we have real nano bots, like the ones in the movies? \n\ni think we have bots which can move through the blood vessels, am i right?\n", "tags": "mobile-robot", "id": "788", "title": "do \"nano bots\" (that can fit inside the human body) actually exist?"}, {"body": "over the last month, i saw many robots that don't have any real purpose, which made me ask myself: \"does this have any value?\" i saw dancing robot on ces, advanced lego based robots and also robots combined for very limited purpose. i saw ten year old children playing with robots, and competitions for them. someone has told me that this is just for education and logic spreading. \n\nin other cases, there were arguments like, \"this is for informing people that everything is going forwards\". i know that people will buy robotic vacuum cleaners because they think that they'll save some time, but these robotic cleaners are not very reliable and i see it only as marketing. \n\ndo these things (children's education, dancing robots, and other instances of selling a pig in a poke) have any value in terms of robotics, and are really advancing the field as manufacturers say?\n", "tags": "research", "id": "790", "title": "do \"toy\" robots move technology forwards?"}, {"body": "microhard systems currently sells several types of 900mhz modems, which are mostly used in robotics and scada applications.  one of their product lines, the 910 series (mhx910, n910, spectra 910), is obsolete and no longer sold.  however, some older equipment is built with oem versions of the 910 series soldered in place.\n\nmicrohard currently sells a 920 series (mhx920, n920, spectra 920) that shares many of the specs with the 910 series, but cannot establish a radio link with a 910 series modem due to differences in encryption and hopping pattern.  therefore, if you want to make new equipment communicate with equipment using the 910 series, your options are:\n\n\nde-solder the old 910 modem and replace it with the footprint-compatible 920 modem, or\nreconfigure a 920 series modem to communicate with the 910 series modem.\n\n\noption 1 is undesirable, since i don't have access to the firmware on the older equipment in question.  does anyone know how to accomplish option 2?\n", "tags": "radio-control", "id": "795", "title": "how can microhard 920 series modems be made compatible with microhard 910 series?"}, {"body": "i also asked this question on ros answers, but it's not getting much interest there.\n\ncurrently the ethercat package in ros uses the slaves' product ids to identify the devices, and load the correct drivers. this works great when all of the devices are manufactured by a single vendor, but are there any plans to prevent product id collisions when multiple vendors make ros compatible ethercat devices?\n\nwe manufacture our own ethercat devices, and are just using some large values for product id, just hoping that these don't collide with anyone else's. ideally, ros would concatenate the vendor and product ids into a single 64-bit value, and use that to identify the correct driver.\n", "tags": "ros", "id": "797", "title": "plans to use vendor id to identify ethercat devices?"}, {"body": "i'm building a quadrupedal robot that will learn how to walk. from the responses i got from asking if  its possible to run a nn on a micro controller i realised i needed to think of a clever system that wouldn't take 1000 years to be effective and would still be able to demonstrate onboard learning. i've designed a system but i'm not sure how effective it will be.\n\nfirstly i hardcode 5-20 positions for the legs.\n\ni set up a (simple) neural network where each node is a different set of positions for the legs, which i will write.\n\nthe robot moves from one node to another and the weight of the joint is determined by how far forward the robot moves.\n\neventually there will be strong connections between the best nodes/positions and the robot will have found a pattern of moves that are most successful in walking.\n\n\n  how effective would this be in learning to walk?\n\n\nnote: instead of positions i could write short gaits and the process would work out which sets work best when combined.\n", "tags": "microcontroller machine-learning walk", "id": "801", "title": "simple neural network with hardcoded positions for walk optimisation"}, {"body": "we are currently designing a mobile robot + mounted arm with multiple controlled degrees of freedom and sensors. \n\ni am considering an architecture in two parts:\n\n\na set of realtime controllers (either raspeberry pis running an rtos such as xenomai or bare metal microcontrollers) to control the arm motors and encoders.  let us call these machines rtx, with x=1,2,3\u2026 depending on the number of microcontrollers.  this control loop will run at 200hz.\na powerful vanilla linux machine running ros to compute slam, mocap, and execute high-level logic (decide the robot\u2019s task and compute the motors' desired position and speed).  this control loop will run at 30hz.\n\n\ni know my framework needs to be scalable to account for more motors, more sensors, more pcs (eg. for external mocap). \n\nmy main problem is to decide how to have the different rtx communicate with pc1. i have looked at papers related to robots architecture (e.g. hrp2), most often they describe the high level control architecture but i have yet to find information on how to have the low level communicate with the high level and in a scalable way. did i miss something?\n\nin order to connect the fast rt machines ensuring the motor control with pc1, i have considered tcp/ip, can and uart:\n\n\ntcp/ip: not deterministic but easy to put in place. is non determinism a real issue (as it will only be used at at slow speed 30hz anyways)?\ncan: slow, very reliable, targeted to cars ( have seen there are some exemples using can with robots but it looked exotic)\nuart: if i had only had one rt machine for motor control i would have considered uart but i guess this port does not scale well with many rtx\nis tcp/ip really a no go because of its non-deterministic characteristics? it is so easy to use\u2026\n\n\nat the moment no solution really seems obvious to me. and as i can find no serious robot example using a specific reliable and scalable solution, i do not feel confident to make a choice. \n\ndoes anyone have a clear view on this point or literature to point to?  are there typical or mainstream communication solutions used on robots?\n", "tags": "control design communication", "id": "807", "title": "how can i control a fast (200hz) realtime system with a slow (30hz) system?"}, {"body": "i'm running out of digital ports, and have no sensors that fit the definition 'analog'. would it be possible to run a touch sensor, a quadrature encoder, or an ultrasonic sensor on an analog port? \n\ni'm thinking not, but i didn't run across anything that said otherwise.\n", "tags": "sensors", "id": "810", "title": "can ultrasonic and button sensors be run in a vex analog port?"}, {"body": "i have a small motorized vehicle with gears as wheels running up and down a track made of gear racks. how can this robot know when it has run half the track? and what's the best method to keep it from running off its track at the end and then return to start.\n\nthe robot is carrying water, not exactly the same amount each time, so it will not weigh the same. therefore it might not be the same amount of steps in the stepper-motor each time.\n\nhere i have some ideas that might work, though i am a beginner, and don't know what's the best solution.\n\n\ngps tracking it (overkill on such a small scale?)\nsome kind of distance measurer\nhave a knob it will hit at the middle of the track, telling program to delay for a given time\ntrack amount of steps the motor has performed (won't be as accurate?)\n\n", "tags": "mobile-robot arduino sensors", "id": "812", "title": "how to find out how far a motor has taken a vehicle?"}, {"body": "i'm working on a rather low budget project, and need some way to control four or more motors using one arduino. i've looked at motor shields a little, but i have a shield on top of it already, it does have female input on the top though, so a motor shield may work. any suggestions?\n", "tags": "control arduino microcontroller motor power", "id": "815", "title": "arduino motor control"}, {"body": "we have an electric wheel chair, and are looking to add a rotary encoder to each wheel.  we don't want to hack the motor itself, so want to add the encoder without harming the motor-to-wheel connection.  we will be using an arduino to read the signal.\n\ndoes anyone have any experience adding rotary encoders to already assembled wheel assemblies?  \n", "tags": "arduino microcontroller", "id": "819", "title": "adding rotary encoders to an electronic wheel chair"}, {"body": "i'm working with a lifesize (~130cm) humanoid robot (hubo+) and looking for a way to easily program new motions and gestures into him. obviously, i could write my own tool, but i am looking for a solution that can leverage existing tools or standards for robot motion. my first thought was trying to use animation software like blender or maya, and writing a script to extract the joint angles at keyframes. however, few robotics researchers are probably proficient with maya. (i know i'm not!)\n\nis there already some kind of 3d posing tool for robotics that is a standard? the only things i have seen so far that comes close is the pose utility in roboplus and choregraphe for the nao, but both programs seem limited to particular robots and don't appear to be extendable to hubo.\n\nso my questions are:\n\n\nare there standard file formats for robot motion? not 2d wheeled robot motion. arm and leg motion! something equivalent to the .bvh file format used in motion capture.\ndo you know of any wysiwygish tool for creating robot motion using keyframes and inverse kinematics?\n\n", "tags": "software motion", "id": "826", "title": "can i use digital animation software to define the movements of humanoid robots?"}, {"body": "i'm currently designing a robotic arm with 6-dof, and my goal is to be able to give setpoints for 3d position, velocity and orientation ($x,y,z,\\dot{x},\\dot{y},\\dot{z},\\theta,\\alpha,\\gamma$).\n\ni only had feedback-control for siso systems so far in college, so, taking the learning curve of multivariable control in consideration, should i approach this problem trying to model the system as a mimo or multiple sisos?\n\nif possible please mention possible disadvantages and advantages in each strategy.\n", "tags": "control manipulator robotic-arm", "id": "829", "title": "which is model is best for feedback control of robotic manipulators: mimo or parallel siso?"}, {"body": "i have been trying to write code to connect a hitechnic prototype board to my lego brick. although i am using msrds studio, that isn't the issue; reading and writing to the serial port that the device is connected to works fine. \n\nwhere i am lacking is that i don't understand the data is that is being sent and received. it goes out and comes back in the form of a byte array. for example:\n[128]\n[15]\n[0]\n[2]\n[16]\n[2]\n[8]\n\nis this byte array converted from hex? what is this response telling me? \n\nobviously i am a total newbie at this, i can program but i don't really understand electronics and i am trying to make that connection between what i have read about how an i2c controller works and what is happening when i send and receive data over a serial port. \n", "tags": "nxt i2c", "id": "832", "title": "how do i interpret this data, received by the i2c controller on an nxt 2 brick?"}, {"body": "i want to injection-mold several thousand of a part that fits in a 6\" x 6\" x 2\" bed.\n\ni would like to be able to use only tooling that i can make myself, so i can rapidly iterate on the tooling as production problems are discovered.\n\ni know that typical injection-mold \"hard tooling\" is created using electrical discharge machining, which requires first cncing a carbon positive and then using that as an electrode to spark-burn out a negative mold from hard steel.\n\nhowever, i do not have the equipment for edm. instead, i would prefer to directly cnc the negative mold. i know that a soft enough steel to be cnced will not last very long as an injection mold, but like i said, my run size is tiny, and i am ok with making a new mold every 500 units or so if necessary.\n\ni am open to buying an endmill that is diamond-tipped, to work with harder steel, but then the limitation will probably be how much torque the cnc can produce on the endmill.\n\nwhat are some recommendations or links to helpful resources? in particular, what is a good cnc with enough torque, and what blend of steel should i use? thanks!\n", "tags": "cnc", "id": "835", "title": "cncing an injection mold"}, {"body": "i am working on building my own quadcopter from scratch. i noticed that many solutions available online use arduino, but i am not a fan of arduino. so my questions are: what microcontrollers should be used, what are the crucial features of those microcontrollers etc. i would like to build it from total scratch. i was thinking about pic microcontrollers.\n\nalso what should be used for esc, since i would build that from scratch too.\n\nsumming it all up:\n\n\n4 escs\ngyro,acceloremeter,gps\ntransceiver\n\nwhich is about 8 slaves and one master microcontroller.\n\n", "tags": "microcontroller quadcopter esc", "id": "838", "title": "what microcontroller should be used for quadcopter flight control and esc?"}, {"body": "for avoiding obstacles during 2d robot navigation what is the best position/angle to place the sonar sensors? how many should there be?\n\ni would like to know if there is some theory or examples for the problem of placing. i realize that it depends on the way that the robot moves and its geometry, but i am searching for general answers.\n", "tags": "mobile-robot sensors navigation acoustic-rangefinder", "id": "839", "title": "sonar for obstacle avoidance: how many sensors and where to place them?"}, {"body": "i am reading research papers about robotics and many of them follow the same pattern:\n\n\nsome construction is established\nkinematical formulas are read from the mechanical structure\nthe state space is analysed (e.g. how far the robot can reach, what the maximum speed can be, what is left underspecified and how to handle such mathematically incorrect systems and so on)\n\n\nis there some tool or software product that can receive (as input) the mechanical structure and then output the kinematical formulas?  preferably, it would provide some kind of plots, analysis, suggestions for optimal design parameters (e.g. length, angles of the sturcture, optimum parameters of motors and so on).  does this exist?\n", "tags": "software design inverse-kinematics research kinematics", "id": "840", "title": "is there a tool for building and analysing robots (kinematics, control) visually?"}, {"body": "using a depth sensing camera like kinect, i would like to retrieve the position of an predetermined object (e.g. a cup, fork etc so that i would ultimately be able to grab the object). what would be a way to achieve this?\n", "tags": "computer-vision algorithm", "id": "845", "title": "how to identify objects in space"}, {"body": "i am learning about i2c on the arduino. i was looking at a sample program to scan for i2c devices and saw this:\n\n\n\nwith the following code. \n\n\n\nas far as i understand it, a bit is just 1.  so, why how do 7 bits loop from 0 - 127? \n", "tags": "arduino i2c", "id": "848", "title": "why must i loop 127 times for a \"7-bit\" address in this example?"}, {"body": "i'm studying various optimal control methods (and implements them in matlab), and as test case i choose (for now) a simple pendulum (fixed to the ground), which i want to control to the upper position.\n\ni managed to control it using \"simple\" feedback method (swing-up based on energy control + lqr stabilization for the upper position), and the state trajectory is show in figure (i forgot the axis description: x is theta, y is theta dot.\n\n\n\nnow i want to try a \"full\" optimal control method, starting with an iterative lqr method (which i found implemented here http://homes.cs.washington.edu/~todorov/software/ilqg_det.m)\n\nthe method requires one dynamic function and one cost function ( is the motor torque (one motor only)):\n\n\n\nsome info on the pendulum: the origin of my system is where the pendulum is fixed to the ground. the angle theta is zero in the stable position (and pi in the unstable/goal position).\n is the bob mass,  is the rod length,  is a damping factor (for simplicity i put , , )\n\nmy cost is simple: penalize the control + the final error.\n\nthis is how i call the ilqr function\n\n\n\nthis is the output\n\n\n  time from 0 to 10. initial conditions: (0.785398,0.000000). goal: (-3.141593,0.000000)\n   length: 1.000000, mass: 1.000000, damping :0.300000\n  \n  using iterative lqr control\n  \n  iterations = 5;  cost = 88230673.8003\n\n\nthe nominal trajectory (that is the optimal trajectory the control finds) is \n\n\n\nthe control is \"off\"... it doesn't even try to reach the goal...\nwhat am i doing wrong? (the algorithm  from todorov seems to work.. at least with his examples)\n", "tags": "control", "id": "851", "title": "optimal control for a simple pendulum"}, {"body": "i have the following chassis along with an arduino and a motor shield. \n\ni'm in the process of developing a tracking mechanism for use with differential drive.\n\nnormally, a photo reflector can be placed adjacent to the wheel that will reflect when each spoke passes through therefore allowing code to be written that will accurately measure each wheels position.\n\nthe problem i have is that you cannot see the wheels from inside the chassis, only small holes for the driveshaft. placing sensors on the outside would look ridiculous and a wall crash would cause havoc.\n\nwould i be able to use a photo reflector on the gears (as shown) if i accurately placed it to count each spoke on the gear itself? i'm a bit hesitant though because even a small bump could misalign the sensor - again causing havoc.\n\nso does any one have an idea on how to track the wheel movements?\n", "tags": "arduino two-wheeled", "id": "854", "title": "sensors for differential drive"}, {"body": "is there a way of initializing a kalman filter using a population of particles that belong to the same \"cluster\"? how can you determine a good estimate for the mean value (compute weighted average ?) and the covariance matrix ? each particle is represented as $[ x , y , \u03b8 , weight]$.\n", "tags": "localization kalman-filter particle-filter", "id": "857", "title": "can you seed a kalman filter with a particle filter?"}, {"body": "i need to simulate a stream of vehicles, such as on an assembly line. automatons are performing operations on the vehicles when they come within reach.  the automatons do not keep track of the individual vehicles, they simply collect data.\n\nwe need to choose a method of matching the data gathered by each automaton with the vehicle it belongs to.  for example, we could guess the identity of a vehicle using its timing when arriving in the operation range (sensors) of an automaton.\n\ni have to check the possible problems we will face, so i would like a little (hopefully simple) video/simulation tool that i could play with.\n\n\nvehicles could be symbolized has moving black squares\nautomatons/sensors could be static points or circles.\nit should be possible to change the time interval between two vehicles, and their speed, and add some random delays.\n\n\nwhat kind of software should i search for, or where should i look?\n\nshould i consider to developing it from scratch? \n", "tags": "simulator", "id": "863", "title": "how do i simulate an assembly line?"}, {"body": "i am creating a cnc machine on a budget, using old motors out of printers/scanners/etc. \n\ni am limited to about 650ma for the whole system, so my fear is that when the cutting bit touches the material, the stepper might be moving too quickly and won't have enough torque.  this would mean it will become one rotation behind, which could really mess up a cnc project.\n\ndetecting when the motor \"misses\" a step would allow me to readjust the motor speed until it reaches a balance between working quickly and having adequate torque.  how can i achieve this?\n", "tags": "arduino stepper-motor current cnc", "id": "865", "title": "how to tell a stepper motor's position, or detect slippage"}, {"body": "i am very new to robotic design and i need to determine what parts i will need to assemble an arm joint.  the joint will contain one timing belt pulley which a remote motor will be turning, a forearm that the pulley will be rotating and an upper-arm piece that will actually be two parallel arms that will grip the pulley on top and bottom in order to brace the pulley from off axis torque from the timing belt.\n\ni am kind of at a lost as to how to mount all of these together.  i would like to mount the forearm directly to the pulley and then the two parallel arms (comprising the upper-arm) sandwich the top of the pulley and the lower part of the forearm.  this would be attached using a turn table.  any ideas on how a shaft would mount to these?  or how to attach the pulley to the arms themselves?\n\nany kind of direction or links would be greatly appreciated, i don't even know the names of the parts i would be looking for.\n\nin this ascii art model the dashed lines (-) are the arms.  the arm on the left is the forearm and the two arms on the right are the two parallel parts of the upper arm.  the stars are the belt and the bars (||) are the pulleys at the elbow |e| and shoulder |s|.  \n\n\n\ni am thinking of mounting the pulley to the left arm directly (a bushing?) and then maybe using turntables to mount the pulley to the top arm and another turn table to mount the left arm to the bottom arm.\n\nhere is a picture of the design to help you visualize:\n\n\n", "tags": "design arm joint", "id": "869", "title": "building robotic arm joint"}, {"body": "i'm working on a basic airplane flight stabilization system, as the precursor to a full autopilot system. i'm using a salvaged wii motion plus and nunchuk to create a 6dof imu. the first goal is to keep the wings level, then mix in the users commands. am i correct in saying that this would not require a gyro, just a 3 (2?) axis accelerometer, to detect pitch and roll, then adjust the ailerons and elevator to compensate?\n\nsecondly, if we extend my design goal from \"keeping the wings level\" to \"flying in a straight line\" (obviously two different things, given wind and turbulence), does the gyro become necessary, insofar as this can be accomplished without gps guidance?\n\ni've tried integrating over the gyro values to get roll, pitch &amp; yaw from that, however (as evidenced by this question), i'm at a level in my knowledge on the topic where i'd prefer simpler mathematics in my code. thanks for any help!\n", "tags": "uav accelerometer imu gyroscope", "id": "872", "title": "do i really need a gyro for an airplane flight stabilization system?"}, {"body": "i have a quadcopter robot that has a kinect on it and i want to do 3d mapping with it. \n\n\nis kinect reliable on a moving robot (i.e., can it give me stable images and maps with this movement)?\nis there an sdk for producing 3d maps from kinect data?  will slam algorithms work?\nis the arduino board on the copter (atmega 2560) powerful enough to handle this?\n\n", "tags": "arduino slam kinect quadcopter", "id": "873", "title": "3d mapping from a quadcopter with kinect"}, {"body": "before i start asking you for help let you know that i am newbie in electronic field.\n\nall i want to know is the principle of wheel rotation (left-right) from remote car gadget. i am not talking about changing the spin rotation of dc motor (up,down buttons from remote), i am asking about left and right movement of wheel.\n\ni know that spin change depends on polarity of dc motor, so changing polarity changes spin, but what is the principle of changing the left and right positions of front wheels.\n", "tags": "control wheel", "id": "876", "title": "remote car controlling"}, {"body": "i have 3d printers at my school, but unfortunately they are not super high quality. i want to try 3d printing a model i made on google sketchup, but i would like for it to be fairly accurate.\n\nwhat measures can i take to prevent error in the model? i understand that i need to export the file as an stl; is there anything i can do to the model before hand to ensure accuracy? \n\nwhat can i do to calibrate a 3d printer for best results?  \n", "tags": "3d-printing", "id": "878", "title": "accurate 3d printing w/sketchup"}, {"body": "i recently start a project to measure the force on a bathroom grab bar. the force/load is applied by the person who need to the grab bar for assistant. what i want to measure is the load against the wall and do the the real-time monitoring of the load for further analysis to improve the design.\n\ni am not quite sure about what kind of sensor would be suitable to do the measurement. i am looking at different load cells but cannot get the idea how to mount commercial load cells to do the measurement. what i am trying right now is using strain gauge to measure the strain near the end of the bar(wall side) and roughly calculate the load. i think (might be wrong) there may exists some kind of force/load sensors that can clamp on the bar to do the measurement.\n\nany sensor types/models or suggestion are welcome.\n\ni also posted this question to ee forum\nhttp://electronics.stackexchange.com/questions/57197/how-to-measure-force-that-applied-on-grab-bar\n", "tags": "sensors force", "id": "882", "title": "force measurement on grab bars"}, {"body": "i have been experimenting with different fitness functions for my webots robot simulation (in short: i'm using genetic algorithm to evolve interesting behaviour).\n\nthe idea i have now is to reward/punish aibo based on its speed of movement. the movement is performed by setting new joint position, and currently it results in jerky random movements. i have been looking at the nodes available in webots, but apart from gps node (which is not available in aibo) i couldn't find anything relevant.\n\nwhat i want to achieve is to measure the distance from previous location to current location after each movement.  how can i do this?\n", "tags": "mobile-robot reinforcement-learning simulator", "id": "883", "title": "measuring speed of movement in webots"}, {"body": "is there a matlab toolbox available to use sick lasers in windows?\n\ni found one toolbox for matlab in gnu/linux.  is there another way to use sick laser via matlab in windows?\n", "tags": "mobile-robot localization", "id": "884", "title": "using a sick laser with matlab in windows"}, {"body": "does anyone know if this is possible? it's just an i2c device right? i mean you would have to cut the cable and make it so you could plug into the pins on the arduino but you should just be able to use the wire library and say something like. \n\n\n\nthe nxt hardware developers kit tells you what pins are which http://mindstorms.lego.com/en-us/support/files/default.aspx \n\nthanks\n\nedit. turns out this is very possible. the main problem was that hitechnic says the address is 0x10 and it is actually 0x08 but here is a short sketch that reads and prints some into about the device, i.e. the manufacturer and version. \n\n\n", "tags": "arduino", "id": "885", "title": "would is be possible to connect a hitechnic prototype board to an arduino?"}, {"body": "i'm a programmer by trade, and an amateur aerospace nut, with some degree-level training in both fields. i'm working on a uav project, and while the good people over at diy drones have been very helpful, this question is a little less drone-related and a little more general robotics/electronics. essentially, i'm looking at options for ground stations, and my current rough plan is something like this:\n\ni have a pc joystick with a broken sensor in the base, which i plan to dismantle, separate the handle from the base, insert an arduino nano into the (mostly hollow) handle and hook it up to all the buttons and the hat thumbstick. then, where the hole is that used to accept the stem to the base, i fit a bracket that runs horizontally to hold a smallish touchscreen (think razer's project fiona tablet with only one stick), behind which is mounted a raspberry pi. the nano talks to the rpi over usb as a hid input. the rpi will be running some custom software to display telemetry and other data sent down from the uav.\n\nmy main question whether that nano would have enough power to run the xbee that provides the telemetry link without causing lag in the control inputs. it's worth mentioning that the uav will be doing fly-by-wire moderation, so slight stutters won't result in wobbly flying, but serious interruptions will still be problematic - and annoying. it's also worth mentioning that this will only be used as a simplified \"guiding hand\" control; there will always be a regular remote control available (not least because of eu flight regulations) so this is just for when i don't want to use that. if that nano won't do, what are my options? my first thought is to get a second nano and get that to drive the xbee (the rpi has two usb ports after all) but there may well be a better way.\n", "tags": "arduino control uav raspberry-pi radio-control", "id": "891", "title": "arduino nano + raspberry pi = uav ground station?"}, {"body": "is it possible to achieve arbitrary precision to the calibration of the extrinsic parameters of a camera or is there a minimum error wich can not be compensated (probably dictated by the camera's resolution)?\n", "tags": "computer-vision calibration", "id": "892", "title": "is it possible to achieve arbitrary precision in camera calibration?"}, {"body": "i am in the process of building a stereo vision system to be used on a ugv. the system is for a robot that will be used in a competition wherein the robot is teleoperated to find relatively small colored rocks in a large outdoor field. i understand how to calibrate such a system and process the data for a stereo vision system. i do not however know how to select cameras for such a system. what are the best practices for picking cameras for a stereo vision system?\n", "tags": "computer-vision stereo-vision cameras", "id": "896", "title": "how to select cameras for a stereo vision system?"}, {"body": "i'm trying to control a higher voltage motor than an arduino can source with a pin, with an arduino. i am trying to hook it up to a transistor. the battery pack is not supposed to be 4.8v, it's 6v, 4 d batteries.\n\nhere is the setup:\n\n\n\nhere is the arduino code i'm trying to run to it:\n\n\n\ncode gives me no errors, but no motor movement happens. what would make this work? thanks.\n", "tags": "arduino motor", "id": "897", "title": "high voltage motor control with arduino"}, {"body": "given workspace constraints, load and task to be done, how do i select the best configuration of my robot? how do i select between a cartesian or scara robot for instance? how do i select a manipulator? how do i determine how many axes that i need?\n\nmost of what i have seen is based on experience, rules of thumb and readily available standard devices, but i would like a more formal answer to quantify my choice. is there some technique (genetic algorithm?) which describes the task, load, workspace, budget, speed etc. and rates and selects an optimal robot configuration or maybe even multiple configurations? how can i be mathematically ensure i ultimately chose the optimal solution?\n\nthe only thing i found online was a thesis from 1999 titled automated synthesis and optimization of robot configurations: an evolutionary approach (pdf, cmu-ri-tr-99-43). it is a synthesis and optimization tool called darwin2k presented in a thesis written by chris leger at cmu. i am surprised no one has updated it or created a tool similar to it.\n\nto provide some context for my question, we are developing a robot to assist the elderly with domestic tasks. in this instance, the robot identifies and picks food items from a previously stored and known location. the hand opens the package and place it in the oven. the pick and place locations are fixed and nearby so the robot is stationary.\n", "tags": "design algorithm industrial-robot theory manipulator", "id": "900", "title": "how do i select the best configuration for a known workspace, load and task?"}, {"body": "a 2d laser scanner is mounted on a rotary axis. i wish to determine the transformation matrix from the center of the axis to the center of the scanner, using only the input from the scanner and the angle of rotation.\n\n\n\nthe 2d scanner itself is assumed to be calibrated, it will accurately measure the position of any object inside the plane of the laser, in regards to the scanner origin.\n\nthe rotary axis is calibrated as well, it will accurately measure the angle of its own movement.\n\nthe scanner is aligned and mounted close to the center of rotation, but the exact offset is unknown, and may drift over time.\n\nassume it is impractical to measure the position and orientation of the scanner directly. \ni am looking for a way to determine the exact values for the 6 degrees of offset the scanner may have in relation to the axis, determined solely on the 2d information from the scanner and the rotation angle from the axis.\n\n\n\ni am mainly interested in the 4 offsets depicted here, since the other two do not matter in regard to generating a consistent 3d point cloud from the input data.\n\nby scanning a known calibration object, it should be possible to determine these offsets. what are the mathematical formulas for this? \n\nwhat sort of calibration information is required at a minimum?\nis it for example possible to determine all parameters simply by scanning a flat surface, knowing nothing about the surface except that it is flat?\n\n(the transformation matrix from rotation axis to world is unknown as well, but that one is trivial to determine once the transformation from axis to camera is known.)\n\n\n\nexample\n\n\n\non the left the camera is placed exactly on the rotational axis.  the camera scans a planar object with reference points a b and c. based on the laser distance measurements and the angle of the axis, this planar object can be reconstructed.\n\non the right, the camera has an unknown offset to the axis. it scans the same object. if the point cloud is constructed without knowing this offset, the planar surface maps to a curved surface. \n\ncan i calculate the offset based on the surface curvature?\n\nif i know the real-world distances and angles between a, b and c, how can i calculate the camera offsets from that? what would be the minimum number of reference points i need for all 4 offsets?\n", "tags": "calibration", "id": "907", "title": "calibrate a 2d scanner mounted on a rotary axis"}, {"body": "i've been looking into a makeblock robotics kit but have found no information on the web that comes from end-users, and one of the main advertised features is not clear to me:  the slot threads shown below are straight, while the screw thread that will mate with them is angled.  is there just very little contact between screw thread and rail thread vs. regular screw hole threads?  or would the screw want to rest angled somewhat- and then the head would not be flush with the rim of the rail?  or would the screw deform the aluminum rail if over-torqued?\n\nthis is a close up picture of the slot with screws:\n\n", "tags": "mechanism kit", "id": "908", "title": "how does the makeblock threaded slot work?"}, {"body": "i'm looking for a gps tracking device without screen or apps. i just need it to look for the current position of a bus and send it to a server through tcp/ip protocol. this process must be constant so i can have a real-time tracking. the bus already has a wireless access point. \n\nwhat device can be useful? do i need another piece of hardware to send the coordinates to the server? i have no experience but... can something like an arduino connected to the gps send the data?\n", "tags": "gps", "id": "909", "title": "gps tracking device"}, {"body": "i am designing a new platform for outdoor robotics and i need to calculate the power and/or torque that is needed to move the platform. i have calculated that i need about 720 w of total power to move it (360w per motor), but i don't know how to calculate the torque that i need. \n\nis it really just about having the required power and ignoring the torque or is there a way to calculate it easily?\n\nalready known parameters of the platform are:\n\n\nweight of the whole platform: 75 kg.\nnumber of wheels: 4.\nnumber of powered wheels: 4.\ndiameter of wheels: 30 cm.\nnumber of motors: 2.\nwanted speed: 180 rpm (3 m/s).\nwanted acceleration: > 0.2 m/s^2\n\n", "tags": "mobile-robot design motor", "id": "913", "title": "are power and torque required related in some way?"}, {"body": "does anybody know if kinect data can be stored directly onto a usb drive?? \ni have a kinect for windows that i cannot use on linux(ros). however what i plan is to mount the kinect on my robot, store the captured frames on a usb and then un mount the usb ,transfer to linux and process them on ros.\n\nis this possible?? any suggestions.  \n", "tags": "kinect ros", "id": "918", "title": "storing kinect data on a usb drive"}, {"body": "i'm trying to connect a camera module to my arduino mega, connect my mega to my android phone (throught bluetooth or other), and send the live view of the camera to the mobile phone.\n\ni saw a video online that showed this for still images -- an image captured by the camera module on the arduino was sent to android and the output image was viewed after a couple of seconds (the time to send image by bt).\n\nis this doable with live video instead of image?  if yes, please guide me; if no, please suggest some workarounds.\n", "tags": "arduino cameras", "id": "921", "title": "how can i send video from my arduino camera module video to my android screen?"}, {"body": "i need to make an omni wheeled robot platform (4 wheels), which should go at a minimum speed of 15 cm/s.  i have an idea for the design, but since this is my first time doing something like this i have made a lot of assumptions. \n\ni decided to choose the tgy-s4505b servos as my motor system. i intend to attach these servos to fxa308b wheels. finally, i intend to power my servos with one turnigy lsd 6.0v 2300mah ni-mh flat receiver packs (not sure if lipo is a better choice). i need to be able to run the servos continuously for roughly 8 minutes. you can ignore the microcontroller and other stuff, relatively speaking they will consume much less power. the robot will have four wheels (thus, four servos).\n\nthe basic specifications of each servo is:\n\n\ntype: analog\ngear train: plastic\nbearings: dual\nmotor type: carbon brushed\nweight: 40g (1.41oz)\nlead: 30cm\ntorque: 3.9kg.cm @ 4.8v / 4.8kg.cm @ 6v\nspeed: 0.13sec 60\u00b0@ 4.8v / 0.10 60\u00b0 @ 6v\n\n\nso based on my battery pack, i will be running the servos at 6v. that gives me a speed of 60 degrees per 0.10 seconds. i plan on modifying these servos for continuous rotation, and connected them directly to the wheel. since the wheel has a diameter of ~5 cm, it has a circumference of ~15 cm. based on these specs, it seems to me that my robot can move at roughly 15 cm/0.6 seconds, or 25 cm/s (quite fast actually). i don't intend to run it constantly at that speed, so in the 8 minute run, assume my average speed to be 20 cm/s.\n\nare these assumptions reasonable, and are the calculations correct?  i would really appreciate any insight, advice, recommendations, and criticisms you may have.\n", "tags": "mobile-robot wheel rcservo", "id": "922", "title": "how do i design for a target speed?"}, {"body": "do any of the ti arm socs, e.g. omap or da vinci, have a version with stacked ram? (e.g. ddr2 or mddr) for miniature robots like micro drones, it would be really nice to not need to spend board area on an external ram chip. thanks!\n", "tags": "arm", "id": "924", "title": "ti arm with stacked ram"}, {"body": "for my robot, i am using two continuous rotation servos to spin a threaded rod. i am trying to make this project as cheap as possible. here are the servos that i can find:\n\n\nservo #1: this is a very cheap option and it has half of the torque i need.\nservo #2: this has all of the torque my project requires, but it is much more expensive that two of servo #1.\n\n\ncan i hook up two of servo #1 to each end of the rod and have them move synchronized? i can spare a few extra pins on my microprocessor that i am using; that isn't a issue. i know hooking two together will increase torque, but i don't want 75% of the torque i want in this situation. also, i don't care if i only have 98% of my torque \"goal\" with the extra weight (which probably won't happen) but i don't want to, like i said earlier, have 70, 80, 90% of my \"target goal\" of torque if possible.\n\nany help appreciated. thanks in advance.\n", "tags": "motor rcservo", "id": "939", "title": "will connecting two servo motors double the torque?"}, {"body": "i'm doing robotics research as an undergraduate, and i understand the conceptual math for the most part; however, when it comes to actually implementing code to calculate the forward kinematics for my robot, i am stuck. i'm just not getting the way the book or websites i've found explain it.\n\ni would like to calculate the x-y-z angles given the link parameters (denavit-hartenberg parameters), such as the following:\n\n$$\\begin{array}{ccc}\n\\bf{i} &amp; \\bf{\\alpha_i-1} &amp; \\bf{a_i-1} &amp; \\bf{d_i} &amp; \\bf{\\theta_i}\\\\\n\\\\ \n1 &amp; 0 &amp; 0 &amp; 0 &amp; \\theta_1\\\\\n2 &amp; -90^{\\circ} &amp; 0 &amp; 0 &amp; \\theta_2\\\\\n3 &amp; 0 &amp; a_2 &amp; d_3 &amp; \\theta_3\\\\\n4 &amp; -90^{\\circ} &amp; a_3 &amp; d_4 &amp; \\theta_4\\\\\n5 &amp; 90^{\\circ} &amp; 0 &amp; 0 &amp; \\theta_5\\\\\n6 &amp; -90^{\\circ} &amp; 0 &amp; 0 &amp; \\theta_6\\\\\n\\end{array}$$\n\ni don't understand how to turn this table of values into the proper transformation matrices needed to get $^0t_n$, the cartesian position and rotation of the last link. from there, i'm hoping i can figure out the x-y-z angle(s) from reading my book, but any help would be appreciated.\n", "tags": "kinematics forward-kinematics", "id": "940", "title": "how do i convert link parameters and angles (in kinematics) into transformation matrices in programming logic?"}, {"body": "i'm new to robot making and just got my first arduino to play around.\n\ni want to make a robot that will wander on a table, and it will last longer i think if i could make it avoid falling from the table.\n\nwhat will be the best way to make it detect the edge of a table so i can make it stop and turn around ? it have to be something reliable and preferably cheap.\n\nit will also be better if i don't need to add extra stuff to the table so i can use it on any surface (my first idea was to draw path lines on the table and make a line follower robot, but i don't like this idea very much).\n", "tags": "sensors", "id": "946", "title": "how can i detect the edge of a table?"}, {"body": "a robotic joint is connected to two actuators, e.g. air muscles. one flexes the joint, while the other extends it. this arrangement is called 'antagonistic'.\n\n\n\nbut what if i had an electric motor instead of the air muscles? in that case it can only pull on one tendon at a time, and it's not antagonistic. what it the arrangement called in this case? untagonistic?\n", "tags": "motor air-muscle", "id": "948", "title": "what is the opposite of 'antagonistic'?"}, {"body": "i'm trying to create a map of the obstacles in a fairly coarse 2d grid space, using exploration.  i detect obstacles by attempting to move from one space to an adjacent space, and if that fails then there's an obstacle in the destination space (there is no concept of a rangefinding sensor in this problem).\n\n (for example)\n\nthe process is complete when all the reachable squares have been visited.  in other words, some spaces might be completely unreachable even if they don't have obstacles because they're surrounded.  this is expected.\n\nin the simplest case, i could use a dfs algorithm, but i'm worried that this will take an excessively long time to complete &mdash; the robot will spend more time backtracking than exploring new territory.  i expect this to be especially problematic when attempting to reach the unreachable squares, because the robot will exhaust every option.\n\nin the more sophisticated method, the proper thing to do seems to be boustrophedon cell decomposition.\n\n\nhowever, i can't seem to find a good description of the boustrophedon cell decomposition algorithm (that is, a complete description in simple terms).  there are resources like this one, or this more general one on vertical cell decomposition but they don't offer much insight into the high-level algorithms nor the low-level data structures involved.\n\nhow can i visit (map) this grid efficiently?  if it exists, i would like an algorithm that performs better than $o(n^2)$ with respect to the total number of grid squares (i.e. better than $o(n^4)$ for an $n*n$ grid).\n", "tags": "algorithm coverage planning", "id": "952", "title": "what's an efficient way to visit every reachable space on a grid with unknown obstacles?"}, {"body": "i am having some issues with the ardrone parrot 2.0 and hope someone else may be running into the same thing.\n\nwhile hovering, the drone is (seemingly) randomly losing altitude then recovering . it is doing so while not being commanded any velocity inputs and should hold altitude. \n\nwe are using the drivers from the ardrone_autonomy (dev_unstable branch) on github. we are able to watch the pwm outputs being sent to the motor and they are dropping from the hover command do a small value before exponentially returning to the hover value when this drop occurs.\n\nthe issue could be a communication between the imu and the onboard controller or on our software control implementation.\n\nhas anyone seen a similar problem or suggestions to test/troubleshoot what is happening?\n", "tags": "ros quadcopter pwm", "id": "953", "title": "dropping pwm on ardrone parrot 2.0"}, {"body": "i'm using teensy hardware specifically.  i have a teensy 2.0 and a teensy 3.0, and from the documentation it seems like there are two 16 bit timers available, and each should be able to control 12 servos.  however, i've attached a logic analyzer and have confirmed that only the first 12 servos attached ever function.\n\nis there anything special i have to do with my sketch in order to convince the servo library to allocate the second timer for servos attached beyond number 12?\n\nthis works:\n\n\n\nbut this, below, only ever shows activity on the first twelve pins attached:\n\n\n", "tags": "arduino rcservo", "id": "954", "title": "controlling more than 12 servos with the arduino servo library"}, {"body": "i have come across a number of methods for developing wall-climbing robots.\n\n\nsuction\nchemical adhesion\ngecko like hair adhesion\nelectroadhesion\n\n\nwhich method would be the best for heavy robots (5kg+)? are there any other methods that i have missed?\n", "tags": "mobile-robot", "id": "957", "title": "adhesion for a heavy wall-climbing robot"}, {"body": "i asked this question on answers.ros.org and gazebo.ros.org but still haven't got any answer. i'm posting my question here with the hope someone can help me.\n\nin our robot, the kinect can be mounted on the side of the arm, as shown in the screenshot below. when running the simulation in fuerte, i found this weird behaviour. as you can observe on the image, the point cloud does not match the robot model (we see a partial image of the hand/arm at the bottom left of the screenshot, which should be on the robot model).\n\n\n\nas soon as i rotate the kinect against its x axis (so that the kinect is horizontal as you can see on the second screenshot), then the point cloud and robot model are aligned properly.\n\n\n\nthe kinect xacro and dae are the one from the turtlebot. i'm simply attaching them with a rotation:\n\n\n\nthe code can be seen on github.\n\nany help is greatly appreciated!\n", "tags": "kinect ros simulator", "id": "963", "title": "simulated kinect rotation around x [gazebo bug?]"}, {"body": "in the prediction step of ekf localization, linearization must be performed and (as mentioned in probabilistic robotics [thrun,burgard,fox] page 206) the jacobian matrix when using velocity motion model, defined as\n\n$\\begin{bmatrix} x \\\\ y \\\\ \\theta \\end{bmatrix}' = \\begin{bmatrix} x \\\\ y \\\\ \\theta \\end{bmatrix} + \\begin{bmatrix} \\frac{\\hat{v}_t}{\\hat{\\omega}_t}(-\\text{sin}\\theta + \\text{sin}(\\theta + \\hat{\\omega}_t{\\delta}t)) \\\\ \\frac{\\hat{v}_t}{\\hat{\\omega}_t}(\\text{cos}\\theta - \\text{cos}(\\theta + \\hat{\\omega}_t{\\delta}t)) \\\\ \\hat{\\omega}_t{\\delta}t \\end{bmatrix}$\n\nis calculated as \n\n$g_{t}= \\begin{bmatrix}\n  1 &amp; 0 &amp; \\frac{\u03c5_{t}}{\u03c9_{t}}(-cos {\u03bc_{t-1,\u03b8}} + cos(\u03bc_{t-1,\u03b8}+\u03c9_{t}\u0394{t})) \\\\\n  0 &amp; 1 &amp; \\frac{\u03c5_{t}}{\u03c9_{t}}(-sin {\u03bc_{t-1,\u03b8}} + sin(\u03bc_{t-1,\u03b8}+\u03c9_{t}\u0394{t})) \\\\\n  0 &amp; 0 &amp; 1\n \\end{bmatrix}$.\n\ndoes the same apply when using the odometry motion model (described in the same book, page 133), where robot motion is approximated by a rotation $\\hat{\\delta}_{rot1}$, a translation $\\hat{\\delta}$ and a second rotation $\\hat{\\delta}_{rot2}$ ? the corresponding equations are:\n\n$\\begin{bmatrix} x \\\\ y \\\\ \\theta \\end{bmatrix}' = \\begin{bmatrix} x \\\\ y \\\\ \\theta \\end{bmatrix} + \\begin{bmatrix} \\hat{\\delta}\\text{cos}(\\theta + \\hat{\\delta}_{rot1}) \\\\ \\hat{\\delta}\\text{sin}(\\theta + \\hat{\\delta}_{rot1}) \\\\ \\hat{\\delta}_{rot1} + \\hat{\\delta}_{rot2} \\end{bmatrix}$.\n\nin which case the jacobian is\n\n$g_{t}= \\begin{bmatrix}\n  1 &amp; 0 &amp; -\\hat{\\delta} sin(\u03b8 + \\hat{\\delta}_{rot1}) \\\\\n  0 &amp; 1 &amp; -\\hat{\\delta} cos(\u03b8 + \\hat{\\delta}_{rot1}) \\\\\n  0 &amp; 0 &amp; 1\n \\end{bmatrix}$.\n\nis it a good practise to use odometry motion model instead of velocity for mobile robot localization?\n", "tags": "localization kalman-filter", "id": "964", "title": "extended kalman filter using odometry motion model"}, {"body": "i'm having some technical problems... i'm trying to use firmata for arduino but over nrf24, not over serial interface. i have tested nrf24 communication and it's fine. i have also tested firmata over serial and it works. \n\nbase device is simple \"serial relay\". when it has data available on serial, read it and send it over nrf24 network. if there is data available from network, read it and send it through serial.\n\nnode device is a bit complex. it has custom standard firmata where i have just added write and read override. \n\n\n\nread override id handeled in  method in this way:\n\n\n\nfirmata  is little changed method of  where  reads data directly from , and in this method we pass data down to method from network. this was tested and it should work fine.\n\nwrite method is overloaded in a different way. in  i have added an method pointer that can be set to a custom method and used to send data using that custom method. i have then added custom method call after each of the  call:\n\n\n\ni have then set the overrided write method to a custom method that just writes byte to network instead of .\n\n\n\nall stages pass right (i guess) and then i don't get any response from node when i request pin states\n\n\n\nthere is no response. any ideas why would that happen? node receive all messages correctly and code for handling pin states exist.\n", "tags": "arduino serial c++", "id": "965", "title": "firmata over nrf24"}, {"body": "im in the process of making a robot which requires 12 3x10mm cylindric magnets for the construction. they are 30mm from the center of the robot where i plan to have the imu. \n\ni was thinking about using mpu-6050. do magnets affect the values? if yes, is there a solution for it? like maybe i could have a shield or something around the imu?\n", "tags": "sensors imu", "id": "970", "title": "do magnets affect imu values?"}, {"body": "i know that temperature influences the characteristics of semiconductors and other materials, but we know how and can take that into account. furthermore, lower temperatures makes electronics more efficient, sometimes even superconducting.\n\ni remember reading somewhere that engineers building curiosity even considered low temperature electronics for the motors driving the wheels but still decided against it in the end.\n\nwhy is it, apparently, so hard to build components with operating temperatures matching those on mars, europa, or in space?\n\nedit: none of the answers address my question thus far. i know that all parts, both electronic and mechanical, and greases and so on have relatively narrow working temperatures. my question is, why don't we build special cold metals and cold greases and cold chips that have their narrow operating temperature band at -100 c or whatever?\n\nvalid answers could be: it's too expensive, insufficient science has been done to determine materials appropriate for such cold, such cold materials cannot be manufactured in the sweltering heat of planet earth.\n", "tags": "heat-management cooling", "id": "975", "title": "why do space probes need heating?"}, {"body": "i have a small quadruped with three degree of freedom legs which i have been working on: 3dof mini quadruped.\n\nmy original code for it was a simple servo controller on the arduino, and scala code which would send servo commands over the wire.  i did all the inverse kinematics and gait logic in scala, and got it to walk: 3dof quadruped first gait.\n\nmy gait logic in scala was somewhat naive; it depended on the legs being in the right position at the beginning (one side extended fore and aft, the other side in toward each other).  the logic was simply translate all four feet backward by 1mm along y, and whenever a coxa angle became excessively rearward, stop and perform a little routine where that foot is lifted 10mm in z, then translated forward 60mm along y, and set back down.  naive, but effective.\n\nnow, i have rewritten my ik code in arduino c, and i'm trying to decide how to move forward with the gait dynamics.  i've had a hard time finding good, easy to understand resources about gaits.  i do have some knowledge about the difference between dynamically stable gaits (like creep gaits) where the body is a stable tripod at all times and dynamically unstable gaits (walking, trotting), where two legs are off the ground at a time and the body is essentially falling forward into the advancing leg.\n\ni had some thoughts about state machines and trying to calculate whether the body center falls within a triangle made by the remaining feet to decide which foot was safe to lift, but i'm not sure if these are ideas worth exploring.\n\ni know this is kind of an overly general question, but i'm interested to see how other people have attacked this problem, and about all i've been able to find are research papers.\n", "tags": "mobile-robot walk", "id": "976", "title": "what is a good approach to a quadruped gait?"}, {"body": "i'm building a quadcopter and have discovered that most esc have a built-in bec, but i was wondering if it wouldn't be better to use only one.\n\nwhat if i delivered power to my four esc with a unique bec ? would that work ?\ni think this would be easier to configure (you have to set it up only once for the four esc) and it would prevent each esc from having it's own behavior.\n\nam i doing it wrong ?\n\nhere is an image of what i'm talking about :\n\nedit : trying to find the original image and upload it.\n\ngiven the answer by ian mcmahon it appears that this schema is not the right thing to do, since i had misunderstood the role of becs.\n\nso would the right schema would look like this ?\nedit : trying to find the original image and upload it.\ni'm still not sure if i'm getting it.\n\ndo i need 4 escs with integrated becs and connect all three cables to flight controller ?\n", "tags": "electronics quadcopter bec esc", "id": "979", "title": "one bec for multiple esc (quadcopter)"}, {"body": "here is the background. i am trying to write a service for the hitechnic prototype board. \n\nusing the appendix 2 from the blue tooth developers kit from lego's site i am able to understand what is going on with this service i am trying to build however the response i get is always 221 = 0xdd = \"communication bus error\" or 32 = 0x20 = \"pending communication transaction in progress\". \n\ni figured out that the hitechnic prototype board is using i2c address 0x08 so i modified the brick code to use that address instead of the standard 0x02. it goes out and configures the device, i get a response and then it does an lswrite which seems ok then i get a get an error when it does the lsgetstatus. \n\ni know this thing works - i can bit bang it all day long with an arduino but i only did that to test it out - see this link\n\ni am not sure what else to try. here is how i am setting it up in the connect to brick handler. \n\n\n\ni have also tried setting anyport as well so that it will hit the testportfori2csensorhandler that just does what i explained before - it seems to set the mode fine and then gets an error when it tries to read the device information. \n\nhere is the data. - this first part is the set input more - both the message and response - you can see it is totally fine. \n\nsend command data. \n\n\n\nthen it does an lswrite - everything still seems fine... you can see i have modified the nxtcomm code to use 0x08 instead of 0x02 which it would normally use, then the last byte is also 0x08 which is the starting address of the manufacturer. it's asking for 16 bytes which would be the manufacturer and sensor type. like i said - i know that works i can print that info out using the arduino.  \n\n\n\nthen it tries to get the status\n\n\n\nhere is the response... \n\n\n\nit's either 32 or 221. it's making me nuts... \n\nif anyone has anything that might help me out i would so much appreciate it. at this point i am running out of ideas. i can see what is going on, i can understand the entire transaction but can't seem to figure out why it just errors out like that. \n\nalso - just for grins i tried 0x10 which is what they tell you on the hitechnic website. that gets a response of 2,14,0,0 from the nxt brick - that would indicate there is no data but as i pointed out i can get data using the arduino. how could i have two different i2c device addresses? \n", "tags": "nxt i2c", "id": "984", "title": "does anyone know what might be giving me this error coming from an i2c device"}, {"body": "i was able to find a small esc for about $12 off of ebay.  if you were designing a robot, would you see that and think?  \n\n\n  \\$12 bucks for an esc that connects to simple pulse-wave interface -\n  sign me up!\n\n\nor would you think:\n\n\n  \\$12 just to control a motor? i could throw together an h-bridge for\n  $0.50 and be done with it.\n\n\nmy robot in particular actually has two motors and therefor $24 to control the two of them.  but the interface is really easy (plus has the added advantage of being r/c vs computer controlled with a simple change of connectors.\n\nwhich way would you go?\n", "tags": "design electronics wheeled-robot", "id": "985", "title": "which is easier/cheaper: hbridge vs esc for controlling a motor?"}, {"body": "i have written code to send data from controller to pc through serialport using interrupt\nbut it echos garbage value exactly 3 times back.\n\n\n\nedit:\nfunction used to set baud rate..\n\n\n", "tags": "serial avr", "id": "987", "title": "problem in serial communication between pc and atmega 8535(avr)"}, {"body": "i am doing local localisation with sonar, particle filter (i.e all particles are initially with robot pose).\n\ni have grip map of environment. when i execute algorithm in environment (where doors are closed/open), particles are not able to followup the robot.\n\ni don't have random particles since i know the initial position of the robot exactly.\n\nadding random particle will change the pose of robot (i am find median of particles as robot pose).\n\nany idea/methods how to improve local localisation?\n\ni want to know, do i need random variable if i am doing local localisation? and how do i improve localisation if there are many changes in the map without adding random particles?\n", "tags": "mobile-robot localization odometry particle-filter", "id": "989", "title": "local localisation with particle filter"}, {"body": "i am looking to augment a gps/ins solution with a conventional land vehicle (car-like) model. that is, front-wheel steered, rear wheels passive on an axle.  i don't have access to odometry or wheel angle sensors.\n\ni am aware of the bicycle model (e.g. chapter 4 of corke), but i am not sure how to apply the heading/velocity constraint on the filter.\n\nso my questions are:\n\n\nare there any other dynamic models that are applicable to the land vehicle situation, especially if they have the potential to provide better accuracy?\nare there any standard techniques to applying such a model/constraint to this type of filter, bearing in mind i don't have access to odometry or wheel angle?\nare there any seminal papers on the topic that i should be reading?\n\n", "tags": "gps dynamics", "id": "993", "title": "conventional land vehicle dynamic models for gps/ins augmentation"}, {"body": "my team is building a robot to navigate autonomously in an outdoor environment. we recently got a new integrated imu/gps sensor which apparently does some extended kalman filtering on-chip. it gives pitch, roll, and yaw, north, east, and down velocities, and latitude and longitude.\n\nhowever, we also have some encoders attached to our wheels, which provide linear and angular velocities. before we got this new imu/gps sensor, we made our own ekf to estimate our state using the encoders and some other low-cost sensors. we want to use this new sensor's on-chip filter, but also incorporate our encoders into the mix.\n\nis there any problem with chaining the filters? what i mean is, we'd use the output of the imu/gps sensor's on-chip ekf as an update to our own ekf, just as we use the data read from the encoders as an update to our ekf. it seems reasonable to me, but i was wondering what is usually supposed to be done in this case. \n", "tags": "kalman-filter imu navigation", "id": "994", "title": "chaining kalman filters"}, {"body": "as a holiday project we are building a surveillance robot that is capable of transmitting live images using a webcam and is also capable of lifting small objects.  \n\nit uses a cc2500 module for communicating with the robot.  the interface is designed in visual basic 6 and it allows us to set the port of the computer to which the transreceiver is connected. it is connected via a usb to rs232 port (usb side is connected to the computer).\n\nwe tried the settings as shown below and we get an error that the config is unsuccessful. we have tried the same settings in 4 different computers so far and it did not work.  \n\n\n\ncircuit diagram for the robot:\n\n\n\nit is designed using an atmel 89s52.\n\nplease tell us what settings to try to make it work\n", "tags": "electronics computer-vision wheeled-robot robotic-arm", "id": "997", "title": "how do we make our robot work?"}, {"body": "we are making a junior soccer robot and we just got our brilliant motors from maxon.\nsetting the pwm timer to low-frequencies (around 39khz or 156 khz ) the robot acts as expected. but this produces some problems. \n\n\nit puts a heavy current on batteries (around 1.5a for 3 motors which is far too high).\nthe high current causes our motor drivers (l6203) to heat up very quickly and even heat-sinks won't help them.\nthe motors make such a bad sound as they are screaming and this is not normal.\n\n\nin contrast when i configure the timer on high-frequencies (such as 1250 khz or 10000 khz) the current drops off to 0.2a which is ideal and the sounds quit down.\nbut this causes a problem that our 3 motors when set to run on their highest speed (pwm set to 255) don't run by the same rpm. like one of them runs slower than others making robot turn to a specific side and so our handling functions fail to work correctly.\n\nasking someone he told me that the drivers don't respond the same to frequencies thus resulting in different speeds and because on low frequencies the difference is very small i won't notice it but on higher frequencies the difference becomes bigger and noticeable.\n\nso is there any workaround for this problem? or i should continue using low frequencies? \n\nps: i'm using atmega16 as the main controller with a 10 mhz external crystal. \n", "tags": "pwm avr", "id": "998", "title": "motors response different with high-frequency pwm"}, {"body": "i am following a guide that recommends using stepper motors and it has an approximate holding and operating torque. it says that if you don't know the operating torque, it is often half of the holding torque. i am adapting this to use with a servo and i was wondering can this same formula be used with a servo. my servo has approximately  of torque so does that mean that i can estimate the operating torque would be ?\n\na couple of things:\n\n\ni know operating torque and holding torque are different. this is just a estimate-it isn't an exact science.\ni know servos are harder to find their location (75 degrees, etc.) than to use a stepper and assume that it worked. i have external means of finding the location.\n\n", "tags": "rcservo stepper-motor", "id": "999", "title": "for servos, can it be implied that `holding torque = operating torque * 2` like with steppers?"}, {"body": "i am trying to control the force of a solenoid. my current system has a bank of capacitors connected to a relay. in order to control the force (how hard i am trying to hit the object) i am increasing or decreasing the the time the relay is on. the problem is this works but it either hits with too much force or way too much force. i can turn the relay on for 5 ms or more. if i try to turn it on for 1 ms it does not even respond. (i am using a mechanical relay.)\n\ni would like to have more control on how much of the energy i discharge so i can control how hard/soft the solenoid moves (say discharge only 10 percent of the total energy stored so it hits slower). while searching i found out about solid state relays which according to wikipedia can be switched on an off way faster that mechanical relay (of the order of microseconds to milliseconds).\n\nso my question is am i on the right track? or is there something better to achieve what i am trying to achieve?\n", "tags": "control", "id": "1005", "title": "controlling the power of a solenoid"}, {"body": "in probablistic robotics by s. thrun, in the first section on the extended kalman filter, it talks about linearizing the process and observation models using first order taylor expansion.  \n\nequation 3.51 states:\n\n$g(u_t,x_{t-1}) \\approx g(u_t,\\mu_{t-1}) + g\\prime(u_t, \\mu_{t-1})(x_{t-1} - \\mu_{t-1})$\n\ni think $\\mu_{t-1}$ is the state estimate from the last time step.  my question is: what is $x_{t-1}$?  \n\nalso, the ekf algorithm following this (on table 3.3) does not use the factor $(x_{t-1} - \\mu_{t-1})$ anywhere, only $g\\prime(u_t, \\mu_{t-1})$.  so after being confused about $x_{t-1}$, i'm left wondering where it went in the algorithm.\n", "tags": "kalman-filter theory ekf", "id": "1006", "title": "taylor series expansion for ekf"}, {"body": "i'm looking to use 4 of these 3.2v lifepo4 batteries. i intend to have 2 pairs of 2 in series in parallel. so two 6.4v battery packs in parallel. because my setup will be run off of this, it will also be easiest to recharge the batteries using the same setup. to accomplish this, i'm looking to charge all the batteries at once using this 6.4v lifepo4 smart charger. from a simplistic standpoint, the resulting voltage should be correct and this should work fine. however, i know (from a previous question) that lifepo4 battery chargers are a bit more complex then a basic voltage supply and check. would the setup i've described work correctly? and in general, will a lifepo4 smart charger be able to charge several batteries of the correct voltage at the same time so long as it doesn't try to charge them at too high an amperage? or does a lifepo4 battery also have a minimum amperage cutoff point to charge such that trying to charge more than one battery at a time will cause problems? any other issues i didn't mention? thank you much!\n", "tags": "battery", "id": "1011", "title": "charging multiple lifepo4 batteries at the same time?"}, {"body": "i saw this art drawing robot on youtube:\n\nhttp://www.youtube.com/watch?v=wo15zxhfdzo\n\nwhat do i need to learn in order to build something like that? what are some beginner oriented projects that could lead up to building something like this?\n\ni'm an experienced programmer but i have very little hardware experience.\n", "tags": "robotic-arm", "id": "1014", "title": "how would i go about making an art drawing robot like this?"}, {"body": "i am using the pololu micro serial servo controller connected to an arduino and multiple other servos (4 total) to make a robot arm.  two of the four servos require 4-6 volts, while the other 2 require 7-10 volts, so i am planning on powering all the servos separate from the pololu.\n\ni have the arduino and pololu connecting to each other correctly (flashing green led), but the servo(s) don't move when plugged in to the control pins.  all the servos work correctly when plugged into a servo-tester.\n\ni think that this problem could be fixed by connecting the grounds of the servos to the ground of the pololu, but would like advice because i am not sure if it will work, or will end up frying one of the parts (we already fried a pololu).\n\nwould connecting the grounds of the batteries to the ground of the pololu help, or damage the parts?\n\n, but i couldn't figure out how to show the micro serial servo controller.\n", "tags": "rcservo", "id": "1018", "title": "connecting multiple different voltage servos to the same controller"}, {"body": "i would like to build a robot which follows a virtual path (not a visible path like a 'black line on a white surface', etc).\n\ni'm just enthusiastic by seeing some sci-fi videos which show robots carry goods and materials in a crowded place. and they really don't follow a physical line. they sense obstacles, depth, etc.  \n\ni would like to build one such robot which follows a specific (virtual)  path from point a to b.\n\ni have tried a couple of things:\n\n\nusing a magnetic \"hall effect\" sensor on the robot and wire carrying current (beneath the table). the problem here was that the vicinity of the hall effect sensor is so small (&lt; 2cms) that it is very difficult to judge whether robot is on the line or off the line. even using series of magnets couldn't solve this issue, as my table is 1 inch thick. so this idea flopped :p\nusing an ultraviolet paint (on a line) and using uv leds on the robot as sensors. this will give more zig-zag motion for the robot. and due to potential threats of using uv light source, even this idea flopped :p\n\n\ni finally thought of having a camera on top and using image processing algorithms to see whether robot is on the line or diverging.\n\nis there any better solution than this? really looking for some creative and simple solutions. :)\n", "tags": "mobile-robot localization wheeled-robot industrial-robot line-following", "id": "1020", "title": "how to make an \"invisible line following robot\"?"}, {"body": "does anyone know if small mechanical actuators exist which can be controlled electrically, sort of like a miniature joystick, but in reverse.  instead of it picking up mechanical movement and outputting electrical signals, i want it to generate mechanical movement controlled via my electrical input signals.  i\u2019ve searched for : electromechanical actuators, not finding what i need. think of a pencil attached to a surface which can pivot to point anywhere in its half dome.  i\u2019m thinking small, on the order of an inch.  it will not be load bearing. \n\nmy goal is to programmatically control the normal pointed to by a small flat surface attached to the end of each joystick rod.  accuracy is more important than speed.  from across a small room, say 10' by 10', i'd like the surface normal to accurately point to arbitrary objects in the room, say a person walking across the room.  if i can cheaply buy/build such mechanisms to control the movement of these small flat surfaces, i would like dozens places across the walls of the room.\n\nits for an electromechanical sound project i\u2019m planning. \n", "tags": "control actuator", "id": "1021", "title": "looking for a miniature joystick, but in reverse"}, {"body": "i'm building a quadruped and i'm not sure of the features i should be looking for in a servo motor. e.g. digital vs analog, signal vs dual bearings. some of the ones i'm considering are here\n", "tags": "rcservo walking-robot", "id": "1023", "title": "servo motor considerations for a quadruped"}, {"body": "i am working on this project that involves using the kinect for xbox 360s with ros.\n\ni did all the steps mentioned in the ros tutorials to have openni installed and the prime sense and other drivers. and when i go to openni samples i see a output. but in ros i do a roscore and in another terminal do a roslaunch openni_launch openni.launch. and it loads with the regular calibration warnings and service already registered errors. then in another terminal i open rviz which gives a error /.rviz/display_config does not exist. and even though i accept the error and go ahead i see a black window which shows no output ,even if i do all tasks mentioned at the rviz tutorials. also i tried running \"rosrun image_view image_view image:=/camera/rgb/image_color\" and it shows up a blank window with no output. how do i resolve this and get ros to show my kinect data??\n\ni need to run rgbdslam and use this kinect later.\n\ni am on ubuntu 12.04 and ros-fuerte.\n\nwell when i launch the openni.launch it starts as usual except for the errors \u00a8tried to advertise a service that is already advertised in this node.\n\nand when i run a rostopic it just says subscribed to the /camera/depth_registered/points and cursor keeps blinking.\neven subscribing to the rectified topics just says subscribed and nothing more happens.\n", "tags": "kinect ros", "id": "1025", "title": "cant see kinect data in ros"}, {"body": "i'm considering experimenting with piv control instead of pid control. contrary to pid, piv control has very little explanation on the internet and literature. there is almost a single source of information explaining the method, which is a technical paper by parker motion. \n\nwhat i understand from the control method diagram (which is in laplace domain) is that the control output boils down to the sum of:\n\n\nkpp*(integral of position error)\n-kiv*(integral of measured velocity)\n-kpv*(measured velocity)\n\n\nam i correct? thank you. \n", "tags": "control servos pid", "id": "1032", "title": "how is piv control performed?"}, {"body": "i'm trying to learn about servo control. i have seen that the most generic position control method for servos is pid, where the control input is position error. however, i am not sure about what is the actuated quantity. i am guessing that it is one of:\n\n\nvoltage applied to the motor\ncurrent applied to the motor\n\n\ni am then guessing that the actuated quantity gets turned into one of:\n\n\ntorque that the motor exerts\nangular velocity that the motor runs at\n\n\ni haven't been able to get my hands on and explicitly control a physical servo so i cannot confirm that the actuated quantity is any of these. i know very little of the electronics that controls the motor. it might well be that the controlled quantities are different for different series servos. \n\nmy bet is on torque control. however, assume that the servo is holding a weight at a distance (so it is acting against gravity), which means an approximately constant torque load. in this case, if the position error is zero and the servo is at rest, then each of p, i and d components are zero, which means the exerted torque is zero. this would cause the weight to sink, which is countered by the error in its position causing p,i components to increase. wouldn't this situation cause the lifted weight to oscillate and balance at a constant position which is significantly different from the goal position? this isn't the case with the videos of servos i have seen lifting weights. or is this the case and friction is smoothing everything out? please help me understand. \n", "tags": "servos pid", "id": "1033", "title": "what is the actual physical actuated quantity when controlling the position of a servo?"}, {"body": "is there web mapping tool that allows developers to use it to plot gps data of autonomous vehicles/robots?\n\n forbids it. see 10.2.c.  terms of use link jumps to the same page.  maps looks the similar (see 3.2.(g)).\n\nwhat i want is a internet-based tool that shows either/both satellite images and/or map, which can overlay plot using its api. i'm making a generic gps plotter on  that could be used both for slow robots or fast vehicles/cars.\n\nthanks!\n", "tags": "gps visualization", "id": "1036", "title": "web mapping that can be used for autonomous vehicles/robots"}, {"body": "i've already built a two wheeled balancing robot using some continuous rotation servos and an accelerometer/gyroscope.  i upgraded the servos to some geared dc motors with 8-bit encoders with the goal having the robot drive around while balancing.   \n\ni'm kind of stuck on how to program it to drive around while still balancing.  i think one way would be to just have the control input to the motors act sort of like pushing it.  so the robot would be momentarily unbalanced in the direction i want it to travel.  that seems kind of clumsy to me though.  there must be a better way of doing?  i think i need to combine the dynamic model for the balancer with the differential drive but this is a bit beyond the control theory that i know.  \n\nupdate \nfrom anorton's answer i have a good looking state matrix now. \n\nnow about pole placement:  the a matrix will will have to be 4x4 based on the new state vector.  and b will then have to be a 4x2 matrix since i can only control the left/right wheel torque (u = 2x1 vector).  \n\ni may need to read more about this but is there a systematic way to determine the a matrix by pole placement?  it seems to me for this example and even more complicated examples, determining a by guess and check would be very difficult.  \n\nupdate #2\nafter a bit of reading i think i understand it now.  i still need the dynamics of the robot to determine the a matrix.  once i have that i can do the pole placement using matlab or octave.   \n", "tags": "mobile-robot control dynamics", "id": "1037", "title": "building a balancing robot with differential drive"}, {"body": "i am writing a method (java) that will reset the position of e-puck in webots. i have been following tutorial on supervisor approach. i have two controllers in my project:\n\n\nsupervisorcontroller extends supervisor - responsible for genetic algorithm and resetting e-puck's position\nepuckcontroller extends robot - drives the robot\n\n\nrobots are communicating via emitter and receiver, and everything works fine but the position reset.\nthis is what i'm doing in supervisorcontroller:\n\n\n\nand as a result i get this exception:\n\n\n\nepuck variable is null. i tried calling different methods on epuck, and they all resulted in nullpointerexception. the name of e-puck matches the world file. \n\n\n\ni would appreciate any advice on how to get a handle to the robot or where to look for issues in simulation/code.\n", "tags": "mobile-robot simulator", "id": "1039", "title": "resetting position of e-puck in webots using supervisor node - problem with getting a handle to the robot"}, {"body": "i want to make a list of what knowledge is necessary for sensor fusion. since it has a wide array of possible applications, it is not clear where to begin studying. can we please verify add topics that are in-scope, and specify to what extent?:\n\n\ndigital signal processing - course: https://www.coursera.org/course/dsp\nprobability - course http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-spring-2006/lecture-notes/\nmachine learning - course at coursera from stanford university\nprogramming robotic car - course at udacity\nknowledge of matlab and simulink - tutorials on mathworks webpage and offline help.\nbasic knowledge about integrals, matrices operations, differential equations.\n\n", "tags": "sensors sensor-fusion", "id": "1046", "title": "overview - what skills are needed for sensor fusion?"}, {"body": "currently i am reading a book of mr. thrun: probabilistic robotics. i find it really helpfull to understand concept of filters, however i would like to see some code in eg. matlab. is the book \"kalman filter for beginners: with matlab examples\" worth buying, or would you suggest some other source to learn the code snippets from?\n", "tags": "kalman-filter books", "id": "1047", "title": "source to learn kalman fusion, explanatory code snippets"}, {"body": "i want to built a robot and i need bunch of modules to track it like gsm/gps wifi and camera\nif i try to buy each of module separately it will cost me 300 dollar each aprox in pakistan. on the other hand an android enable phone can be purchased on just 250$ having all of them. i was wondring if it is possible to interface android phones like (huawaii or google nexus) with 8-bit microcontrollers or arduino? the only port available with android phones are usb and arduino supports usb. it is possible to some how attach both of them?\n", "tags": "arduino usb", "id": "1049", "title": "is it possible to interface android mobile as gsm and gps module with arduino based robotic applications?"}, {"body": "for example, if a rover has working temperature range of -70 to +120 celsius, how does it survive and then restore itself if the temperature drops to -150 degrees for several months?\n", "tags": "electronics ugv reliability", "id": "1050", "title": "how do space rovers survive at very low temperatures?"}, {"body": "within robotics programming, orientation is primarily given in terms of x, y, &amp; z coordinates, from some central location.  however x, y, z coordinates aren't convenient for rapid human understanding if there are many locations from which to select (e.g., {23, 34, 45}, {34, 23, 45}, {34, 32, 45}, {23, 43, 45} is not particularly human friendly, and is highly prone to human error).  yet more common english orientation descriptors are frequently either too wordy or too imprecise for rapid selection (e.g.,  \"front-facing camera on robot 1's right front shoulder\" is too wordy; but \"front\"/ \"forward\" is too imprecise - is the camera on the leading edge or is it pointing forward?)\n\nin the naval and aeronautical fields vehicle locations are generically talked about as fore, aft (or stern), port, and starboard. while, direction of movement that is relative to the vehicle is frequently given in reference to a clockface (e.g., forward of the the fore would be \"at 12\", rear of the aft would be \"at 6\", while right of starboard and left of port would be \"at 3\" and \"at 9\", respectively).  this language supports rapid human communication that is more precise than terms such as \"front\" and \"forward\".  are there equivalent terms within mobile robotics?\n", "tags": "mobile-robot control design localization navigation", "id": "1051", "title": "what are human-friendly terms for mobile-robot orientation and relative direction of non-robot objects?"}, {"body": "i need help with figuring out the following things:\n\ni'm developing a hexapod r-hex type model with a tripod gait. however, the angles obtained during the robot's walking in real life are not perfectly aligned. because of this the robot often collapses and falls even on perfectly straight terrain.\n\nmy configuration is: \n\n\n60 rpm dc motors for each leg\nh-bridge motor drivers for each dc motor\natmega 8\n\n\nshould i change the gait type?\n\nor is tripod sufficiently stable?\n\nare dc motors providing fine enough control or do i need servos?\n\ndo i need a dc motor with an encoder? \nwhat will be its benefits?\n\nwhat could be done to improve performance of the robot?\n\nadded: would a stepper motor work as well, instead of a servo?\n", "tags": "motor wheeled-robot gait", "id": "1056", "title": "perfecting tripod gait - building a r-hex robot"}, {"body": "i want to know if there is best algorithm and technique to implement self learning maze solving robot in 8 bit limited resource micro-controller? i was looking for some well optimized algorithm and/or technique. maze can be of any type. of-course first time it has to walk all the way and keep tracking obstacles it found. \n\ni think best technique would be neural networks but is it possible to do this in such a short resources of 8bit? any example online with similar kind of problem? \n\nmy wall detection is based on units, well, i have counted the wheel turns and it is almost 95% accurate in integers. for sensing the walls ultrasonic range finding is used. wheel can remeber its current position in let say, 3 feet staight, 2 feet right, etc.\n", "tags": "algorithm machine-learning mapping micromouse", "id": "1060", "title": "self learning maze solving robot using 8bit microcontroller?"}, {"body": "i want to prototype a therapeutic device that will have a lot of tiny mobile-phone type vibration motors like this one in it, and i want to be able to activate them in any configuration i want.  i'm going to need analogue control, and support for logic like perlin noise functions and so on.  i'm not really going to need sensor data or any other kind of feedback beyond a few buttons for control.  i just need fine control over lots of little motors.  depending on what results i can get out of, say, 25 motors on the initial prototype, i may decide that i'm done, or that it needs more motors.  i also don't have an enormous budget.\n\nso the question is, is arduino a good fit for a project like this?  is it feasible to get that many motors working off the same controller?  i know some of the arduino boards have up to 50-something serial outputs, but from what i can tell, that may only translate to 25 or so motors, so i'd need a way to extend the board with more serial outputs if i wanted to try more.\n\nadditionally, if arduino isn't a good fit, what would be better?  could i try something directly out of a serial port on my pc?  i've never tried to home-cook a robotics application before, so i'm not really aware of the options.\n", "tags": "arduino motor", "id": "1064", "title": "prototyping a device with 25-100 small dc 3.0v motors, is arduino a good fit?"}, {"body": "i currently have a working kinematic chain made by a set of ten links in d-h convention (with usual parameters [ $a_i, d_i, \\alpha_i, \\theta_i$]).\n\nbut my task currently requires the inversion of some of them. basically, i would have a part of the chain that is read from the end-effector to the origin, using the same links (and thus the same parameters). is it possible? how to do so?\n\n\n\nplease notice that this is not related to the inversion of the kinematic chain. it's more basic: i want to find the dh parameters of the inverted forward kinematic chain.\n\nlet's put it simple: i have dh parameters for a 2 link planar chain from joint 0 to joint 1, so i can compute its direct kinematics. but what if i want to compute the direct kinematics from joint 1 to joint 0?\n\ngiven dh parameters [ $a_i, d_i, \\alpha_i, \\theta_i$], i can retrieve the transform matrix with this formula:\n\n$g = \\left[\\begin{matrix}\ncos(\\theta) &amp; -sin(\\theta)*cos(\\alpha) &amp;  sin(\\theta)*sin(\\alpha) &amp; cos(\\theta)*a \\\\\nsin(\\theta) &amp;  cos(\\theta)*cos(\\alpha) &amp; -cos(\\theta)*sin(\\alpha) &amp; sin(\\theta)*a \\\\\n          0 &amp;              sin(\\alpha) &amp;              cos(\\alpha) &amp;             d \\\\\n          0 &amp;                        0 &amp;                        0 &amp;             1\n\\end{matrix} \\right]$\n\nthis is the transform matrix from the i-th link to the (i+1)-th link. thus, i can invert it to obtain the transform matrix from the (i+1)-th link to the i-th link, but the problem is that this is not working. i believe that the reason is related to the fact that the dh convention doesn't work any more as it is.\n\nany help?\n", "tags": "control inverse-kinematics kinematics", "id": "1066", "title": "how to invert d-h parameters"}, {"body": "i am trying to run a nxt motor using the mindsensors motor multiplexer at a slow speed.  when i turn it on, it tends to jump approx 20 to 40 degrees before moving at a slow speed.  has anyone seen this behavior?\n\ni am using nxt 1.0 with firmware down loaded from .  sample code in nxc (i am using bricx command center as my ide) is as follows:\n\n\n", "tags": "nxt mindstorms not-exactly-c", "id": "1068", "title": "mindsensor motor multiplexer jump on run_unlimited"}, {"body": "is there a way to use a single dc motor output for two different loads (by using gears, relays etc) ? please see the illustration below:\n\n\n\nto clarify the illustration, i get the dc motor power at \"output 1\" gear which is extended over an idler gear to output 2 gear. all three are in contact (though the picture doesn't quite show it). load 1 and load 2 are two separate gears connected to different loads (wheels etc) and initially not in contact with the bottom gears. on switching on the relays 1, 2 - the load bearing gears, move towards the output1 and output2 and mesh with them to drive load 1, 2.\n", "tags": "motor", "id": "1069", "title": "is there a way to use a single dc motor output for two different loads?"}, {"body": "this question was asked in electronics stackexchange. i want to know if is it possible to make a robot that can fly kites. is this idea practical? i was thinking that making a kite is just like making some flying quadcopter or helicopter. i just want to know is this idea really implementable?  is there an example or similar work in reference to this?     \n", "tags": "control quadcopter radio-control", "id": "1072", "title": "is it possible to make kite flying robot?"}, {"body": "i'm building a quadcopter and i've received my motors and propellers.\n\nwhat's the right way to assemble those together?\n\ni'm not confident with what i've done, as i'm not sure the propeller would stay in place on a clockwise rotating motor.\n\ni mean, if the motor rotates clockwise, will the screw stay tightly in place, even with the prop's inertia pushing counter-clockwise?\n\nhere's what i've done (of course i'll tighten the screw...) :\n\n\n", "tags": "brushless-motor", "id": "1074", "title": "how to assemble brushless motors and propellers?"}, {"body": "for a pet project, i am trying to fly a kite using my computer.  i need to measure how far a cord extends from a device.  i also need to somehow read out the results on my computer. so i need to connect this to my pc, preferably using something standard like usb.\n\nsince the budget is very small, it would be best if i could get it out of old home appliances or build it myself.\n\nwhat technology do i need to make this measurement?\n", "tags": "wheel usb encoding", "id": "1078", "title": "how do i measure the distance that a cord (string) has moved?"}, {"body": "i have a 4-bar linkage arm (or similar design) for a telerobot used in the vex robotics competition.  i want to be able to press buttons on my ps3-style controller to have the arm raise to certain angles.  i have a potentiometer to measure the 4-bar's angle.\n\nthe potentiometer measures the angle of one of the joints in the shoulder of the mechanism, which is similar to this:\n\n\n\nwhat type of control should i use to stabilize the arm at these angles?\n", "tags": "mobile-robot control arm", "id": "1082", "title": "stabilizing a robot arm at a specified height"}, {"body": "the textbook i'm using doesn't have the answers to the practice questions, so i'm not sure how i'm doing. are the following dh parameters correct given the frames i assigned?\n\nthe original question is as follows:\nthe arm with 3dof shown below is like the one in example 3.3, except that joint 1's axis is not parallel to the other two. instead, there is a twist of 90 degrees in magnitude between axes 1 and 2. derive link parameters and the kinematic equations for $^bt_w$ (where b means base frame and w means wrist frame).\n\nlink parameters:\n\n$$\\begin{array}{ccc}\n  &amp; const.        &amp; and      &amp;     &amp;               \\\\\n  &amp; angle         &amp; distance &amp;     &amp;               \\\\\ni &amp; \\alpha_{i-1}  &amp; a_{i-1}  &amp; d_i &amp; \\bf{\\theta_i} \\\\\n\\\\\n1 &amp; 0             &amp; 0        &amp; 0   &amp; \\theta_1      \\\\\n2 &amp; -90           &amp; l_1      &amp; 0   &amp; \\theta_2      \\\\\n3 &amp; 0             &amp; l_2      &amp; 0   &amp; \\theta_3      \\\\\n\\end{array}$$\n\nframe assignments: \n", "tags": "inverse-kinematics kinematics forward-kinematics", "id": "1083", "title": "assigning frames and deriving link parameters"}, {"body": "how do i increase the rotation range of a standard servo? most servos have a rotation range of ~ 180 degrees. i would like to access the entire 360 degree range on the servo, partially because i would be attaching the servo's shaft to the robotic wheel and would like it to be able to make full rotations. or is that not possible? \n\ni would not like to however lose the 'encoder' part of the servo which allows me to clearly specify which angular position the wheel should stop at. \n\nif i use gears in order to transform this system range, would it lead to loss of precision?\n\nin addition, would such a transform allow the wheels to continuously rotate in one direction? from what i understand, this won't work.\n\nwould a stepper motor with an external encoder or a dc motor with an external encoder work?\n", "tags": "motor rcservo", "id": "1086", "title": "increasing the rotation range of a servo motor"}, {"body": "i have two quaternions that indicate the initial orientation of a four wheel robot, each one in relative to one reference systems. \n\nthe robot's orientation given by a quaternion q is not the same in the two reference systems: for one reference system the quaternion q1 might refer to the robot looking at positive x while the same quaternion components q1 in the second reference system might refer to the robot looking at the negative x.\n\ni have two matrices which indicate the position of the robot in time in its correspondent reference system.\n\ni want to find the correspondent points of the first matrix in to the second reference system. each matrix is built with a different sensor, so the results will be similar but not exactly the same.\n\ni think i should find the transformation from the first reference system to the second and then apply it for each point of the first matrix. how can i find this transformation? the translation part i think is clear, but the rotation not at all.\n\nedit:\n\n@wildcrustacean\n\nthe solution proposed does not solve the problem, i think that the reason is because each system represents the robot in a different way. \n\nin the initial one (a) the robot moving forward with no rotation would increase in the x axis. in the goal referential system (b) the robot moving forward with no rotation would increase in the z axis. (see figure for more details) \n\n\n\ni think i have to apply an extra rotation to change the initial quaternion that belongs to the first system to be in accordance with the second system before applying the transformation of your post.\n\none rotation of 180 degrees around x followed by one of 90 around y. would rotate from a to b \n\nthis is how i tried to solve it:\n\n\n\na2b_quaternion is the quaternion that i use for the rotation but still doesn't perform the right rotation. any idea?\n", "tags": "mobile-robot localization odometry", "id": "1089", "title": "how to perform this reference system transformation?"}, {"body": "i'm building a motion detection and object recognition camera with feedback control for a hexy robot. fortunately most of the servo control is handled by the analog servo controls and the high-level logic can be implemented in python on a raspberry pi. what's the right combination of python modules to implement:\n\n\na daemon/service to trigger and execute image capture and processing\na daemon/service to regularly update the hexy with the latest motion plan and servo setpoints\nthe image processing for recognition and tracking of objects from the webcam\n\n\ni'm currently using python-daemon for the services and comparing the various pypi opencv libraries to see if any of them look promising. anyone have experience with these on a raspberry pi or arm processor in a robotics application? \n\n\nremotecv                  - remotecv is an opencv server for face recognition\nctypes-opencv             - ctypes-opencv - a python wrapper for opencv using ctypes\npyopencv                  - pyopencv - boost.python and numpy\nopencv-cython             - an alternative opencv wrapper\ncvtypes                   - python opencv wrapper using ctypes\ntippy                     - another toolbox for image processing, based on opencv\n\n\nthese each depend on a deep list of low-level libraries and/or compilers like boost->numpy->gfortran or cython->gcc or ctypes. i'm concerned about compatibility and performance of these lowlevel libraries on raspbian and an arm processor.\n\nanyone with a known working architecture for image processing and real-time control in python on an arm processor will get their answer upvoted and/or accepted.\n", "tags": "raspberry-pi real-time", "id": "1091", "title": "python libraries for image processing and feedback control on raspberry pi"}, {"body": "are there any decent python numerical package libraries besides numpy for python? numpy relies on gfortan which itself must be compiled correctly for your platform to avoid hidden/insidious numerical errors in numpy. \n\ni need a matrix algebra package to do kinematics, path planing, and machine learning in python that isn't sensitive to  gfortran version and compiler options.\n", "tags": "kinematics python", "id": "1092", "title": "numpy alternatives for linear algebra and kinematics in python?"}, {"body": "shall i filter (kalman/lowpass) after getting the raw values from a sensor or after converting the raw values to a usable data? does it matter? if so, why? \n\nexample:\nfilter after getting raw values from imu\nor \nfilter after converting raw values to a usable data eg. flight dynamics parameters.\n", "tags": "kalman-filter imu", "id": "1097", "title": "at which stage should filtering be applied to the sensors data?"}, {"body": "the definition of a robot is as follow: \"a robotic paradigm can be described by the relationship between the three primitives of robotics: sense, plan, and act.\"\n\nan example could be the famous \"kuka robots\". the kuka robot is preprogrammed and does mainly one loop over and over again. some of them could have measurement sensors but that is all. they do not think or plan nor do they make decisions. \n\nan automatic door opener, used in a building is not a robot either but according to the robotic paradigm definition they are more a robot than a kuka machine. they actually get some data from a sensor followed by planning and acting. \n\nso why are kuka machines called robots?\n", "tags": "industrial-robot", "id": "1100", "title": "why are industrial machines called robots?"}, {"body": "i am trying to implement a mechanism to make robots avoid being too close (say in a distance less than ). i am not familiar with those systems and i have to implement a strategy to avoid robots being too close to each other. could anyone recommend me some readings for such a problem or a set of keywords to search for? i don't know yet how to start.\n", "tags": "algorithm movement", "id": "1110", "title": "robots minimum distance"}, {"body": "i was looking up the motor parameters for some stepper motor where they listed the torque of the motor at different current/voltage but the torque they listed was in kg/cm.\n\nhow is kg/cm even a remotely acceptable unit for torque?\n\nhow do i calculate the torque in nm from kg/cm?\n\nclarity note: its not kgcm which represents [0.098 kilogram force = 1 nm.]\n\nwebsite where this happens.\n", "tags": "stepper-motor torque", "id": "1113", "title": "torque in kg/cm?"}, {"body": "i am currently reading into the topic of stereo vision, using the book of hartley&amp;zimmerman alongside some papers, as i am trying to develop an algorithm capable of creating elevation maps from two images.\n\ni am trying to come up with the basic steps for such an algorithm. this is what i think i have to do:\n\nif i have two images i somehow have to find the fundamental matrix, f, in order to find the actual elevation values at all points from triangulation later on. if the cameras are calibrated this is straightforward if not it is slightly more complex (plenty of methods for this can be found in h&amp;z).\n\nit is necessary to know f in order to obtain the epipolar lines. these are lines that are used in order to find image point x in the first image back in the second image.\n\nnow comes the part were it gets a bit confusing for me: now i would start taking a image point x_i in the first picture and try to find the corresponding point x_i\u2019 in the second picture, using some matching algorithm. using triangulation it is now possible to compute the real world point x and from that it\u2019s elevation. this process will be repeated for every pixel in the right image.\n\nin the perfect world (no noise etc) triangulation will be done based on\n\n\n\nin the real world it is necessary to find a best fit instead.\n\ndoing this for all pixels will lead to the complete elevation map as desired, some pixels will however be impossible to match and therefore can't be triangulated.\n\nwhat confuses me most is that i have the feeling that hartley&amp;zimmerman skip the entire discussion on how to obtain your point correspondences (matching?) and that the papers i read in addition to the book talk a lot about disparity maps which aren\u2019t mentioned in h&amp;z at all. however i think i understood correctly that the disparity is simply the difference x1_i- x2_i?\n\nis this approach correct, and if not where did i make mistakes?\n", "tags": "computer-vision stereo-vision", "id": "1117", "title": "how to obtain stereo correspondences and what exactly is a disparity map?"}, {"body": "and if so, what was the highest score so far?\n\nsome news articles suggest only parts of tests were aced.\n\n\n\nupdate since people censored this question and closed it. there was an ai that has taken an iq test and scored similar to a 4 year old.\n\nhttp://phys.org/news/2015-10-ai-machine-iq-score-young.html\n\n\n  the ai system which they used is conceptnet, an open-source project run by the mit common sense computing initiative.\n  results: it scored a wppsi-iii viq that is average for a four-year-old child, but below average for 5 to 7 year-olds\n\n\nabstract\n\n\n  we administered the verbal iq (viq) part of the wechsler preschool and primary scale of intelligence (wppsi-iii) to the conceptnet 4 ai system. the test questions (e.g., \"why do we shake hands?\") were translated into conceptnet 4 inputs using a combination of the simple natural language processing tools that come with conceptnet together with short python programs that we wrote. the question answering used a version of conceptnet based on spectral methods. the conceptnet system scored a wppsi-iii viq that is average for a four-year-old child, but below average for 5 to 7 year-olds. large variations among subtests indicate potential areas of improvement. in particular, results were strongest for the vocabulary and similarities subtests, intermediate for the information subtest, and lowest for the comprehension and word reasoning subtests. comprehension is the subtest most strongly associated with common sense. the large variations among subtests and ordinary common sense strongly suggest that the wppsi-iii viq results do not show that \"conceptnet has the verbal abilities a four-year-old.\" rather, children's iq tests offer one objective metric for the evaluation and comparison of ai systems. also, this work continues previous research on psychometric ai. \n\n\n\n\nupdate. a robot has passed the japanese college entrance test and has an 80% chance of being accepted. since it scored more than the average, that would make the iq > 100, especially since college applicants have an iq greater than average, and especially since japanese are smarter than average humans. http://gizmodo.com/an-ai-program-in-japan-just-passed-a-college-entrance-e-1742758286\n\n\n  the wall street journal reports that the program, developed by japan\u2019s national institute of informatics, took a multi-subject college entrance exam and passed with an above-average score of 511 points out of a possible 950. (the national average is 416.) with scores like that, it has an 8 out of 10 chance of being admitted to 441 private institutions in japan, and 33 national ones.\n\n", "tags": "artificial-intelligence", "id": "1120", "title": "has a robot ever taken a complete iq test?"}, {"body": "from an old dust buster i've got this electro motor, the included battery pack and the charger:\n\n\n\n\ni ripped everything apart (the dust buster was broken) and the motor still works. after playing around with it for a while and letting it lying around for about two weeks it suddenly revs a lot slower. i supposed the battery pack was drained so i hooked up the battery pack to the charger and let it charge for a night. unfortunately the motor still turns very slow.\n\nsince i want to use this motor for my first home robotics project (making a kite fly with my computer), off i went to the local electronics store where they measured the charger to give 16v (even though it says 21v) and the battery pack to give about 5v. i then hooked up the motor directly to the charger, but unfortunately it doesn't even move an inch then.\n\nso now i wonder:\n\n\nwhy doesn't the motor spin at all when hooking it up to the charger? (could that be because the 250ma is too low?)\nwhy doesn't the battery pack charge at all? (this bothers me the most!)\n\n\nall tips are welcome!\n", "tags": "motor battery", "id": "1128", "title": "why is this electro motor going slower?"}, {"body": "\n\ni am trying to find a joint like these for a robot i'm building. it is often called a swivel joint or a universal joint, but with a modified spider. i can't find one anywhere and would prefer not to make it. searching for 'universal joint' returns the standard automotive type. any help would be appreciated\n", "tags": "joint", "id": "1130", "title": "what is the name of this mechanical linkage?"}, {"body": "i am a high school student, doing a research project in ai and robotics. how should i choose a robotics kit (for example, will it be better to learn the basics by using a hexapod or robotic arm?)\n\ni know c at good level.\n", "tags": "arduino artificial-intelligence beginner", "id": "1132", "title": "how should i choose an educational robotics kits for beginner programmers?"}, {"body": "this is a follow-up to this question:  prototyping a device with 25-100 small dc 3.0v motors, is arduino a good fit?\n\ni've decided based on the answer that sending the control signals through multiple tlc5947 chips, then sending the pwm signal to the motors is the best way to go.  what i need to know is how to turn the pwm signals into something of the required power, since the tlc5947's won't be able to drive the motors by themselves.\n\ni'm guessing an amplifier is what i'll need to make, but what's the best way to boost that many signals?\n", "tags": "motor power pwm", "id": "1143", "title": "how do i interface a tlc5947 with small motors?"}, {"body": "i'm trying to handle food grains like rice, wheat in an automated way (to cook simple dishes). for this i have to transfer grain from a larger container to a weighing scale. i know i can use solenoid valves for liquids but all solid handling valves seem to be too big (gate valves etc) and for larger applications. is there any better way to do this ? \n", "tags": "automatic", "id": "1145", "title": "what would be the best way to handle food grains?"}, {"body": "the dynamic programming algorithm refers to the bellman equation. an open-loop control decides movement at the initial point while a closed-loop control decides control during the movement. now most robotic application looks like closed-loop control: in every point, it checks how it is doing with respect to some reward function, this is my thinking. now most participants in threads such as how mature is real-time programming in robotics? do not differentiate their scope, perhaps they haven't thought about it. anyway, i am interested to know:\n\nhow is dynamic programming used in robotics? is there any research about dp usage in robotics?\n", "tags": "research dynamic-programming", "id": "1148", "title": "dynamic programming algorithm aka bellman equation in robotics?"}, {"body": "fpga has good points such as a lot of io points but then again you need to think things on very low level with flip-flops and pioneer on areas where things are not yet mature -- for example see this question here about development-tools on fpgas -- this is my understanding currently! now fpga has been used to create excellent dexterity in robotic hands like here. now some people market fpga for fast prototyping and \"forward looking\" designs like here, i don't fully understand them: if you don't need a lot of io points for things such as sensors, why to choose fpga for a robot? so\n\nwhen should fpga be chosen for a project in robotics?\n", "tags": "design research logic-control", "id": "1153", "title": "when should fpgas be used in robotics?"}, {"body": "some vector math is involved here so prepare yourself.\n\ni am developing a robotic arm that moves in two dimensions.  it is a rotary-rotary design which looks roughly like the picture in this post:\nbuilding robotic arm joint\n\ni am now trying to limit the speed of the end-effector.  i am using simulink and believe that the best way to limit the speed is the limit the rate of change of the x and y coordinates that i tell it to move to.  \n\nnow, i also want the end-effector to be able to move in a straight line and believe that i can accomplish this by defining functions that calculate the maximum rate for movement in the x or y direction based on the distance the arm is trying to travel.  the equasion i came up with is this :\n\n\n\nso basically, xrate: distance in x / max between distance in x and distance in y.\n\n\n\nnow, for the actual problem.  because this limits the speed in both x and y, the end-effector can travel (for instance) 1 in./sec in both directions at the same time.  meaning that it is travelling at over 1 in./sec overall.  if, however, it is only moving in one direction then it will only move at that 1 in./sec speed because there is no second component.  it boils down to the fact that the max speed the arm can move is 'sqrt(2)' and the minimum is '1'.\n\nmy main question is:  given that i need to calculate a max xrate and a max yrate, how can i limit the overall speed of the end-effector?\n\nsecondarily, is there a way for me to implement a rate control that will limit the overall rate instead of limiting x and y independantly using simulink?\n", "tags": "design", "id": "1158", "title": "equation to limit rate of change of end-effector in x and y coordinates"}, {"body": "i have an old gamecube that doesn't work and i want to gut it and fill it with arduino boards and/or raspberry pi if necessary.  i want the project to eventually have some kind of ai aspect, but i'm also toying with the idea of using a wireless gamecube remote and wavebird to issue commands at the push of a button.  \n\ni guess this would be mostly good for testing purposes, but i'm mostly curious if and how i would go about making my raspberrypi understand gamecube remote input.  furthermore, would this kind of idea be feasible?\n", "tags": "control arduino raspberry-pi", "id": "1160", "title": "can i use the raspberrypi to receive the gamecube remote's rf signals?"}, {"body": "i'm building a quadcopter and i've seen that a li-po battery must not be entirely discharged, otherwise it could damage it.\n\nhow do you know when you have to stop your quadcopter or robot in order to prevent damages, since the voltage doesn't drop? which part should control the battery charge? escs? bec? flight controller?\n", "tags": "battery", "id": "1163", "title": "how to know when a li-po battery is discharged?"}, {"body": "i'm building an arduino controlled pump system to be able to move fluids around. i need this to be fairly accurate, but extreme precision isn't required. since there will be a variety of liquids moved through the pump, i've determined a peristaltic pump the best fit. but i don't think i fully understand them, and had a few questions..\n\n\nsince i'll need to purge the system... can a peristaltic pump push air? let's assume you have a 2m of tubing, and you pump a bunch of water through it. can you remove the tube from the water reservoir so it is open to the air, and effectively purge the system of any remaining water?\nsince i want to fairly accurately measure flow, could i simply count milliseconds instead of using a flowmeter? ... will a peristaltic pump always pump at a constant rate, regardless of the viscosity of the fluid? that is, will maple syrup come out at the same rate as water?\nshopping question, ignore i suppose ... anyone know where i may find a fast/high flow peristaltic pump? i'm looking to be able to pump, at a minimum, .5oz/sec\nwould be determinant upon #3 ... what sort of relay would i want for toggling this on/off with an arduino?\n\n", "tags": "arduino", "id": "1167", "title": "can the rate of peristaltic pump's flow be accurate across changes in fluid viscosity?"}, {"body": "i'm starting out with gazebo (1.5) at the moment and am following a tutorial off the internet. in order to get gazebo to find the model, the author advocates manually exporting the  environment variable via \n\n\n\nbut that will only work for the current terminal. so i wanted to change the environment variable permanently. \n\nthe gazebo user guide claims that , along with all the other environment variables, is set by  but my (virgin) gazebo install doesn't list it:\n\n\n\nbut when i start gazebo,  is already set to , so it must be set somewhere. i guess i could probably simply add  to the  script, but since it is set somewhere, i'd still like to know where and whether it is better practice to set it in there.\n", "tags": "gazebo", "id": "1170", "title": "where does gazebo set the gazebo_model_path environment variable?"}, {"body": "see the video below of my balancing robot.\nbalancing robot\n\ni was having trouble getting it to balance on hard surfaces but finally got it after playing with the pid gains a lot.  previously it was balancing just fine on carpet.  \n\ni set the pid gains by just picking a kp, then increasing ki until the robot oscillated very badly and tried to smash it's self into the ground,  then increasing kd until it was finally stable. here's the gains i'm using in the video.\n\n\n\nit will sit in one spot balancing without any problem.  you can see in the video that it can even stop from falling after i give it a pretty good kick.  the problem is that it stops from falling over but then greatly overshoots the other direction.  in the video you can see when i give it just a small tap it runs the other way for a while before finally becoming stable again.  \n\nany suggestions on what to try next?\n", "tags": "pid", "id": "1174", "title": "pid tuning to make my balancing robot better"}, {"body": "in a quadrotor we need to change each motor's speed depends on its position in space. more frequency will result more stability ( i mean if we can change motor's speed 400 times per second instead of 100 times per second we may stabilize our uav quadrotor far better ).\n\nnow my question targeting people who made a uav quadrotor before or have any information about escs. i wanna know whats the minimum refresh rate for escs in a quadrotor to make it stable ? for example may an esc with 50hz refresh rate enough for stabilizing quadrotor or not ? i'm asking this question because high speed escs are more expensive than lower speed ones.\n\ni have this one. may it work ?\n", "tags": "quadcopter esc", "id": "1175", "title": "minimum speed controller refresh rate"}, {"body": "i need a microcontroller that can process minimum 2mb data per second.\nhow do i determine what processors will be able to do this?\n\nalso how can i calculate the processing speed in per second of any microcontroller?\n\ni am very much scared with my college project and i need help.\n", "tags": "microcontroller", "id": "1178", "title": "how can i calculate processing speed of microcontroller"}, {"body": "i read many sources about kalman filter, yet no about the other approach to filtering, where canonical parametrization instead of moments parametrization is used. \n\nwhat is the difference?\n\n\n\nother questions:\n\n\nusing if i can forget kf,but have to remember that prediction is more complicated link \nhow can i imagine uncertainty matrix turning into an ellipse? (generally i see, area is uncertainty, but i mean boundaries) \nsimple addition of information in if was possible only under assumption that each sensor read a different object? (hence no association problem, which i posted here\n\n", "tags": "kalman-filter algorithm sensor-fusion", "id": "1180", "title": "information filter instead of kalman filter approach"}, {"body": "i want to fuse objects coming from several sensors, with different (sometimes overlapping!) fields of view. having object lists, how can i determine whether some objects observed by different sensors are in fact the same object? only then i can truly write an algorithm to predict future state of such an object. \n\nfrom literature i read those 4 steps:\n\n\nplot to track association (first update tracks estimates and then associate by \"acceptance gate\" or by statistical approach pdaf or jpdaf)\ntrack smoothing (lots of algorithms for generating new improved estimate, e.g.: ekf, ukf, pf)\ntrack initiation (create new tracks from unassociated plots)\ntrack maintenance (delete a track if was not associated for last m turns. also: predict those tracks that were associated, their new location based on previous heading and speed)\n\n\nso basically i am questioning point 1, acceptance gate. for a single sensor i can imagine it can be just a comparison of xy position of object and sensor measurement, velocity with heading eventually. my case is however, i have already ready object lists from each sensor in every cycle, there are some algorithms how to merge informations about an object collected by different sensors (great source is e.g. here: http://www.mathworks.de/matlabcentral/fileexchange/37807-measurement-fusion-state-vector-fusion), but question is how to decide which objects should be fused, and which left as they were? fields of view may overlap partly, not totally.\n", "tags": "kalman-filter algorithm sensor-fusion", "id": "1181", "title": "object level sensor fusion for multiobject tracking"}, {"body": "these days, one often hears of cyber-physical systems. reading on the subject, though, it is very unclear how those systems differ from distributed and/or embedded systems. examples from wikipedia itself only make them look more like traditional distributed systems. for example:\n\n\n  a real-world example of such a system is the distributed robot garden at mit in which a team of robots tend a garden of tomato plants. this system combines distributed sensing (each plant is equipped with a sensor node monitoring its status), navigation, manipulation and wireless networking.\n\n\nobviously, any distributed system consists of sensing, actuations (which can easily include navigation) and networking.\n\nmy question is, how exactly does cyber-physical systems differ from traditional distributed systems? is it just a fancy name, or is there something considerably different with it?\n", "tags": "distributed-systems embedded-systems", "id": "1182", "title": "shedding light on \"cyber-physical systems\""}, {"body": "i'm planning on programming a prebuilt robot to solve a maze as fast as possible.  the robot has forward obstacle sensors (no side sensors) and 3-axis accelerometer.  i'm planning on using the wall following algorithm.  is this the fastest possible algorithm?  also, since there are no side sensors, the robot needs to continuously turn to check if there is a wall on its side, so is there a clever way to use the accelerometer and sensors?\n", "tags": "mobile-robot", "id": "1192", "title": "fastest maze algorithm for robot"}, {"body": "i am looking for a3 size poster of mars exploration rover spirit/opportunity for robotic education. \n\nwww.sunstartoys.com gives a little postcard-size of the mer along with its components on board when you buy the toy. but this is not large enough for classroom purpose.\n\ndoes anyone know where to buy a3 size poster of these mer for robotic education?\n", "tags": "wheeled-robot", "id": "1195", "title": "where can i get a3 poster size of mars exploration rover spirit/opportunity?"}, {"body": "i'm a newbie in robotics, and i'm doing a project on dynamic braille interface. basically it's a 8*8 array of pins, which can be either totally up or down. how to use least motor as possible?\n\ni'm thinking of using arduino for easy interface with computer.\n", "tags": "design", "id": "1198", "title": "dynamic braille interface"}, {"body": "i am looking for a specific name of the wire used for the robotic arm movement control and where can i find some of this online. i want to control it using the micro controller so please suggest some good development kit.\n", "tags": "microcontroller robotic-arm movement", "id": "1200", "title": "what is the wire used for hand movement of robot called ? where can i find it online ?"}, {"body": "we have an air bearing for a planar xy motion. today it consists of four pockets according to picture. \n\n\n\nin the current design there are no sealings around the peripheries of the pockets and we suspect that is the reason we get vibrations. \n\nin the current design we control the pressure, same for all for recesses. the flow is adjustable individually for each recess. in practice it is very hard to tune it.\n\nfor the non recess surfaces we have used slydway as we need to be able to operate it without pressure occasionally.\n\nto try to solve the problem we plan to develop a prototype where we can try out the effect of using sealings around the periphery of the pockets. the idea is something like this:\n\n\nquestions\n\n\nis the idea with adding sealings good? (sanity check)\nsuggestions for sealings? (i'm thinking a porous material like felt or cigarette filter) \n\nof course all suggestions are welcome.\n\n\nedit\n\ni'm going to try and add grooves around the recesses to evaquate the air that leaks. my thinking is that this will give us a more defined area under pressure.\n", "tags": "linear-bearing", "id": "1202", "title": "problem with vibrations in air bearing"}, {"body": "i would like to prevent a shaft from being pulled through it's bearings - that is, press a plastic ring around it on either side.  what are these rings called? they're not bearings or hubs.  and where can i find them?\n", "tags": "mechanism", "id": "1205", "title": "plastic shaft supports"}, {"body": "i have a 9 channel rf rx/tx and want to connect 3 motors to it.  i am able to connect channel 1 with motor 1 but unable to connect channel 2 with motor 2 simultaneously with arduino.\n\nhere is the code i am currently using:\n\n\n", "tags": "arduino", "id": "1207", "title": "read multiple channels of rx-tx with arduino"}, {"body": "today was my quadcopter's first \"flight\". i'm running megapirate on a crius aiop v2 with a turnigy talon v2 frame.\n\ni only touched the throttle stick on my remote, nothing else. when i felt the quadcopter was about to take off, i pushed the throttle just a little bit more, and the quadcopter oscillated 2 or 3 times and the just flipped over, landing on the propellers.\n\nso, i broke 2 props, my frame feels a bit loose, i'll probably have to tighten the screws (i hope...). how can i tune the software so it will stabilize nicely after take off?\n\nedit :\ni don't know if it was true oscillation or just random air flows making it unstable. i made some more tests yesterday and it was quite ok (even if i crashed a few times). this time, it was really oscillating but it was quite windy outside and the quadcopter managed to stabilize after all. so i'll probably have to tune my pids and find a way to do it without crashing.\n\nedit 2 : after some pid tuning, i managed to stabilize my quadcopter pretty well but it's still oscillating just a little bit. i guess i'll have to slightly change the values to get a perfect stabilization.\n", "tags": "quadcopter stability", "id": "1209", "title": "how to stabilize a quadcopter"}, {"body": "i have a small device that's picking up small rocks from a pile and moving them to another place. its a kind of crude way of trying to push the whole pile onto a bigger gear and hoping one of them are pushed to one of the spaces between gears and taken around and falls off on the other side of the spinning gear. here i want to know if the machine successfully got a rock here, if not it should spin the gear until it turns up a single rock on the other side of it. if a rock is present at the spot, the gear should stop spinning until the rock is taken care of by the rest of the machine. \n\nwhat kind of device can i use to sensor if i successfully succeeded in getting a rock on the other side of the gear? \n\n\n\nthis is just a part of a bigger system, to sum up, i need the sensor to signal when a rock is signalled out and separated from the rest so it can continue work on that single rock.\n\ni am building this using an ardiuno to move the gear around, so the sensor need to be something that can be controlled by an arduino\n", "tags": "motor sensors", "id": "1211", "title": "what kind of sensor do i need for knowing that something is placed at a position?"}, {"body": "in order to build and operate a space elevator moving crafts and people into space, there are two big challenges that have not been solved yet:\n\n\nfinding a cable with enough tensile strength,\nmoving stuff along the cable at a reasonnable speed.\n\n\napart from those two ones, what are the other technical challenges to solve, especially things that do not exist yet in robotics, and need to be invented?\n", "tags": "mobile-robot research", "id": "1214", "title": "space elevator: what is still needed, apart from the cable and propulsion?"}, {"body": "this is part two of my larger robot, it follows up what happens with the small rocks here: what kind of sensor do i need for knowing that something is placed at a position?\n\nnow i am taking the rocks down a tube for placement. in the case they need to be altered so they always will stand up before they enter the tube. obvioulsy a rectangular rock wont fit if it comes in sideways. the dimensions here are pretty small. the rocks are about 15 mm x 10 mm. the tube i use is actually a plastic drinking straw. and the material i use for the rest of the robot is lego powered by step motors which draw the conveyor belts to move the rocks. the control is arduino.\n\n\n\n(sorry for the lousy illustration, if you know a good paint program for mac like the one used to draw the picture in my other post, please tell me :-))\n\nthe rocks will always enter one at a time and have as much time they need to be adjusted to fit and enter the tube so the fall down. the question is, how to ensure all rocks are turned the right way when they get to the straw. im not sure if using lego when building the robot is off topic here, but a solution involving lego is preferable. and it has to be controlled by an arduino. \n\ngeneral tips in how to split a complex task into subtasks robots can do is also good, is there any theory behind the most common sub tasks a job requires when designing multiple robots to do it?\n", "tags": "arduino motor microcontroller motion", "id": "1218", "title": "how do i adjust objects on a conveyor belt into the proper orientation?"}, {"body": "imagine a \"drone\" and a target point on a 2d plane. assuming the target is stationary, there are eight parameters:\n\n\n\nthe drone's job is to get to the target as fast as possible, obeying max torque and max thrust. there are only two ways to apply the torque, since this is only in a 2d plane. thrust is restricted to only go in one direction relative to the orientation of the craft, and cannot be aimed without rotating the drone. neglect any resistance, you can just pretend it is floating around in 2d outer space. let's say the drone checks an equation at time interval  (maybe something like every .01 seconds), plugs in the parameters, and adjusts its torque and thrust accordingly.\n\n\nwhat should the equations for thrust and torque be?\n\n\nwhat have we tried?\n\nwe know that the time it takes for the drone to reach the target in the x-direction has to be the same for the same time in the y-direction. there is going to have to be some integral over time in each dimension to account for the changing thrust based on total thrust, and total thrust in each direction given the changing angular position. i have no idea how to tie the torque and thrust together in a practical way where a function can just be called to give what thrust and torque should be applied over the interval  unless there is some other technique.\n", "tags": "design algorithm kinematics navigation", "id": "1219", "title": "drone targeting"}, {"body": "we are using arduimu (v3) as our quadrotor's inertial measurement unit. (we have a separate board to control all motors, not with arduimu itself). \n\nas mentioned here , the output rate of this module is only at about 8hz. \n\nisn't it super slow to control a quadrotor ? i'm asking because as mentioned in this answer a quadrotor needs at least 200hz of control frequency to easily stay in one spot, and our escs is configured to work with 450hz of refresh rate. any working pid controller i saw before for quadrotors used at least 200-400hz of control frequency.\n\ni asked similar question before from ahmad byagowi (one of the developers of arduimu ) and he answered:\n\n\n  the arduimu calculates the dcm matrices and that makes it so slow. if\n  you disable the dcm output, you can get up to 100 hz gyro, acc and so\n  on.\n\n\nso, what will happen if i disable dcm from the firmware ? is it really important ? we did a simulation before and our pid controller works pretty well without dcm.\n", "tags": "arduino quadcopter imu pid", "id": "1221", "title": "quadrotor control using arduimu"}, {"body": "i'm in the planning stages for a project using the arduino uno to control 8 distance sensors, and have run into a little road block, the uno only has six input pins. so i'm wondering, is there any way for this to work? if so, how?\n", "tags": "arduino microcontroller input", "id": "1227", "title": "connecting more than six analog input pins to arduino"}, {"body": "i have a robot with two wheels/motors and each has a quadrature encoder for odometry.  using the wheel/motor/encoder combo from pololu, i get 48 transition changes per rotation and my motors give me a max of 400rpm.  i've found it seems to miss some of the encoder state changes with the pololu wheel encoder library.\n\nwould i run into issues or limitations on my arduino uno using interrupts to track the quadrature encoders while using pwm to drive my motors through an h-bridge chip?  \n", "tags": "arduino pwm encoding interrupts", "id": "1229", "title": "limits of pwm, timers and interrupts?"}, {"body": "i would like to create an arduino based robot with 2 wheels, quadrature encoders on each wheel, a h-bridge driver chip (or motor controller) and a caster.  i want to use the pid library to ensure the speed is proportional to the distance to travel.  \n\nat a conceptual level, (assuming the motors do not respond identically to pwm levels) how can i implement the pid control so that it travels in a straight line and at a speed proportional to the distance left to travel? \n", "tags": "arduino pid driver encoding", "id": "1232", "title": "how can i use the arduino pid library to drive a robot in a straight line?"}, {"body": "how does rocker-bogie mechanism keep the body flat / keep the solar panel almost flat all the time? i know there is an differential system that connect both rocker bogie (left and right) together. but how does it actually work?\n\nedited: please provide relevant references.\n", "tags": "wheeled-robot", "id": "1235", "title": "how does rocker bogie keep the body almost flat?"}, {"body": "so i have a quadrocopter, it does come with a remote but i intend to run certain modifications to the copter, like installing a camera, a mechanical manipulator, and other random modifications. the remote that comes with the copter isn't flexible enough to help with such functions and plus it lacks any more buttons. \n\ni was wondering if i could somehow program the quadrocopter to respond to my xbox controller. i was planning on using my laptop's bluetooth connection to talk to copter. the xbox controller which is connected to the computer would be then used to control the quadrocopter. so my question is, how exactly do i program the controller? how do i go about making all of this possible? \n\ni understand this question is really vague and that there are too many options out there, but i do need help figuring this out. \n", "tags": "quadcopter", "id": "1246", "title": "using an xbox controller to fly a quadrocopter"}, {"body": "expanding upon the title, i am querying the use of robotic skeletons to augment human strength and speed. if such a robot had the capacity for example to bear weight 5 times heavier than the wearer and move its robotic limbs twice as fast as the wearer, is there not a danger because such powerful and sharp movements could break their bones and seriously injure them because it moves beyond their human capabilities?\n\nthe robots means of producing movement i would think is important here but unsure how so. the nature of passive or actively powered movement and when each mode is used will also determine performance of the exoskeleton. i am not well versed in this area so will appreciate any feedback.\n", "tags": "mobile-robot", "id": "1249", "title": "would the strength and speed of a robot skeleton be a danger to its wearer?"}, {"body": "while doing a literature review of mobile robots in general and mobile hexapods in particular i came across a control system defined as \"task level open loop\" and \"joint level closed loop\" system.\n\n\n  the present prototype robot has no external sensors by\n  which its body state may be estimated. thus, in our simulations and experiments, we have used joint space closed\n  loop (\u201cproprioceptive\u201d) but task space open loop control\n  strategies.\n\n\nthe relevant paper is a simple and highly mobile hexapod\n\nwhat is the meaning of the terms \"joint-level\" and \"task-level\" in the context of the rhex hexapod?\n", "tags": "mobile-robot control walking-robot hexapod", "id": "1253", "title": "what is the difference between task-level and joint-level control systems?"}, {"body": "i have tried following a number of guides on the internet but most of them fall down as libfreenect does not exist in opkg, which is the apt-get of angstrom. has anyone got it working and if so what is the method?\n", "tags": "kinect", "id": "1257", "title": "how can i get windows kinect working on angstrom on beaglebone?"}, {"body": "i've been watching too much how it's made, and i've been wondering how they build devices that spray/inject/dispense a finite amount of liquid (to within some amount of error).  i wanted to try this for a hobby project. i'm working on that dispenses dry goods in the amount i specify.\n\ndo i use some kind of special nozzle/valve which can open and close at high speeds? how can i dispense a known quantity from a reservoir of a fluid substance onto each individual unit passing along an assembly line, or an amount specified by the user into another container?\n", "tags": "mechanism manufacturing", "id": "1259", "title": "how to measure and dispense a finite amount of powder or liquid"}, {"body": "the latest osx documentation i found on the website is from 2011, and the latest build is from over a year ago. i'm a complete n00b to all things ros and wanted to start playing with it. what is the easiest way?\n\nedit: this version of the installation instructions is more recent (april 2013), but it says that\n\n\n  osx is not officially supported by ros and the installation might fail for several reasons. this page does not (yet) contain instructions for most higher level ros packages, only for the base system. this includes the middleware and command line tools but not much more.\n\n\n\"does not contain instructions\" also means it doesn't work? what do osx users who work on ros usually do? run it on an ubuntu vm? install it just fine on their own on osx, even though there aren't detailed instructions on the website?\n", "tags": "ros", "id": "1263", "title": "what is the easiest way to install ros on osx mountain lion?"}, {"body": "let's think of the following situations:\n\n\nyou are teaching a robot to play ping pong\nyou are teaching a program to calculate square root\nyou are teaching math to a kid in school\n\n\nthese situations (i.e. supervised learning), and many others have one thing (among others) in common: the learner gets a reward based on its performance.\n\nmy question is, what should the reward function look like? is there a \"best\" answer, or does it depend on the situation? if it depends on the situation, how does one determine which reward function to pick?\n\nfor example, take the following three reward functions:\n\n\n\n\nfunction  says:\n\nbelow a certain point, bad or worse are the same: you get nothing\nthere is a clear difference between almost good and perfect\n\nfunction  says:\n\nyou get reward linearly proportional to your performance\n\nfunction  says:\n\nif your performance is bad, it's ok, you did your best: you still get some reward\nthere is not much difference between perfect and almost good\n\n\n\nintuitively, i'd think  would make the robot very focused and learn the exact pattern, but become stupid when dealing with similar patterns, while  would make it more adaptable to change at the cost of losing perfection.\n\none might also think of more complex functions, just to show but few:\n\n\n\nso, how does one know which function to pick? is it known which behavior would emerge from (at least) the basic ,  and  functions?\n\n\n\na side question is would this be fundamentally different for robots and human kids?\n", "tags": "machine-learning", "id": "1266", "title": "what reward function results in optimal learning?"}, {"body": "how can i detect when a stepper motor has stalled?\n\na google search led me to some people who say that\nwhen the stepper motor stalls, the current spikes up,\nwhich is easily detectable with a hall sensor.\n(or, i suppose, by any of the other current sensors mentioned at\n\"how can i sense the motor's current?\"\n).\n\nhowever, i measured the current through (one of the 4 wires of) my stepper motor,\nand it's always within a few percent of 0.5 a, whether my stepper driver is holding one position, moving it normally (which in my application is very slowly), or the stepper driver thinks it is telling the stepper to move normally, but the motor has pegged out against the hard limit.\nmeasuring the current in the +12v power supply going to the stepper motor driver, also seemed to give a fairly constant current.\nthis may be because i turned down the current limit to that amount on my \"chopper\" stepper motor driver.\n\nam i missing some key detail in the \"measure the current\" approach?\n\na google search led me to some other people that measure the back-emf (bemf) in one coil of the stepper during the time the stepper driver is only driving the other coil.\nbut that only seems to distinguish between \"a motor moving quickly\" vs \"a motor stopped\", and doesn't seem to distinguish between my case of \"a motor moving slowly\" vs \"a motor stopped\".\n\nis there some way to apply the bemf approach even in a system where i always drive the stepper slowly, and never spin it quickly?\n\ni'm currently using a stepper driver board with the ti drv8825 chip on it, and i hoped the \"fault\" pin would tell me when the stepper motor has stalled against my hard stop.\nbut it doesn't seem to be doing anything -- is it supposed to tell me about a stall, but i just have it wired up wrong?\n\nis there some other chip or drive technique that detects when the stepper has stalled out against the hard stop?\n\nis there some other technique for detecting a hard stall that i can \"add on\" to a system using an off-the-shelf stepper motor driver?\n\n(is there some other stackexchange site that is more appropriate for questions about motors and motor drivers?)\n", "tags": "stepper-motor stepper-driver force-sensor", "id": "1270", "title": "how to detect when a stepper motor has stalled?"}, {"body": "i bought my kid a robotics kit with several motors and an infrared remote control (you can steer the robot using ir remote control).\n\nnow i want to take it to the next level and control the robots from a pc or a raspberry pi.\n\nwhat is the simplest approach to do this?\n\ni am thinking about 2 possible ways:\n\n\nfind out the protocol the existing remote control uses and then emulate the ir signals using arduino (arduino is sending the ir signals).\nfind a piece of hardware, which presses the buttons on the remote control and control it via to arduino (arduino is sending signals to the button pushers, the remote control is sending the ir signals to the robot).\n\n", "tags": "mobile-robot arduino raspberry-pi", "id": "1274", "title": "how to connect an infrared remote control to pc or arduino or raspberry pi?"}, {"body": "i have a three wheeled vehicle in a tricycle configuration attached to a fixed frame. each wheel is powered by an ac electric motor. the ac motors are fed by motor controllers that take a speed demand. the single main wheel (which is also steerable) has a lower gear ratio than the rear wheels so it has a theoretical higher top speed. \n\nwhen the vehicle drives in a straight line each of the motor controllers are given identical speed requests. unfortunately feedback from the controller indicates that some motors are pushing while some are pulling. in particular we have a common scenario where one rear wheel is pushing while the front wheel is trying to slow down. the third wheel will often have almost no current. \n\nwhat can be done to make all three motors work together and avoid situations where they fight?  is there a way to change the request to the motor controller to encourage the drives to work together? do we have to switch from a speed request setup to a current control setup? if so what is the appropriate way to control the motors then? \n\nlet me know if i haven't included any important details and i will update my question.\n", "tags": "control motor", "id": "1276", "title": "how can a load be balanced between multiple ac electric drive motors?"}, {"body": "i am developing a robot which paints using an airbrush (3d painting). i intend to use several colors as a cmyk printer, but i do not know how to do the conversion of rgb colors in the computer to the dosage of colors in cmyk.\n", "tags": "robotic-arm", "id": "1277", "title": "how can i convert rgb colors to cmyk for my airbrush robot?"}, {"body": "i am interested in building a robot like the ez-b, sold by ez-robot.com. it comes with an sdk for visual studio and has direct scripting in runtime through a usb, bluetooth, wi-fi, irc or https connection.\n\nif i get a regular arduino board, will i be able to control it remotely in the same way? from what i've read, an arduino needs to hold the instructions in its own memory, but i would rather have the brain in the computer, feeding signals back and forth to the microcontroller.\n\nalso, is arduino alone, a step down as the website niceley puts it?\n", "tags": "arduino microcontroller research machine-learning artificial-intelligence", "id": "1279", "title": "how can i create a robot like the ez-b using a regular arduino?"}, {"body": "i noticed that some imu units are tuned to be sensitive to small changes, other to large changes and some that can be adjusted between different sensitivities. i am familiar with the use of a kalman filter to normalize readings, but i was wondering if my uav could benefit from a second imu where the two are set at high and low sensitivities to get even more accurate and timely information.\n", "tags": "kalman-filter quadcopter imu uav", "id": "1286", "title": "is there a benefit to using 2 imu units on a uav set at different sensitivities?"}, {"body": "i have arduino code for operating 2 servos, but we are using 4 servos and am having trouble getting the other 2 to talk. \n\nthe program so far as i can make out is that the angles for the servos that are calculated by the processing side are being sent out one after the other (shoulder, elbow, wrist, wrist2) then repeated. the arduino program gets this data and stores in into an array and then is written to the pin of the appropriate array segment. so 0 is shoulder, 1 is elbow, 2 is wrist and 3 is wirst2. \n\ni can easily get 2 servos to run with no problem.  but when i try and add 1 or 2 more we get no response.  can anyone please help me to get the other 2 servos to work?  my knowledge on this code is rather limited, so any help is appreciated.  \n\nprocessing data being sent to the arduino:\n\n\n\narduino code:\n\n\n\nsorry for the lengthy post but have been stuck for a while. \n", "tags": "arduino kinect robotic-arm", "id": "1290", "title": "issues upgrading arduino code for kinect controlled arm from 2 servos to 4"}, {"body": "concerning robots which rotate at high speed by spinning the drive motors in opposite directions, while still being able to simultaneously move in a direction (translate):\n\nas far as i know this originated with competitive fighting robots, where it is known as \"melty brain\" or \"tornado drive,\" according to wikipedia, and is based on alternately slowing down the motors on either side as they revolve around the centre of mass.\n\nhowever, with the whole body spinning so fast how is the current \"heading\" of the robot established and maintained?\n", "tags": "navigation movement", "id": "1294", "title": "what are the mechanics of translational drift?"}, {"body": "i'm trying to send arduino sensor data to a server using a gprs shield (sim900 shield from geeetech). i have this particular set up because the data will be updated to a website and the device will be roaming. i can't use http://www.cosm.org because to the best of my knowledge that only updates every 15 minutes, i need to update about every 5-10 seconds.\n\nin order to connect i tried the code below to form udp connection but it does not get sent through to the receiving ip and port. i don't know why. no errors occur on the arduino side, and the server side has been shown to work with an iphone app that sends a udp message.\n\n\n\nthe server side is as follows (written in python):\n\n\n\ni can see a possible solution might be to change it to a tcp connection, but i don't know how to do that...\n", "tags": "arduino sensors raspberry-pi programming-languages python", "id": "1299", "title": "send arduino sensor data to server with gprs shield"}, {"body": "i just got a kit and im not sure if its me or not but it appears one of the continuous servos might be broken. what happened first when i plugged it into the microcontroller, it made a humming sound when i sent it commands. the second continuous servo didnt work at all\n\ni played around with different ports on the aurdino based board, and to no avail, just a hum.\nthen i removed the humming servo altogether and just placed the second servo alone. the second continuous servo started to move in whatever direction i asked it to. \n\ni plugged the first one in, only the second moved.\n\nthen i tried spinning them by hand, the second has much resistance, while the first one has dramatically less resistance, maybe 60% easier to spin by hand.\n\nis this something i can fix? has anyone experienced these problems before?\n\nthanks in advance, you guys are great!\n", "tags": "arduino microcontroller servos wheeled-robot", "id": "1301", "title": "what are the signs that a servo might be broken?"}, {"body": "following, the previous question, i am trying to calculate how much one rocker would rotate when the other is being rotated. i attached my calculation here.\n\ni am trying calculate the rotation of gear b that connects to right rocket. given gear a rotates at 0.05 rad, what is the rotation of gear b in rad? gear ratio a:d is 4:1, and d:b is 1:4.\n\nat the end, i ended up with rotational gear a = gear b. this somewhat puzzles me. is my calculation correct?\n\n\n", "tags": "wheeled-robot", "id": "1302", "title": "rotation ratio between left rocker and right rocker in rocker-bogie system"}, {"body": "i recently have been working on a little project. unfortunately, i've ran into a bit of a road block with controlling servos using serial commands. the servos do appear to move when i put in any character into serial, but only a little. when i type in say, 90 characters of random gibberish, both servos connected to my arduino move several degrees. here's my code:\n\n\n\nany help would be much appreciated.\n\nedit: another note, nothing is printed in the serial monitor.\n\nalso, these are micro towerpro rc servos.\n", "tags": "arduino rcservo serial c++", "id": "1305", "title": "controlling robots through serial"}, {"body": "i was jogging the abb irb1410 and i noticed that the servo motors are humming even when the joints are not moving. the motor cuts off only when the guard switch in the flex pendant is released.\n\nwhat kind of mechanism which require the drive motors to keep running even when the joints are not moving ? i went through the manual but no luck. i suppose the holding torque is provided by some braking mechanism so i think i can rule it out.\n", "tags": "industrial-robot servomotor", "id": "1306", "title": "why are the irb 1410's servos running even when the joints are not moving?"}, {"body": "i am currently debugging and tuning an ekf (extended kalman filter). the task is classical mobile robot pose tracking where landmarks are ar markers.\n\nsometimes i am surprised how some measurement affects the estimate. when i look at and calculate the numbers and matrices involved, i can work out how the update step got executed, what and why exactly happened, but this is very tedious. \n\nso i wonder if anyone is using some technique, trick or clever visualization to get a better feel of what is happening in the ekf update step?\n\nupdate #1 (will be more specific and show first approximation of what i have in mind)\n\nwhat i am looking for, is some way to visualize one update step in a way that gives me a feel of how each component of the measurement affects each component of the state.\n\nmy very first idea is to plot the measurement and it's prediction together with some vectors taken from the k matrix. the vectors from k represent how the innovation vector (measurement - measurement prediction, not plotted) will affect each component of the state.\n\ncurrently i am working with an ekf where the state is 2d pose (x,y,angle) and the measurements are also 2d poses.\n\n\n\nin the attached image(open it in new page/tab to see in full resolution), the (scaled) vector k(1,1:2) (matlab syntax to take a submatrix from 3x3 matrix) should give an idea how the first component of the ekf state will change with the current innovation vector, k(2,1:2) how the second component of ekf will change, etc. in this example, the innovation vector has a relatively large x component and it is aligned with vector k(2,1:2) - the second component of the state (y coordinate) will change the most.\n\none problem in this plot is that it does not give a feel of how the third component(angle) of the innovation vector affects the state. the first component of the state increases a bit, contrary to what k(1:1:2) indicates - the third component of the innovation causes this, but currently i can not visualize this.\n\nfirst improvement would be to visualize how the third component of the innovation affects the state. then it would be nice to add covariance data to get a feel how the k matrix is created.\n\nupdate #2 now the plot has vectors in state-space that show how each component of measurement changes the position. from this plot, i can see that the third component of the measurement changes the state most.\n\n\n", "tags": "ekf visualization", "id": "1314", "title": "visualizing and debugging an ekf"}, {"body": "i am building 4-wheeled, knee-high robot with music and speakers on top that will follow a target person as the target moves around. i would like some help with the setup for tracking the target. the most obvious solutions are ultrasound or infrared sensors or some kind of vision tracking, but for this application, i don't want to use them. \n\nimagine that the robot is placed into a crowded area and asked to move towards a particular person in the area (for the sake of simplicity, assume the person is less than 5 meters away, but could be obscured by an object). ideally, if someone walked between the target and the robot, the robot would not lose it's path (as would happen with vision-based sensing).\nthanks!\n", "tags": "sensors computer-vision navigation", "id": "1315", "title": "non-vision based target tracking"}, {"body": "i am building a robot that will follow a target as the target moves around. i would like some help with the setup for tracking the target. the most obvious solutions are ultrasound or infrared sensors, but for this application, they won't work. imagine that the robot is placed into a crowded area and asked to move towards a particular person in the area (for the sake of simplicity, assume the person is less than 5 meters away). is there some kind of radar or radio solution to this, or anything?\n", "tags": "mobile-robot sensors electronics", "id": "1318", "title": "robotics location following and tracking?"}, {"body": "i bought a new roboduino atmega 328 board. basically roboduino is a modded version of arduino uno made by robokits.co.in. the problem is \n\non windows plaform:\nwhen i tried to upload a simple blink program that's listed in the examples of arduino ide 1.0.4, i got error that \navrdude: stk500_getsync(): not in sync: resp=0x00\n\ni chose the correct com port after verifying it with the device manager. i installed the prolific drivers for the board. i selected the board as arduino uno in arduino ide.\n\nthe complete verbose for the upload is as follow:\n\n\n\nwhen i plug in the board the power led is on. the 13 pin led blinks once. when the ide shows uploading the 13 pin led blinks 3-4 times and then the error appears on the screen. in between also sometimes it blinks randomly for 5-6 times. i also tried other example programs but the same follows.\ni'm using 32 bit windows 7 ultimate and the baud rate is set to 9600.\n\non ubuntu 13.04:\ni downloaded the ide from software center. i was added to the dialouts group on  the first run. after connecting the board to my pc i ran two commands  which returned following output:\n\n\n\nand then . after this when i tried to upload the same program of blink, it gave me following error:avrdude: stk500_recv(): programmer is not responding. i'm using 64 bit ubuntu 13.04 and selected arduino uno as the board.\n\nthank you for reading this long. please provide me suggestions for the problem.\n", "tags": "arduino", "id": "1320", "title": "problem uploading to roboduino atmega328"}, {"body": "how can i provide more power to a dc motor that is in series behind a receiver circuit hacked out of a cheap rc car without burning up the receiver board? the board runs off two aas at about 3v. i'm replacing the stock motor with a slightly larger one (12v, taken from a printer) and remounting it on a chassis for a homebrew robotics project... just messing around to learn more. i imagine i could go safely to 4.5v or even 6v with the receiver but i don't want to go much higher since half the stuff is epoxied and i can't really tell what's in there.\n\nwhat i'd like to be able to do is add an additional two aa batteries behind the receiver to run the receiver system at 6v but add another two 3v 123a batteries to have the motor at 12v with the ability to run with the higher current draw due to the heavier load the motor will handle on its fancy new chassis... but without pulling that current through the receiver circuit.\n\nmy first thought is to simply connect my 123as negative to the motor and positive to a common ground... but i'm really not sure and i want to be careful to not damage the circuit or batteries. my next thought is to simply build a single power supply out of my 123as and use a current divider but i've only read about them and never actually tried so.\n\ni've been doing some of those kiddie \"electronic playgrounds,\" a few books and probably cost google an extra few bucks in energy costs and i'm still kinda at a loss.\n", "tags": "motor power", "id": "1324", "title": "additional power to dc motor via second power source"}, {"body": "i ran into confusion while reading about motors.\n\nconsider a motor with these specs:\n\n\nmaximum motor voltage - 6vdc\nno load current - 250ma max.\nstall current - around 1a\n\n\ni am considering using the texas instruments l293d, with these specs:\n\n\noutput current - 600 ma per channel\npeak output current - 1.2 a per channel\n\n\nif i use the l293d to run 1 motor (back and forth), is this safe?  what would happen if my motor requires more than 600ma?  does this simply mean i need different driver ic?\n\nalso, the specs say that if i want to drive 2 motors then i'll need to compensate for the current. is it current from my power supply or from the motor driver?\n", "tags": "mobile-robot motor", "id": "1327", "title": "how do i interpret these specs for a motor and motor driver?"}, {"body": "i used a turnigy 2200mah 3s 25c lipo battery pack with turnigy balancer &amp; charger 2s-3s for about a month. yesterday i left the battery plugged into four escs of my quadrocopter. today i've found the battery totally discharged. when i tried to charge it, the charger showed it as faulty. after replugging it to the charger it showed it as fully charged.\n\nhow can i charge it now?\n\np.s. i've got a multimeter, but i do not know what and how to measure... the battery pack has two plugs: one is connected to the charger and the other to the escs...\n", "tags": "battery", "id": "1329", "title": "battery pack discharged"}, {"body": "i would like to ask which is better to design the multicopter with odd or even number of propellers? and why?\n", "tags": "quadcopter", "id": "1332", "title": "multicopter odd or even"}, {"body": "i bought an rc car about a year ago. a few days later i integrated an arduino nano into the vehicle. the only thing the arduino does is to receive the rc signal and pass it on to the esc/servo. so, basically it just does a big amount of nothing :)\n\nright now the wiring looks like this:\n\n\n  [remote] -> [rc receiver] -> [arduino] -> [servo/esc/lights]\n\n\ni added lights and i did some experiments with distance sensors and i will try to integrate car control via xbee + processing. this works via serial already.\n\nwhat else could be possible with a setup like that? here are some of my ideas:\n\n\nperhaps some sort of autonomic driving? the car is built for offroad and the suspension is not too bad but it is pretty fast (40 km/h) so a crash would be fatal.\nfpv (first person view) driving? i could add another servo to move a small camera.\n\"swarm intelligence\"? i have built two of those vehicles. both feature the arduino nano, a zigbee and led front lights.\nsteering correction? i could integrate a gyro sensor to check if the car is not driving straight when it should.\ntelemetry to another arduino? i could build some sort of arduino-zigbee-handheld that shows me some information for both cars like motor temperature, current speed, uptime, battery voltage, sensor values etc.\n\n\nany ideas, anyone? right now it is just driving like it normally would. i integrated an arduino into an rc toy that does an awesome amount of nothing. makes me feel pretty stupid.\n", "tags": "arduino sensors radio-control automatic", "id": "1334", "title": "arduino controlled rc car. what now?"}, {"body": "i have a matrix of m measurements and n objects. each cell contains a cost of assignment a particular measurement to the object. i want to assign them optimally. as the condition, only one measurement can go to one object, and one measurement can go to only one object. i want to set some cost threshold, in effect there may be some measurement or object, which is not assigned at all.\nhow can i do it?\ni was recently thinking of the auction algorithm, which however will never leave any unassigned measurement or object. if that is false, correct me please. or help with some alternative solution. thanks for your time!\n", "tags": "artificial-intelligence sensor-fusion", "id": "1336", "title": "association of multiple measurements to multiple objects"}, {"body": "okay, this might sound like a stupid question but, is there some sort of a permission in the us i might require to fly a quadcopter or a uav for that matter? i couldn't find much help anywhere else.  \n", "tags": "quadcopter", "id": "1339", "title": "permission to fly uavs"}, {"body": "i'm looking for a robot that is capable of moving around and has arms that can get objects in one place and drop in another. something akin to what we see in most sci-fi movies, though much simpler. it may run on legs, wheels or tracks; it may have claws or hands. i'm looking for open-sourced design, schematics, specifications of the parts, coding - the whole package. it may be specific cases or projects/initiatives with a growing collection of robots.\n\nas long as it can take out the trash, it's perfect. ;d\n", "tags": "mobile-robot", "id": "1342", "title": "open source \"sci-fi\"-like robot projects irl"}, {"body": "so building a quadrocopter from scratch has a lot of decision making, and i need some input on the material choice. \n\ni have short listed aluminum and carbon fiber for the arms and support of the quadrocopter. \ni am a little short on cash to experiment with both of them. \n\nso considering that i have enough money to buy either of those, and assuming that i have access to general tools like a table saw, horizontal band saw, cnc router and a water jet. \n\nwhat would be a better material to work with\n\nedit:\ni will be deciding the specs around the frame so as to allow me some design liberty. so right now, my goal is to assemble a very durable, as-light-as possible frame, which can withstand a lot of experimentation on the electrical side.\n", "tags": "design quadcopter", "id": "1344", "title": "aluminum vs carbon fiber"}, {"body": "i am looking to upgrade the motors for seaperch underwater rovs so we can carry heavier payloads and more equipment.  \n\nmy question is, should i look for motors which have a higher rpm and lower torque, or with lower rpm but higher torque to gain a substantial power increase? if the latter, what threshold of rpms should i stay above to maintain speed?  \n\nwe are currently running jameco  motors with ~1 1/2\" props (same setup as here). they are mainly run at max power as our esc currently consists of a fuse and a toggle switch.  \n", "tags": "motor underwater", "id": "1346", "title": "upgrading the motors on a seaperch rov - more torque, or more rpms?"}, {"body": "some time ago i saw a demo of a small 'toy tank' with a single camera mounted on it. this tank was able to drive around the floor and detect objects and then move/steer to avoid them.\nthe interesting part was that it used a single camera vision system and as far as i remember was taking advantage of the floor being flat. and then using the rate a feature was moving in the scene relative to the motors and directions of travel to evaluate and hence map the scene.\n\ncan anyone send me pointers what to search for to get some more information on this, or some pointers to codebases that can do this.\n\nthe reason i ask is that this was a single camera system from a number of years ago (5+) and therefore (from what i remember) was a relatively low compute load.\ni was intending to try this out on a raspberry pi to build a car/tank that maps a room or set of rooms.\n", "tags": "raspberry-pi computer-vision", "id": "1348", "title": "single camera vision and mapping system"}, {"body": "i have an autonomous lawn mower(alm) which can mow a certain lawn area when that area is bounded by a perimeter wire. even when that perimeter wire is removed, it has to mow the above mentioned area accurately without slipping into a neighboring area.\n\nconstraints and problems:\n\n\nthe alm is an open loop system.\ndifferential gps was tried, but it did not yield proper results.\nany iterative pattern of area coverage can be used provided the error in each iteration is not added cumulatively which can result in unpredictable error in the end.\n\n\ni do not expect full fledged solution. but i need a starting point to understand motion planning particularly for unbounded robotics to solve this problem. \n\ni searched on internet to know about the knowledge sources about motion planning but could not get good results. can anyone guide me to know about such sources preferably books and articles on internet which can help me to solve this problem?\n\nedit:\naddition of information:\n\n \n\nthe above picture shows the irregular lawn area which does not have any enclosures and perimeter\nwire\n1.the red mark shows the center point of lawn .\n\n2.the grey area is the initial scaled down area which resembles in shape to the larger area .i could not draw the grey area which exactly resembles the larger green area .\n\n3.the grey lines are the contours which from the tracks to be followed by the lawn mower\n\nidea description:\n\n1.using planimeter app for onetime , the shape and dimension of the lawn area (green area) can be known\nlink:https://play.google.com/store/apps/details?id=com.vistechprojects.planimeter&amp;hl=en\n\n2.center of polygon can be found by using the method in the following link\nhttp://en.wikipedia.org/wiki/centroid#centroid_of_polygon\n\n3.calculation of area of grey shape in the above figure .\n\n4 . grey shape is the least possible area which can be grazed by the alm . grey shape is similar to the green area shape and it is formed when green area is scaled down\n\nto determine the scale down factor which is a numerical value \u2018 n\u2019 (n&lt;1) \nwhere grey area = n * green area\n\nonce the grey area is known , the number of contours or tracks to be grazed by alm have to be determined manually .\n\nthe width of contour is equal to the distance between the blades on the either end i.e. the width which can be grazed by alm in a single stroke .\n\ngreen area = grey area + area of track 1 + area of track 2 + area of track3 + . . . . . . + area of track n\n\n5.once the lawn mower is switched on ,it should reach the center of the lawn (red mark showed in the above figure)\n\n6.then, alm should graze the least possible area or grey area .\n\n7.after that alm should switch to contour circumscribing the grey area . it should continue circumscribing in each track till all the tracks are completed( decision has to be made by validating against the calculated and preset value ' no.of tracks' in alm)   \n\nin this way entire lawn can be mowed without the need of perimeter wire and also alm would not mow the neighbor\u2019s lawn \n\nchallenges :\n\na. enable alm to reach the center point of the lawn\n\na. to make alm mow the grey area accurately\n\nb. to make the alm switch from one track to track .\n\nc. to bypass the obstacle in track and return to the same track .\n\nwhen i mentioned this idea to my colleague ,he mentioned the about possible cumulative addition of error in each iteration resulting in an unpredictable error in the end .\n\ni intend to minimize the error and fix the boundary as correct as possible. \nin fact this deviation should be predictable before it can be corrected .\n", "tags": "motion-planning", "id": "1349", "title": "working of autonomous lawn mower(alm) in an unbounded area without a perimeter wire"}, {"body": "visible worms, pests, and diseased parts of plants emit a unique odor (volatile organic compounds with different concentrations). i understand that sensors which can quantitatively detect these compounds development are being developed. my idea is to build a swarm of robots which can spray pesticides by detecting vocs on three targets present on plants across the fields.\n\n\ntarget 1: visible worms, pests, larvae. may these can be mechanically eliminated \ntarget 2: invisible pathogens on certain areas of a plant \ntarget 3: areas where pesticides have to be sprayed to prevent disease \n\n\nfor these targets, pesticide has to be administered in the correct concentration \nthis idea can optimize the use of pesticides and treat the plant properly \nquestions:\n\n\nis swarm robotics still sci-fi or did any one implement it?\nare the  any specific scenarios where implemented swarm robotic systems  are coming to help and establish the ease?   \nwhich is the implemented system or idea in conception that can help in formation of solution for the above problem? \nhow much time is required approximately to realize this idea?\n\n\nhope this question does not sound sci-fi and is practical and the intention is to solve a definite problem \nhow can i work by following some steps to further  make this idea concrete\n", "tags": "sensors research", "id": "1350", "title": "building a autonomous pesticide spraying system using swarm robotics based on odor (volatile organic compounds) detection"}, {"body": "i'm trying to use a pid to stabilize a system described from the following difference equation:\n\n$$y_{k+1} = a y_k \\sqrt{(1-y_k)}~~~ + b y_{k-1} ~+ c u_k$$\n\ncan i use ziegler-nichols's rules to find pid parameters in this situation?\n\nto be more precise. my system is an apache http server, in particular i'm trying to model how the cpu load can change in function of keepalive parameter. when keepalive grows the cpu load should decrease.\n\nso:\n\n$$cpu_{k+1} = a \\cdot cpu_k \\sqrt{(1-cpu_k)}~~~ + b \\cdot cpu_{k-1} ~+ c \\cdot keepalive_k$$\n\nobviously the cpu load is a scalar $\\in [0,1]$ , $keepalive$ is just a time and the $a,b,c$ parameters are known to me through experimental data and multiple regression on them.\n", "tags": "pid", "id": "1356", "title": "can i use ziegler-nichols's rules to find pid parameters for a non linear system"}, {"body": "while looking up information for the right propellers for my quadcopter, i realized that they had different orientations i.e. clockwise and counterclockwise. on further research i found that all multi-rotors have different combinations of these orientations. so my question is why? how does it matter if the propeller is turning clockwise or anti-clockwise?   \n", "tags": "quadcopter multi-rotor", "id": "1367", "title": "prop orientation on a multirotor"}, {"body": "i want to find the instantaneous center of rotation of a differential drive robot.\n\nassuming i know that the robot will travel with a particular linear and angular velocity $(v,w)$ i can use the equations (given at a path following a circular arc to a point at a specified range and bearing) which come out to be:\n\n$$x_c = x_0 - |\\frac{v}{w}| \\cdot sin(\\theta_0)$$\n$$y_c = y_0 - |\\frac{v}{w}| \\cdot cos(\\theta_0) $$\n\ni'm using the webots simulator and i dumped gps points for the robot moving in a circle (constant v,w (1,1)) and instead of a single $x_c$ and $y_c$ i get a center point for every point. if i plot it out in matlab it does not look nice:\n\n\n\nthe red points in the image are the perceived centers, they just seem to trace the curve itself. \n\nis there some detail i am missing? i'm really really confused as to what's happening. \n\ni'm trying to figure out the center so i can check whether an obstacle is on this circle or not and whether collision will occur. \n", "tags": "mobile-robot motion", "id": "1369", "title": "instantaneous center of rotation for a differential drive robot"}, {"body": "this question stems from previous question, where i asked why does the prop orientation matter so much for a multirotor. but on further research&dagger; i found that these reasons need not apply to a tri copter. and then again. why? \n\nare these reasons general for all multi rotors with odd number of motors? or even rotors?\n\n&dagger; this forum talks a lot about tricopters and prop orientations but nothing really answers the question. \n", "tags": "multi-rotor", "id": "1370", "title": "prop orientation for tricopters"}, {"body": "using ardupilot software (fixed wing, arduplane), i know that after i boot up i need to keep the system sit still while the gyros initialise.\n\nwhen i have ground station in the field it's easy to know when it's safe to launch because the telemetry message tells me. but i don't always fly with a ground station. in these situations i currently just sit and wait for a while before arming, then again before launching. \n\nis there some reliable rule of thumb? information in the blinking of the arming switch or buzzing that i haven't worked out yet? this uav has px4 autopilot hardware (with both px4fmu and px4ioboard), including with buzzer and illuminated arming switch. the leds on the board are obscured (but i could make light channels from them if required).\n\n(note: i'm asking this question here to test the theory that robotics stack exchange night be an appropriate forum for these sorts of questions, which has been suggested a couple of times in response to the area51 drones proposal.)\n", "tags": "uav", "id": "1378", "title": "how to tell when an ardupilot has finished initialising its gyros (without referencing telemetry)?"}, {"body": "i'm robotic engineer, using openscad to model robotic components (gears, pulleys, parts, etc). but i need an application to model the physics and interaction of the components (for i.e. how will robot move if i rotate a given gear).\n\nso, is there any software i can use for modelling interactions in linux? google sketchup is good, but i can't use it in linux.\n", "tags": "software", "id": "1383", "title": "software for robot parts interaction modeling"}, {"body": "i am trying to write a c code for a pan-tilt unit model ptu-d46 using visual studio 2010 in windows 7, but i can't find any tutorial or reference on how to do so. all the user's manual mentions is that there is a c programmer's interface (model ptu-cpi) available, but it doesn't say where to find it nor how to use. i looked for it on google but couldn't find anything.\n\nthere is a command reference manual along with the user's manual, but it only shows the different commands to control the tilt and does not explain how to make a c program that connects to the tilt controller and sends queries to it. \n\ndoes anyone please have an idea of where i should look or if there are any open source programs for that. i'm not trying to make a complicated program. i just need it to connect to the tilt controller (the computer is connected via usb cable to the host rs232 of the tilt controller) and makes it nod to say \"yes\" and \"no\" !\n", "tags": "c", "id": "1388", "title": "programming pan-tilt unit with c"}, {"body": "does anyone have experience with this ez-b, it is sold by ez-robot.com and comes with an sdk for visual studio\n\nit has direct scripting in runtime and through usb or bluetooth, wifi, irc, https\n\nmy question is, if i get a regular arduino board, will i be able to do the same?\nfrom what ive read, arduino needs to hold the instructions on its own memory, but i rather have the brain in the computer, and feed signals back and forth to the microcontroller\n\nalso, is arduino alone, a step down as the website niceley puts it\n\nthanks for your help in advance\n", "tags": "arduino microcontroller", "id": "1391", "title": "creating a robot from ez-b or regular arduino"}, {"body": "i am using a lsm303 sensor to compute a heading and i want to turn my robot to a heading.\n\ni have the simple code here:\n\n\n\nin a function called with a speed and angle (heading), the trex part tells the motor controller to turn on a point and the while loop should test for when the desired heading is reached. however, testing using a couple of instances of  i have determined that once inside the while loop, mag never changes which just means the robot turns indefinitely. \n\ni have no idea why this would happen. perhaps someone here does?\n\nthanks.\n", "tags": "arduino", "id": "1392", "title": "why does my lsm303 magnetometer reading not change in a while loop?"}, {"body": "i'd like to know if anyone has had success detecting a warm-bodied mammal (ie. human) using standard off the shelf, inexpensive sensors?  \n\nideally, i'd like to use an inexpensive sensor or combination of sensors to detect a person within a room and localize that person. i would like the robot to enter a room, detect if a human(s) is/are present and then move to the detected human. the accuracy does not need to be 100%, as cost is more of a factor. i'd like the computational requirements of such a sensor to be such that it can run on an arduino, although if it's impossible, i'd be willing to utilize something with more horespower, such as a raspberry pi or a beaglebone black. i have a few thoughts; however, none of them are ideal:\n\n\npir sensor - can detect movement within a large field of vision (ie. usually 120 degrees or more).  might be the closest thing to a \"human\" detector that i'm aware of; however, it requires movement and localizing/triangulating where a person is would be very difficult (impossible?) with such a large field of vision.\nultrasound - can detect objects with good precision.  has a much narrower field of view; however, is unable to differentiate between a static non-living object and a human.\nir detectors - (ie. sharp range sensors) can again detect objects with great precision, very narrow field of view; however, it is again unable to differentiate objects.\nwebcam + opencv - possibly use face detection to detect human(s) in a room.  this may be the best option; however, opencv is computationally expensive and would require much more than an arduino to run.  even on a raspberry pi, it can be slow.\nkinect - using the feature detection capabilities of kinect, it would be relatively easy to identify humans in an area; however, the kinect is too expensive and i would not consider it a \"cheap\" solution.  \n\n\nperhaps someone is aware of a inexpensive \"heat-detector\" tuned to body heat and/or has had success with some combination of (#1-4) above and would like to share their results?\n", "tags": "sensors sensor-fusion", "id": "1393", "title": "what is the cheapest / easiest way of detecting a person?"}, {"body": "i am working on a robotic application under r.o.s. groovy galapagos.\ni would like to make a tutorial about how create a template app with catkin_create_qt_pkg.\ni'm unable to call the script  from my catkin workspace.\ni found it at the root : _/opt/ros/groovy/qt_ros/qt_create/script_\nbut even if i try to execute it as sudoer i got an error.\n\n\n  importerror: no module named qt_create\n\n\ni'm unable to determine what i have to do to make it work.\n\nwhy?\n", "tags": "c++ ros", "id": "1394", "title": "qt ros tutorial issue"}, {"body": "i've worked with wiimote accelerometer, but i think now i want to move past that. mostly because i want to have a wider range of available gestures and i think that using only one accelerometer has too many limitations for what i want to do. i'm looking for something compatible with arduino or rpi. does anyone have recommendations on how i should do this?\n", "tags": "control sensors accelerometer", "id": "1399", "title": "wearable accelerometor"}, {"body": "i want to embed environmental data collected from sensors into a live video stream from a camera. has anyone done this or know how i would go about doing something like this? is there a library available for the arduino or rpi?\n", "tags": "sensors cameras", "id": "1400", "title": "stream real-time video with environmental data overlaid"}, {"body": "so i want to program something that will simply push a button, but controllable over ethernet.  i'm new to robotics so i don't know where to start.  what's the best way to control an actuator over a network connection?\n", "tags": "arduino actuator", "id": "1408", "title": "pushing buttons remotely over ethernet"}, {"body": "in general, is a raspberry pi processor powerful enough for a mobile chatbot? i want to make a small mobile robot that is like a chatbot. is a raspberry pi processor powerful enough for any type of ai robotics?\nas far as a mobile robot, i want to make a wheeled robot about one foot in every dimension. the chatbot abilities will be from programpy-sh, a new chatbot program that uses xaiml databases. the chatbot works by looking through a database for a match of the user's input (vocal or text-based). it then acts according to the instructions given by the xml-like database.\n", "tags": "mobile-robot raspberry-pi artificial-intelligence", "id": "1413", "title": "is a raspberry pi processor powerful enough for a mobile chatbot?"}, {"body": "i am currently working on a legged hexapod which moves around using a tripod gait. i have two sets of code to control the tripod. \n\nset 1: time based control\n\nin this code set, i set the tripod motor set to move at their rated rpm for a required amount of time before shifting to the other tripod motor set.\n\npid control would be based on counting the number of transitions using an optical speed encoder, calculating the error based on difference between actual speed and required speed and then adjusting the error with fixed kd and ki values.\n\nset 2: transitions based control\n\nin this code set i count to the number of transitions required to complete one rotation of the leg(tripod motor set) before starting the other leg(tripod motor set).\n\npid control would be time based. calculation of error would be the difference in time taken for individual motors of the motor set.\n\nquery:\nthe set 2 shows promising results even without pid control, but the first set does not.why so? the motors are basically set to move 1 rotation before the other set moves. \n\nwould the speed differences between the motors cause it to destabilize?\n\nhow often do i update the pid loop?\n\nmy robot seems to drag a little bit. how do i solve this?\n", "tags": "mobile-robot pid legged walking-robot hexapod", "id": "1414", "title": "pid conundrums for legged robots"}, {"body": "i'm involved in research on psychologically plausible models of reinforcement learning, and as such i thought it'd be nice to try and see how well some to the models out there perform in the real world (i.e. sensory-motor learning on a mobile robot). this is already been done in some robotics labs, such sutton's implementation of the horde architecture on the \"critterbot\". however, these implementations involve robots custom-build by robotics experts in order to deal with the trials and tribulations of learning on a long time-scale: \n\n\n  \"the robot has been\n  designed to withstand the rigors of reinforcement learning\n  experiments; it can drive into walls for hours without damage or burning out its motors, it can dock autonomously\n  with its charging station, and it can run continuously for\n  twelve hours without recharging.\"\n\n\nunfortunately i'm no expert when it comes to designing robots, and don't have access to a high quality machine shop even if i did; i'm stuck with whatever i can buy off-the-self or assemble by hand. are these constraints common enough for amateur robotics suppliers to cater to, or should i expect to have to start from scratch?\n", "tags": "mobile-robot battery reinforcement-learning", "id": "1416", "title": "building robots with high reliability, durability, and battery life"}, {"body": "how would you go about building a robot that can use a computer? type on the keyboard, move &amp; click mouse? i am talking about physically manipulating the hardware inputs, and the robot would be able to see the screen. not connected to anything. it's purely autonomous. my hope is that this will replace human qa testers.\n", "tags": "wheeled-robot artificial-intelligence robotic-arm", "id": "1417", "title": "what is the best way to go about building a robot hand that can type on keyboard, move &click mouse, swipe touchscreens?"}, {"body": "a little background of my aim\n\ni am in the process of building a mobile autonomous robot which must navigate around an unknown area, must avoid obstacles and receive speech input to do various tasks. it also must recognize faces, objects etc. i am using a kinect sensor and wheel odometry data as its sensors. i chose c# as my primary language as the official drivers and sdk are readily available. i have completed the vision and nlp module and am working on the navigation part. \n\nmy robot currently uses the arduino as a module for communication and a intel i7 x64 bit processor on a laptop as a cpu.\n\nthis is the overview of the robot and its electronics: \n\n\n\n\n\n\nthe problem\n\ni implemented a simple slam algorithm which gets robot position from the encoders and the adds whatever it sees using the kinect (as a 2d slice of the 3d point cloud) to the map.\n\nthis is what the maps of my room currently look like:\n\n \n\nthis is a rough representation of my actual room:\n\n\n\nas you can see, they are very different and so really bad maps.\n\n\nis this expected from using just dead reckoning?\ni am aware of particle filters that refine it and am ready to implement, but what are the ways in which i can improve this result? \n\n\n\n\nupdate \n\ni forgot to mention my current approach (which i earlier had to but forgot). my program roughly does this: (i am using a hashtable to store the dynamic map)\n\n\n\ngrab point cloud from kinect\nwait for incoming serial odometry data\nsynchronize using a time-stamp based method\nestimate robot pose (x,y,theta) using equations at wikipedia and encoder data\nobtain a \"slice\" of the point cloud\nmy slice is basically an array of the x and z parameters\nthen plot these points based on the robot pose and the x and z params\nrepeat\n\n", "tags": "arduino slam kinect odometry", "id": "1419", "title": "how can i improve the map in my mobile autonomous robot using kinect"}, {"body": "i need to control quadrotor from a pc, without using a joystick.\n\ni have got a mini-beetle quad v929 beetle 4-axis  and also have this nrf24l01+ wireless transceiver module chip (2.4ghz transceiver)\nis it possible to write an arduino program to make them speak to each other?\n\ni did some research and found that the quad v929 model uses flysky protocol and only works with a7105 nrf24l01 2.4ghz transmitter chip not the one which i mentioned above\n\nare there any other better ways of controlling the quad from pc or arduino board?\n", "tags": "arduino quadcopter radio-control", "id": "1427", "title": "controlling a quadrotor from a pc"}, {"body": "currently, i'm using a pan tilt bracket that works with the servos i have (spark fun product 9065), and are generic to all servos. however, servos aren't accurate enough. i'm looking at other options, but have limited knowledge about them. what would you recommend?\n\nan important consideration is that i need my motor to tilt in two dimensions, to any random $x, y$ coordinate accurately. to do this, i think i'll need some sort of attachment to the motor shaft, like the pan tilt bracket.\n\nwhat motor and bracket would you recommend i use to go to any random $(x,y)$ coordinate?\n", "tags": "servos stepper-motor motion", "id": "1429", "title": "pan and tilt bracket for a stepper motor"}, {"body": "i'm looking at laser scanners and i see a huge range of detection distances.  the furthest i've see are 30m if you exclude the very expensive ones that claim up to 150m.  my question is what is the reason for the huge difference in range/price.  i would think that with a laser it wouldn't be that difficult to detect something at distances greater than 30m.  what's the physical limitation that makes it so much more expensive to go above 30m for a laser scanner or is there one?\n", "tags": "localization", "id": "1435", "title": "laser scanner distance"}, {"body": "i haven't made or flown any quadcopter, but i am fascinated by them.\nbut when looking at the frame of a lot of designs, after whachting this video i wondered why a lot the frames are in an x shape. since the most efficient shape would according to the video be something like this >-&lt;, where each corner is 120\u00b0.\ni also did a quick search on the internet and found this blog which stated the same (however he did not mention the exact angle) and said: \"even though this is not entirely a new idea, it has not yet been widely accepted by the community.\"\n", "tags": "design quadcopter", "id": "1437", "title": "most material efficient quadcopter frame"}, {"body": "how do you know if a commercial driver is working with trapezoidal or sinusoidal commutation? if you measure the 3-phase voltage applied to the pmsm by means of an oscilloscope, will you see a difference?\n", "tags": "brushless-motor", "id": "1440", "title": "trapezoidal vs sinusoidal commutation"}, {"body": "the last robotics project i worked on involved autonomous outdoor navigation, using a microcontroller for lower-level control and a computer for image processing and decision making. i worked more with the higher-level software and another guy on the team did the electrical and embedded systems. i would like to be capable of doing everything, including stuff with embedded, but i'm not sure where to look for information. if i were to have done the project from scratch on my own, i'd need to know:\n\n\nwhat microcontroller to use\nwhat motors are required \nwhat motor controllers to get, and how to interface with the controllers\nwhat encoders to use for motor feedback, and how to write drivers for them\nwhat batteries to use and how to safely power everything \n\n\nif i were trying to learn about higher-level software, i'd probably take a few courses on udacity. are there any good resources out there like that for this kind of low-level stuff?\n", "tags": "microcontroller power embedded-systems", "id": "1443", "title": "learning about embedded systems"}, {"body": "i'm doing a project related to telemetry, and i want to make ardupilot (programmed with arduplane 2.73) send through a serial port the sensors informations such as height, gps position, etc.. i have tried to use ardustation, but i could not change its firmware to do what i want. \n\nthe idea would be to read the ardupilot's serial port using an arduino uno, and then saving it in a sd card in real-time. so ardupilot needs to send data without any user input or something like that. i've already tried to manipulate arduplane source code to do that, but i couldn't either.\n\nhas someone here did something like that before? i need some advice!\n", "tags": "arduino ardupilot", "id": "1445", "title": "how to get data from ardupilot through serial port"}, {"body": "i am trying to implement a line following robot that can solve mazes similar to the pololu robots you can watch on youtube. my problem is the maze that i am trying to solve is looped and therefore simple left/right hand rule can not solve the maze. i have done some research and think either flood-fill or breadth-first-search algorithm will be able to solve these looped mazes. solving the maze is reaching a large black area where all the sensors will read black. when the robot is following the line some of the sensors will read white and the central ones black.\n\nis there any other algorithms that can solve looped mazes? i understand how the flood-fill algorithm works but am unsure how it could be implemented in this situation.\n\nmy robot has 3 sensors on the bottom. the one in the centre is expected to always read black (the black line it follows) and the sensors on the right and left are expected to read white (while following a straight line) and then black once a junction or turn is reached. my robot has no problem following the line, turning etc. i need to find an algorithm that can solve looped mazes (that is, find a path from an entrance to an exit).\n", "tags": "mobile-robot motion-planning line-following routing", "id": "1448", "title": "maze solving algorithm for mazes with loops"}, {"body": "i'm trying to understand how noise is represented for accelerometers, gyroscopes, and magnetometers so that i can match the requirements of my project with the standard specs of these sensors. \n\ni want the output of a 3 axis inertial sensor to be values in meters/sec/sec, gauss, and radians/sec for each axis, and noise to be represented by a range around the true value (so x as in +/- x m/s/s, gauss, and radians/sec) for accelerometers, magnetometers, and gyroscopes respectively. switching out gauss for teslas, meters for feet, radians for degrees, etc. would all be fine.\n\nafter looking at a few datasheets, i'm surprised to find that... \n\n\naccelerometer noise is measured in \"lsb rms\" and \"\u03bcg/\u221ahz\"(https://www.sparkfun.com/datasheets/sensors/accelerometer/adxl345.pdf, http://dlnmh9ip6v2uc.cloudfront.net/datasheets/sensors/accelerometers/mma8452q.pdf)\ngyroscope noise is measured in \"\u00ba/s-rms\" and \"\u00ba/s/\u221ahz\" (https://www.sparkfun.com/datasheets/sensors/gyro/ps-itg-3200-00-01.4.pdf)\nmagnetometer noise is measured in \"\u00b5t rms\" and \"gauss/\u221ahz\" (http://dlnmh9ip6v2uc.cloudfront.net/datasheets/sensors/magneto/mag3110.pdf, http://www.vectornav.com/products/vn200-rug?id=54)\n\n\nwhat do these units mean, and how do they (or can they) translate into what i want?\n", "tags": "imu accelerometer gyroscope noise magnetometer", "id": "1449", "title": "how are units of noise measurement related to units of a sensor's data measurement?"}, {"body": "i want to do a project where i control motors, and the arduino seems ideal for that. however, i'm going to need to do quite a bit of processing on the back end first. connecting the arduino to a computer isn't an option, because it has to be quite mobile. does anyone have any experience controlling arduino with raspberry pi?\n", "tags": "arduino raspberry-pi", "id": "1450", "title": "how would i go about interfacing the raspberry pi and arduino?"}, {"body": "i was wondering if it is possible to plug a motor shield on top of an ethernet shield, even though the direction pins on the motor shield would be connected to the same pins as the spi bus.  i was thinking that it would work if, in the coding, i disabled both chip selects on the ethernet shield before i used the motors.\n", "tags": "arduino", "id": "1454", "title": "motor and ethernet shields together"}, {"body": "tl;dr:\ncan anyone point me to a good adaptive path fill algorithm?\n\nhey there, my name is james, and my daughter built an awesome painting robot with her friends over at evil mad scientist labs, even brought it to the white house, very fun stuff!\n\nanyways, i'm a web programmer by day, and though it might be fun to try and write some software to get the robot to do some cool stuff, and for some crazy reason i decided to do this using standard web technologies like svg and javascript, and.. it actually works! [see the project at github.com/techninja/robopaint]\n\nbut there's a problem: to fill in colors for given shapes, it requires some kind of path filling algorithm using a given size shape to cover every part of the shape internally, not to mention it has to take into account overlapping paths and occlusions.\n\ni have successfully created \"fake\" fills, by following known paths like spirals and back and forth hatch lines over paths, while detecting occlusion using browser internal functions for detecting what object lies at a given x/y coordinate, but these fill functions fall incredibly short of doing anything other than simply following paths, and can be incredibly inefficient at filling certain paths (like filling borders, letters, or u shaped areas).\n\nthe question: i need an adaptive path filling algorithm. i know they're currently being used in similar cnc setups for milling, and similar algorithms are used by the roomba and 3d printers to figure out coverage in the most efficient way possible. the issue comes in that i don't think any have ever been done in javascript, using native svg paths.\n\nanyone out there know where i should look? i'm not too afraid to attempt to port something over to js, or possibly even use it as is for a native node.js module. all my work will be sure to go back to the community and become open source as well.\n\nthanks for the help!\n", "tags": "algorithm motion-planning motion cnc", "id": "1455", "title": "help with adaptive fill algorithm for water color painting robot"}, {"body": "3d cad:\n\n\nafter some editing:\n\ni'm building the frame myself from aluminum and polycarbonate.\n\nedit: all frame is polycarbonate except of the arms (and all screws &amp; nuts &amp; washers)), they are form aluminum covered in polycarbonate. the total length (from one motor to another) is exactly 60cm\n\napproximate mass: 1.736 kg\n\nwhat i thought to put in:\n\n\nbattery: zippy flightmax 4000mah 3s1p 20c (changed from this 1500mah battery)\nmotors: turnigy aerodrive sk3 - 2822-1090kv\npdb: hobby king quadcopter power distribution board\nesc: hobbyking ss series 18-20a esc\ncore: arduino nano\nimu: invensense mpu-6050\npropellers: 10x4.5 sf props 2pc standa## heading ##rd rotation/2 pc rh rotation (changed from 8x4.5 ones)\n\n\ni'm gonna program it myself (or atleast try to).\n\ni want to use it as a uav (self controlled)\n\nquestions:\n\n\nis this a good setup?\nit costs to much for me, any way to keep the frame and change the\nparts to be cheaper and yet a good quadcopter?\ndoes the propeller fit the motor?\nany place to buy these for cheaper?\nis it a good frame?\nam i missing some parts?\n\n\nthanks alot,\ndan\n\nedit:\n\n\ni checked it using this site and it gave no errors (guess its a good sign).\nthe camera on the image is just for the picture (no camera is intended there).\n\n", "tags": "arduino motor quadcopter", "id": "1457", "title": "is this a good quadcopter build?"}, {"body": "if a motor has more power (w) from another it means that it is better?\n", "tags": "motor quadcopter power", "id": "1463", "title": "does more \"power\" (w) means better motor?"}, {"body": "just out of curiosity, can the concept of a quadcopter be applied to an rov? would it work the same way underwater as it would be in air? if not what kind of modifications it would take to implement that idea, underwater?\n", "tags": "quadcopter", "id": "1466", "title": "quadcopter application underwater"}, {"body": "i am interested in starting a robotics club at my high school next year, since we don't have one, and i want to know what the pros/cons are of common kits. i already have a vex kit since my uncle is one of the regional suppliers for indiana, but i'm not sure that vex is the best choice. what are the pros/cons of the other kits out there?\n\nnote: i looked at lego mindstorms, and have used them when i was learning robotics, but do not want them for this. also, i own a raspberry pi.\n", "tags": "raspberry-pi mindstorms beginner children", "id": "1475", "title": "pros/cons of common robotics \"kits\" at the high school level"}, {"body": "i'm trying to find the optimal-time trajectory for an object (point mass) from point  with initial velocity  to point  with velocity . i'm considering a simple environment without any obstacles to move around. the only constraint is a maximum acceleration in any direction of  and optionally a maximum speed of .\n\nthis is easy to work out in 1d but i struggle with the solution 2d and 3d. i could apply the 1d solution separately to each dimension, but this would only limit acceleration and speed in a single dimension. the actual acceleration might be larger in multiple dimensions.\n\nare there any textbook, closed-form solutions to this problem? if not, how can i solve this in a discrete-event simulation?\n", "tags": "control motion-planning", "id": "1483", "title": "optimal-time trajectory in 2d and 3d with simple constraints"}, {"body": "i'm endeavoring to prototype a challenging sorting mechanism inside a fridge and would appreciate any constructive tips on how to get from the specs to a plausible design.\n\nproblem\n\nthe aim of the game is to identify and sort food items in the limited space of a fridge\n-   such that a user would push their unsorted shopping into a chamber at the top of the enclosure\n-   and the machine inside would then try to identify the contents with help of bar-codes (first big problem)\n-   and then sort and move the items according to their identities into different chambers below (second big problem). \n\nsolution?\n\nare there any existing devices that already serve such functions (automatic bar-coding and sorting), the designs of which could perhaps inform the mechanics of the device i'm planning to construct?\n\n\ni'm thinking maybe manufacturing plants\nor packing factories with conveyor belts etc may use systems that already solve such problems? \nor filtering mechanisms in candy dispensers,\nmechanized lifting forks? \ntextbook engineering mechanisms?\n\n", "tags": "arduino motor sensors cameras", "id": "1489", "title": "automatic sorting with barcode identification inside a refrigerator"}, {"body": "it seems popular to build robotic systems with separate supplies for the electronics and motors/servos, but what would be involved in a circuit that powers both from one source?\n\nif it helps, here is my particular situation. i want to use a single 7.4v li-po pack to power an embedded system (2.5-6v operating voltage input, max 1a current draw) and up to 18 standard-sized servos (hs-645, 4.8-6v operating voltage). this would be a hexapod, so it is unlikely that all servos would be stalling at once with normal gait patterns.\n", "tags": "electronics servos power legged", "id": "1492", "title": "single power source for electronics and actuators"}, {"body": "my team developed a filling machine for liquids which uses a piston to measure and deliver volumes 0.1 to 1 liters into bottles. it is built mostly with mechanical parts and we'd like to replace most of them with electronic ones.\n\nhow do i build a machine to pull liquid from a reservoir and fills a bottle using a piston, with electronic parts such as stepper motor, linear actuators and sensors?\n\ni understand this is somewhat vague. any aligned response is appreciated.\n\nupdate:\n\nthis machine should, at its max speed, fill a 1 litter bottle with water in 2 seconds (to deliver 30 bottles per minute). higher viscosity liquids may take longer.\n\nit should not spill so liquid needs some filling acceleration control.\n\nyou may assume two operation modes: with bubbles and without bubbles. the first is a plus.\n\ni'd like to be able to change the volume electronically (via a lcd menu).\n\ni thought of a single main valve that switches between the reservoir and the bottle. that should be controlled electronically too. i could use two valves too.\n", "tags": "sensors stepper-motor mechanism", "id": "1494", "title": "how to build a liquid filling machine using piston?"}, {"body": "i'm looking for an electrical explanation of the statement here:\n\n\n  warning: do not connect power (red + &amp; black -) from the rc10 (a10) &amp;\n  rc11 (a11) connectors to the servos, just use the signal lines. power\n  the servos via the pwm outputs connectors. these solutions will avoid\n  the scenario that can possibly happen when the pan/tilt servo draw too\n  much current and cause the apm to brownout (reset)\n\n\ni examined the apm2.5 board drawing and schematic here and the grounding is a little unclear. on the schematic, they all just go to gnd, but on the board drawing some of the ground pins appear unconnected to traces. i checked for continuity, and there is no continuity between the pwm grounds and the a10/a11 ground pins. by the way, my power setup is that i have j1 enabled and i am using an esc to power the board.\n\ncan anyone figure out, electrically, what is between these two sets of ground pins?\n\nground pins appear unconnected to the traces:\n\n\n\npwm ground just connected to gnd:\n\n\n\nanalog output ground also connected to gnd:\n\n\n", "tags": "power ardupilot", "id": "1496", "title": "on the ardupilot board, what is the difference between a10 / a11 ground pins and the pwm ground pins?"}, {"body": "i am building a robot where power density is critical, and looking into using lithium thionyl chloride (socl2) batteries. i will be drawing around 20a constantly and need between 12 and 17v. the batteries i have found so far are aa-sized and designed to deliver 100ma, 3.6v, 2.4ah, and weigh 19g each. i could picture a pack with 4 blocks of these batteries in series, where each block has 20 batteries in parallel. that would mean 48ah, which is way more than the ~10ah that i need, and 1.52kg, which is more than i would like the robot be carrying.\n\nso, the question is, is there a way to achieve 10ah at 20a and 14.4v (i.e. for 5 hours) using socl2, carrying much less weight than 1.52kg?\n", "tags": "power", "id": "1500", "title": "lithium thionyl chloride batteries to generate 20a @ 14.4v"}, {"body": "are there any python3 modules used to program robotic movement by declaring a device or component and then providing the instructions? i am not looking for modules that test the components.\n", "tags": "python", "id": "1503", "title": "python3 modules for motor movement"}, {"body": "so i built three little bots so far. \n\none with a raspberry-pi, (6v motors), one with an arduino (12v motors), and another with an arduino but a hacked remote car (7ish, volt motors):\n\n\n\n\nthe problem i have with all these is that the motors spin so fast the car just bumps into something in a few seconds. (i have a small house)\n\ni tried to use pwm to control the speed, but at a little less than full throttle (255) they jam up and they can't take the weight i put on them.\n\nshould i buy one of these chassis that slow the motor down and give it torque with a gearbox, or is there anything else i can do?\n", "tags": "motor pwm", "id": "1514", "title": "reducing motor speed without jamming up"}, {"body": "i'm looking for a way to create a non-rotating persistence of vision device. i have all the electronics set up but i'm stumped with the mechanical design.\ni tried these two designs:\nbut these didn't work so well. everything shakes violently and it doesn't go nearly as fast as i need (about 20 swipes per second)\nany ideas on how i can build this?\n", "tags": "motor mechanism", "id": "1515", "title": "building a non rotating persistence of vision device"}, {"body": "i have two kinects (each on its own usb card) whose cameras i'm watching in rviz through openni, and the structured light pattern of one is flashing intermittently - it's only there for a short flash every two seconds. obviously, the structured light depth calculations only work if the pattern is always projected when the camera is looking at it.\n\nwhat causes this, and how do i fix it?\n\nedit: i was also having this issue with a single kinect, for which i discovered the issue, as detailed in my own answer. however, the problem persists when two kinects are plugged in, with one kinect flashing and the other functioning normally.\n", "tags": "ros kinect openni", "id": "1517", "title": "light pattern is flashing intermittently using rviz/openni with two kinects"}, {"body": "if a street is extremely crowded to an extent that the terrain is not visible from the point of view of the lidar (e.g. in google's self driving car), can it still manage to localize itself and continue to operate? i recall sebastian thrun saying that google's car cannot navigate through snow filled roads since the onboard lidar cannot map the terrain beneath the snow (e.g. here).\n\n[edit : based on the comments] clarifying the context, here \"not visible\" means there is an obstacle between the lidar and the terrain\n", "tags": "ugv lidar", "id": "1518", "title": "how do visual obstructions impact the ability to localize using lidar?"}, {"body": "does anyone here know px4 software? i'm using eclipse to program it and i want to open a new uart door and write on this, i'm doing the following commands...\n\n\n\nbut this is not working! anyone have any idea why?\n", "tags": "microcontroller communication", "id": "1519", "title": "px4 communication"}, {"body": "i'm working on building a line follower robot and want to optimize its performance. it was suggested that i use a pid algorithm. i read a lot about pid but am confused a bit regarding following:\n\ni've calculated the error_value using $k_p * proportional + ...$\nbut regarding the change in the motor speed i'm confused as to what to use during comparison the difference (i.e. currentposition - setpoint) or the errorvalue. that is should i use \n\n\n\nor \n\n\n\nalso is there any specified range for the values of the constants $k_p$, $k_i$ and $k_d$?\ni'm using a differential wheeled robot for my line follower.\n\nalso i would be happy if someone suggests me any other advanced optimization algorithm for improving the line follower robot.\n", "tags": "arduino pid line-following", "id": "1524", "title": "line follower optimization"}, {"body": "i have built a particles filter simulator and i wanted to add the following functionalities.\n\n\nlimited range vision (robot can see up to 50 meters)\nlimited angle vision (robot can see within a certain angle w.r.t its current orientation.  e.g. if the current orientation is 30 degree then it can see in the range from 0 to 60 degree.)\n\n\ni have managed to add the limited range vision functionality but unable to add limited angle vision.\n\nmethod to sense the landmarks distance within the range\n\n\n\n\n\nmethod to calculate the probability of this particle\n\n\n\ncoordinates are relative to the robot and for distance calculation i am using the euclidean distance method and my robot gets localized correctly. \n", "tags": "localization particle-filter visualization", "id": "1525", "title": "how to implement bounded angle vision in particle filter?"}, {"body": "i was considering using a raspberry pi controlling a usb relay to dim 12v led lights, but i'm having trouble finding a solution that isn't a simple on/off.  what type of device would i need for dimming?\n", "tags": "raspberry-pi", "id": "1530", "title": "how do you dim 12 volt leds?"}, {"body": "i am interested in using this miniature motor (squiggle micro motor) to create very tiny horizontal movements. however, due to very limited space, i can only place it vertically within my project.\n\nassuming this motor is placed as follows, how can one adapt it to simultaneous movement at a right angle? (ideally with the x-axis movement matched to the y-axis movement as well as possible.)\n\n", "tags": "motor motion movement driver linear-bearing", "id": "1538", "title": "how to convert vertical motion to horizontal"}, {"body": "i have no problem in reading circuit schemes, connecting wires, resistors etc. (basing on for example instructables.com), but i've only tried to learn java (a while ago) and it's difficult to me to find out what's going on with all this c-code-stuff.\nare there any tutorials that are focused on the programming part?\nthanks\n", "tags": "arduino", "id": "1544", "title": "arduino c/c++ progamming tutorials"}, {"body": "many robot applications (actually - the most practical and appealing ones) include the robot's reaction to (and impact on) the evironment, e.g. due to stochastic nature of the environment (e.g. when robot should handle non-rigid objects like clothes) or due to the variability of environment (e.g. harvesting robots should be prepared to pick fruits of different sizes and shapes).\n\nthe question is - is there a robotics simulator that can simulate not only robot but also the environment as well? e.g. that can simulate the response of robots action on cloth folding or fruit picking and so on. i guess that such simulator is really non-trivial but maybe there is some ongoing project for it?\n", "tags": "simulator industrial-robot", "id": "1547", "title": "how can i simulate a changing environment with non-rigid objects?"}, {"body": "hy, i just found useful to post my idea here. \n\ni've seen videos about automated quadcopters: http://www.ted.com/talks/raffaello_d_andrea_the_astounding_athletic_power_of_quadcopters.html and http://www.ted.com/talks/vijay_kumar_robots_that_fly_and_cooperate.html. \n\ni surfed pages from the companies presenting this research and other information on the internet, but i haven't found why they use quadcopters specifically. \ni understand, how accelerating, rotating and rolling works in those quadcopters - it's simple, but they claim that quadcopters have minimum number of working parts to fulfill their needs, which i don't agree and i think that tricopters are better in this (duocopters can't rotate horizontally, but tricopters can by inclining and then powering the remaining left or right propeller). \n\ni rode forums, calculated and draw drafts of both tri and quad and found, that tri is much more efficient just in everything than quad with same props and battery when taken in account that best properties has the smallest copter with the largest props so: \n3:4 moving parts (no vectored yaw in tri), 9.5:16 size, building y instead of x construction take far less material 1.5:2.82, lesser maximum power input 3:4, better power efficiency makes longer flight time and tricopters have also improved agility over quadcopters. \n\nthe only disadvantage i see is a bit complicated horizontal rotating in tricopter without vectored yaw, which can be problem in man controlled machines but easily solved by simple algorithms in automated machines -> it's not a real disadvantage, just a small work to be done. i was thinking about doing that in my bachelor thesis, but for now i am looking for your opinions, thanks!\n\nedit: maybe the torque is the problem, because on tricopters you can can have all 3 props in 1 direction or 2 in 1 direction and 1 in the opposite and it's symmetrical in neither way, but i'm not sure if this is the main problem...\n", "tags": "quadcopter multi-rotor", "id": "1552", "title": "use of automated tricopters instead of quadcopters?"}, {"body": "if this has already been answered, by all means please point me to it.\n\ni am in the process of building a quadcopter which i eventually plan to run autonomously by allowing it to track an object and take a video feed of it moving.\n\ngps is one of the options i've considered, basically:\n\n\ngps antena on moving object (person, car, bike, surfer)\ngps antena on quadcopter\nradio to transmit coordinates from moving object to quad copter\n\n\nsome of the challenges i can foresee are\n\n\nline of sight for camera. how does the camera know exactly where to point?\nangle, how can i pre-program the quad to always record, say... 10m to the right of the moving object, or even better, program a set of angles to record from whilst keeping up with the object\ngps accuracy, what happens if the gps lock is weak?\n\n\nwhat are some of my other options? i saw this ted talk where the quads are following a ball shaped sensor? i believe it uses kinect cameras and lots of them which is not really an option for this challenge.\n\nso i'm open to hearing some ideas before i start research and development of these features.\n", "tags": "arduino quadcopter gps", "id": "1554", "title": "track a moving object"}, {"body": "an answer to the question why do quadcopters have four propellers? (besides the name)  said:\n\n\n  you need 4 degrees of freedom to control yaw, pitch, roll and thrust.\n  \n  four props is therefore the minimum number of actuators required. tricoptors require a servo to tilt one or more rotors which is more mechanically complicated.\n\n\nin a comment, i asked:\n\n\n  how do you get pure yaw motion with a quadcoptor and if that's possible why won't this work with a tricoptor? i don't understand how can you get yaw motion with any system where all rotors are in a plane without first tilting and moving. i would have thought that the main difference between quadcopters and tricoptors would be the kinematic calculations would be more complex.\n\n\nanother answer explained:\n\n\n  you get pure yaw in the following way:\n  \n  north and south motors rotating the same speed but collectively at a higher (or lower) speed than east and west motors which are also at the same speed. \n\n\nthis explains why it works with a quadcopter, but doesn't explain why it won't work with a tricopter.\n\nis it simply the fact that the asymmetry means that you can't imbalance the torque effects to provide yaw movement while still keeping the thrusts balanced to keep pitch and roll constant?\n", "tags": "design quadcopter uav", "id": "1562", "title": "if you can create pure yaw motion with a quadcoptor then why won't this work with a tricoptor?"}, {"body": "i am trying to launch a file from remote computer but i could not success. actually i can connect to remote computer but i think the problem is with including a file from remote computer. in other words, i am looking for a machine tag for include. here is the my code:\n\n\n", "tags": "ros", "id": "1564", "title": "roslaunch include file remotely"}, {"body": "for a dc motor with a stall current of 950 ma, what should the h-bridge's current rating be?  what will happen if we use our h-bridge l293d whose max. output current is 600 ma?\n", "tags": "motor electronics h-bridge", "id": "1567", "title": "h-bridges and stall current"}, {"body": "i found a good explanation on how to remove accelerometer bias (when on flat table only one axis should show values, the other two should be 0). i've calculated s and b factors (page 3):\n\n\n  record $b_x^{0g}$, $b_y^{0g}$, $b_z^{0g}$, $s_{xx}$, $s_{yy}$, and $s_{zz}$ in eeprom or flash memory\n  and use these values in all subsequent calculations of acceleration to\n  get the corrected outputs.\n\n\ni don't know how to incorporate these into the final calculation of accelerations. i guess the bias should be substracted from my sensor reading. what about sensitivities (s)?\n", "tags": "accelerometer calibration errors", "id": "1570", "title": "accelerometer bias removal"}, {"body": "for halloween, i'd like to build a spider that drops from the ceiling when it detects motion, and then rewinds itself back up to scare the next kids. i already have a lightweight foamish spider from hobby lobby that i'd like to use, but i need help adding the smarts to it to scare the kids.  \n\nideally it'd be able to detect how tall/far away the kid is to drop to a custom height each time, but i'd settle for a standard dropping height if that's too much. i even had an idea of having a motion sensor and having it shoot silly string webs in front of people. \n\ni have a very technical background, but i'm a total n00b when it comes to robotics, so ideas as to what components and considerations i'd need would be greatly appreciated!\n", "tags": "motor rcservo", "id": "1571", "title": "how to raise/drop a spider"}, {"body": "i've already asked a related question (accelerometer bias removal) here on robotics and got a bit better results on corrected accelerometer output. to get even better results i found the calibration equations (7th &amp; 8th paragraph) from vectornav which are just a bit enhanced than the solution in the linked question:\n\n \n\nhowever, six more variables are needed:\n\n\n  sensitivity of sensor x-axis to y-axis inputs ($m_{xy}$)\n  \n  sensitivity of sensor x-axis to z-axis inputs ($m_{xz}$)\n  \n  sensitivity of sensor y-axis to x-axis inputs ($m_{yx}$)\n  \n  sensitivity of sensor y-axis to z-axis inputs ($m_{yz}$)\n  \n  sensitivity of sensor z-axis to x-axis inputs ($m_{zx}$)\n  \n  sensitivity of sensor z-axis to y-axis inputs ($m_{zy}$)\n\n\nbelow it is also stated:\n\n\n  ieee-std-1293-1998 [...] provides a detailed test procedure for\n  determining each of these calibration parameters\n\n\nhowever, after searching through the 1293-1998 standard (especially page 201 in google docs) i didn't find any clue on how to calculate the $m$ values. also, $b_{d}$ and $v_x$ values from vectornav equations is not explained anywhere. can someone point me further?\n", "tags": "sensors accelerometer calibration errors", "id": "1579", "title": "accelerometer calibration - how to get cross-axis sensitivities"}, {"body": "i have a project which requires a robot to move around a room with a flat surface (concrete floor).\n\nthe robot must carry a laptop. i estimated that the total weight would be 6-7kg (including motors, battery, laptop, motor controller board and other mics items). i would like it to move at about the same speed as a roomba moves. the robot will have two motors and a castor.\n\ni have tried doing the calculation to determine the type of motor to use, but i'm very confused.\n\ncan someone advise me on the type of motor and type of battery (lipo/sla) to use?\n", "tags": "motor wheeled-robot battery", "id": "1581", "title": "choosing motor and battery for a robot"}, {"body": "i'm starting to attempt to fly fpv on my quadrotor. i am using a frsky d8r-ii 2.4 ghz frequency hopping diversity receiver (two antennas) for control and recently added a no-name 910 mhz 1.5 watt analog video transmitter for fpv flying:\n\n\n\nwhen the video transmitter is powered up, my control range drops from about 1.5km to under 50m. i'm surprised that the 910 mhz video channel affects my 2.4 ghz control channel this way.\n\nis this sort of interference expected? is it because my transmitter is low quality? what changes are recommended&nbsp;\u2014 should i switch to a uhf control radio? or a different frequency (eg 5.8 ghz?) for the video radio? or just try moving it a few more inches (they are already about 5in apart)?\n", "tags": "multi-rotor", "id": "1582", "title": "interference between 900 mhz video transmitter and 2.4 ghz control radio"}, {"body": "our robot has a circular array of 12 sonar sensors that looks like this:\n\n\n\nthe sonar sensors themselves are pretty good. we use a low-pass filter to deal with noise, and the readings seem pretty accurate. however, when the robot comes across a flat surface like a wall, something weird happens. the sonars don't show readings that would indicate a wall, instead, it appears like a curved surface. \n\nthe plot below was made when the robot was facing a wall. see the curve in the blue lines, as compared to the straight red line. the red line was produced by using a camera to detect the wall, where the blue lines show filtered sonar readings. \n\n\n\nwe believe this error is due to crosstalk, where one sonar sensor's pulse bounces off the wall at an angle and is received by another sensor. this is a systematic error, so we can't really deal with it like we would with noise. are there any solutions out there to correct for it?\n", "tags": "sonar sensor-error", "id": "1584", "title": "how to deal with sonar crosstalk"}, {"body": "i want to compute an existence probability of an object in a sensor fusion on the high level (having from each sensor list of objects already filtered with e.g. kalman filter).\n\nthere are these formulae:\n\n$$lr(g)_{old} = \\frac{p(ex_{out})_{old}}{1 - p(ex_{out})_{old}}$$\n$$\\alpha = \\frac{p(ex_{in})_{old}}{p(ex_{in})_{old}*(1 - p(ex_{in})_{new})}$$\n$$lr(g)_{new} = lr(g)_{old} * \\alpha$$\n$$p(ex_{out})_{new} = \\frac{lr(g)_{new}}{1 + lr(g)_{new}}$$\n\nwhere $p(ex)$ is the probability of existence and $lr$ is the likelihood ratio.  \n\nthe idea is that $p(ex)_{in}$ is some probability existence of local object, which was fused into the global $p(ex_{out})$, and its probability influences that global one.  $old$ would mean values from previous cycle. \n\nhow do you condition that computation to avoid situations of dividing by zero, obtaining , or ? also, if $p(ex_{in})_{new}$ is almost 1, then $\\alpha$ will be huge, increasing output, and increasing it enormously in each later cycle, so that the object will live forever. how to prevent it?\n", "tags": "sensors kalman-filter sensor-fusion", "id": "1589", "title": "existence probability of an object in fusion"}, {"body": "what is the name for the transfer function, gh, in a simple feedback control system like\n\n$$y=\\frac{g}{1+gh}u$$\n\nwhat do you call g? what about (g/(1+gh))?\n\ni am confused by the fact that that  \"open-loop transfer function\" and \"loop transfer function\" are used to mean gh by different people. which is academically correct or widely accepted?\n\nthere are several terms: \"closed-loop transfer function\"\n\"open-loop transfer function\"\n\"overall transfer function\"\n\"loop transfer function\"\n\"loop gain\"\n\"loop ratio\"\n\nthanks\n", "tags": "control", "id": "1590", "title": "what is the name for the transfer function, gh"}, {"body": "i'm quite new to mechanical engineering and not familiar with gears and motors. a few days ago, i bought a second-hand gws servo motor for my project, and it didn't include gears.\n\ncan someone help me understand the correct measurement of my motor so i can buy the correct gear to fit on my servo motor. specifically, the measurement of the gear hole:\n\n\n", "tags": "rcservo", "id": "1595", "title": "measuring gears for servo motors"}, {"body": "i am software engineering student and don't know much about hardware.\n\ni recently have started a project in my ai course. the project is playing a 3x3 tic-tac-toe game between computer and a human. \n\nin a tic-tac-toe board, suppose you play first and you put a cross mark in a certain place.(in tic-tac-toe board your position is (3,1)).\n now my computer will take a picture with webcam and analyse this picture with help of opencv(it is a open source c++ library for image processing. for details opencv.org). after finding the position of your cross mark it will find the optimal position with the help of my algorithm at which it will put a circle. for output it will just speak out the position.\n\nnow, i want to make a robotic hand that can draw this circle as output. would anybody please help me to find the hardware and other materials that needed to make that robotic hand? it will be very helpful if anybody suggest me some tutorials.\n\ni googled and found many many suggestions, but i am confused about what to choose. \n", "tags": "robotic-arm", "id": "1598", "title": "making a robotic arm that can draw a circle"}, {"body": "i want to give my robot a differential mechanism for the system of turning and steering. considering the case of turning a right-angled corner, the robot will achieve this by following a gradual circular arc through the intersection while maintaining a steady speed. to accomplish this end, we increase the speed of the outer wheel while slowing that of the inner. but supposing i want the turn to be within a definite radius, how do i calculate what ratio the 2 speeds have to be in? can someone give me an insight into this? \n\nwhat ive done is this, although i have my doubts.\n\nif the speed of the right wheel is $v_r$ and the speed of the left wheel is $v_l$, then the ratio of their speeds while turning will be equal to the ratio of the circumferences of their corresponding quadrants.\n\ntherefore\n$$v_r :v_l =\\frac{r+a}{r}$$\n\nis this right? i have a sinister feeling im missing something out..\n\n\n", "tags": "kinematics", "id": "1602", "title": "how to design a differential steering mechanism?"}, {"body": "i am thinking of creating a robot that can navigate using a map. it is controlled from a pc. an 8-bit controller performs low level tasks and the pc is doing the image processing. i plan to implement it in a single room where the robot is placed and the robot and environment are tracked by a camera from a height or from the ceiling of the room. first, the robot needs to be mapped, like this http://www.societyofrobots.com/programming_wavefront.shtml\n\nto do:\n\n\ntrack the robot from some height  using camera following the\nwavefont algorithim to locate robot and obstacles.\n\n\nprocedure:(just my idea)\n\nthe camera will give image of the robot surrounded by obstacles in the random places.\n using some opencv technique draw some grind over the image. \n\n\nlocating the  grid which contain  robot(by having some colored\nsymbol over the robot) and locating the grids containing the \nobstacle.\nnow the grids with obstacle is thought as wall and the remaining is\nthe free space for the robot to navigate.\nrobot is going to get  the goal place which should be reached is\ngiven from the pc(may be like point the place to reach in the image\nby mouse click).\n\n\nunknowns :\n\n\nmapping the room and locating the robot\n\n\nhow  to do that? the robot should know where it is in the map or the image. we cannot believe only the camera is enough to locate the robot. so i thought of adding triangulation mapping like placing two irs in the room and a receiver in the robot.\n\nthe doubt i have in this is how an ir receiver can know from which direction it is receiving the ir signal (from left or right ). i think it knows only that it receives ir not the direction. then how is the triangulation going to happen if i don't know the angle and direction?\n\n\ncoming to the image processing, how can i implement the wavefront\n   algorithm(that is capture the live vedio and draw grids over it to\n   find robot and the obstacles)?\n\n\ni have hc-05 bluetooth module, arduino, bluetooth dongle, chassis with dc motors and driver, and a dc supply.\n", "tags": "algorithm mapping", "id": "1603", "title": "how to implement the wavefront algorithm"}, {"body": "i'm moving from controlling a robot arm with basic time based motors controlled using a raspberry pi and python to a more advanced robotic arm that has servos (e.g hs-425bb). with the time based motors i must constantly keep track of the arms position (guess work) and make sure it doesn't over turn.\n\ndo servos automatically stop if you give them a position that is outside of their boundaries rather than grinding the gears?\n", "tags": "rcservo", "id": "1607", "title": "do servos stop at their limits automatically?"}, {"body": "i'm a newbie to electronics/ robotics, but i love to do it as a hobby. \n\nso i want to build a circuit (a really small in size would be much better) with a motion sensor that can communicate data (basically when it sense a motion send a signal) to my computer over wifi. is this something possible? \nif so how do i do it, may be a schematics diagram, or someway to start the project would be a grate help.\n\nthank you!!  \n", "tags": "sensors wifi communication circuit", "id": "1610", "title": "how to make a motion sensor circuit, that can communicate with my lan?"}, {"body": "i am building a 2 wheel robot to carry 7kg of load. i used this link to get the torque required for each motor which is 11kg-cm. so i choose 2 of these motor which has 36kg-cm(2x more) torque. i noticed that the stall current is 14a. \n\ncan i use adafruit motor shield for arduino which is rated 3a peak current capability?\n\nif not what current rating driver should look into?\n\nthanks!!\n", "tags": "motor driver current", "id": "1613", "title": "motor driver selection"}, {"body": "we are using arduimu (v3) as our quadrotor's inertial measurement unit. (we have a separate board to control all motors, not with arduimu itself). \n\nnow we have a problem with arduimu's sensors output. when we put our quadrotor steady on the ground with motors on, instead of getting 0 degree in roll and pitch we have a noisy output something like the image below( -6 to 6 degree error ):\n\n\n\ndelta_t = 0.2s\n\nwe are sure that this isn't a mechanical problem, because we checked the mechanical joints and everything.\n\ni should mention that with motors off everything is going well. also we checked that if we vibrate the device slowly on yaw axis or any other axis, it still shows the noisy output.\n\nwe are using dcm filter inside arduimu, also we tested with kalman filter but no difference.\n\nwe also tested fri low-pass filter, results is good but there is about 3 seconds delay in the output.\n\nwe also checked that if we separate the arduimu's power from our circuit, it still no difference.\n\nwhat's the problem with arduimu and how we can get rid off this noisy output ?\n\nupdate: \nwe think that the problem with our pid controller is because of these noises ... is this a true assumption ? we can't tune our pid parameters ( using ziegler\u2013nichols method ) when we have noisy data. we tested ziegler\u2013nichols method when we have low rate noises and we successfully tuned our pid but when noise appears we are unable to tune pids. is there anyway for us for tuning our pid in such situation ? is this problem is because of the noises or the pid itself can get rid of them ?\n", "tags": "arduino quadcopter noise ardupilot", "id": "1614", "title": "arduimu noisy output in quadrotor"}, {"body": "i was at a robotics conference earlier today and one of the speakers mentioned robots not being able to function as well in a crowd because they can't single out audio like a person can.  \n\nwhy can people single out audio so well?  and what would it take for a robot to do the same?\n\ni'm aware of active noise reduction (anr) like on bose aviation headset, but that is not what i'm talking about.  i am thinking about the ability to take everything in but process only what you feel is important. \n", "tags": "artificial-intelligence", "id": "1615", "title": "why can humans single out audio in a crowd? what would it take for a robot to do the same?"}, {"body": "what is the minimum amount of power that a beaglebone needs to start up? this would be with no   peripherals attached besides host usb. the getting started guide claims that it can run off of a computer's usb power, but makes no mention of how many amps are actually needed. i saw a mention of older kernels limiting current draw to .5 amps when working off of usb, although that was all i could find. \n\ncould one start a beaglebone black off of .3 amps? if not, how many?\n", "tags": "microcontroller", "id": "1618", "title": "beaglebone black power draw"}, {"body": "how would you motorize the joints in an iron man suit? you need something fairly shallow, i would think, probably dual servos sitting on either side of an elbow or knee joint or either side of your hips, but how do you get motorized action there without dramatically adding to the thickness of the joint? \n\nbicycle-style chain drives wouldn't work, i would think, since the length of the chain would need to vary depending on what position you're in for at least a lot of joints.\n\nhow would you motorize the joints?\n", "tags": "design mechanism", "id": "1622", "title": "how do i motorize the elbow socket and other joints in a powered exo-skeleton?"}, {"body": "i've been thinking about starting a quadcopter project, maybe building it from scratch. one of the main barriers-to-entry for me is the motors: it seems like most quadcopters use brushless motors. i have some experience with dc motors and using pwm signals to regulate speed, but no experience with brushless motors. as i understand it, brushless motors are more expensive than the typical dc motor i would use on a land robot, and they also require electronic speed controllers (escs), which seem to make them (from my perspective) even more expensive and more complicated to use.\n\nso, my question: what is it about brushless motors that make them useful in a quadcopter? is it more torque, less weight, something to do with efficiency? and would it be significantly harder (or even possible) to achieve lift using dc motors instead? \n", "tags": "quadcopter brushless-motor", "id": "1627", "title": "why do quadcopters use brushless motors"}, {"body": "i have some crude time based motors taken from a robot arm that we upgraded to proper servos. i want to be able to power a conveyor belt with one of them and i was wondering how i would go about the following setup:\n\na ball drops through a hole onto the conveyer belt hitting a lever switch on its way through. this switch triggers the motor to start. when the ball gets to the top of the belt and falls off it hits another lever switch that turns the motor off.\n\ni could handle this logic by hooking it up to my raspberry pi and using python to start and stop the motor depending on which gpio pin received input (top or bottom lever). or i could use a single lever and set a constant time interval to stop the motor. i would prefer to use both to handle any change in scale/construction.\n\ni was wondering however if this could be done with the breadboard alone, using logic gates or similar?\n", "tags": "motor raspberry-pi", "id": "1630", "title": "controlling a conveyor belt with a time based motor"}, {"body": "whenever i try using , it works normally, however, when i try viewing an image using the kinect's rgb or depth camera, or even recording a simple  with data from the kinect, i am unable to see any picture and  does not record any data, and after a few seconds of running  or , i got this error:\n\n\n\nafter searching around and trying various fixes, i figured it might be a problem with openni and started using freenect, however i encountered the same problems, i could not record any data using bagfiles or see any images from the kinect (using  or )\n\nthen someone asked me to use something completely unrelated, , however that too gave me a black screen.\n\n shows that all 3 parts of the kinect are connected and i've been able to control the kinect's motor through ubuntu so i know that there is at least a connection established between both.\n\nadditional info:\n\n\ni run ros on ubuntu using virtualbox v.4.2.14 and windows 7 with usb 2 ports\ni am using ubuntu 12.04 and ros-groovy (all up to date)\ni've had the exact same errors on my mac osx lion\nwhen i try using rviz with the kinect, virtualbox crashes all together\n\n\ni would appreciate anyone's help on the matter.\n", "tags": "ros kinect", "id": "1633", "title": "how can i get data from my kinect?"}, {"body": "im designing a differential steering mechanism for my robot. supposing my robot is going in a straight line and i want it to change it direction by a certain angle( $\u03b8$ in the diagram). what should the velocity ratio be of the 2 wheels so that it gradually turns and starts moving along a line that is $\u03b8$ degrees to the initial line of movement?\nif there's any ambiguity in the question please take a look at my earlier question which is similar. how to design a differential steering mechanism?\n\n", "tags": "kinematics", "id": "1634", "title": "how to control velocity ratio when turning angle is \u03b8?"}, {"body": "i am a graduate student trying to make my own line follower robot for my minor assessment, i've all hardware parts and all data-sheets with me, i've attended a workshops of robotics and studied a lot on line follower robot. i have a good knowledge of c programming and embedded systems, but the problem is i've a very limited amount of time(2 days).\n\nplease help me to suggest a good paper work about my project - line follower robot, where should i start from ? i am getting myself confused should i start from programming or should i first do circuit simulations as i know it is not a better approach to use directly hardware. \n\nplease suggest me a fine paper work or some links/videos so that i can make my robotics projects fast. any help would be really appreciated, thanks.\n", "tags": "mobile-robot", "id": "1637", "title": "paper work before i do make my own line follower robot"}, {"body": "i have been looking online for a while, and i cannot seem to find any steppers without any torque ratings. (operating torque, not holding torque.) i even looked on hobby sites that usually have all the ratings, including adafruit and sparkfun. i only have found one ever that said the operating torque, however, it didn't seem that reputable and it didn't have holding torque, so it might be likely that it's a mistake. i might contact them and ask.\n\nam i missing something? can i calculate what it will run at with certain factors? (how long in between each step, etc.)\n\nthe reason that i say that is i found a tutorial saying how much torque (didn't specify which kind, but i kinda assume it isn't holding) you need for a cnc machine (what i'm building).\n\n\n\nequation (from this site):\n\n\n\nalso on the page:\n\n\n  by the way, we are talking about torque during a continual turning motion, not at a holding position.\n\n\nthat seems like operating torque, but what makes it the most confusing is they sell steppers and they only list the holding.\n\n\n\nwhat am i missing?\n", "tags": "stepper-motor cnc", "id": "1638", "title": "why are there no operating torque specifications on steppers?"}, {"body": "i need  my robotic arm to ring a desk bell. i one on the maplins site  usb robotic arm.\n\nit does seem very slow.  what can i hack on it to boost the downwards and upwards speed. i need it hit the bell tip/platform quickly once or twice.\n\n\n\nthis is purely a lol project for work. ever time we get an order we want the arm to ring the bell. :)\n\n\n\n-edit\n\nthis is the gearbox assembly - and it much much to slow - what can i change in here to speed up one gearbox by at least 4 times?\n\nthe grabber gearbox is different though. the gear marker p7 is white and seems to move the grabbers at a faster speed.\n\n\n", "tags": "robotic-arm", "id": "1642", "title": "how to speed up robotic arm?"}, {"body": "on my stepper's datasheet, it has the category \"rotor torque\" (labeled in n-cm). what does that mean? is this the torque it has can supply when turning? (hopefully)\n", "tags": "torque stepper-motor", "id": "1644", "title": "what is rotor torque?"}, {"body": "i have a apm 3dr quad with a 3dr radio telemetry kit.  i would like to send real-time sonar data to my laptop (running windows 7) in order to manipulate it in an additional arduino sketch.  \n\nthe sonar sensor is connected to an analog in channel on my arduino. that data is processed for altitude calculations, and i would like to send this altitude data to some sort of ground station on my computer through the use of a telemetry kit (2 3dr radios: 1 on the quadcopter and 1 on my computer).\n\ni am not quite sure how to go about this task.  is there a way that i can modify the source code (gcs.h or gcs_mavlink.pde) in conjunction with mission planner mav 1.0 ground station to do this?  or would i need to write a python module to accomplish this?  \n", "tags": "quadcopter python sonar", "id": "1649", "title": "how can i manipulate real-time sonar data from my arducopter in arduino?"}, {"body": "how do you calculate or update the position of a differential drive robot with incremental sensors?\n\nthere is one incremental sensor attatched to each of the two differential wheels. both sensors determine the distance $\\delta left$ resp. $\\delta right$ their wheel has rolled during a known time $\\delta t$.\n\nfirst, let's assume the center between both wheels marks the position of the robot. in this case, one could calculate the position as:\n\n$$\nx = \\frac{x_{left}+x_{right}}{2} \\\\\ny = \\frac{y_{left}+y_{right}}{2}\n$$\n\n\"deriving\" those equations under the assumption that both wheels rolled in a straight line (which should be approximately correct for small distances) i get:\n\n$$\n\\frac{\\delta x}{\\delta t} = \\frac{1}{2}\\left( \\frac{\\delta left}{\\delta t} + \\frac{\\delta right}{\\delta t}\\right)cos(\\theta) \\\\\n\\frac{\\delta y}{\\delta t} = \\frac{1}{2}\\left( \\frac{\\delta left}{\\delta t} + \\frac{\\delta right}{\\delta t}\\right)sin(\\theta)\n$$\n\nwhere $\\theta$ is the angle of orientation of the robot. for the change of this angle i found the equation\n\n$$\n\\frac{\\delta \\theta}{\\delta t} = \\frac{1}{w} \\left( \\frac{\\delta left}{\\delta t} - \\frac{\\delta right}{\\delta t}\\right)\n$$\n\nwhere $w$ is the distance between both wheels.\n\nbecause $\\delta x$ and $\\delta y$ depend on $\\theta$, i wonder whether i should first calculate the new $\\theta$ by adding $\\delta \\theta$ or if i should rather use the \"old\" $\\theta$ ? is there any reason to use one over the other?\n\nthen, let's now assume the center between both wheels does not mark the position of the robot. instead i want to use a point which marks the geometric center of the robot's bounding box. then $x$ and $y$ change to:\n\n$$\nx = \\frac{x_{left}+x_{right}}{2} + l\\, cos(\\theta)\\\\\ny = \\frac{y_{left}+y_{right}}{2} + l\\, sin(\\theta)\n$$\n\n\"deriving\" the first gives:\n\n$$\n\\frac{\\delta x}{\\delta t} = \\frac{1}{2}\\left( \\frac{\\delta left}{\\delta t} + \\frac{\\delta right}{\\delta t}\\right)cos(\\theta) - l\\,sin(\\theta)\\,\\frac{\\delta \\theta}{\\delta t}\n$$\n\nnow there is a dependance on $\\delta \\theta$. is this a reason to use the \"new\" $\\theta$ ?\n\nis there any better method to do simulatenous update of position and orientation? may be using complex numbers (same approach as with quaternions in 3d?) or homogeneous coordinates?\n", "tags": "mobile-robot kinematics motion two-wheeled forward-kinematics", "id": "1653", "title": "calculate position of differential drive robot"}, {"body": "what is the difference between a robot and a machine? at what point does a machine begin to be called a robot?\n\nis it at a certain level of complexity? is it when it has software etc?.\n\nfor instance: a desktop printer has mechanics, electronics and firmware but it is not considered a robot (or is it). a roomba has the same stuff but we call it a robot. so what is the difference.\n\ni have always believed that a robot is a robot when it takes input from it's environment and uses it to make decisions on how to affect it's environment; i.e. a robot has a feedback loop.\n", "tags": "industrial-robot", "id": "1654", "title": "what is the difference between a robot and a machine?"}, {"body": "i would like to know if there are any other solutions to implement slip compensation into a half-size micromouse other than the conventional method. i have spoken to a few japanese competitors, and they told me that the only solution they have to such a problem is creating a table of predetermined values and using these values to increase or decrease the before turn/after turn distances. the values used are determined by the mouse's intelligence. due to the fact that this method has too many limitations, i would like to hear more suggestions from people who are familiar with this matter.\n", "tags": "micromouse", "id": "1658", "title": "implementing slip compensation into a half-size micromouse"}, {"body": "i am working on a homemade vending machine project that serves milk and cookies, using arduino and some basic servos and stuff.\n\nthe problem is: i really have no clue on how to protect the milk to last long, or how to even know if the milk is still ok to drink.. all i really know is that air is bad for the milk (and the cookies), so here is what i came up with:\n\n\n\ntwo solenoids that activates at the same time, to allow air in, and milk out. all of this should be inside a \"slightly\" colder place.\n\ni'm sure this design might sound stupid to some of you, but this is where i need your help please, do you think this design can work ? (would that solenoid on top make any difference to protect milk?) how to improve it to make the milk last as long as possible ?\n\ni'v heard about the big guys making machines that keep milk fresh for weeks even months, while i'm probably sure my milk won't stand a couple of hours..\n\nany idea or any information, link, or clue would be greatly appreciated. thank you.\n", "tags": "arduino", "id": "1660", "title": "how to protect the milk in a homemade vending machine?"}, {"body": "i have a box (cuboid) lying on floor or table. so there are 6 surfaces of the box and 1 surface of the floor. if i take each pair of surface such that the surfaces are \"adjacent\" to each other, i get two kind of pairings:\n\n1) two surfaces of the box: the surface normals of the surfaces diverge from each other.\n\n2) 1 surface of the box + surface of the floor : the surface normals converge and intersect at an angle of 90 degrees. ( 8o to 100 degrees, if we want to add some tolerance).\n\ni want to distinguish these two cases by representing through a function? what function can distinguish between these two situations?\n\nin both cases, the the normalized dot product of the surface normals is 0, since the angle b/w them is 90 degrees. so this is not the right solution...\n", "tags": "kinect computer-vision machine-learning", "id": "1666", "title": "classify if two adjacent surfaces belong to same object"}, {"body": "i\u2019m trying to inject some kind of rubber around an aluminum strut to form \u201cfeet\u201d for a robot. i\u2019ve already milled the mold, but i\u2019m having trouble finding an inexpensive and readily available rubber compound that will cure without exposure to air. ideally it should cure to about the consistency of a silicone o-ring. i\u2019ve tried silicone gasket-maker (the automotive stuff), however a week later it hasn\u2019t cured in the mold, as there is no exposure to the air.  is there anything out there with a similar consistency to silicone, but doesn\u2019t require air to cure?  or is there a way to get what i\u2019m currently using to set up without waiting a millennium?  there aren\u2019t any real mechanical requirements, i\u2019m just trying to clean up the look of the robot and prevent its legs from scratching my table. \n", "tags": "cnc", "id": "1670", "title": "moldable rubber for \"feet\""}, {"body": "i'm trying to control the speed of this motor, with this motor driver and pic16f690. \n\npwm in my program\n\n\n\nstarting with 50% duty cycle the motor doesn't run.\nbut moving from say, 80% to 50% (i.e. program my pic with 80% duty cycle, and then re-program it with 50%), the motor will run at 50% (of course at a lower speed). i consider this funny. anyone to explain this?\n\nmy motor powered by 5v.\n", "tags": "motor", "id": "1676", "title": "funny behaviour or what - dc motor control"}, {"body": "i made a small crawler robot a little while ago that had two legs with two degrees of freedom each, so 4 rc servos total. while i was programming the movement of the legs i noticed that they moved rather stiffly. it makes sense that the rc servo's internal controller would have a very quick response to position commands, but i wanted my crawler to move in a way that seems a little more smooth and life-like.\n\nmy solution was create a cubic function of time that describes the path of the servos, and then set their position in small time increments, resulting in more smooth motion. essentially what i did was solve for the $a_i$ coefficients in a cubic equation using the time interval, starting and ending position of the servo, and starting and ending rates the servo should move (which is just the derivative of the position):\n\nsolve for $a_0$, $a_1$, $a_2$, and $a_3$:\n\n$$ position(t) = a_0 + a_1t + a_2t^2 + a_3t^3 $$\n$$ rate(t) = position'(t) = a_1 + 2a_2t + 3a_3t^2 $$\n\ngiven: $position(0)$, $position(t_f)$, $rate(0)$, $rate(t_f)$\n\ni set the rate of the servo between a pair of movements to be zero if the movements were in opposite directions, and positive or negative if the movements were both in the positive or negative direction, respectively. \n\nthis worked pretty well, but this solution is limited in a few ways. for one, it's difficult to decide what exactly the rates between movements that go in the same direction should be. i used the average of the slopes ahead and behind of a particular position between movements, but it isn't clear to me that is optimal. second of all, cubic curves could take the servo to a position outside of the range of the positions at the beginning and end of a movement, which may be undesirable. for example, at some point during the time interval, the curve could cause the servo to go beyond the second position, or below the first position. thirdly, curve generation here does not consider the maximum rate that the servo can turn, so a curve may have the servo move at a speed that is unrealistic. with that, a minor concern is that the maximum turning rate depends on the response of servo's internal controller, and may change depending on the size of the position interval. \n\nneglecting that last concern, these issues may be solved by increasing the degree of the polynomial and adding constraints to solve for the coefficients, but i'm now starting to wonder...\n\nis there a better way than this to make servo movement smooth and seem more life-like?\n", "tags": "servos kinematics", "id": "1678", "title": "smooth servo movement for a crawling robot"}, {"body": "i am working on a robotics project with c++ (drawing signs on board), on crs catalyst5 arm.\n\ni have faced a problem:\n\ni have many methods move in different directions, gotolocalizations, etc, but the problem is that when i run many of them in main without sleep() function between each function they does not run properly. i think that the first one needs time (the time of robot movement) but when i put sleep(10000) between them (i guessed that 10 seconds are enough for the movement) all is ok. this is very ineffective and slow solution. would you like to give me some solutions to avoid the use of sleep ?\n", "tags": "activerobot", "id": "1680", "title": "management of asynchronous commands"}, {"body": "i am in the concept phase of a driving robot. the two wheels on the front axle will be powered, while the rear will be dragged along. the rear is also responsible for steering but this has noting to do with my question.\n\nsince the robot is required to make relatively sharp turns at high speed. therefore i have two options to compensate the different speeds on both sides. on the one hand, a differential gear in the front axle could be used. it would be powered by one motor then. on the other hand, i could simply use two motors directly powering each a front wheel. this way i could simulate the differentiation in software.\n\ni'd like to go for the first approach, using the hardware differential. but i have the one concern with it. would a robot vehicle with differential gear still move straight, without explicit steering applied?\n\nmy imagination is that, with those wheels not being solidly connected, the robot would move in random curves which i'd have to compensate with a lot of steering then. i know that for real cars, differential gears are standard and do their work, but now i am talking about a small robot measuring about 6 inches.\n", "tags": "motor wheeled-robot motion wheel", "id": "1682", "title": "does a vehicle with defferential gear still move straight?"}, {"body": "i've got an arm attached to a shaft. the arms dimensions are 40x5 inches the arm weights about 10 lbs.\n\nif i have a wind acting on the side of the arm, how would i translate the wind force into torque on the shaft?\n\nto give some more information, i'm rotating the arm using a stepper motor, and i would like to know how to size the motor depending environmental conditions.\n\nwhat should my formula look like in order to arrive a required oz-in of torque given my requirements being:\n\n\ni need to be able to accelerate the arm from 0 to 12 rpm in 1.5 seconds\nthe wind speed can be as high as 30 mph\n\n\nusing the formula p = .00256 x 30^2 i find the wind pressure per square foot being 2.304\n\nusing the formula f = a x p x cd for calculating force, i get 1.389 x 2.304 x 2 = 6.4 \n\nso i know that the wind force on my arm is 6.4 lbs. but now how do i translate this to torque on my arm? \n\n\n\nsource: http://k7nv.com/notebook/topics/windload.html\n", "tags": "force torque", "id": "1684", "title": "wind force impact on torque mechanical arm"}, {"body": "i'm going to be embarking on an autonomous robot project and i was going to be using gps to navigate to waypoints (i'm aware of the margin of error when it comes to localization with gpd but i live in a lovely area with many open fields). \n\ni was going to use adafruit's ultimate gps breakout board with my raspberrypi, and i was wondering how i should protect or mount the gps to protect it from the elements. do all gps units need to be face up and unobstructed (ex. wood or plastic) in order to work? if so, how can i still protect a gps unit from the outdoors?\n", "tags": "gps protection coverage", "id": "1687", "title": "can gps modules work inside plastic enclosures?"}, {"body": "for the dagu wild thumper 6 wheeled platform, or any multiple motor system, do i really need 1 battery for each motor? or should i just buy 2 for either side of the platform. in addition, for larger motors like the ones on this platform, how do i deal with the power generated from a coasting motor?\n\ni want to jump into the deep end with robotics, as i already hold all the programming skills, and i realize a platform of this magnitude may be a difficult endeavor.\n\nrecommended motor voltage is 2 \u2013 7.5 volts, so should one use two 22 volt batteries for the left and right side, or six 7.5 volt batteries?\n", "tags": "motor battery", "id": "1693", "title": "do 6 motors require 6 individually-assigned batteries?"}, {"body": "i'm in the process of writing my own simple quadcopter controller for experimental use, and i'm having trouble getting my head around how to convert from the degrees which my pid controller demands to an appropriate 1k-2k range for pwm output. for example, take the roll axis on a '+' configured 'copter (pseudo-code):\n\n\n\nhow do i take the roll demanded by my pid controller and convert it to a value useful to the motors, that is to say, where does  come from? my first instinct is to use a simple linear relationship, i.e.:\n\n\n\nhowever this seems far too simplistic to work.\n\nor should i be doing more calculations before everything goes through pid control? any help would be great. \n", "tags": "pid motion multi-rotor", "id": "1695", "title": "help with pid \"units\" in a quadcopter control system"}, {"body": "i bought this mpu-6050: link\n\naccording to the manufacture site, the sensor logic level is 3.3v (though the ebay page says )\n\nshould i use a 4 channel bi-directional logic level converter (like this one) for the  channels? or can i connect it directly to my arduino nano?\n\ni saw some places that says i should use it with a logic level converter and some who say it's ok without it. (i guess it depends on the sensor board, so please take a look, link above)\n\ncurrent setup:\n\n\n\ni still don't have the parts so i can't test it, and i'm probably going to use jeff rowberg library to communicate with the sensor (i2c)\n", "tags": "arduino sensors quadcopter logic-control", "id": "1697", "title": "mpu-6050 + arduino nano - logic converter or not?"}, {"body": "i have a usb webcam and a wifi module which it can convert serial data to wifi and vice versa.\n\nthe question is can i simply convert the data coming from the webcam to serial with a usb to serial ic (like ft232r ) and then hand it over to my wifi module?\n\nupdate:\n\nthe wifi module datasheet is here\n", "tags": "cameras wifi usb", "id": "1707", "title": "build an wifi ip camera with webcam"}, {"body": "consider a differential drive robot that has two motorized wheels with an encoder attached to each for feedback. supposed there is a function for each dc motor that takes a float from -1 to 1 and sets the pwm signals to provide a proportional amount of power to that motor. unfortunately, not all motors are created equal, so sending each motor the same pwm signal makes the robot veer left or right. i'm trying to think about how to drive the robot straight using the encoders attached to each motor as input to a pid loop.\n\nhere's how i would do it: i would take the difference between the left and right encoders, bound the error between some range, normalize it to be from [-1, 1], and then map it to the motor powers 0 to 1. so if i and d were zero, and we get an error of 1 (so the left motor has turned much more than the right motor), then left motor would be set to 0, and the right motor set to 1 (causing a hard left). \n\nare there any issues with this? what is a better approach?\n", "tags": "pid differential-drive", "id": "1711", "title": "approach to using pid to get a differential robot driving straight"}, {"body": "i am working on a robot with focus on speed. at the moment i am looking for a suitable motor but it world help if i understood the difference between the various options.\n\nto provide some background, i have not worked with rc model components before, but i think this is the only place for me to find the components needed for my robot, such as the motor.\n\ni have already figured out how much power the motor needs to accelerate my robot as desired, taking energy conversion efficiency and tractional resistance into account. it's about 170 watts, depending on the final weight.\n\nto limit my search further, i need to decide on either using a rc car motor or a rc helicopter motor now, but i don't understand the difference between these options.\n\nfocussing on brushless motors (if that matters), what are the differences between rc car and rc helicopter motors which might need to be taken into account when choosing between them?\n", "tags": "motor brushless-motor", "id": "1712", "title": "what is the difference between rc motors for cars and helicopters?"}, {"body": "i know that the complementary filter has the functions of both lpf and hpf. but i think my understanding on the principal behind it is still unclear.\n\ni am quite new on digital signal processing, and maybe some very fundamental explanations will help a lot.\n\nsay i have a complementary filter as follows:\n\n$$y =a\\cdot y+(1-a)\\cdot x$$\n\nthen my parameter $a$ may be calculated by $$a=\\frac{\\text{time constant}}{\\text{time constant}+\\text{sample period}}$$\nwhere the $\\text{sample period}$ is simply the reciprocal of the $\\text{sampling frequency}$.\n\nthe $\\text{time constant}$ seems to be at my own choice.\n\nmy questions:\n\n\nwhat is the theory behind this calculation?\nhow do we choose the $\\text{time constant}$ properly?\n\n\nnote: i also posted this question on stack overflow, as the answers there are likely to be slightly different in emphasis.\n", "tags": "gyroscope magnetometer", "id": "1717", "title": "how to determine the parameter of a complementary filter?"}, {"body": "need a way to dispense micro liter amounts of water (lets say 1-10ul). only thing i've found is piezoelectric dispensers and they are >$100. any suggestions? \n\ni can build, but preferably would be an off-the-shelf component.\n", "tags": "electronics", "id": "1724", "title": "off-the-shelf micro fluid dispenser"}, {"body": "i'm working on a robotics project, and i am using grayscale sensors to automatically follow a black line: turning 90 degrees, going round in a circle, and passing through gaps in the lines etc.  i was wondering what is an effective way to detect the colours and move it through the lines, with five or six grayscale sensors.  \n\nthank you very much.\n", "tags": "mobile-robot sensors automatic line-following", "id": "1728", "title": "what is an effective distribution of grayscale sensors on robot"}, {"body": "i'm doing some groundwork for a project, and i have a question about the current state of slam techniques.\n\nwhen a slam-equipped device detects an object, that object's position is stored. if you look at the point cloud the device is generating, you'll see points for this object, and models generated from it will include geometry here.\n\nif an object is placed in a previously-empty space, it is detected, and points are added. subsequent models will feature geometry describing this new object.\n\nhow does the device react if that object is removed? as far as i've seen, slam systems will tend to leave the points in place, resulting in \"ghost\" geometry. there are algorithms that will disregard lone points caused by transient contacts, but objects that remained long enough to build up a solid model will remain in the device's memory. are there any systems that are capable of detecting that previously-occupied space is now empty?\n", "tags": "slam", "id": "1729", "title": "how do slam algorithms handle a changing environment?"}, {"body": "i'm trying to put together a simple simulation for a delta robot and i'd like to use forward kinematics (direct kinematics) to compute the end effector's position in space by passing 3 angles.\n\ni've started with the trossen robotics forum delta robot tutorial and i can understand most of the math, but not all. i'm lost at the last part in forward kinematics, when trying to compute the point where the 3 sphere's intersect. i've looked at spherical coordinates in general but couldn't work out the two angles used to find to rotate towards (to e(x,y,z)).\ni see they're solving the equation of a sphere, but that's where i get lost. \n\ncan someone please 'dumb it down' for me ?\n\nalso, i've used the example code to do a quick visualization using processing,\nbut the last part seems wrong. the lower leg changes length and it shouldn't:\n\n\n", "tags": "kinematics forward-kinematics", "id": "1730", "title": "how to correctly compute direct kinematics for a delta robot?"}, {"body": "i want to use my atmega8 uc as a h-bridge.\n\ncan anybody give me the source code using c, so that the microcontroller acts as an h-bridge.\n", "tags": "microcontroller c h-bridge avr", "id": "1737", "title": "h-bridge using atmega8 microcontroller"}, {"body": "i am using sim900a for some purpose and want to know the number of the sender from where a message comes. i am unable to find the specific at command related to receiving message which give me number from where latest message comes.\n\ni had used at+cnmi (it corresponds to notification regarding latest received message), but am unable to find sender number.\n\ni had seen at+cmgl=&lt;stat&gt;[,&lt;mode&gt;] will give you a string which will have oa i.e. originating address and once that is stored in a string i can easily parse it out, but when i had data format of that string. need help or any suggestion if somebody can help me out with any other possible solution.\n", "tags": "arduino microcontroller", "id": "1743", "title": "at command in sim900a gsm/gprs module to find out originating address of an sms"}, {"body": "i am planning to build a robot.\n\n1) what free or low cost robot modelling tools exist.\n", "tags": "design", "id": "1745", "title": "what modelling tools are available to design a robot"}, {"body": "i have been studying about building a tricopter.  but i couldn't find the design calculations or mathematical modeling of the tricopter any where over the internet. \n\nwhat are the mathematical relationships or equations of motion and forces in tricopter?  how do i calculate the requirements of the structural design and the energy requirements of the motors?\n", "tags": "design", "id": "1750", "title": "design calculations & mathematical modeling of tricopters"}, {"body": "this might be a out of league question and may seems to be very odd.i am using multiple arduino uno boards over network and want to assign a guid and serial number to each board so that when ever they send any data to a central server, server is able to find which device it is if we have assign name for each device.\n\n\nfirst way to do this is to assign guid and serial number of device before each message that is send to central server manually while programming and then burn that hex to arduino.\n\n\nnow is there any way out that we can burn a program that always give a output as a string (guid+serial number of device) like we program eeprom for this and then burn our main code in arduino which pick the guid+serial id combo from eeprom and write it before every message which arduino is pushing to central server.\n\nor my another way of asking is can we program eeprom with a different program and our arduino separately like 2 files running in parallel or is it not possible?\n\nis there any other way of doing this?\n", "tags": "arduino microcontroller communication", "id": "1753", "title": "assigning serial number and guid to a microcontroller"}, {"body": "i have several apm 2.5 boards and need to identify them based on some globally unique hardware signature that does not change with programming.\n\narduinos and atmel avr chips in general do not have (also this thread) an accessible serial number.\n\nhowever, it seems that the ardupilot has so many integrated sensors and other ics that one of them must have something unique i can use ( see schematic )!\n\ni will be checking datasheets for mpu-6000, hmc5883l-tr and ms5611, but in the meantime, if someone has already figured this one out, please answer.\n", "tags": "arduino ardupilot", "id": "1755", "title": "any globally unique signature in ardupilot hardware, or arduino in general?"}, {"body": "i'm looking for a laser / photosensor pair (or product of similar function) for detecting when a beam is interrupted (no more than 3ft apart, probably more like 1ft).\n\ni'd like these to run off of 5v, since i'm using an arduino. my main requirement, however, is that these parts have nice housings, ideally with some mounting screw holes or something along those lines. this is going into a project where sturdiness and durability are important.\n\ni don't know how to search for parts like what i am looking for. could you please point me either to some good product sources, give me some better keywords for searching, or link me directly to potentially useful products?\n", "tags": "arduino sensors", "id": "1757", "title": "laser / photosensor pair or similar"}, {"body": "we hope to build a simple line follower robot and we got a problem when we were discussing about pic programming.\n\nwe planed to write a endless loop, check the sensor panel reading and do the relevant stuff for that reading.\n\nbut one of our friends told us to use a timer interrupt to generate interrupts in definite time periods and in each interrupt check the sensor panel reading and do the relevant stuff for that reading.\n\nbut we can't figure out which is best: the endless loop in main method or timer interrupt method.\n\nwhat is the best way, and why?\n", "tags": "sensors microcontroller interrupts", "id": "1758", "title": "polling or timer interrupt?"}, {"body": "i used to think that the higher gps antenna position the better until i read the following on gpsd faq:\n\n\n  one common error is to place the gps or antenna as high as possible.\n  this will increase multipath effects due to signal bounce from the\n  ground or water, which can cause the gps to mistake its position and\n  the time signal. the correct location for a boat gps antenna is on the\n  gunwale rail or pushpit rail, close to the water and as far from the\n  mast as possible (to reduce signal bounce from the mast). if you're\n  outside or in a fixed location, put the gps antenna as far from\n  buildings as possible, and on the ground.\n  \n  if you're in a car, don't\n  put the gps antenna on the roof, put it on the towbar or some similar\n  location. if you're driving in a heavily built up area, you're going\n  to get signal bounce off buildings and reduced accuracy. that's just\n  how the physics works. note, however, that as your velocity goes up it\n  becomes easier for the convergence filters in your gps to spot and\n  discard delayed signal, so multipath effects are proportionally less\n  important in fast-moving vehicles.\n\n\ndoes anyone has experience placing gps antenna on a towbar of the car as suggested? does it give reasonable effect?\n\nmy concern is that placing antenna there will not reduce an error that much, but will expose the device (antenna) to possible mechanical damage.\n\nso, are there any better positions apart from roof and towbar?\n\nthanks\n", "tags": "gps ugv", "id": "1765", "title": "place for gps antenna on autonomous vehicle"}, {"body": "background:\n\ni am implementing a simple kalman filter that estimates the heading direction of a robot. the robot is equipped with a compass and a gyroscope.\n\nmy understanding:\n\ni am thinking about representing my state as a 2d vector $(x, \\dot{x})$, where $x$ is the current heading direction and  $\\dot{x}$ is the rotation rate reported by the gyroscope.\n\nquestions:\n\n\nif my understanding is correct, there will be no control term, $u$ in my filter. is it true? what if i take the state as a 1d vector $(x)$? then does my $\\dot{x}$becomes the control term $u$? will these two methods yield different results?\nas we know, the main noise source comes from the compass when the compass is in a distorted magnetic field. here, i suppose the gaussian noise is less significant. but the magnetic distortion is totally unpredictable. how do we model it in the kalman filter?\nin kalman filter, is the assumption that \"all the noises are white\" necessary? say, if my noise distribution is actually a laplacian distribution, can i still use a kalman filter? or i have to switch to another filter, like extended kalman filter?\n\n", "tags": "localization kalman-filter gyroscope compass", "id": "1766", "title": "how to model unpredictable noise in kalman filter?"}, {"body": "i am implementing a simple kalman filter that estimates the heading direction of a robot. the robot is equipped with a compass and a gyroscope.\n\nsay at time $t-dt$, the compass reports a reading $\\theta_{t-dt}$, and the gyroscope reports a reading $\\omega_{t-dt}$. then i assume from time $t-dt$ to $t$, the rotation rate can be regarded as a constant. thus, my current heading direction is $$\\theta_{t}=\\theta_{t-dt}+\\omega_{t-dt}\\cdot dt$$\nas can be seen, the $\\theta$ can be easily time-updated.\n\nbut what about my $\\omega$? the robot is not at my control. so its rotation rate at next moment is unpredictable.\n\nhow should i do the time update in this case?\n", "tags": "kalman-filter gyroscope compass", "id": "1767", "title": "what to do when the control input of the kalman filter is unknown?"}, {"body": "in this paper, the author says that during slam process, pseudo segments that appear from any momentary pause of dynamic objects in laser data would make the map unsatisfied.\n\nhow is this caused?  \n\nif the dynamic object moved, won't laser data update and eliminate the segment of dynamic objects?\n", "tags": "sensors slam", "id": "1774", "title": "in slam, how does a laser range finder produce pseudo-segments from dynamic objects?"}, {"body": "how can i control the position of a pneumatic piston?\n\nthe only way i know about is using a magnetic reed switch (magnetic sensor) with a matching piston and use some type of control algorithm, like pid for instance, to keep the piston where the sensor is.\n\nthe problem with that is that it gives you only limited control of the position, it just adds another \"state\" (open, closed, sensor position) and not full control. for example i want it to be 43% once and 70% the other time, but without using a sensor for each position because i would like all the \"options\" to be available (i mean that the percentages aren't pre-defined)\n\nthis is an example of the pistons i use:\n\n\n\nthis is a good example of what i want: http://www.youtube.com/watch?v=a8lz15uiuxu\n", "tags": "sensors control pid movement", "id": "1775", "title": "how to control the position of a pneumatic piston?"}, {"body": "where can i find a good documentation about the uwsim in ros. actually having the source files is not enough and it is actually hard to follow all the functions. for example, how can i use these command correctly :\n\n&amp; rosrun uwsim gotoabsoluteposition   0 0 0 0 0 0\n\ni know that there is a node 'gotoabsoluteposition' in the package 'uwsim' and i knwo the variables, but i cannot set the two topics properly.\n", "tags": "ros", "id": "1776", "title": "running uwsim commands in ros"}, {"body": "i'm building quadcopter and most of the control systems use one accelerometer and gyro. i've read few papers and usually accelerometer is used as reference to the ground because gyro slowly drifts away in time. but if quadcopter does some crazy maneuvering when force direction from accelerometer does not have to point to the ground than accelerometer data is useless. as well there is problem with centripetal force if the accelerometer is not directly in the centor of mass.\n\ni was thinking about using multiple accelerometers. to fully determine position and motion of quadcopter one would need three accelerometers(if i have done the math right). this would kind of solve the problem with centripetal force\n\nso i would like to know if anyone tried to use multiple accelerometers for better orientation estimation. \n", "tags": "quadcopter imu accelerometer gyroscope", "id": "1777", "title": "tracking with accelerometer and gyro versus multiple accelerometers"}, {"body": "so i got this idea waay back when i was in highschool as a kind of electromagnetic analogue to a biological muscle. it is basically a long stack of thin electromagnets connected in parallel. \n. \nwhen current is applied gaps between electromagnets shrink thus providing contraction of the whole chain. \n\ni am pretty sure it can work. it can't offer great contraction range (up to 50% i would guess) but it has potential to provide good speed and be compact so that multiple chains can be combined to form stong and fast linear actuators. the thing is, i never heard of this type of actuator being used. so what is the catch? is there a better alternative? is there a design flaw? too much heat generated making them unpractical?\n", "tags": "actuator", "id": "1782", "title": "actuator design. plausible?"}, {"body": "i'm trying to get the \"torobot\" usb servo controller to work with angstrom linux on a beagle board xm.\n\nthe servo controller registers as a usb device. the device just takes simple text commands, but there is no tty associated with it. so i'm not sure how to send commands to it.\n\ncan i just send data like this (assuming that 002/005 is the device):\n\n\n\nor do i need to associate it with the generic usb device? if so, how do i do that?\n", "tags": "control microcontroller rcservo usb embedded-systems", "id": "1787", "title": "how do i send text to a torobot usb device?"}, {"body": "i am a robotics enthusiast and planning to make a small and simple four wheel car whose motors are supposed to be controlled by an android device housed inside the car by means of the micro usb port of the device. the car has to move forward or backward only as directed by the signals from the android device.\n\nso my assumption is that there should be a circuit board which accepts the signals from the microusb/usb of the android device and controls the power to the electric dc motor. also the power for the motor will be supplied from a battery pack inside the car.\n\ncould anyone suggest me a cheap motor driver circuit which supports micro usb or usb? and where can i get the parts for this online? i did a lot research but very confused with the technical terms which i am not familiar with.\n", "tags": "mobile-robot motor wheeled-robot", "id": "1790", "title": "motor controller with micro usb interface"}, {"body": "i consider using a brushless outrunner motor, designed for helicopters, in my driving roboter. how can i control such a brushless motor with my micro controller? of course i'll have a separate power source.\n\nthe roboter should be able to move forwards and backwards, so i need to control the motor in a way to determine direction of rotation, too. i think this isn't related to the question, but i need to ensure high acceleration.\n\nspecially, i am talking about this motor which is listed in a german shop.\n\n\n", "tags": "motor control microcontroller power brushless-motor", "id": "1791", "title": "how to control a brushless motor?"}, {"body": "i have a 300cm x 300cm room with a 25cm high ceiling (yes twenty five centimeters). it contains 50 small wheeled robots (about 20cm x 20cm). a central computer will orchestrate the movements of the robots, using a wireless network to send position commands to them. the robots will perform their own closed loop position control to save wireless bandwidth. the robots have 32-bit arm microcontrollers. they have wheel position sensors, and the ability to do accurate wheel control. \n\nproblem: the robots can't actually do this yet because they have no way to measure their position in the room.\n\nquestion: how can the robots be given the ability measure their position and orientation to an accuracy of better than \u00b15mm? i am looking for an accurate and robust solution, which is not affected by occlusions, and doesn't require a high power pc on each robot. whatever sensors are necessary for localisation can easily be added to the robots. the microcontrollers can easily be changed for more powerful ones if the localisation solution demands it.\n", "tags": "localization wireless swarm", "id": "1795", "title": "localizing a swarm of robots"}, {"body": "outline:\n\ni'm trying to work with an arduino and analog thumb stick to get values for a simple differential drive robot i'm working on. the keyes_sjoys arduino joystick module i have in my possession is giving me some strange numbers. \n\nfollowing axises data i have:\n\n\nx-axis range of 0 to a shaky 470-520 with a center value of 40.\ny-axis range of a solid 4 to solid 1023 with a center value of 605.\n\n\nproblem\n\ni haven't used analog sensors before but it seems pretty obvious that my x-axis ranges should feel somewhat similar to the y-axis but they don't. in addition, the x-axis hits zero way way before even coming close to the edge for its operating range.\n\nis my sensor broken (it's new), or is there some way i can recalibrate the potentiometer?\n\nnote, i also asked this over on electrical engineering stack exchange.\n", "tags": "microcontroller", "id": "1797", "title": "how do i calibrate analog thumb stick?"}, {"body": "i'm trying to make differential in google sketchup using this tutorial http://support.ponoko.com/entries/21249896-gears-and-joints-with-sketchup-sketchy-physics for gears modeling.\nbut i have problem: gears don't collide with any objects (and other gears). what's wrong? how to fix this?\nhow to make a bevel gear placed at 90 degrees relative to each other and conical cylindrical gears joints?\n\np.s. is there something like sketchup and sketchyphisics in linux?\n", "tags": "design mechanism 3d-printing", "id": "1806", "title": "gears modeling in google sketchup and sketchyphisics"}, {"body": "i need to assemble a small (about 8cm x 5cm x 5cm maximum), actuator with as much torque as i can get at that size. it will be driving a small reel and pulley (reel is ~1.25cm^3, 5mm center diameter), and needs to respond to load (eg. stop if the load increases beyond a certain threshold). power to the actuator will be provided via a common bus line, so the space limit isn't further limited by the size of the battery.\n\nmy thought is to use a worm drive for this (for torque) and watch for change in current/voltage (for load), but i'm not sure if that is mechanically sound. it seems like the mechanical advantage provided by the worm would make it hard to detect a change in load.\n\nplan b\n\ni could add another sensor that would gauge the force being exerted. i'd prefer to avoid adding too many points of failure to the system, but if i did what sort of sensor would i use?\n\nhow should i approach this?\n", "tags": "sensors control actuator", "id": "1808", "title": "tiny high torque actuator/sensor design"}, {"body": "this question is further to localizing a swarm of robots.\n\nin summary: i want to create a swarm of robots which can each measure their own position in a 3x2m room with a 25cm high ceiling, to an accuracy of \u00b15mm. \n\nthere were some good answers, but most of them were optical methods. i would be interested to hear some non-optical localisation ideas, so i will impose the following further constraints:\n\n\nlocalisation method cannot use optical means, either visible or invisible.\nnothing can be added to the floor or ceiling.\nthere's no appreciable gap between the top of the robots and the ceiling.\nthere are no walls, and equipment can be added around the perimeter of the room.\n\n\ni look forward to hearing some creative ideas.\n", "tags": "wireless swarm", "id": "1811", "title": "localising a robot swarm non-optically"}, {"body": "which software can be used to prototype/design robot parts (mechanical parts, body, gears, etc)>\n\ni have some crazy idea i would like to try (quadripedal walking robot, animal-like), but i'd like to design the mechanism and test (to some degree) the mechanism in some kind of simulator before i start wasting money on parts/materials. what tool could i use for that? \n\ni'm only interested in mechanical design (chassis + servo/motor placement + cogs/gears), not in electronic design. i'm not interesting in robot control software, because i'll be probably able to slap something like arduino onto it and program behavior i want (experienced programmer)\n\ndetails (what i'd like to see):\n\n\nshould work in 3d. i.e. finished system should be viewable in 3d.\ni should be able to cut materials like plywood/metal, drill holes, place gears on it, etc.\nit would be nice if it had some kind of part catalog so to place a gear/cog i wouldn't need to design it from scratch.\nit would be nice i could test if parts can actually move. i don't need full-blown simulation, just to see if gears can turn or if they'll get stuck.\nnot interested in electronic circuitry, just need mechanical parts, but should be able to place servos.\nit would be nice if it could produce blueprints.\ncheap/inexpensive, if possible.\n\n\nbasically, i should be able to construct robot mechanism in it (by placing/connecting parts like gears,cogs, motors, springs), or some kind of clock, and test (to some degree) if it actually works.\n\ni know that i could use blender3d for that, but it wasn't exactly designed for this purpose.\n\ni also heard that solidworks could be used for designing mechanical parts, but it is too expensive, especially for one-time-project. \n\nany recommendations?\n", "tags": "software", "id": "1813", "title": "software for designing mechanical systems/robotic parts"}, {"body": "i need an equation or a some hints to solve the following problem.\n\nimagine a roller screw drive. i apply a torque of  to translative move my load mass . i assume my screw has an efficiency of 90%. now an additional axial force affects my mass in the opposite moving direction. is this force completely transformed into torque (of course considering the efficiency) or is it possible, that my whole roller screw is moving, because it is not fixed? i just found papers/books/articles for movable slides/loads, but fixed shafts. but in my case motor and shaft are part of an osciallation system.\n\ni'm not a mechanical engineer, so i'm sorry if the answer may is trivial.\n\ni made a little sketch now \n\nthe process force  is pushing my mass, most of the force is transformed into a load torque  which acts against my drive torque . some of the energy is lost by friction. the question is, if there is also a partial force  which is affecting the bearing and therefore exciting my chassis.\n", "tags": "movement torque differential-drive", "id": "1815", "title": "roller screw drive - axial movement instead of friction"}, {"body": "today i was going to buy a motor online, and saw that 10 rpm and 1000 rpm dc motors cost the same. how is it possible to change the rpm without requiring any additional parts cost?\n", "tags": "motor", "id": "1826", "title": "why do 1000 rpm and 10 rpm dc motors cost the same?"}, {"body": "i want to make a mathematical model of quadcopter in simulink. i have studied quadcopter, although i am new and not build any flying robot before. i studied so far that i have to use four brushless dc motors pid speed control, two motors will rotate clock wise and two anti clock wise. i want to make very simple mathematical model. \n\nthe input of the model will be the xyz locations on 3d space, copter will always fly from 0,0,0 path. \n\nso far i decided that i will increment the coordinates step by step for example if i want the next location of the to be x=10, y=10, z=10; then i will increment in these locations and input to a flight control block. \n\nmy question is how can i decide the speed of motors according to x,y,z next location and how to convert that speed into yaw pitch and roll and finnally convert the yaw, pitch and roll into x,y,z coordinates. \n\ni need the convertion formulas that can be easily implemented into simulink. \nplease provide help thanks \n", "tags": "quadcopter simulator", "id": "1831", "title": "quadcopter parameters calculations for simulink model"}, {"body": "i am trying to build an arm that will have about 5 by 5 by maybe 7 or so centimeters of room for a rotary motor capable of lifting it. the joint will basically allow the arm to rotate in a single degree of freedom in a circle that is perpendicular to the ground.the rest of the arm will probably be around 64 centimeters long and weigh around a minimum of 9 kilograms before it lifts anything. \n\nwhat kind of motor type would give it the best chance of lifting the arm quickly&dagger; and reasonably accurately&ddagger;? \n\n&dagger; raising from straight down to out 90 degrees in around 1 to .5 seconds maximum.\n&ddagger; at least a centimeter at the end of the arm which means probably at the very least 300 positions for the motor.\n", "tags": "motor robotic-arm", "id": "1835", "title": "effective motor type for robotic arm?"}, {"body": "i'd like to start with robotics, but unfortunately i know very little about hw engineering. moreover i used to use such languages as python, c# and java, and do not have much experience in c. still i want very much to be able to program a robot, and i have very big interest in computer vision and ai. are there any platforms/kits that you can buy, and with little time spent you already can program them, preferably in high-order languages?\n\ni'd prefer something wheeled (something flying would also be nice, but it may be too hard to be the case for a first robot), with a camera and some additional sensors. would be also nice to have there something, that could help to avoid obstacles, like laser distance sensor or ultra-sonic sensor. ideally i would like to build a robot that can navigate in the room without the help of operator. i'd like to look at slam some time in future, but for now i just need something to get familiar with the robotics.\n\nalso it should probably be not very expensive, at least not before i will be very sure that i am ready to go deeper into robotics. something for 300-500$ would be awesome.\n\ncan somebody suggest kits/platforms/tutorials/any other info?\n", "tags": "beginner", "id": "1837", "title": "choosing a platform to start"}, {"body": "i am doing simulation of quadcopter in simulink. i want to know how yaw, pitch and roll effect the flight of quadcopter? and how these are different from a single rotor helicopter? \nmainly how to change the rpm to change the x,y,z coordinates of the quadcopter? \nis there any differential equation that can convert the rpm to yaw pitch and roll? please help.\n", "tags": "quadcopter", "id": "1838", "title": "how yaw, pitch and roll effect the flight of quadcopter?"}, {"body": "i'm new to the whole visual servoing area.\n\ni'm now reading the tutorial visual servo control \npart i: basic approaches\" and i don't understand something fundamental - what information is available to the robot?\n\n\nis the 3d location of the tracked features in the current frame known?\nis it known for the desired frame?\nis it known for both?\n\n\nif it's known for both - then what would be the best thing to do? \nwould it be to compute the current and desired 3d location and orientation of the robot, and plan an optimal path accordingly, essentially knowing everything in advance?\n\nalso, in what sense could a control law (i.e translation + rotation path) be optimal for a visual servo?\n", "tags": "localization research visual-servoing", "id": "1839", "title": "what frame of reference is used during visual servoing?"}, {"body": "my quad copter can balance itself in the air using data collected from mpu6050. with the sonar sensor, it can hover at a specific height, but it  moves on the horizontal plane in a random direction.  if i put an object below it, it will ascend to keep the distance between the sonar senor and the object.\n\nnow i want to make it have the ability to hover stably. is it possible to add a downward-facing camera to calculate the speed of optical flow in order to keep it hovering on the same point in the horizontal plane?  could i use a forward-facing camera to stabilize its vertical speed?\n", "tags": "sensors quadcopter cameras visual-servoing", "id": "1844", "title": "stabilizing a quadcopter with optical flow"}, {"body": "i need your advice if someone experienced something similar. i try using digital servo but when i tried to connect it to board by this tutorial https://blogs.oracle.com/hinkmond/entry/connect_robot_servo_to_rpi3\nservo motor only shakes for first ten cycles but after that turns normally. i have no idea why is that but in every article i read that controlling digital servo is same as analog with no need to program them after unboxing.\nthanks for any idea\n", "tags": "raspberry-pi servos", "id": "1853", "title": "digital servo shaking"}, {"body": "i am tasked with creating a system that will recognize fish pulled out of a lake. the system should be able to identify the type of species of fish. typically, i turn to arduino for projects like this. however, based on what i've read about image processing, it's sounding like arduino doesn't have the processing power. does anyone have any suggestions for development boards and cameras that can easily interface with the board?\n\ni've look at this option, http://www.finboard.org/videos/introducing-finboard?utm_campaign=embedded%20processing%20and%20dsp%20newsletter%2013-q3%20na&amp;utm_medium=email&amp;utm_source=eloqua. it seems like it would be a nice all in one type of thing. has anyone used anything like this?\n\nthanks!\n", "tags": "arduino microcontroller raspberry-pi cameras", "id": "1856", "title": "need suggestions for object recognition"}, {"body": "i'm kicking around the idea of building a small passive sonar for an autonomous submarine. i've looked through the net for parts and finding a good transducer for converting the sound underwater into an electrical impulse. after looking at parts i got into the piezoelectric materials used for doing this such as barium titanate or lead zirconate titanate. from what i've read on the web, some of these materials are toxic. \n\nmy question is, are there piezoelectric materials that one could to build a sensor from scratch that does not possess the toxic qualities? something that could preferably thrown in a pool w/ my kids and not give them or me any defects.\n", "tags": "sensors", "id": "1857", "title": "transducer for underwater applications (passive sonar)"}, {"body": "careful inspection of page 35 (figure 58) of the adxl345 datasheet shows that under gravitational loading only, the chip uses a left-handed coordinate system.  my own experiments with this chip confirm this.  \n\ni typically only use the chip to indicate the gravity vector.  so when using this chip, i simply negate the values to get a right handed coordinate system.  but this doesn't seem right.  i assume there is a logical and mathematical explanation for the left-handed coordinate system but i can't figure out what it might be. \n\n\n", "tags": "sensors imu accelerometer calibration", "id": "1858", "title": "why do 3-axis accelerometers seemingly have a left-handed coordinate system?"}, {"body": "i have a question about car-like robot localization using only dead-reckoning. given: \n\n\nrobot position (at current time step) in the form $\\begin{bmatrix}x &amp; y &amp; \\theta\\end{bmatrix}$ (theta is the heading)\nsteering angle \ndistance traveled between two time steps\n\n\nhow can i estimate the position of the robot at the next time step?\n", "tags": "mobile-robot localization", "id": "1861", "title": "question about car-like robot localization based on dead-reckoning"}, {"body": "i want to build some simple application. i need a 5 or 6 degrees of freedom robotic arm. the arm must have feedback, so that i can control it preciously. the arm must be able to handle at least 5 lbs. and the arm would be able to work 10 hours a day.\n\nmy budget is usd$300 . any suggestion?\n", "tags": "robotic-arm", "id": "1863", "title": "may i have some suggestion on inexpensive robotic arm?"}, {"body": "i was looking into the  razor imu from sparkfun, and realized that the only example code on sparkfun's website was for it was meant for hooking it up to a computer (the ahrs head tracker used a serial to usb chip). i looked on google and saw nothing but stories about how it did not work. \n\nis there any good way to hook up the  razor imu to an arduino uno (or any arduino without hardware support for more than one serial port), and if so does example code exist?\n", "tags": "arduino imu", "id": "1864", "title": "razor imu arduino interfacing"}, {"body": "i am a programmer by profession and new to robotics. i have studied ece, so know electronics, but not very familiar with mechanical aspects of robotics. i am working on a learning project with dagu rover 5 platform.\n\ni am trying to control the 4 dc motors with pwm and want to use the optical encoders for feedback. i am looking for some algorithms, example code in c to effectively control the rover. i know how to control the gpio, pwm and interrupts from the processor. i am more interested in learning the algorithm that controls the motors based on this. for now, i am working on a manual robot, controlled with up/down/left/right keys. in future, i would like to add sensors, camera etc and work on autonomous aspects. any pointers would be helpful.\n\nfor reference, i am working on the raspberry pi platform to control the rover.\n", "tags": "mobile-robot algorithm pwm c", "id": "1874", "title": "where i can learn algorithms or and find examples of code for controlling a rover?"}, {"body": "is it possible to get two images from the raspberry pi camera mounted on a remote controlled bot and have them sent to a computer through wi-fi and process the images in the computer to generate a depth map?\n\nall this is to be done in a very short time so that the robot can be helped with its locomotion without making it completely autonomous.\n", "tags": "raspberry-pi", "id": "1876", "title": "depth map with raspberry pi"}, {"body": "i am going to start a project on controlling robots using hand gestures.\nthough i have used matlab for this purpose earlier, i realized it is extremely slow for any practical real-time system.\ni am currently planning to use opencv for this purpose.\ni want suggestions on, if opencv is the best alternative, are there any other alternatives and if i use opencv, which language should i go for, c, c++ or python?\n", "tags": "real-time", "id": "1877", "title": "controlling bot using video and image processing"}, {"body": "i ask this since assembly language is really close to the micro-controller hardware, and what micro-controller would you reccomend.\n\nthe bot has to search for object that i show it before and then i 'lose' the object. note, i do not know anything about micro-controllers.\n", "tags": "microcontroller software programming-languages", "id": "1881", "title": "should you learn assembly language for robotics?"}, {"body": "i'm interested in simulating quadcopter control and swarm co-ordinations. was wondering if blender or specifically morse was going to be good enough? according to the limitations of morse, it states:\n\n\n  \n  morse was never meant to be a physically accurate simulator: while we rely on a state-of-the-art physics engine (bullet), do not expect to accurately simulate robot arm dynamics or fine grasping. other projects are doing that much better (like opengrasp for grasping).\n  while on-going efforts try to tackle this issue, we do not consider morse to have a good enough temporal accuracy and time synchronization capabilities for application like hybrid simulation (where some robots are simulated while others are physically operated).\n  \n\n\nwas wondering if anyone has experience in using blender for these types of applications.\n", "tags": "simulator", "id": "1884", "title": "blender a good robotic simulator for quadcopters / swarm simulations?"}, {"body": "first, i'm a beginner in mcu/robotic world (been working with atmega+cvavr, but that's all). so please bear with me.\n\ni'm making a prototype data glove (like keyglove, but much more simpler), it consist of:\n\n\nimu sensors (mpu 9150 9dof, all reading is fused with built in dmp) ->\nreads hand position and orientations\nminimum of 2 flex sensors -> reads figer flexion\nmcu (well, arduino to be specific)\n\n\nthe sensors are plugged in to the arduino and the reading will be filtered (e.g low pass, kalmann) in the arduino before being transferred over serial to pc. the pc will then translates the data into virtual gripper to move an object (vr)  \n\ninitially i planned to use uno +  pansenti\u2019s mpu9150 library in my code, then i realised the flash memory size left would be so tiny (i.e mpu9150 lib code size is ~29k, uno has 32k).\nmy project is still in very early stage, so a lot things are expected to be changed and added, with so little flash memory left. i can only do so much.  \n\ni immediately looking for mega as replacement (256k flash) but i realised there is also newer due with faster processor. \n\nthey cost effectively the same as for now.\n.\n\nmy main concern here is the robustness and compatibility when:\n\n\ndesigning  the hw interface to arduino (making circuits, addding\nshield)\ncode development (available library)\nstreaming and filtering the sensor readings (would 32 bit mcus helps, or it's overkill?)\n\n\ni know this question might sound as too localized, but i believe a lot of projects that utilize multiple sensor reading + filtering similarly will also benefits from this discussion.\n\ni'll revise the question if it's needed. the main question is probably\nwould 32 bit mcus perform significantly better in multiple sensor reading and signal filtering compared to 8 bit mcus?\n\nor in my case.. should i go with mega or due?\n", "tags": "arduino imu", "id": "1895", "title": "arduino for simple data-glove. should i go with mega or due?"}, {"body": "i'm looking to make an automatically shifting bicycle for my senior design project (along with some additional features tbd). however, i come from an electrical/software background; not a mechanical one. so i need to figure out a decent way to shift gears. i was thinking of leaving the gear system in place as is and using some sort of motor (servo or stepper motor with worm gears) to pull and release the wire cable as needed. however, i have some concerns with this; namely the amount of torque needed to pull the wire and finding something with enough holding torque. perhaps my best option is to use the trigger shifters on as well and perhaps use a solenoid. my other concern (namely with the worm gear) is that it'll be too slow. \n\nso i would like to pick your brains here for a moment. thanks\n", "tags": "motor automatic", "id": "1897", "title": "mechanism for changing gears on a bicycle"}, {"body": "i'd like to assemble a prototype of brushless servo system using a rc brushless motor (heavily geared down), a sensored electronic speed controller for rc motors, and a microcontroller to do the pid control.\n\n\n\ni'd add three hall sensors around the motor, and feed those signals into the esc and the microcontroller. the mcu will run a pid controller, and output an rc servo compatible pwm signal to the esc.\n\nquestion: is this likely to work, or will i find that the esc is trying to be clever? i have one rc car which only switches into reverse if you double pulse the reverse signal.\n\n(note, the reason i'm trying to get this working using an off-the-shelf esc, rather than designing my own proper one is that development time is much more expensive than parts cost at the moment).\n", "tags": "servos brushless-motor esc", "id": "1899", "title": "making a brushless servo using hall sensors"}, {"body": "so i'm doing some reading on monte carlo localization, and it sounds like the approach is based on using a predefined map, but i just need to make sure (because i haven't read anywhere that it absolutely needs a predefined map). i just want to make 100% sure that my understanding is correct:\n\ndoes it absolutely need a predefined map?\n\n[maybe i need to add the below stuff as another question, but here goes nothing]\nand what other localization approaches are there that don't need a predefined map? so far i've only read about slam (which sounds to me like a general approach instead of a specific implementation).\n\nthanks in advance!\n", "tags": "localization slam mapping", "id": "1901", "title": "does monte carlo localization need a predefined map?"}, {"body": "i'm working on a robot with a team, and we're building our robot out of acetal polyoxymethylene (pom) (specifically, delrin) plastic. however, we'd like to prototype the robot before we build it out of pom, as pom is somewhat expensive.\n\nthere are several critical areas that plywood would be used in place of pom:\n\n\nover sliding surfaces\naround gearboxes \nunder weight stress\n\n\nwe'd like to take into account the friction coefficients, smoothness, and rigidity of the materials in deciding whether plywood is a valid prototype substitute. the material will be 1/4\" thick.\n\nwhat differentiates plywood from acetal pom with respect to the relevant points?\n", "tags": "design", "id": "1903", "title": "is finished plywood a comparable prototyping substitute for polyoxymethylene?"}, {"body": "i am part of my college robotics team. we are participating in robocon 2014 and are thinking about using mecanum wheels. we have done our research but one thing id like to clarify is: does the number of rollers in the mecanum wheel effect its strafing? if yes then how?\n", "tags": "wheeled-robot", "id": "1904", "title": "strafing of mecanum wheels"}, {"body": "in earlier versions of the arduino ide there was a option to burn a the bootloader using an arduino as the programmer. as of current there is only a burn bootloader option. was the burn using a arduino as a isp integrated into the still existing one, or did it disappear?\n", "tags": "arduino", "id": "1909", "title": "arduino isp bootloader burning"}, {"body": "would like a product that enables me to use my computer to throw an small dc on / off switch. seems like a stupidly simple thing to do, but for the life of me i can't seem to find such a device when i search online.\n\nis there a device floating around out there that i can order? or is there some kind of term i should be searching for?\n\nthanks so much!\n", "tags": "control", "id": "1910", "title": "use computer to throw a small switch"}, {"body": "i just got my rover 5 chassis with 4 motors and 4 quadrature encoders and i am trying to utilize the optical encoders. i know the encoders generate pulse signals which can be used to measure speed and direction of the motor.\n\ni want to know how 4 separate optical encoders add value for the controller of rover 5 like platform. the controller normally uses pwm to control the speed of the motor. if two motors are running at same speed then the encoder output will be same. so, why should the controller monitor all 4 encoders?\n", "tags": "mobile-robot motor control", "id": "1911", "title": "how are the optical encoders used in platforms like rover 5?"}, {"body": "i've been having trouble installing morse.\ni am trying to install it on ubuntu 12.04 and on a virtualbox with ubuntu 13.04 (i don't need it on a virtualbox, i'm just trying to make something work). \non ubuntu 12.04 i get the following errors at the cmake stage:\n\n\n\non a fresh vmbox with ubuntu 13.04, after 'morse check' succeeds, i try \"morse create mysim\" and get:\n\n\n\nany suggestions?\n\nupdate:\n\ni've managed to install morse on ubuntu 12.04. make sure your blender was compiled with the same version of python (i.e python 3.2.2) that morse was compiled in. i used this blender: \nhttp://download.blender.org/release/blender2.62\n", "tags": "simulator", "id": "1918", "title": "installing morse simulator on ubuntu 12.04"}, {"body": "i need to actuate 3 or 4 cnc-like nema 23 (~1n.m torque) stepper motors, i would like some cable solution to connect easily the motor to the motor driver. \n\ni have not yet bought anything, i have searched various robotic stores and ebay, but did not yet found a triple (motor, cables, driver) which would be \"plug and play\". \n\nas stepper motors usually have 4 to 6 cables, and there are multiple motors, manual soldering everything would be too time consuming, error prone and messy.\n\nis there a standard way to deal with cables for stepper motors ?\n", "tags": "stepper-motor wiring", "id": "1919", "title": "how can we manage stepper motor cables?"}, {"body": "i am in the fll (first lego league), and while we are waiting for the competitions, we want to work on a robot. anyone have any ideas?\n", "tags": "sensors", "id": "1924", "title": "any ideas for a robot?"}, {"body": "i could use some guidance regarding a coordinate system transform problem. my situation is this: my system begins at some unknown location, which i initialize the location (x y) and orientation (roll, pitch, and yaw) all to zero. i establish a frame of reference at this point, which i call the \"local\" coordinate frame. it is fixed in the world and does not move. at system startup, the body frame is perfectly aligned with the local frame, where body +x points forward, +y to the right and +z down. the body frame is fixed to my system, and travels with the system as it moves.\n\ni have an estimation routine that provides me with the x and y position, as well as the roll, pitch, and yaw of the system. yaw is rotation about the z axis of the local frame. pitch and roll are with respect to the body frame (i.e.,if the robot pitches up, i always get a positive value. if it rolls right, i get a positive value.)\n\nhow can i take the known roll and pitch values and transform them to be with respect to the local (fixed) frame?\n", "tags": "mobile-robot kinematics", "id": "1925", "title": "performing the proper coordinate system transformation"}, {"body": "i found many tutorials and online calculators for the selection of dc motor to drive wheel, i understood how the torque affect the driving of wheel.\n\nbut what happen when i change the orientation of motor and load? what is the main criteria for a dc motor to work when i want to rotate a plate which is vertically mounted on motor's shaft, when the motor is placed vertically also (as shown in the picture)?\n\n\ni am not an engineering student so please provide me an answer as simple as possible.\n", "tags": "motor torque", "id": "1927", "title": "dc motor direct loading"}, {"body": "i'm trying to learn about a very basic motor, the servo motor.  can these be found a thrift stores like goodwill in old toys?  are these \"robotic\" quality?  what toys or other kinds of things would i scavenge?  all i want to do is get a motor.  after that i want an arduino and make it \"work.\"  nothing complex.\n", "tags": "servomotor", "id": "1931", "title": "is a thrift store a good place to get a servo motor?"}, {"body": "if i was controlling a normal brushed motor as a servo, i would measure the motor's position, and adjust the pwm signal to control the voltage. this way i could achieve a precise velocity/position profile if my control was good enough.\n\n\n\nwhen doing field oriented control (foc) of a brushless motor, there are two parameters i can control: the voltage angle, and the voltage magnitude. there are three things i can measure, the current angle and magnitude, and the rotor position.\n\ni want to achieve a precise velocity/position profile including good control down to zero speed and reverse.\n\nquestion: how can i calculate the correct voltage field angle (or phase lead) and magnitude?  do i need two pid algorithms?\n\n\n\nassume i can take any reasonable measurements of the motor state, including rotor position and winding current.\n", "tags": "servos pid brushless-motor", "id": "1934", "title": "field oriented control of brushless motors"}, {"body": "i live in an apartment that has sliding windows in it. the apartment is naturally warm because we live above a mechanical room, such that we either opened the windows or ran the air conditioning through the winter. i want to create a device than can open and close the windows in the apartment depending on temperature. the software and electronics are already figured out, i just need to figure out how to move the windows.\n\n\n\nthis is a sample of the window. it takes about 4 lbs of force to pull it open, and they only open 6 inches(since i'm 16 stories high).\n\nultimately, i want to make this cheap enough that i could replicate it on 6 windows.\n\nmy first thought was a linear actuator, but most of the ones i have seen are designed for moving 100+lbs and cost several hundred dollars. pneumatic actuators are cheaper, but i'd have to run a network of air lines and solenoids, and would have a compressor that would randomly kick in. a double winch system would be very complicated to set up and prone to failure. lastly, i was thinking of a cheap metal gear servo(dealextreme has 15kg/cm servos for under $15.00), but it would be somewhat difficult to use a series of turnbuckles and arms to translate into 6 inches of linear movement.\n\nany help would be appreciated.\n", "tags": "design actuator", "id": "1935", "title": "how to open a sliding window?"}, {"body": "has anybody experimented with gmaw for additive manufacturing? the thing is, welding wire is so much cheaper than abs or pla filaments and, well, it is steel you are printing in, not some flimsy plastic! i imagine the arc deposition printhead would be constructed similarly to one used in plastic filament printers, except there is no need for the heating element (so, even simpler). welding often requires fast z speed (to finely control the arc) so i think delta (deltamaker) chassis would work best. gmaw calls for some sort of inert gas to insulate heated metal from oxygen. it would make sense to seal off most of the interior of the printer and fill it with heavier than air inert gas during printing. \n\ni would highly appreciate any pointers on existing 3d printer designs employing this deposition method as well as flaws in design i outlined here.\n", "tags": "3d-printing", "id": "1944", "title": "arc welder for 3d printing"}, {"body": "i have been researching on slam. i came across ekf slam which uses odometry to measure the robot's initial position in the map and as well as landmarks which helps the robot's position to be more accurate. based on the slam for dummies, it has a problem of loop closure. in another journal, it was compared to fastslam and ekf has a big-o function of $o(k^2)$ where $k$ is the number of landmarks while fastslam has $o(m\\log(k))$.\n\nit was also said that the most promising slam algorithm from the journal \"the vslam algorithm for navigation in natural environments\"\n is fastslam \nhowever, the vslam used by an experiment done by the university of pennsylvania is the occupancy grid slam.\n\ni want to ask what would be the most approriate slam algorithm for vslam given an unmanned aerial vehicle like the quadrotor and rgb-d camera + imu? also are there any algorithm that can be extended to support co-operation?\n", "tags": "localization slam quadcopter mapping", "id": "1946", "title": "what is the most appropriate slam algorithm for quadrotors with rgb-d camera?"}, {"body": "i have this project i'm working on where i'll need the speed of the stepper motor to change set speed at a certain distance, i just can't figure out a way to do it. i'm using arduino and a stepper motor, this is the current code.\n\n\n\nwhat i want it to do basically is to first   at the current speed 200 then after 2500 i want it to increase speed to 400. after it has moved 5000 it turns and moves back to position but that's implemented already.\n", "tags": "arduino control stepper-motor", "id": "1947", "title": "i want my stepper motor to switch speed while traveling (not acceleration wise)"}, {"body": "in the book of slam for dummies, why do we even need the odometry when the robot would use the data retrieved from the laser scanner which is more accurate than odometry? why not just rerly on the laser scanner and do away from the odometry? is there any contribution by the odometry that the laser scanner does not have? also, are all slam algorithms feature-based?\n", "tags": "localization slam mapping", "id": "1949", "title": "in ekf-slam, why do we even need odometry when there is a more reliable sensor?also, are all slam algorithms feature-based?"}, {"body": "the hc-sr04 is directly connected to an arduino board with the receiver end(echo) connected to analog pin 2 and the transmitter (trigger) connected to digital pin 4.\n\ni am wondering if i can use the sensor to sense the change in saturation from when object block its path. the receiver and transmitter will be positioned like this \n\n\n\nthe line in the middle is supposed to be a paper. i'll be using it to see the difference between one paper and two paper when they travel trough the two. \n\nnow i'm not sure if this is possible but the way i see it working is kind of similar to an ir led arduino program connected to an led, where when one paper passes trough the light gets a little bit weaker and with two it takes a heavier hit.\n\nis this possible?\n", "tags": "arduino sensors", "id": "1954", "title": "is it possible to use hc-sr04 ultrasonic range sensor to indicate thickness of a material"}, {"body": "i am working on a 2d space where my robot needs to follow a trajectory while avoiding some obstacles.\n\ni've read recently about methods for path planning as \"vector field histogram\" and the \"dynamic window approach\".\n\nis it worth to use these kind of algorithms for a 2d space or should i go with something as potential fields or rapidly-exploring random trees?\n", "tags": "mobile-robot motion-planning", "id": "1955", "title": "choosing path planning and obstacle avoidance algorithm for 2d space"}, {"body": "i have read that you can wire a unipolar stepper to a bipolar driver, which i have, by ignoring the two extra wires. one concern i have is whether connecting a unipolar stepper to a bipolar driver will cause it to lose torque (holding or operating)? \n\nwill it be the same? increase? \n\ni've read that bipolars are more bang for your buck energy-wise, and since you can \"transform\" a unipolar stepper to a bipolar good enough that the driver will still work right, i would think that it might run more efficiently. is this true?\n", "tags": "torque stepper-driver stepper-motor", "id": "1959", "title": "will wiring a unipolar stepper to a bipolar stepper driver decrease the holding torque?"}, {"body": "i'm pretty new to the world of uas after a ten year holiday from rc flying.\n\ni'm looking at ardupilot and am wondering what purpose telemetry serves? is it just to get in flight data back to a ground station or can it also be used to program the system in flight? are there other capabilities that i am missing?\n", "tags": "uav ardupilot", "id": "1965", "title": "what is telemetry used for?"}, {"body": "this question is to anyone familiar with object (specifically vehicle) detection research.\n\ni'm new to computer vision and am confused about training object detection classifiers. specifically, the objective is vehicle detection. i've been reading through vehicle detection literature for weeks now, but i'm still a bit confused.\n\nwhat i'm confused about is evaluation. for the evaluation of a system, the research community usually has a benchmarked dataset which can be used for testing data. but the performance of a system also depends very much on the data that was used to train it, no? \n\nso aren't there any training datasets out there, too? that would make for far more uniform method comparisons. i seem to keep finding papers using benchmarked datasets for evaluation, but making no mention of where they got their training data from.\n", "tags": "computer-vision", "id": "1966", "title": "public training data for vehicle detectors in computer vision?"}, {"body": "i want to steer a rc car in a straight line.the car has 4 sharp ir sensors on each corner of the car to help it steer the corridor.the corridor is irregular and looks something similar to the picture below.\n\nthe car needs to be stay exactly at the middle(shown by lighter line) and take help of the ir sensors to correct its path.\n\nthe car has a servo on the front wheel to steer and another that controls the speed.\n\ni tried running it using a algorithm where it summed the values on each side of the car and took the difference.the difference was then fed to a pid control the output of which went to steer the car.the greater the value from the pid (on either sides), the greater the value of the steering angle till it reaches the middle.\n\nit works for the part where the walls are at similar distance from the center and even then it oscillates a lot around the center and fails miserably around the bumps in the corridor.\n\ni need to make changes to the algorithm and need some help in steering me  in the right direction.\n\nthe ir sensors are too finicky and is there a way to filter out the noise and make the readings more stable?\n\nany help regarding the changes that needs to be implemented is much appreciated.\n\ncurrently the car only uses 4 ir sensors to guide.i can also use 2 ultrasonic sensors.\n\n\n", "tags": "mobile-robot sensors", "id": "1974", "title": "autonomous car steering using ir sensors"}, {"body": "i belong to an auv team at my university. we are planning to have a multibeam 2d imaging sonar (the blueview p900) for our auv to detect obstacles underwater.\n\ni have the following questions to ask on the feasibility of testing/implementing such sonars on auvs.\n\n\nas we know that these multibeam sonars have multiple reflections arriving at different times from various surfaces while testing in a pool, is there any recommended way to filter these noises in the image obtained from the sonar pings?\nare such sonars in use/test by any other team/organization anywhere else who do pool testing other than a ocean/reservoir testing where multiple reflections are almost zero except the reflections from the obstacle(s)?\nalso i would like to know the recommended image processing algorithms that can be implemented/used to detect obstacles from the sonar images.\n\n", "tags": "sonar", "id": "1976", "title": "usage of multibeam 2d imaging sonar for auvs, testing them in the pool environment"}, {"body": "recently i am working with two accelerometers: bma020 and bma180. i will try to explain my problem using bma020 as example because it is less accurate therefore problem is more visible. when i hold my acc in neutral position i get correct average result: -1g. now i turn my acc upside down but this time my average result is +0,91g. the same problem occurs for other axis. for bma180 problem is less visible (-1g in normal position and +0.98g upside down). do you know why accelerometer behaves like this ?\n", "tags": "accelerometer", "id": "1978", "title": "accelerometers error (bma020 and bma180)"}, {"body": "i've been able to use pymavlink.mavutil to read telemetry from a .tlog created by missionplanner.\n\nto do this, i create a  like this:\n\n\n\nnow i want to read the flight parameters (settings) from the .tlog . the method mavlogfile.param_fetch_all() appears to be designed only to work with a live telemetry link rather than a log. it sends a parameter request command, which obviously has no result when you are linked to a log rather than an actual aircraft.\n\ni know the parameters are in the .tlog... how do i get them out?\n", "tags": "python ardupilot", "id": "1979", "title": "how to retrieve parameters from mavlink .tlog using pymavlink?"}, {"body": "i need to track a point in space. the point is less than 2&nbsp;m away, it has to be passive, no batteries, and no charging.\n\ni don't always have a line of sight. i need to pinpoint it to about a centimeter. i need to sample it at a frequency of 10&nbsp;hz or more.\n\ncan it be done at all? does such a solution exist?\n", "tags": "sensors", "id": "1981", "title": "1 cm accuracy radio rangefinder?"}, {"body": "i'm a first year electronics engineering student. i love almost all the aspects of robotics - the electronics, algorithms, control theory etc. i can't stand the mechanical aspect of robotics though.\n\ncan i have a fulfilling career in robotics if i hate mechanics but love all other parts of robotics? i'm ready to learn mechanics if i absolutely have to, but would strongly prefer not to learn anymore than the absolute basics.\n\nthanks.\n", "tags": "software electronics mechanism", "id": "1983", "title": "can you have a career in robotics if you hate mechanics?"}, {"body": "first, is it possible to build map without landmarks for a robot in 2d? let's say we have an aisle surrounded  by two walls. the robot moves in this environment. now is it feasible to build such a slam problem? or landmarks must be available to do so?\n", "tags": "mobile-robot slam", "id": "1985", "title": "slam without landmarks?"}, {"body": "i'm looking to build an outdoor robot and i need to know if time-of-flight cameras like the swissranger\u2122 sr4500 work in fog, does anybody have some experiences on that?\n", "tags": "mobile-robot cameras", "id": "1991", "title": "are time-of-flight cameras like the swissranger affected by outdoor fog?"}, {"body": "the state vector is \n$$ \\textbf{x} = \\begin{bmatrix} x \\\\ y \\\\ v_{x} \\\\ v_{y} \\end{bmatrix}$$\n\ntransition function is \n$$\n\\textbf{x}_{k} = f(\\textbf{x}_{k-1}, \\delta t) =\n\\begin{cases} \nx_{k-1} + v_{xk} \\delta t \\\\\ny_{k-1} + v_{yk} \\delta t \n \\end{cases} \n$$\n\n$z_{b} = atan2(y, x)$ and\n$z_{r} = \\sqrt{ x^{2} + y^{2}}$\n\nthe jacobian of the observation model:\n$$\n\\frac{\\partial h}{\\partial x} =\n \\begin{bmatrix} \\frac{-y}{x^{2}+y^{2}} &amp; \\frac{1}{x(1+(\\frac{y}{x})^{2})} &amp; 0 &amp; 0 \\\\\n                 \\frac{x}{\\sqrt{ x^{2} + y^{2}}} &amp; \\frac{y}{\\sqrt{ x^{2} + y^{2}}} &amp; 0 &amp; 0 \n \\end{bmatrix}\n$$\n\nmy question is how  the jacobian of the observation model has been obtained? and why it is 2x4?\n\nthe model from kalman filter.\n", "tags": "kalman-filter", "id": "1992", "title": "jacobian of the observation model?"}, {"body": "i lead a university robotics team that needs pid controllers for four drive motors and two additional motors that are used in a secondary system. i would strongly prefer to buy pre-built pid controllers that provide just about any reasonable interface for setting pid constants, motor velocity and direction, as the controllers are not remotely central to the difficult, interesting problems we're trying to solve. to my astonishment, the internet doesn't seem to be saturated with such controllers (talk about reinventing the wheel - hundreds of tutorials but almost no pre-built solutions! did willow garage build their own pid controller for the pr2?!). \n\ndoes anyone have recommendations/experience, preferably pointers to such controllers? \n\ni've googled around quite a bit, and so far this is the best option i've found. it's a cape for a beaglebone black (which is the board we're using). the problem is that the python library is not finished - it resets pid constants at every call, it doesn't support changing the direction of the motor, and it seems to only support setting motor power, not velocity, which gives me the impression that it's not actually using the pid controller at all.\n\nadditional details:\n\n\nthe stall current of our drive motors is 6a. they are brushless dc motors with quadrature encoders. the secondary motors are much smaller, and we're building our own encoders for them.\nour code base is in python, and we're running on a beaglebone black using the latest debian image from robert nelson (that guy's awesome!). our batteries provide 14.8v, and we already have 3.3v and 5v rails.\nour robot is fairly small, about 1x1x2 feet, and weighs about 9 pounds. this info is meant to give perspective with regard to scale.\n$350 or so is the comfortable top range of what we could spend to get all 6 motors pid-controlled.\n\n\nany help would be greatly appreciated!\n", "tags": "motor pid brushless-motor", "id": "1995", "title": "pre-built pid motor controller"}, {"body": "for a 6dof robot with all revolute joints the jacobian is given by:\n$$\n\\mathbf{j} = \n\\begin{bmatrix}\n\\hat{z_0} \\times (\\vec{o_6}-\\vec{o_0}) &amp; \\ldots &amp; \\hat{z_5} \\times (\\vec{o_6}-\\vec{o_5})\\\\\n\\hat{z_0} &amp; \\ldots &amp; \\hat{z_5}\n\\end{bmatrix}\n$$\nwhere $z_i$ is the unit z axis of joint $i+1$(using dh params), $o_i$ is the origin of the coordinate frame connected to joint $i+1$, and $o_6$ is the origin of the end effector.  the jacobian matrix is the relationship between the cartesian velocity vector and the joint velocity vector:\n$$\n\\dot{\\mathbf{x}}=\n\\begin{bmatrix}\n\\dot{x}\\\\\n\\dot{y}\\\\\n\\dot{z}\\\\\n\\dot{r_x}\\\\\n\\dot{r_y}\\\\\n\\dot{r_z}\n\\end{bmatrix}\n=\n\\mathbf{j}\n\\begin{bmatrix}\n\\dot{\\theta_1}\\\\\n\\dot{\\theta_2}\\\\\n\\dot{\\theta_3}\\\\\n\\dot{\\theta_4}\\\\\n\\dot{\\theta_5}\\\\\n\\dot{\\theta_6}\\\\\n\\end{bmatrix}\n=\n\\mathbf{j}\\dot{\\mathbf{\\theta}}\n$$\n\nhere is a singularity position of a staubli tx90xl 6dof robot:\n\n\n\n$$\n\\mathbf{j} = \n\\begin{bmatrix}\n          -50     &amp;    -425     &amp;    -750      &amp;      0     &amp;    -100      &amp;      0\\\\\n       612.92     &amp;       0     &amp;       0      &amp;      0     &amp;       0      &amp;      0\\\\\n            0     &amp; -562.92     &amp;       0      &amp;      0     &amp;       0      &amp;      0\\\\\n            0     &amp;       0     &amp;       0      &amp;      0     &amp;       0      &amp;      0\\\\\n            0     &amp;       1     &amp;       1      &amp;      0     &amp;       1      &amp;      0\\\\\n            1     &amp;       0     &amp;       0      &amp;     -1     &amp;       0      &amp;     -1\n\\end{bmatrix}\n$$\n\nyou can easily see that the 4th row corresponding to $\\dot{r_x}$ is all zeros, which is exactly the lost degree of freedom in this position.\n\nhowever, other cases are not so straightforward.\n\n\n$$\n\\mathbf{j} = \n\\begin{bmatrix}\n          -50   &amp;   -324.52  &amp;    -649.52   &amp;         0   &amp;   -86.603   &amp;         0\\\\\n       987.92   &amp;         0  &amp;          0   &amp;         0   &amp;         0   &amp;         0\\\\\n            0   &amp;   -937.92  &amp;       -375   &amp;         0   &amp;       -50   &amp;         0\\\\\n            0   &amp;         0  &amp;          0   &amp;       0.5   &amp;         0   &amp;       0.5\\\\\n            0   &amp;         1  &amp;          1   &amp;         0   &amp;         1   &amp;         0\\\\\n            1   &amp;         0  &amp;          0   &amp;    -0.866   &amp;         0   &amp;    -0.866\n\\end{bmatrix}\n$$\n\nhere you can clearly see that joint 4 and joint 6 are aligned because the 4th and 6th columns are the same.  but it's not clear which cartesian degree of freedom is lost (it should be a rotation about the end effector's x axis in red).\n\neven less straightforward are singularities at workspace limits.\n\n\n\n$$\n\\mathbf{j} = \n\\begin{bmatrix}\n          -50     &amp;     650   &amp;       325  &amp;          0    &amp;        0     &amp;       0\\\\\n       1275.8     &amp;       0   &amp;         0  &amp;         50    &amp;        0     &amp;       0\\\\\n            0     &amp; -1225.8   &amp;   -662.92  &amp;          0    &amp;     -100     &amp;       0\\\\\n            0     &amp;       0   &amp;         0  &amp;    0.86603    &amp;        0     &amp;       1\\\\\n            0     &amp;       1   &amp;         1  &amp;          0    &amp;        1     &amp;       0\\\\\n            1     &amp;       0   &amp;         0  &amp;        0.5    &amp;        0     &amp;       0\n\\end{bmatrix}\n$$\n\nin this case, the robot is able to rotate $\\dot{-r_y}$ but not $\\dot{+r_y}$.  there are no rows full of zeros, or equal columns, or any clear linearly dependent columns/rows.  \n\nis there a way to determine which degrees of freedom are lost by looking at the jacobian?\n", "tags": "robotic-arm inverse-kinematics industrial-robot", "id": "1997", "title": "is there a way to determine which degrees of freedom are lost in a robot at a singularity position by looking at the jacobian?"}, {"body": "i have an unscented kalman filter (ukf) that tracks the state of a robot. the state vector has 12 variables. each time i carry out a prediction step, my transfer function (naturally) acts on the entire state. however, my sensors provide measurements of different parts of the robot's state, so i may get roll, pitch, yaw and their respective velocities in one measurement, and then linear velocity in another.\n\nmy approach to handling this so far has been to simply create sub-matrices for the covariance, carry out my standard ukf update equations, and then stick the resulting values back into the full covariance matrix. however, after a few updates, the ukf yells at me for trying to pass a matrix that isn't positive-definite into a cholesky decomposition function. clearly the covariance is losing its positive-definite properties, and i'm guessing it has to do with my attempts to update subsets of the full covariance matrix. \n\nas an example taken from an actual log file, the following matrix (after the ukf prediction step) is positive-definite:\n\n\n\nhowever, after processing the correction for one variable (in this case, linear x velocity), the matrix becomes:\n\n\n\nthe difference between the two matrices above is \n\n\n\nas you can see, the only difference between the two is the value in the location of the variance of linear x velocity, which is the measurement i just processed. this difference is enough to \"break\" my covariance matrix.\n\ni have two questions:\n\n\nupdating a subset of the filter doesn't appear to be the right way to go about things. is there a better solution?\nalternatively, am i missing a step that would keep my covariance matrix as positive-definite?\n\n\nthanks!\n\nedit:\n\nit looks like i'm not properly placing the values back into the original covariance matrix. simply copying the values back isn't sufficient. i need to track the correlation coefficients for the covariance matrix, and make sure that when i update a variance value, i update all the values in its row/column to maintain the correlation coefficient value. i have to do some more testing to verify that this is my issue, but some initial analysis in matlab suggests that it is. if i'm correct, i'll answer my own question.\n\nedit 2:\n\ngiven the response below and after trying it, i can see that my original edit idea won't fly. however, i have one more question:\n\nas this is a ukf, i don't actually have jacobian matrices. i think i see how i would make it work within the ukf update equations, but even in an ekf - and i ask because i have one of those as well - my state-to-measurement function $h$ is going to end up being the identity matrix, as i am directly measuring my state variables. in the case, i take it my \"jacobian\" would just be an $m \\times n$ matrix with ones in the $(i, i)$ location, where $i$ is the index of the measured values in the measurement vector?\n", "tags": "kalman-filter", "id": "2000", "title": "maintaining positive-definite property for covariance in an unscented kalman filter update"}, {"body": "this is a follow up to \n\nmaintaining positive-definite property for covariance in an unscented kalman filter update\n\n...but it's deserving of its own question, i think.\n\ni am processing measurements in my ekf for a subset of the variables in my state. my state vector is of cardinality 12. i am directly measuring my state variables, which means my state-to-measurement function $h$ is the identity. i am trying to update the first two variables in my state vector, which are the x and y position of my robot. my kalman update matrices currently look like this:\n\nstate $x$ (just test values):\n$$\n\\left(\\begin{array}{ccc}\n0.4018 &amp; 0.0760\n\\end{array} \\right) \n$$\n\ncovariance matrix $p$ (pulled from log file): \n$$\n\\left(\\begin{array}{ccc}\n0.1015 &amp; -0.0137 &amp; -0.2900 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0195 &amp; 0.0233 &amp; 0.1004 &amp; 0 &amp; 0 &amp; 0 \\\\\n-0.0137 &amp; 0.5825 &amp; -0.0107 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0002 &amp; -0.7626 &amp; -0.0165 &amp; 0 &amp; 0 &amp; 0 \\\\\n-0.2900 &amp; -0.0107 &amp; 9.6257 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0015 &amp; 0.0778 &amp; -2.9359 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n0.0195 &amp; 0.0002 &amp; 0.0015 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n0.0233 &amp; -0.7626 &amp; 0.0778 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1.0000 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n0.1004 &amp; -0.0165 &amp; -2.9359 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1.0000 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 \\\\\n\\end{array} \\right) \n$$\n\nmeasurement $z$ (just test values):\n$$\n\\left(\\begin{array}{ccc}\n2 &amp; 2\n\\end{array} \\right) \n$$\n\n\"jacobean\" $j$:\n$$\n\\left(\\begin{array}{ccc}\n1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n\\end{array} \\right) $$\n\nmeasurement covariance $r$ (just test values):\n$$\n\\left(\\begin{array}{ccc}\n5 &amp; 0 \\\\\n0 &amp; 5 \\\\\n\\end{array} \\right) $$\n\nkalman gain $k = pj^t(jpj^t + r)^{-1}$:\n\n$$\n\\left(\\begin{array}{ccc}\n0.0199 &amp; -0.0024 \\\\\n-0.0024 &amp; 0.1043 \\\\\n-0.0569 &amp; -0.0021 \\\\\n0 &amp; 0 \\\\\n0 &amp; 0 \\\\\n0 &amp; 0 \\\\\n0.0038 &amp; 0.0000 \\\\\n0.0042 &amp; -0.1366 \\\\\n0.0197 &amp; -0.0029 \\\\\n0 &amp; 0 \\\\\n0 &amp; 0 \\\\\n0 &amp; 0 \\\\\n\\end{array} \\right) $$\n\n$k$ is 12x2, meaning that my innovation - and therefore both measurement and current state - would need to be 2x1 in order to have a 12x1 result to add to the current full state:\n\n$x' = x + k(z - h(x_s))$\n\nwhere $x_s$ is a vector containing only the parts of the full state vector that i am measuring. \n\nhere's my question: $k(z - h(x_s))$ yields\n\n$$\n\\left(\\begin{array}{ccc}\n    0.0272 \\\\\n    0.1969 \\\\\n   -0.0948 \\\\\n         0 \\\\\n         0 \\\\\n         0 \\\\\n    0.0062 \\\\\n   -0.2561 \\\\\n    0.0258 \\\\\n         0 \\\\\n         0 \\\\\n         0 \\\\\n\\end{array} \\right) $$\n\ndoes it make sense that this vector, which i will add to the current state, has non-zero values in positions other that 1 and 2 (the x and y positions of my robot)? the other non-zero locations correspond to the robot's z location, and the x, y, and z velocities. it seems strange to me that a measurement of x and y should yield changes to other variables in the state vector. am i incorrect in this assumption?\n\nincidentally, the covariance update works very well with the jacobean in this form, and maintains its positive-definite property.\n", "tags": "kalman-filter", "id": "2009", "title": "ekf partial state update question"}, {"body": "i am trying to control the rover 5 robot using an android app with a touch-based joystick control in the app ui. i want to calculate the speed of the left and right motors in the rover when joystick is moved.\n\nfrom the joystick, i get two values, pan and tilt. i convert them into the polar coordinate system with  and . where r ranges from 0 to 100 and theta from 0 to 360. i want to derive an equation which can convert the  to  for rover. the speed values also are in the [0;100] range.\n\nnow, here is what i have figured out till now. for any value of , \n\nif  then  (turning right on spot) \nif  then  (moving forward at speed r) \nif  then  (turning left on spot) \nif  then  (moving backwards at speed r) \n\nfor other values, i want it moving and turning simultaneously. for example,\n\nif  then  (moving forward while turning right) \n\nso, basically for any , i can set speeds as,\n\n\n\ni need to formulate an equation where i can generalize all these cases by finding  and  based on .\n\nhow can i do this? is there is any existing work i can refer to?\n", "tags": "control kinematics movement", "id": "2011", "title": "how to calculate the right and left speed for a tank-like rover?"}, {"body": "i am new in robotics. may be this question looks like too naive but i want to know which is a better board among arduino uno r3 or roboduino atmega168 or arduino mega 2560 r3 for my purpose mention below - \n\n\na simple robot with wheels, can move around.\ncan have ir sensors and camera.\nis powerful enough to deal with computer vision computations.\n\n\narduino mega 2560 r3 looks more powerful than the other too, i just want to know if my purpose can be solved with other two boards too?\n\nthanks\n", "tags": "arduino", "id": "2018", "title": "arduino uno r3 or roboduino atmega168 or arduino mega 2560 r3 which board is better for small robots"}, {"body": "i'm using the telemetry kit from 3dr robotics (433mhz) to interface with ardupilot mega 2.6, controlling a quadcopter. the mission planner (v1.2.84) by michael oborne works well with the telemetry kit, transmitting flight data (imu, compass, gps etc.) from the quadcopter to the gcs and displaying them in their gui.\n\nhowever, i would like to see the same data in the hyperterminal (windows system). the radio receiver on the gcs connects to my pc through a usb drive. i have tried calling the remote radio station using all possible baud rates, starting from 110 to 921600 (including 57600). i've set the data bits to 8 and stop bits to 1. 'none' for parity and flow control.\n\nhowever, all that i ever get on my terminal is either gibberish or no data at all. i also tried burning this software to the radio receiver and tried using at commands on the radio. \n\nit connects ok with '+++', but keeps returning error for at1, att etc.\n\nplease give me an idea about how to get flight data at the hyperterminal.\n\nthanks.\n", "tags": "quadcopter ardupilot", "id": "2021", "title": "telemetry with ardupilot 2.6"}, {"body": "this is my first question on this site, might be a little subjective :)\n\nthere is an ongoing process of many cool cyclonic changes of technology in the electronics and software industry.\n\nconcurrency and parallelism\n\nwhat will be the implications of gpgpu, multi-core  and \"programming in the large\" model in the specific case of embedded software, and how will it influence the styles and conventions in the community?\n\nsingle board multicore hardware like soon to be released parallela can be an example?\n\nprogramming language research\n\nthe results have been excellent. functional programming has supposedly started reaching the masses. it was late night previous weekend when i briefly skimmed through an example of functional reactive programming to solve real time problems. ai people are also suggesting that we should be programming our robots in declarative domain specific languages soon. it would be nice to know the implications on the robotics community. \n\nthere has been a tremendous growth of frameworks like ros and urbi too!! that should be the region to look upon.. \n\nmost of the robotics, embedded and high performance ai codebase directly depends on c/c++ , and though languages like rust and d are bubbling up, wouldn't it take massive amount of time to adopt the new languages, if ever adaptation begins? \n\nai\n\ncorrect me, but it seems like a lot of time has passed and there are not many major production results from the ai community. i've heard about cognitive architectures of old like act-r and 4caps. they seem to be in hibernation mode! \n\nthere seems to be a lot of work lately on otherwise intelligent systems (solved problems) like computer vision and data mining, but these problems cater to supercomputing and industrial crowd more. could there be any possible shift towards low powered systems soon?\n\nthanks\n", "tags": "software artificial-intelligence programming-languages beginner embedded-systems", "id": "2022", "title": "how will the currently evaluated computer technology influence robotics and embedded systems in the forseeable future?"}, {"body": "i have an old beat-up netbook that is currently collecting dust. i've also only taken stuff apart, without having to worry about putting it back together, so please bear with my possibly stupid questions. \n\na) i imagine it's possible to wire this baby up to servos, breadboards, and all that good stuff. am i correct? \n\nb) i'd like to start with some simple raspberry pi-like projects (think automating my irrigation system, feeding the dog from work, etc). obviously barring the energy expenditure, wouldn't a netbook be more apt than a raspberry pi for handling this type of thing?\n\nc) i have basic python experience, but i wouldn't mind picking up more as i go. would that be sufficient?\n\ncheers!\n", "tags": "raspberry-pi python", "id": "2024", "title": "roboticize an old netbook?"}, {"body": "i'm reading amcl document on ros wiki. in its subscribed topics there is not odometry topic, why? it works only with laser?\n\nsubscribed topics: (from ros wiki)\n\nscan (sensor_msgs/laserscan)\n\ntf (tf/tfmessage)\n\ninitialpose (geometry_msgs/posewithcovariancestamped)\n\nmap (nav_msgs/occupancygrid)\n\nand my next question is how can i use  in  simulator for turtlebot? any tutorial available?\n", "tags": "slam ros navigation", "id": "2027", "title": "ros amcl does not need odometry data?"}, {"body": "i've got a code where i have a motor running back and forth and buttons connected to a scanner, when i press the buttons it causes the motor to stop  and over rides it. i would like them to run parallel to each other so the codes don't interrupt each other.\n\nhere is my code\n\n\n", "tags": "arduino stepper-motor c", "id": "2028", "title": "is it possible to run multiple loops at the same time? (arduino)"}, {"body": "in ros i've recorded a bag file from a custom robot (in real world) that does not provide covariance matrix and i want to use  to feed an ekf, but covariance matrix is 0. how can i calculate it?\n\nnote:\ncovariance matrix is needed by ekf to estimate position.\n\nit's a sample of :\n\n\n", "tags": "localization ros odometry", "id": "2031", "title": "calculate covariance matrix from x,y,z data"}, {"body": "from the designs usually shown of rocker bogie systems, the whole weight of platform seems to be supported by only one rod, be it differential bar or gear. isn't this a bit unstable system because if we have arms of rover at one end we will have a high torque about that rod?\n\nis my understanding such rocker bogie systems correct? if so, are there any solutions to this problem which don't sacrificing the functionality of rover?\n\nto clarify, i want to know how rovers like curiosity are designed so as to balance such a heavy platform with a differential bar mechanism. i am trying to make a small rocker bogie myself and i want to avoid this anticipated problem.\n", "tags": "mobile-robot design wheeled-robot", "id": "2032", "title": "rocker bogie suspension stability"}, {"body": "it would be incredibly useful if i could print my own gearing solutions, even if i have to print the gears one at a time.  however, i do not know how the market's cheapest printers will accommodate this task.\n\n--the gears need be 2-3 inches in diameter, and they will bear only a very light load (much less than 1 foot pound), so the material need not be strong or machinable. \n\n--the tolerances need only be sufficient for the teeth to mate robustly, preventing any hang.  unfortunately, i do not have a sense of what tolerances will allow gears to mate properly.\n\n--(if the machine is precise enough to print a hole to statically mate with a shaft of a specified dimensions due to friction, excellent.  if not, i can probably improvise a tiny shaft hole with adhesive.)\n\n--because this may be used in close proximity to pavement, a melting temperature in excess of 100f is desirable but not required.  \n\n--because any given element will interact kinetically only with other elements that have also been 3d printed (except a metallic shaft), compatibility with external resources is not required.  \n\ni would be grateful to anyone who could shed some light on this issue!\n", "tags": "wheeled-robot", "id": "2034", "title": "cheapest 3d printer for gears?"}, {"body": "i'm dealing with a board that no matter what i do i can't seem to make can work over 125&nbsp;kbit/s. i'll give some detail about the board on the bottom, but i'm going to keep this question generic.\n\nfirst of all, regarding hardware. from what i've gathered, there isn't any need for a pull-up resistor on the tx of can. is that correct? it may perhaps be chip-specific, but wherever i see, it seems that the tx/rx lines are directly connected to the transceiver.\n\nsecond, regarding bit-timing: using different calculators, for example, kvaser or the one from microchip, i can see the following configuration (for 64&nbsp;khz input clock):\n\n\n\ni've seen this from more than one source. furthermore, the numbers fit to the formula in the datasheet of the microcontroller.\n\nhowever, only the configuration for 125&nbsp;kbit/s works for me. i'm using canreal to monitor the messages.\n\ni've tried different configurations for the can, for example with 16 time quanta instead of 8 as well as changing my microcontroller's clock to 16&nbsp;mhz and using again different values. regardless of all that, speeds higher than 125&nbsp;kbit/s result in only errors and warnings in canreal (which are taken from the can driver). note that the same can board, driver and software works with 1&nbsp;mbit/s with some other hardware i have.\n\nthis all is made harder since, as soon as i put a probe from my oscillator on the tx line, it becomes a continuous 0-1 alteration like the following:\n\n\n\nwhich is not something i would be outputting by software. in fact, as soon as i remove the probe, the messages start arriving (again, only at 125&nbsp;mbit/s). so basically, i don't seem to be able to have any oscillator debugging available.\n\nback to my \"first of all, regarding hardware\", the shape of the signal suggests a pull-up resistor may be necessary, but i haven't seen the need for that in any datasheet i found. furthermore, my microcontroller configures the pin when used as can, so i don't have control over making it push-pull (since it looks like it's open-drain). not to mention the microcontroller doesn't even have a configuration to make the pin push-pull.\n\nis there any hidden parameter somewhere that should also be set? is a pull-up resistor necessary after all? why would the oscillator probe cause such a behavior?\n\n\n\ndetails from the board:\n\n\nmcu: p18f45k80. can is connected to its default rb2 and rb3.\ncan transceiver: iso1050  \ncompiler: mikroc\n\n", "tags": "microcontroller can", "id": "2036", "title": "making high can baud rates work"}, {"body": "i want to model the quadrotor using experimental method \"i have not built it yet\" what i want to do is: turn only one motor at a specific speed ,then plot the x,y,z and the angles then identify the transfert functions from the plots,x/w1,y/w1,... and so on, but i don't know if it's possible or what graphs i will have,so if you know about the subject or maybe tried something like that, and feel free to add anything you think might be helpfull\n", "tags": "control quadcopter brushless-motor uav", "id": "2039", "title": "quadrotor experimental identification"}, {"body": "i am building a machine and need 2 stepper motor for that.\nthe motors are driven using by a 3.3v arm device.\n\ni have made the following selections regarding the stepper motor, stepper motor driver and the power supply.\n\npower supply 12 volt power supply - 3.5 amp single output\n\nstepper motors stepper motor: unipolar/bipolar, 200 steps/rev, 42\u00d748mm, 4v, 1.2 a/phase\n\nstepper motor driver drv8825 stepper motor driver carrier, high current\n\ni tried my best to research the compatibility and came up with these.\n\nis this a good selection considering the fact that the power supply will be driving 2 of these motors.\n\ni will be running the motors at 1/16 step for high resolution.as far as the speed is concerned,it's going to be pretty slow but they will be running continuously for hours in end.basically what i am trying to do here is make a v-plotter.as far i can tell, there will be loads of start stop motion in the motors though.\n", "tags": "stepper-motor power stepper-driver", "id": "2042", "title": "choosing correct power supply for stepper motors"}, {"body": "i'm interested in getting a create for a project i'll be working on, and wanted some information about it from somebody that already has one:\n\n\nhow much weight can it safely carry? i talked with irobot's tech support and they told me the maximum is 5lb, but searching on the internet it seems like this limit is actually not as strict as it appears to be. i'm asking because i'd need to put a 3kg laptop on top of it, which would mean ~3.5-4kg if you also consider the kinect and eventual supports for both. i guess i could use a netbook and send the data i need to another computer, but i wanted to avoid the additional overhead of the wireless link.\nfor how long does it run using aa batteries? i'm inclined on not getting the battery pack, since i'd be using the robot in europe, so i'd also need a transformer if i went with the battery pack option.\n\n\nthanks!\n", "tags": "mobile-robot irobot-create", "id": "2045", "title": "questions about irobot create"}, {"body": "i am trying to get precise control over the speed of rover 5 based robot. it has four pwm controlled motors and 4 optical quadrature encoders. i am using 4-channel motor controller with rover 5 chassis. i am using arduino nano for control. i am able to read encoder int output and change pwm based on pulse width to control speed. but, as a result, i am getting heavy oscillations in the control output. that makes, the robot to move in steps, as pwm is changing constantly. i need an algorithm which can minimize this ringing and have a smooth moving robot. here is my arduino code snippet.\n\n\n\nhere req_speed is between -100 to 100, where sign indicates direction. please consider all undefined variables as globals. i experimentally measured that, when motor is running at full speed, the pulse width is around 3200us.\n\nencoders' int outputs (xor of a and b) are connected to a0 thru a3. motor pwm is connected to d3, d5, d6, d9. please let me suggest any improvements to this code and advice me about what am i missing here.\n", "tags": "arduino motor pwm", "id": "2048", "title": "encoder based speed control for rover 5"}, {"body": "i have a chip that is labeled l293d with a small 'st' logo, which does not behave like i believe a l239d should:\n\ni have the chip on a breadboard with pins 4,5,12 and 13 connected to the ground rail. the positive rail gets 6v from a battery pack.\na motor is connected to pins 3 and 6.\npin 2 is connected to the positive rail.\n\nnow, when i connect pin 1 (enable 1) to the positive rail, the motor spins, which is expected.\n\nthe weird thing is that if i connect pin 16 instead of pin 1 to positive, the motor spins, as well. \n\nalso, with the motor connected to 11 and 14, and 15 connected to positive, the motor spins if i connect pin 1 or pin 16 to positive, but not if i connect pin 9 (which should be the enable pin for that side).\n\ndoes any of that make sense? am i missing something here?\n\nthanks!\n", "tags": "motor h-bridge", "id": "2053", "title": "odd l293d behavior: pin 16 seems to act as enable"}, {"body": "i am working right now with arduino uno and hc-05 bluetooth module.i followed the instruction given on this link for wiring. so there are 2 mode of working with this hc-05 module\n\n\nsimple serial communication\nworking in at command mode so as to change the parameters of hc-05 module\n\n\nas long as i work in simple serial communication mode, everything works fine but when i tried to change the parameters of module, it didn't work out. for working in at command mode, pin no 34 of hc-05 module needs to be high which i had taken care of. lately i found that in mu module they had knowingly not connected the berg strip to pin 34, so i connected the pin directly, even though i am not able to change the parameters of module and when i write any command on com port of arduino ide, i get this response\n\n\n\ni think that garbage is due to my code\n\nhere is my code:\n\n\n\ni am not able to figure out what i am doing wrong. i used this command on com port \n and many other commands but every time i get the same response.\n\ndid i mess up with my bluetooth module unknowingly?\n", "tags": "arduino c communication serial", "id": "2056", "title": "bluetooth module hc-05 giving error :(0)"}, {"body": "i'm working on a legged robot and generating joint torques. basically the robot seems to be statically stable to some extend. the robot goes instable if the center of pressure moves to the border of the feet. i'm looking for some method to move away the center of pressure  from the feet edges after having calculated my joint torques. in sentis thesis ( http://ai.stanford.edu/~lsentis/files/thesis-sentis-2007.pdf ) , it is mentioned that he somehow manages to cancel out the internal forces to keep the feet flat against the supporting surfaces. \n\ndoes anyone has got experience in dealing with internal forces? as far as i understood the literature one can modify the nullspace of the calculated torques to achieve that the cop remains in the geometrical center of the considered foot. i'm looking for methods apart from the virtual linkage model as it did not seem to work for me or someone with whom i could discuss the virtual linkage model described in ( http://ai.stanford.edu/~lsentis/files/tro-2010.pdf ) as i might not have it understood it correctly.\n", "tags": "stability legged", "id": "2062", "title": "guidance for compensating internal forces on closed loop chain"}, {"body": "i am trying to use a push button in order to know and print number of time a push button is pressed using a counter.but everytime i press the button , counter get incremented to sometime 3 and sometime 5 and some time counter does start >100 and continue.\n\ni had preferred the this link for wiring push button with arduino.\n\nand here is my code\n\n\n\ni dont know why count is coming absurdly and unevenly.\n", "tags": "arduino c serial", "id": "2063", "title": "unable to read pushbutton press properly in arduino"}, {"body": "i have an rc car. the battery provides power to the esc and then the esc provides 6&nbsp;v back out to the receiver. instead of the receiver i have a raspberry pi, which uses the 6&nbsp;v, steps it down to 5&nbsp;v and provides power to the raspberry pi.\n\nthe problem\n\nevery time we go full power*, there is a lack of voltage and the raspberry pi seems to hard reset.\n\n* by full power we mean direct to 100% and not ranging from 0-100\n\ni am not an expert in electrical circuits, but some of the suggestions are to use a capacitor to provide the missing 5&nbsp;v in the interim. how do i prevent the raspberry pi from dying in the event of full power?\n", "tags": "raspberry-pi electronics esc", "id": "2068", "title": "my raspberry pi is losing power in a surge"}, {"body": "i could swear that it was working for a while. i got back to my desk, tried it again, and it's no longer working. could i have fried the no pins on both sides? this is a dpdt relay. everything works normally on the nc pins. i have never applied more than 5v. i do hear the relay click when i apply 5v to the coil. but when i measure voltage on the no pins, i get 0v. has anyone else seen this? i have two of these relays and i can't seem to get voltage on the no pins with either relay. i should clarify that i'm expecting the same 5v power source to power both the coil and the common pins. if the nc pins work then i don't see why the no pins shouldn't. in both cases the 5v is shared between the coil and any load attached to the nc/no pins. i did try driving the entire circuit off a 9v power supply, but that did not change the results (and that does contradict my earlier statement that i've never applied more than 5v to this relay). my circuit is based on charles platt's \"make: electronics\", p. 59.\n\nhere's a pic of the schematic i am following, except that i am using a 5v relay and a 5v power supply (usb port) and i am using piezo buzzers without resistors instead of leds.\n\n\n", "tags": "electronics", "id": "2074", "title": "omron g5v-2 relay no pins not working"}, {"body": "i'm looking for particle filter implementation in ros to use in mobile robot localization, but it seems the only available package is amcl (adaptive monte carlo), i'm not sure is it possible to use it as particle filter or not, and if it's feasible, how?\n\nnote: the robot (wheeled robot) provides odometry data and another data source is , that provides visual odometry data using fovis.\n", "tags": "mobile-robot localization ros particle-filter", "id": "2075", "title": "particle filter implementation in ros"}, {"body": "i frequently bang my head on my desk after performing a task poorly. i would like to eliminate the unnecessary middle step of actually performing a task poorly. as such, i would like to design a system to hold my head and repeatedly strike it against my desk. alternatively, a system that holds the desk and repeatedly strikes it against my head would be acceptable. requirements are at least 2 strikes per second maximum with ~50cm travel.\n\ncan anybody make any recommendations for a system to base this device off of? denso products, while small and affordable, do not have the required load capacity (some users may have a rather large head, and involuntary resistance is to be expected -- at least near the start of the cycle). i am thinking of something more industrial, perhaps:\n\n\n", "tags": "robotic-arm", "id": "2077", "title": "recommendations for system to repeatedly force contact between head and desk"}, {"body": "i am looking for some material to build a soft clear protective covering for rgb leds. the material needs to be close to transparent to allow light to shine through, be soft and compliant but sturdy enough to withstand someone standing on it. the ultimate goal is to have a floor of these leds that someone can jump in barefoot and change led colors. \n\ni have tried gel candle wax and silicone but neither worked very well. i am looking for other material ideas and this was the most relevant of the stackexchanges that i could find. \n", "tags": "arduino", "id": "2081", "title": "soft led protection material"}, {"body": "i am using arduino uno to read a push button every time it is pressed.earlier i was simply reading the digital io pin to read the count and then i faced the condition of switch debounce regarding which i had asked a question here and get to know that i must use interrupt instead of reading a digital io pin but even after using interrupt, i was facing the problem of switch debouncing.\n\nso i used this link and code given on this link\n\n\n\nand change \n\n\n\nto 10(means read any thing ina time gap of 10 mili second) as the code says.now what is happening, code is running on the board and after some time my board get hang and led stop toggling on any press of push button and then i had to manually reset my board.i also want to add upon a thing that i am also using a serial port in between when led toggles or switch is pressed.\n\ni am totally confused why this is happening.there can beone possibility that this is happening because i reduced time gap between two consecutive events to 10 from 50 miliseconds and that might be making avr get hanged and thus require a manual reset.\n", "tags": "arduino c serial communication", "id": "2083", "title": "arduino uno getting a type of \"hanged\" while runing samll code of switc debounce and serial print"}, {"body": "i have a bag file that contains couple of topics needed for localization, odometry data, kinect data and . what i want is watching robot's movement path in  after initializing robot position (even i don't know how to initial it). any help?\n\nall topics:\n\n\n", "tags": "localization ros", "id": "2084", "title": "fake localization using bag file in ros"}, {"body": "i am making a mobile base for a robot with wheels. i want to use a kinect like a movement sensor (to avoid obstacles, recognition of people, etc...) but i read that there is 2 models, the 360 and the developer.\n\nwhich kinect works well for my job? and another thins, its there another thing that can i use like a movement sensor? to see diferent posibilities,\n", "tags": "mobile-robot wheeled-robot kinect", "id": "2087", "title": "which kinect to a movement base?"}, {"body": "for a certain robotic application (actually for the ftc challenge this year) our team is performing an operation where a servo-driven arm could potentially be forced into an unknown position. we are using nxt+tetrix.\n\nsince this could damage a powered servo working against this forced position (servo holding arm weight on fixed base is now trying to move heavy base relative to fixed arm), we are thinking about somehow de-powering our servos (or servo controller), in order to get the servos to \"relax\" and accept the mechanically-forced position.\n\noriginally, we were thinking of having our robotc code determine the physical position of a given servo and set its desired position to there every loop, limiting how much a servo would try to fight the movement, but to our dismay,  actually gives us the setpoint, and not the physical location (due to the servo being unable to provide this information).\n\nwe also considered setting  to 1(the minimal value) but this only changes the rate of the target location changing relative to the previous target.\n\nso, we're concluding that the only way to really do this is to fully depower the servos. is this possible on nxt/tetrix with robotc?\n\na few notes:\n\n\ni realized as well that one could suggest to rig an encoder(associated with a tetrix motor that does not need an encoder) onto the rotating area. that actually would not work for mechanical constraints.\ni looked into setting  as shown here but am not sure how to send the i2c commands needed. if someone could clue me in to how these commands would be sent in terms of c code, that would be very helpful.\n\n", "tags": "power rcservo nxt robotc", "id": "2088", "title": "powering down servos completely in robotc+tetrix"}, {"body": "i recently purchased at ey-80 from electrodragon: ey-80 all in one 9-axis motion sensor (gyro + acceler + magneto + baro)\n\ni am having a hard time compiling the example code on my arduino:\n\n\n\nthis is what is happening.  so far, i am only copy and pasting the code.  any help? (i am somewhat new to programming, so don't fully understand all of the code)\n", "tags": "arduino sensors gyroscope", "id": "2089", "title": "compiling code for ey-80"}, {"body": "i am about to make an rc car which uses a wifi connection. the body for the car would be made from aluminium and the wifi receiver will be placed inside this aluminium casing. \n\nhow do i make sure that this will work?\n\nwould i be forced to change my material or can i just make an extension for the receiver and make sure it is out of the casing? \n\nif so , would that really help me?\n", "tags": "wifi", "id": "2090", "title": "wifi to pass through aluminium"}, {"body": "i am using an arduino uno to control an esc for my (in progress) quadrocopter.  i am currently using the servo library to control the esc, which works great.\n\nexcept..\n\na count of 100 is max speed, meaning i only have 10 speeds between 90 (stopped) and 100 (motor at full power) to correctly run my quadrocopter, i would like to have many more speed options.  any ideas?  i'm having a hard time using a pwm signal, i might not be doing it right though.\n\nmy current code is here:\n\n\n", "tags": "arduino control quadcopter esc servomotor", "id": "2091", "title": "how to use arduino for esc control?"}, {"body": "i'm trying to add my own robot in morse 1.1 (using ubuntu 12.04). i am struggling to add an armature actuator and armature pose sensor to an existing robot. can someone please explain how this can be done (preferably with some sample code and using the socket interface).\n\nthanks. \n", "tags": "mobile-robot sensors robotic-arm simulator python", "id": "2096", "title": "using armatures in morse robotic simulator"}, {"body": "i searched for gps devices that provide 1 sec updates to server, but i have not found any.\n\ni found this \n\n\n  t=30s.module has sent a monitoring data packet. after 12 seconds the\n  server sends an acknowledgement. in 18 seconds later (t = 30s) module\n  sends the next monitoring data packet\n\n\nare there any products that take less than this time?\n\nwhy do gps devices take this much time to send data?\n", "tags": "sensors gps", "id": "2098", "title": "are there any gps sensors that provide data at 1hz or faster?"}, {"body": "as i understand it, a kalman filter uses a mathematical model of the robot to predict the robot's state at t+1. it then combines that prediction with information from sensors to get a better sense of the state.\n\nif the robot is an aeroplane, how accurate/realistic does the model need to be? can i get away with simple position and velocity, or do i therefore need an accurate flight model with computational fluid dynamics?\n", "tags": "kalman-filter uav", "id": "2101", "title": "do i need an accurate flight model for a uav?"}, {"body": "i'm trying to figure out a way that i can calculate the probability that a particle will survive the re-sampling step in the particle filter algorithm.\n\nfor the simple case of multinomial re-sampling, i can assume that we can model the process like a binomial distribution if we only care about one sample/particle.\n\nso if the particle has a weight of w that is also the probability that it will get selected in a step of the re-sampling. so we use 1 - p(k, p, n) where p is the binomial distribution, k is 0 (we did not select the particle in all our tries), p is equal to w and n is equal to m, the number of particles.\n\nwhat is the case though in the systematic re-sampling, where the probability of a particle being selected is proportional but not equal to its weight?\n", "tags": "particle-filter", "id": "2104", "title": "how to calculate probability of particle survival for particle filter?"}, {"body": "basically, i want to detect an ultrasonic beacon in a radius around the robot.\nthe beacon would have a separate ultrasonic emitter while the robot would have the spinning receiver.\n\nare there any existing ultrasonic sensors that would meet this use case or am i stuck hacking one together myself?\n\nis ultrasonic even the best choice? i was hoping that the beacon would be kept in a pocket, so i figured optical sensors were out.\n\nedit: the beacon and robot will both be mobile so fixed base stations are not an option.\n", "tags": "sensors", "id": "2105", "title": "360 degree ultrasonic beacon sensor"}, {"body": "having read the article \"this car has electric brains\" in popular mechanics, august 1958 i have some questions.\n\nhow practical were his methods? was his work acquired by a car manufacturer or some other company? were his methods developed further?\n\nhow did his corner navigation work? i don't think he needed to know the distances of road segments; i think he could have used sonar or radar to detect a corner but if cars were entering the corner before him, he could misinterpret those cars as a wall and the absence of a corner. additionally i think he'd need two sonar/radar systems on both sides of the cars which aren't mentioned; all that's mentioned is a set of relays.\n\nwhat is the compensator that is mentioned (it's said to function as a gyroscope)? i cannot find any information on this device (that i'm sure is relevant).\n", "tags": "design navigation", "id": "2106", "title": "what happened to butler's car?"}, {"body": "pomdps are used when we cannot observe all the states.\nhowever, i cannot figure out when these pomdps can be useful in robotics. what is a good example of the use of pomdps? (i have read one paper where they used them, but i didn't find it obvious why pomdps should be used) what would be good projects ideas based on pomdps?\n", "tags": "algorithm artificial-intelligence", "id": "2109", "title": "pomdps in robotics"}, {"body": "i have a wl v262 quadcopter and i want to control it using arduino instead of the joysticks on the transmitter. i opened up the transmitter and saw that each joystick has 2 potentiometers on the pcb and that the voltage for each pot goes from 0-3.3v. i used arduino's pwm and a low pass filter and connected the output of the filtered output to the potentiometer's analog pin which is connected to the pcb (i cannot desolder and take out the pots from the pcb) but even with this $v_{out}$ going onto the analog pin, my transmitter's display gave ???? \n\nnow i am really confused and frustrated because i don't know how else to control this transmitter other than attaching stepper motors to the joysticks and manually controlling the transmitter but this is really my last resort. can someone help me with this? i have spent hours and hours trial and error but i am getting nowhere. \n\nhere is the pcb of the transmitter:\n\n\n\n\n", "tags": "arduino sensors radio-control wireless", "id": "2110", "title": "rc transmitter quadcopter with arduino"}, {"body": "a google search on \"bloodstream nanobots\" yields thousands of results and just on the first page, many results of blog posts that date back to 2009. it is nearly 4 years later. \n\ni've had no luck in finding any information on actual approval of these bots.\n\nare there any countries at all who have approved this? people seem to have talked about it like crazy 4 years ago, yet, we're still not seeing anything.\n", "tags": "microcontroller", "id": "2111", "title": "have bloodstream nanobots been approved in any countries?"}, {"body": "i am interested in getting an arducopter with an ardupilot(apm). i read through the documentation and from what i understand, ardupilot is the low level hardware and firmware that directly controls the motors of the arducoptor. \n\ni would like to know if there is a higher level programmatic interface to the ardupilot? the mission planner provides a user interface to control the ardupilot. but is there a programmatic interface to control it?\n\nin other words, would it be possible for a user written 'linux process' to receive and send sensory data to and from the ardupilot respectively?\n", "tags": "quadcopter ardupilot", "id": "2114", "title": "sending and receiving parameters to ardupilot"}, {"body": "for examples if i have this robotic arm:\nexample, for the base rotation (5th dof in the clip at 0:58), we know that the z axis for that joint will be the same as the z axis for the base frame{0}, but i don't know about y and z axises of the base rotation respects to the base frame, should they be the same or not ?\n\nand one more thing, for defining the frame of the base rotation (at 0:58 in the clip), the vertical arm pitch (at 0.47 in the clip) and the horizontal arm pitch (at 0:46 in the clip), it's pretty easy, but i don't know how to continue for defining the frame of wrist roll (at o.12 in the clip) and wrist pitch (0.23 in the clip) since the angle between the z axis of wrist roll and the wrist pitch is now 90o.\n thank you very much.\n", "tags": "localization kinematics robotic-arm", "id": "2117", "title": "defining frames for 5dof robotics arm"}, {"body": "for a university course i have been asked to design a rough \"specification\" for a system that will deburr a plastic box that appears in a workspace. due to irregularities in the boxes edges i cannot use simple position control and must use force control.\n\ni have so far decided on;\n\nusing an ir sensor to detect the box has appeared in the workspace.\n\nuse an epson 2 axis robot to move around the work piece\n\nuse an ati 6 axis force sensor to maintain a constant force against the edge of the box as the deburrer/robot moves around it.\n\nis there a simple means of detecting the end of each side of the box ?\na 0n force value would indicate reaching the edge of a box but it could also mean a breakage in the box which was also specified. how can i distinguish between the two ?\n\nalso does my work so far sound sensible ?\n\nthanks for any help\n", "tags": "control", "id": "2119", "title": "deburring robot (plastic box)"}, {"body": "suppose i have a particle filter which contains an attitude state (we'll use a unit quaternion from the body to the earth frame for this discussion) $\\mathbf{q}_b^e$.\n\nwhat methods should or should not be used for resampling? many resampling schemes (e.g. this paper) seem to require the variance to be calculated at some stage, which is not trivial for $so\\{3\\}$. or, the variance is required when performing roughening.\n\nare there any good papers on resampling attitude states?  especially those that re-sample complete poses (e.g. position and attitude)?\n", "tags": "particle-filter pose", "id": "2121", "title": "resampling attitude states (quaternions, rotation matrix) in a particle filter"}, {"body": "i would like to know if anyone here has used the blueview sdk (linux) for retrieval of images from the pings obtained by a multibeam sonar (p450, p900, etc.) ?\nif so, i'd like to know why would anyone get a null head when i trying to retrieve the head (eventually for the pings to be converted to an image) using the  method. my snippet for retrieving the image from a  file (some_son_data.son) is given below:\n\n\n", "tags": "auv sonar", "id": "2124", "title": "image retrieval through a multibeam imaging sonar"}, {"body": "i'd like some well put video series of like 30 videos. or anything but it needs to thorough and in easy english...less mundane. so far all resources i have found either go upto resistors code or of projects that tell you do this and this and this and tada you got this.\n\nis there really no online resource for people to learn electronics. i want further master analog and do move on to digital cause it's better to spend 0.40 cents.... than\n spend $95 on components and get the whole thing on tiny chip.\n\nplease bare with me like six months i have been searching for legit source, material that is meant to teach you. i like pictures and colors. \n", "tags": "electronics", "id": "2127", "title": "where can i learn electronics from intro to advance digital"}, {"body": "update: this exact problem has been solved in stackoverflow. please read this post there for further explanation and a working solution. thanks!\n\ni am working on an application where i need to rectify an image taken from a mobile camera platform. the platform measures roll, pitch and yaw angles, and i want to make it look like the image is taken from directly above, by some sort of transform from this information. \n\nin other words, i want a perfect square lying flat on the ground, photographed from afar with some camera orientation, to be transformed, so that the square is perfectly symmetrical afterwards. \n\ni have been trying to do this through opencv(c++) and matlab, but i seem to be missing something fundamental about how this is done.\n\nin matlab, i have tried the following:\n\n\n\nwhere r_z/y/x are the standard rotational matrices (implemented with degrees).\n\nfor some yaw-rotation, it all works just fine:\n\n\n\nwhich gives the result:\n\n\n\nif i try to rotate the image by the same amount about the x- or y- axes, i get results like this:\n\n\n\n\n\nhowever, if i rotate by 10 degrees, divided by some huge number, it starts to look ok. but then again, this is a result that has no research value what so ever:\n\n\n\n\n\ncan someone please help me understand why rotating about the x- or y-axes makes the transformation go wild? is there any way of solving this without dividing by some random number and other magic tricks? is this maybe something that can be solved using euler parameters of some sort? any help will be highly appreciated!\n", "tags": "computer-vision cameras", "id": "2130", "title": "transform image using roll-pitch-yaw angles (image rectification)"}, {"body": "i am trying to build an advanced coloured lines following robot with the ability to differentiate between many different coloured lines and follow them. i am looking for the right sensor that will help my robot achieve its objective.\n\nas i was researching i came across the ev3 colour sensor which can detect up to 7 colours.\n\nis this sensor suitable for my project?\n\nwhat other sensors can i use and how?\n\nthank you\n", "tags": "mobile-robot sensors line-following", "id": "2131", "title": "line following robot with ev3 colour sensor"}, {"body": "i am trying to get the  command to work properly in a python script i'm writing on angstrom for the beagleboard.\n\nhere is my code:\n\n\n\ni don't know what's missing, but here is what i do know.\nwhen i don't specify the backend, no backend is found.  when i do specify the backend  i get the following error: \n\n\n\nwhat am i missing so that this will work properly?\n\nthank you.\n", "tags": "rcservo python usb", "id": "2132", "title": "what dependencies do i need for usb programing in python with pyusb?"}, {"body": "given a robot with 2 wheels with radius r on one axle with length d, i want to set the wheel speed so that it turns to an angle phi as fast as possible. the timestep t is 64 milliseconds.\n\ni thought the wheel speed could be set to v = ((desired_heading-actual_heading) * circumference_wheel_trajectory)/(2*pi * t * wheel_radius). this will converge to a somewhat right angle, eventually, but its very slow and becomes slower as i approach the angle i want to be at.\n\nis there an alternative/better way to do this?\n", "tags": "kinematics", "id": "2133", "title": "turning a differential drive robot to a specific angle"}, {"body": "i'm just wondering that is there any case that when algebraic way can't solve the problem while the geometric can ? cause i'm working on a 2dof robotics arm this one, i know the length of l1 and l2, location that i want for the end effector, then i tried calculating the angles by using algebraic but it gave me cos(alpha) > 1, but when i tried solving with geometric, i can find the solution, so is it because i use a wrong way in algebraic ? \nthank you very much.\n", "tags": "localization kinematics robotic-arm inverse-kinematics", "id": "2135", "title": "algebraic and geometric in inverse kinematic"}, {"body": "is there a node or package that can send commands to  to move atrv-jr like 2 meters forward or turn it 90 degree to right/left? i don't want to tell the robot to move with specified speed. for example when i use this command  the robot starts moving forward until i send another command or send  command.\n", "tags": "mobile-robot ros navigation", "id": "2138", "title": "move atrv robot to specific distance using ros"}, {"body": "i am taking information for my project and i need to see libraries ans sdks. searching in the web i found that openni has a lot of functions and when i try to found another sdk, i dont find any other. i am working with a kinect and a xtion so i need an sdk who works in both. \n\nis there any other sdk o set of libraries that works well in both?\n\nthanks!\n", "tags": "kinect openni", "id": "2145", "title": "another sdk like openni"}, {"body": "i am trying to manually calibrate the on-board accelerometer of an apm 2.6 controller.\n\ni am using the following code (i found this somewhere, don't remember where) with arduino 1.0.5 (in windows environment) to fetch the accelerometer and gyro data:\n\n\n\nmy objective use to calibrate the accelerometers (and gyro), so that i can use them without having to depend on mission planner.\n\ni'm reading values like:\n\n\n\nfrom what i understand, spiread(...,...) returns an analog voltage value from the data pins of the sensor, which happens to be proportional to the acceleration values. right?\n\nmy question is: how do i go about calibrating the accelerometer?\n\nwhat i've tried till date: i've tried the \"place horizontal... place nose down... left side, right side\" technique used by mission planner. \n\nbasically, when placed on horizontal position, the sensor is experiencing +1g on it's z axis and 0g in x and y axis. left/right side provides \u00b11g on y axis and nose down/up provides \u00b11g on x axis. \n\nnow for every orientation, i've passed the raw sensor data through a lpf and then computed the mean, median and sd of this sensor data over 100 iterations. i store this mean, median and sd value in the eeprom for each axis (one for +1g and one for 0g).\n\nnow, when i use the sensor, i load the stats from the eeprom, match the mean/median and standard deviation with the current reading of 4/5 iterations. \n\nhere i'm working under the assumption that the values between 0g and +1g (and anything above 1g) can be interpolated/extrapolated from the data using a linear plot. \n\n\nis this the correct approach for calibration?\ncan you suggest a better way?\ni noticed that the maxima/minima for each axis is different. is this\nthe expected outcome or is there something wrong in the code?\nwhat do i do with the gyro? how to calibrate for angular\nacceleration?\n\n", "tags": "arduino accelerometer ardupilot", "id": "2146", "title": "apm accelerometer calibration"}, {"body": "i am totally new to the camera interface and usage in an embedded project, and would like to use a cmos vision sensor like this.this project further will be used to power a small robot with on-board video processing power using processors like arm 9.\n\ni do have a limitation that until now i have worked only on 8-bit micro-controllers like the atmega 8, 16, 32 and on the arduino platform. i think that for better processing we can use arduino due.\n\nwith the data sheet for the cmos camera above, we can build its breakout board. but what next? i haven't i found any useful resources while searching. all i need to do is to capture a small video and store it in a sd card.\n\ni have seen these links but they haven't proved to be very useful as they don't provide me the required form factor. i am looking to interface this module to a customized board.\n\nso what so i need to understand about what commands they accept for their proper functioning like starting to take video and posting them out on a output pin.\n\nif we get a video on a output pin, to which pin should i take that output to on my controller, i.e. on uart or i2c or spi?\n", "tags": "arduino microcontroller computer-vision cameras c", "id": "2147", "title": "how can i interface my cmos camera module to an arduino?"}, {"body": "i am new to robotics and control and i have been thinking about how to deal with problems in real life. i have passed a course in control, but i do not have any idea about control for discrete/digital systems.\nthere are a lot of robots and in general dynamic systems which are controlled by microcontrollers or computers with some software, i.e. simulink. usually there are sensors which send feedback to the microcontroller or the computer and the controller sends a signal w.r.t the input signal from sensors. i was wondering how we decide if the system is discrete or continuous? how one can decide if he should use discrete or continuous blocks in simulink to control a dynamic system. does it really matter which one we use?\nafter all computers are digital and i think it is easier to work with digital signals and also  do we really have continuous signal? i have not passed any signals course, so my questions might be really easy. i did not find any other place for my question.\n", "tags": "control microcontroller", "id": "2148", "title": "continuous or discrete"}, {"body": "i am currently working on a project for school where i need to implement an extended kalman filter for a point robot with a laser scanner. the robot can rotate with 0 degree turn radius and drive forward. all motions are piecewise linear (drive,rotate,drive).\n\nthe simulator we are using does not support acceleration, all motion is instantaneous. \n\nwe also have a known map (png image) that we need to localize in. we can ray trace in the image in order to simulate laser scans. \n\nmy partner and i are little confused as to the motion and sensor models we'll need to use. \n\nso far we are modelling the state as a vector $(x,y,\\theta)$.\n\nwe are using the update equations as follows\n\n\n\nwe thought we had everything working until we noticed that we forgot to initialize  and that it was zero, meaning that there was no correction happening. apparently our propagation was very accurate as we haven't yet introduced noise into the system.\n\nfor the motion model we are using the following matrix for f:\n\n$f = \\begin{bmatrix}1 &amp; 0 &amp; -v*\\delta t*sin(\\theta)  \n\\\\ 0 &amp; 1 &amp; v*\\delta t*cos(\\theta)   \n\\\\ 0 &amp; 0 &amp; 1 \n\\end{bmatrix}$\n\nas its the jacobian of our update formulas. is this correct?\n\nfor the sensor model we are approximating the jacobian (h) by taking finite differences of the robots $x$, $y$ and $\\theta$ positions and ray tracing in the map. we talked to the ta who said that this would work but i'm still unsure it will. our prof is away so we can't ask him unfortunately. we are using 3 laser measurements per correction step so h is a 3x3. \n\nthe other issue where having how to initialize p. we tried 1,10,100 and they all place the robot outside the map at (-90,-70) when the map is only 50x50.\n\nthe code for our project can be found here: https://github.com/en4bz/kalman/blob/master/src/kalman.cpp\n\nany advice is greatly appreciated.\n\nedit:\n\nat this point i've gotten the filter to stabilize with basic movement noise but no actual movement. as soon as the robot starts to move the filter diverges quite quickly and exits the map. \n", "tags": "mobile-robot ros kalman-filter ekf", "id": "2149", "title": "extended kalman filter with laser scan + known map"}, {"body": "i am planning on building a robot with wheels (later legs, if possible), that can move around the room and analyze certain things, using a couple sensors.\nin the later steps more functions such a grabbing are the things i want to add.\n\ncould you recommend me a micro controller?\n\nmy concern about arduino is that there aren't enough slots, raspberry pi seems like it constantly needs a screen for the user.\n\ni am a complete amateur when it comes to robotics. however, i am quite familiar with the computer languages java and python. since i wrote a fun app for android for myself i would love the robot to be compatible with android, too.\n", "tags": "arduino raspberry-pi beginner", "id": "2151", "title": "what micro controller should i use?"}, {"body": "i am testing an industrial robot (abb irb 1410) using three simple micron dial gauges to get x,y,z values at particular point by varying speed, load and distance from home position. \nmy questions are,\nwhether these three parameters influencing the repeatability or only the accuracy?\nusing dial gauges, without any relation to the base frame, is it possible to measure accuracy?\nis any other cost effective method to measure the repeatability and accuracy like above method?\n", "tags": "industrial-robot calibration", "id": "2156", "title": "how to calibrate an industrial robot?"}, {"body": "i'd like to build a robot as small as possible and with as few \"delicate\" parts as possible (the bots will be bashing into each other).\n\ni was wondering if it was possible to use a small chip that could receive bluetooth/ir/wifi commands to move the motors, and in turn, send back feedback based on sensors such as an accelerometer (to detect impact).\n\ni can probably achieve something like this with the picy \n\n\n\nhowever this is slightly bigger than i'd like (due to the size of the pi) and i'm not sure how long the pi would last taking continuous impacts.\n\ni'd therefore like to try to offset the brain (the pi) to the side of the arena and just use a small chip to receive move commands, and send back data from the accelerometer.\n\ndo you have any recommendations for such a chip? wifi would be my choice but if it impacts the size i could try bt\n\nedit: after further research it seems an arduino nano with a wifi redback shield might do the job along with something like this for the motors: http://www.gravitech.us/2mwfecoadfor.html\n", "tags": "raspberry-pi rcservo accelerometer", "id": "2158", "title": "making a tiny robot by using a remote brain"}, {"body": "http://www.youtube.com/watch?v=4vh_r1nlmx0 is from 2011 swarm and shows rc aircraft or combat wings trying to hit each other in the air. \n\nscoring a hit is pretty rare, and i'd like to increase a pilot's chances by using a computer targeting system.  it would be an offline system that gets data from sensors on the airplane.\n\nwhat sensor(s) would work for this application?\n", "tags": "mobile-robot sensors", "id": "2159", "title": "computer aided rc airplane combat"}, {"body": "i'm building a project that uses a cell phone to control a microcontroller via bluetooth.\n\ni've decided to use the hc-05 bluetooth module.\nhc-05 manual: http://www.exp-tech.de/service/datasheet/hc-serial-bluetooth-products.pdf\n\nand the phone i'm using is the nokia c3-00 (series 40).\nhttp://developer.nokia.com/devices/device_specifications/c3-00/\n\nthe hc-05 module uses the spp bluetooth profile while my phone only supports dun, ftp, gap, goep, hfp, hsp, opp, pan, sap, sdap profiles. but to my knowledge the phone api utilizes rfcomm.\n\nquestion is, can i use this bluetooth module with my phone?\n\nthanks in advance and my apologies if my question is too trivial as i'm quite new to bluetooth.\n\n-shaun \n", "tags": "microcontroller", "id": "2165", "title": "spp bluetooth profile compatibility with phone"}, {"body": "i'm trying to get a quad rotor to fly. the on board controller is an ardupilot mega 2.6, being programmed by arduino 1.0.5.\n\ni'm trying to fly it in simple autonomous mode, no radio controller involved. i've done a thorough static weight balancing of the assembly (somewhat like this: http://www.youtube.com/watch?v=3nevteb2nx4) and the propellers are balanced correctly.\n\ni'm trying to get the quadcopter to lift using this code:\n\n\n\nbut the issue is, as soon as the quadcopter takes off, it tilts heavily in one direction and topples over. \n\nit looks like one of the motor/propeller is not generating enough thrust for that arm to take-off. i've even tried offsetting the weight balance against the direction that fails to lift, but it doesn't work (and i snapped a few propellers in the process);\n\n\nis there something wrong with the way the escs are being fired using\nthe servo library?\nif everything else fails, am i to assume there is something wrong\nwith the motors?\ndo i need to implement a pid controller for self-balancing the roll\nand pitch just to get this quadrotor to take off?\n\n\nedit 1:    thanks for all the replies.\n\ni got the pid in place. actually, it is still a pd controller with the integral gain set to zero. \n\nhere's how i'm writing the angles to the servo:\n\n\n\nki is zero, so i'll ignore that.\n\nwith the value of kp set somewhere between 0.00051 to 0.00070, i'm getting an oscillation of steady amplitude around a supposed mean value. but the problem is, the amplitude of oscillation is way too high. it is somewhere around +/- 160 degrees, which looks crazy even on a tightly constrained test rig. \n\n\n\n[  edit 2: how i calculate the term 'perror' - simple linear thresholding. \n\ni've a precomputed data of the average readings (mean and sd) coming out of the gyro when the imu is steady. based on the gyro reading, i classify any motion of the setup as left, right, forward or backward. \n\nfor each of these motion, i increase the perror term for two of the motors, i.e, for right tilt, i add perror terms to motors 2 &amp; 3, for left tilt, i add perror term to motors 1 &amp; 4 etc. (check the comment lines in the code snippet given above). \n\nthe magnitude of error i assign to the perror term is = abs(current gyro reading) - abs(mean steady-state gyro reading). this value is always positive, therefore the side that is dipping downwards will always have a positive increment in rpm.  ]\n\n\n\nas i crank up the derivative gain to around 0.0010 to 0.0015, the oscillation dampens rapidly and the drone comes to a relatively stable attitude hold, but not on the horizontal plane. the oscillation dies down (considerably, but not completely) only to give me a stable quadrotor tilted at 90 - 100 degrees with horizontal. \n\ni'm using only the gyros for calculating the error. the gyros were self calibrated, hence i do expect a fair amount of noise and inaccuracy associated with the error values. \n\n\ndo you think that is the primary reason for the high amplitude\noscillation?\n\n\none other probable reason might be the low update frequency of the errors. i'm updating the errors 6 times a second. \n\n\ncould that be a probable reason it is taking longer to stabilise the\nerror?\n\n\nand, for the steady state error after the wild oscillations dampen, is it necessary to fine tune the integral gain to get rid of that?\n\nplease help.\n\n\n\nedit 3:  i cranked up the frequency of operation to 150+ hz and what i get now is a very controlled oscillation (within +/- 10 degrees). \n\ni'm yet to tune the derivative gain, following which i plan to recompute the errors for the integral gain using a combination of gyro and accelerometer data. \n\n\n\nedit 4:  i've tuned the p and d gain, resulting in +/- 5 degrees oscillation(approx). i can't get it to any lower than this, no matter how much i try.\n\nthere are two challenges about which i'm deeply concerned:\n\nafter 5 to 8 seconds of flight, the quadcopter is leaning into one side, albeit slowly. \n\na) can this drift be controlled by tuning the integral gain?\n\nb) can the drift be controlled by using accelerometer + gyro fused data?\n\nc) given that my drone still shows +/- 5 degrees oscillation, can i consider this the optimal set point for the proportional and derivative gains? or do i need to search more? (in which case, i'm really at my wits end here!) \n", "tags": "arduino quadcopter pid ardupilot", "id": "2167", "title": "quadcopter instability with simple takeoff in autonomous mode"}, {"body": "i am a beginner of ros, kinect and ubuntu. what i want is to visualize kinect's data on rviz environment then run object recognition on it.\n\ni've tried a few tutorials but had no luck. all i got was an empty rviz world.\n\nsince i am a beginner i would appreciate any step-by-step instructions (preferably for hydro or groovy).\n\ni would also like to note that i've managed to get visual from kinect so the device is working fine.\n", "tags": "ros kinect", "id": "2173", "title": "visualizing kinect data on rviz"}, {"body": "if not all, but major types of lawn mower robots are rotary mowers.\ni presume1 that reel mower is more efficient, and is said to leave a better lawn health and cut. so, why industry go to the other option?\n\n\n\n1 - i'm assuming the efficiency, as electrical rotary mowers have at least 900w universal-motors or induction motors, and a manual reel mower is capable nearly the same cutting speed.\n", "tags": "motor mechanism", "id": "2180", "title": "lawn mower robot (type of cutter)"}, {"body": "i was wondering, my team and me are working on a robot communication-oriented and we wanted to add speech recognition on it.\n\nwhat technology should i use ?\n", "tags": "software", "id": "2182", "title": "what should i use for speech recognition?"}, {"body": "that is what i came to understand while reading here and there about flashing a new bootloader/understanding what a bootloader is etc etc\n\nthe bootloader is supposed to be the first thing that runs when i power up my arduino duemilanove (or micro controllers in general). it does some setup then runs my app. it also listens to the usb cable so that if i upload some code it erases the old one and run the new one. there are 2 sections in the memory, one for the bootloader (s1) and one for the app (s2). code on s1 can write to s2 but not to s1 (or strongly discouraged i don't remember).\n\nthere are things that i don't understand though :\n\n\nif i upload some new code while my app is running, the upload works. what happened ? i thought that the bootloader gave hand to my app\nhow can we flash a new bootloader ? if the bootloader is the thing that runs on section 1 (s1) and can only write to s2 and if the bootloader is the only thing that listens to new code uploads, ...\n\n\ncan you help me correct my thoughts and answer my questions ? many thanks !\n", "tags": "arduino microcontroller", "id": "2186", "title": "understanding arduino bootloader"}, {"body": "i'm a newbie in uav stuff, your advice would be very helpful, i want to start mapping using fixed wing uav, but my main choice was apm 2.6, but after some researches, i found that apm 2.6 won't be actively maintained in the future because the future releases will be pixhawk.\n\ni wonder if i should choose apm 2.6 for its stability, on the other side i don't see the benefits of pixhawk apart having long time support. or being a newbie i should start with something experimental like apm 2.5.2 (cheap chinese version for apm).\n\nthanks in advance\n", "tags": "uav", "id": "2189", "title": "what autopilot to purchase apm 2.6 or pixhawk?"}, {"body": "everybody here is probably aware of the sharp distance sensors (gp2y0 series, e.g. gp2y0a02yk0f). they use a diode to emit infrared light and measure the angle of the reflected light with a psd (i.e. they do triangulation). they seem to be the only producers of this technology.\n\ni am only aware of a few similar but incomparable devices (sensors of ambient light and distance or proximity like si114x). which other comparable products are out there?\n\nanother way to ask this question: \"what are the different ways to build a 10cm - 200cm range low-cost ir range sensor, and what is an example of each of those ways?\"\n", "tags": "sensors manufacturing", "id": "2192", "title": "how can i build a 10cm-200cm ir range sensor?"}, {"body": "we are planning to recalibrate abb irb 1410 robot and conduct series of accuracy &amp; repeatability tests using faroarm.\n\nmy questions are\n\ni) is there any physical identification marker on the robot which can be used to identify the location of base co-ordinate frame?\n\nii) if locating the base frame is not possible, can accuracy be measured from fixed arbitrary point in space?\n", "tags": "industrial-robot calibration", "id": "2195", "title": "re-calibration of an articulated industrial robot"}, {"body": "beam robotics seem to be a good approach to teach learners about electronics in robotics. but can these robots be like regular programmed \"cognitive\" robots? can these robots, with just analog circuits, take us to the level of robotic assistants, worker robots and other kinds of self sufficient autonomous robots?\n\ni specifically want to know that, when creating mission critical robots ->\n\n1) what are the areas in robotics which are practically impossible without a real time software system?\n\n2) what areas of the field can be done without programming? if yes, are these areas feasible without an onboard software system?\n\n3) could an intelligent space rover, work without a cpu in the future?\n", "tags": "control software electronics artificial-intelligence embedded-systems", "id": "2196", "title": "robots without microcontrolers (beam robots). are they technologically limited?"}, {"body": "i'm having an event for a boat race.simple boat has to be made.all i have is 5 days.the restriction is 24v motor not more than 1000 rpm.what best material and shape will you suggest to make a boat.i know basic circuits.we have to make a boat with a wired circuit.that circuiting i can do but what can be an ideal shape for boat with maximum speed it can achieve? \n", "tags": "activerobot", "id": "2197", "title": "how can i make a boat"}, {"body": "i'm having a problem with controlling my bldc motor when starting up and when running in low rpm. i have a custom board to measure rotation of the motor using an optical sensor and send servo pwm commands to an esc.\nthe problem is, that i can't start the motor smoothly. when i slowly increase the control signal, it starts stuttering and then jumps directly up to about 1500rpm.\n\nis there a way to improve this situation without using a sensored motor/esc combo?\n", "tags": "motor control brushless-motor", "id": "2205", "title": "low speed control of bldc motors"}, {"body": "i am beginning to learn about the hardware aspect of robotics, and in order for a lot of this new information to be useful to me (whether on this site or elsewhere) i will need a basic understanding of the terminology.\n\none thing that comes up repeatedly is different electric motors: servo, dc motor, brushless motor, step motor, gear motor... etc\n\nis there a comprehensive list? or at least a list of the most common ones, and their descriptions / differences?\n", "tags": "motor", "id": "2209", "title": "what are the different types of electric motors?"}, {"body": "in software engineering startups, you generally go to a room with a computer or bring your own laptop, and write code. i'm interested in how robotics startups work: is there a separate location for designing the robots? take for example, anki. do they have separate research labs for designing robots? how does a robot get from a single design to being manufactured?\n\ni couldn't find a better place on se to post this (the startups business section is defunct): please link me to another se site if there is a better place to ask this question.\n", "tags": "manufacturing", "id": "2215", "title": "how do robotics startups work?"}, {"body": "i have a boe bot pbasic2 stamp based robot that came with rubberband \"tires\" for the wheel. however, they are very tight and i can't figure out how to get them onto the plastic hubs.\nthe furthest i've gotton was mostly covering the outside, but when trying to make it less crooked it came off again.\nis there some trick to getting those pesky tires to stay on?\n", "tags": "wheeled-robot", "id": "2216", "title": "what method should i use for putting rubber bands on wheels?"}, {"body": "i'm programming a pic16f77 with propic 2 which communicates via serial port. as i don't have this port in my pc, i used serial to usb adapter.\ni'm using icprog in windows 8.\n\ni've proggrammed it before but it was in windows xp using the driver who specifies in http://www.ic-prog.com/index1.htm and worked perfectly.\nbut in this os the only difference is the adapter, the program gives some errors while loading the driver:\n\n\n  \"error occured (access is denied) while loading the driver!\"\n  \n  \"privileged instruction\"\n\n", "tags": "microcontroller", "id": "2217", "title": "access denied during pic programming in windows xp"}, {"body": "i have a micro magician v2 micro controller. it has a a3906 dual fet \u201ch\u201d bridge motor driver built in.\n\nin the manual it states \"electronic braking is possible by driving both inputs high.\" \n\nmy first question is, what is the purpose of these brakes? if i set the left/right motor speed to 0, the robot stops immediately anyway. what advantage is there to using these brakes, or am i taking the word \"brake\" too literally?\n\nmy second question is, the driver has \"motor stall flags that are normally held high by pullup resistors and will go low when a motor draws more than the 910ma current limit. connect these to spare digital inputs so your program will know if your robot gets stuck.\" but when my robot hits a wall, the wheels just keep on spinning (slipping if you will), i take it these stall flags can be used on a rough surface where the wheels have more friction?\n", "tags": "motor h-bridge", "id": "2218", "title": "what is the purpose of electronic braking in motors?"}, {"body": "over the last couple of years i've had good success with my technology startups and now looking to enter into robotics. i was interested in robotics and automation ever since i was a kid (yes, that sounds nerdy). so my question is: where to get started, what to build? and how to sell? and lastly, how difficult it is to sell in this industry?\n", "tags": "industrial-robot", "id": "2222", "title": "i am an entrepreneur and i want to start building robots for businesses, where do i start?"}, {"body": "i am wondering if it would be possible to get kinect to work with udoo board (quad). i have found that there is now support for ros + udoo. also saw a question asked about xtion + udoo which shows some more interest. it would really be great if it could be possible for kinect+udoo. was hoping to implement perhaps a miniature version of turtlebot. i wish someone could give some insights on this matter. thanks.\n", "tags": "ros kinect arm embedded-systems", "id": "2224", "title": "udoo board + kinect sensor?"}, {"body": "i'm building a quadcopter. it will be controlled by a beaglebone black with several sensors and a cam.\n\ni new to the quadcopter stuff, therefore it would be nice if someone could have a look at my setup before i buy the parts.\n\n\nframe: x650f - 550mm\nbattery: turnigy nano-tech 5000mah 4s 25~50c lipo pack\nmotor: ntm prop drive 28-30s 800kv / 300w brushless motor\nesc: skywalker 4x 25a brushless\n\n\nthis sums up to ~ 2kg. giving me still some room for about 700g payload.\n\nwhat do you think? did i miss something important? better ideas for some parts?\n", "tags": "quadcopter", "id": "2227", "title": "quadcopter configuration"}, {"body": "i would like to ask a question about zero crossing event in a trapezoidal commutation on a brush-less dc motor. here is a waveform that shows that the zero crossing event occurs every 180 electrical degrees in a sinusoidal commutation: \n\nbut what about trapezoidal commutation. here is the waveform that i found about the trapezoidal commutation: \n\nso as you see, the zero crossing occurs 30 electrical degrees after the previous commutation and 30 electrical degrees before the next commutation.\nin a motor with one pole pair, we would have 30 electrical degrees = 30 mechanical degrees, so we would have this waveform: \nyou see that the zero crossing in phase a occurs when the magnet faces the phase c, or in other words, after 30 electrical degrees from the last commutation.\nmy question in why does the zero crossing happen at that moment, why not after 60 electrical degrees, or 15 electrical degrees?\nis it related to some law's of induction? what are those law and how do this law's appear in this motor?\ncan someone explain to me this with some pics?\n", "tags": "brushless-motor", "id": "2228", "title": "zero crossing events with brushless dc motors"}, {"body": "i would like to ask a question about zero crossing event in a trapezoidal commutation on a brush-less dc motor. here is the waveform that shows that the zero crossing event occurs every 180 electrical degrees in a sinusoidal commutation: \n\nbut what about trapezoidal commutation. here is the waveform that i found about the trapezoidal commutation: \n\nso as you see, the zero crossing occurs 30 electrical degrees after the previous commutation and 30 electrical degrees before the next commutation.\nin a motor with one pole pair, we would have 30 electrical degrees = 30 mechanical degrees, so we would have this waveform: \nyou see that the zero crossing in phase a occurs when the magnet faces the phase c, or in other words, after 30 electrical degrees from the last commutation.\nmy question in why does the zero crossing happen at that moment, why not after 60 electrical degrees, or 15 electrical degrees?\nis it related to some law's of induction? what are those law and how do this law's appear in this motor?\ncan someone explain to me this with some pics?\n", "tags": "brushless-motor", "id": "2234", "title": "zero crossing events in trapezoidal commutation"}, {"body": "i'm programming lua for controlling computers and robots in-game in the minecraft mod computercraft.\n\ncomputercraft has these robots called turtles, that are able to move around in the grid based(?) world of minecraft. they are also equipped with sensors making them able to detect blocks (obstacles) adjacent to them. turtles execute lua programs written by a player.\n\nas a hobby project i would like to program a  function for my turtles. some turtles actually have equipment to remove obstacles, but i would like to make them avoid obstacles and thus prevent the destruction of the in-game environment.\n\ni have no prior experience in robotics, but i have a b.sc. in computer science and am now a lead web developer.\n\ni did some research and found some basic strategies, namely grid based and quadtree based. as i have no experience in this area, these strategies might be old school.\n\nnote that turtles are able to move in three dimensions (even hover in any height). i could share the obstacles as well as obstacle free coordinates in a common database as they are discovered if that would help me out, as most obstacles are stationary once they are placed.\n\nwhat are my best options in this matter? are there any easy fixes? where do i look for additional resources?\n\nthank you very much in advance! :-)\n\nedit: thank you for your feedback!\n\ni started reading the book artificial intelligence: a modern approach, 3rd edition to get up to speed on basic theory as suggested by ian. pointers to other educational resources are appreciated.\n\nalso, i started developing a basic navigation algorithm for moving in unexplored areas, similar to what cube suggested.\n\nthe priority for me is as few moves as possible, as it costs time and fuel cells for each additional move (approx. 0.8 seconds and 1 fuel cell per move in either direction). i plan on using the euclidean heuristics function in a greedy best-first search for computing a path that is expected to be quite optimal in reducing the number of moves to reach the goal, if enough data is available from the shared database from previous exploration.\n\neach time an obstacle is reached, i plan to use the following very basic algorithm, exploiting the fact that turtles are able to move vertically:\n\n\n\nwhen using this algorithm, records are kept of the explored coordinates and uploaded to a shared database. however, there are some cases, that i did not consider:\n\n\n\nmaybe if enough exploration data of the area is available, a jump point search is performed to calculate an optimal path. however this assumes a 2d map. how can i take the 3rd dimension into account?\n\nalso, what would be a good data structure to store the exploration data?\n", "tags": "mobile-robot navigation", "id": "2236", "title": "computercraft (minecraft mod) navigation: collision avoidance and path planning/finding in 2d/3d space"}, {"body": "i am trying to simulate a quadcopter model on simulink. i want to implement a pid controller for each of x,y,z and phi,theta, psi angles. pid gets the error, as input, which is to be minimized.\nfor the x,y and z, the desired values are entered by the user and the actual values are calculated from the accelerometer data, hence, the error is the desired set value - actual value.\n\nfor phi,theta and psi, the actual values may be obtained from the gyroscope and accelerometer (sensor fusion) but i don't actually know how to calculate the desired values for each one of them since the user is usually interested in giving the position values x,y and z as desired not the angle values! the absence of the desired values prevents me form calculating the angular error which is needed for the pid controller.\n", "tags": "design pid quadcopter", "id": "2244", "title": "how to know the desired orientation of a quadcopter?"}, {"body": "i'm reading probabilistic robotics by thrun. in the kalman filter section, they state that \n$$\nx_{t} =a_{t}x_{t-1} + b_{t}u_{t} + \\epsilon_{t}\n$$\n\nwhere $\\epsilon_{t}$ is the state noise vector.  and in\n$$\nz_{t} = c_{t}x_{t} + \\delta_{t}\n$$\nwhere $\\delta_{t}$ is the measurement noise. now, i want to simulate a system in matlab. everything to me is straightforward except the state noise vector $\\epsilon_{t}$. unfortunately, majority of authors don't care much about the technical details. my question is what is the state noise vector? and what are the sources of it? i need to know because i want my simulation to be rather sensible. about the measurement noise, it is evident and given in the specifications sheet that is the sensor has uncertainty ${\\pm} e$.\n", "tags": "kalman-filter noise", "id": "2245", "title": "kalman filter and the state noise vector?"}, {"body": "i need to find a way to solve invrese kinematics for comau smart-3 robot. could you give me a few hints where to start looking? i have no idea about robotics and i couldn't find an algorithm for this specific robot.\n", "tags": "inverse-kinematics", "id": "2250", "title": "finding inverse kinematics algorithm for a specific manipulator"}, {"body": "from what i've read so far, it seems that a rao-blackwellized particle filter is just a normal particle filter used after marginalizing a variable from:\n\n$$p(r_t,s_t | y^t)$$\n\ni'm not really sure about that conclusion, so i would like to know the precise differences between these two types of filters. thanks in advance.\n", "tags": "slam particle-filter", "id": "2251", "title": "difference between rao-blackwellized particle filters and regular ones"}, {"body": "i have an old audio amplifier that has those switches to turn it on. \ni'm looking for the simplest motor/robotic arm (or any other relevant component) to control this switch - eventually via raspberry pi .\n\nare there any options ?\n", "tags": "motor raspberry-pi robotic-arm", "id": "2256", "title": "flipping an old manual switch (physical one)"}, {"body": "can a gyroscopic sensor (comparable to the type that are typically used in smartphones) that is embedded in this black object \n \nthat is rotating around the x axis measure the number of rotations around the x axis if the object may or may not also be rotating at the same time in random ways (number of partial or full rotations, speeds, and directions) around the z axis?\n\nif so, is the z axis rotation irrelevant, or is there special mathematics involved in filtering out the affects of the z rotation on the measurement of x axis rotation?  or does another measurement such as acceleration or magnetism need to be used to solve the problem?\n\nis there any impact in using a 2-axis vs. a 3-axis gyroscopic sensor for this measurement scenario?\n", "tags": "sensors gyroscope", "id": "2261", "title": "how to gyroscopically measure number of rotations on one axis when there is concurrent random motion on another axis"}, {"body": "we want to create robot that will localize itself by the signals of wifi routers.\nwhich sensors should we buy to detect strength of 3 wifi signal?\nwhich of following is necessary for us?\nhttp://www.dfrobot.com/index.php?route=product/category&amp;path=45_80\nor can be any other more suitable variants?\nwe are using arduino as a platform.\n", "tags": "arduino sensors localization wifi", "id": "2262", "title": "how to choose wifi signal strength detecting sensors"}, {"body": "i previously thought that an accelerometer on a quadcopter is used to find the position by integrating the data got from it. after i read a lot and watched this youtube video (specifically at time 23:20) about sensor fusion on android devices, i seem to get its use a little correct. i realized that it's hard to filter out the considerable noise, generated from error integration,  to get useful information about the position. i also realized that it is used along with the gyroscope and magnetometer to for fused information about orientation not linear translation.\n\nfor outdoor flight, i thought of the gps data to get the relative position, but is it so accurate in away that enables position measurement (with good precision)? how do commercial quadcopters measure positions (x,y and z)? is it that gps data are fused with the accelerometer data?\n", "tags": "quadcopter accelerometer navigation sensor-fusion", "id": "2263", "title": "quadcopter position measurement (accelerometer, gps or both)?"}, {"body": "i need help in differentiating between ai and robotics. are ai and robotics two different fields or is robotics a subject in ai?\n\ni want to pursue a career in ai and robotics. so i need your valuable suggestion. i searched the web and also some universities that i want to apply and i cannot find any such thing that i am searching for.\n", "tags": "artificial-intelligence", "id": "2264", "title": "are artificial intelligence and robotics different?"}, {"body": "i am having difficulty sustaining a connection between my raspberry pi (model b running raspbian) and my arduino (uno) while sending signals from the raspberry pi to a continuously rotating servo (powerhd ar- 3606hb robot servo) via python. i'm not sure if there is a more efficent way of sending servo instructions via python to the arduino to rotate the servo. i'm attempting to communicate signals from the raspberry pi to the arduino via usb using what i believe is considered a \"digital serial connection\". my current connection:\n\n\n\n\n\n\n\non the raspberry pi i have installed the following (although not all needed for addressing this problem):\n\n\nxboxdrv\npyserial\npython-arduino-command-api\npygame\nlego-pi\narduino\n\n\nthe sketch i've uploaded to the arduino uno is the corresponding sketch provided with the python-arduino-command-api. *again, i'm not positive that this is the best method means of driving my servo from python to arduino (to the servo).\n\nfrom the raspberry pi, i can see the arduino is initially correctly connected via usb:\n\n\n\nand\n\n\n\nfrom the raspberry pi, i'm able to rotate the servo as a test clockwise for one second, counter-clockwise for one second, then stop the servo, with the following python script:\n\n\n\nthe output via the raspberry pi terminal reads:\n\n\n\nalthough this only performs full-speed in both direction (as well as the calibrated \"stop\" speed of 90), i have successfully alternated from a full-speed to slower speeds, for example, going from 0 up to 90 in increments of 10.\n\nfrom the raspberry pi, i'm able to send input from my xbox controller to drive the servo with a small custom python script i've created along with xboxdrv (which works flawlessly with other projects i'm doing):\n\n\n\nthis script runs, and i'm able to control the servo using the rb button on my controller. however, it eventually fails - sometimes after minutes, sometimes after seconds (rapid and intermittent input seemingly having no influence on expediting a crash). input is no longer read by the script, the terminal comes to a halt, the servo freezes on whatever the last command given was (either spinning endlessly or stopped), and i'm forced to ctrl + c out of the script. if i check to see if the arduino is still connected to the raspberry pi, it shows that it has reconnected itself to the raspberry pi as \"ttyacm1\" (from /dev/ttyacm0 to /dev/ttyacm1):\n\n\n\nwhy does the arduino reconnect itself? is there some other way i should be processing this information? distance to the wireless xbox receiver is not a factor as all of these pieces are adjacent to one another for testing purposes. it will prove impossible to use this servo as a wheel for my robot if i'm constantly tending to this issue.\n", "tags": "arduino raspberry-pi servos python", "id": "2273", "title": "unwanted arduino reconnect: servo + arduino + python (raspberry pi)"}, {"body": "i've spent quite some time researching this, but most of my google search results have turned up academic research papers that are interesting but not very practical.\n\ni'm working on a target/pattern recognition project** where a robot with a small camera attached to it will attempt to locate targets using a small wireless camera as it moves around a room. the targets will ideally be as small as possible (something like the size of a business card or smaller), but could be (less ideally) as large as 8x10 inches. the targets will be in the form of something easily printable.\n\nthe pattern recognition software needs to be able to recognize if a target (only one at a time) is in the field of vision, and needs to be able to accurately differentiate between at least 12 different target patterns, hopefully from maybe a 50x50 pixel portion of a 640x480 image.\n\nbefore playing with the camera, i had envisioned using somewhat small printed barcodes and the excellent zxing library to recognize the barcodes.\n\nas it turns out, the camera's resolution is terrible - 640x480, and grainy and not well-focused. here is an example still image. it's not very well-suited for capturing barcodes, especially while moving. i think it could work with 8x10 barcodes, but that's really larger than i'm looking for. (i'm using this particular camera because it is tiny, light, cheap, and includes a battery and wi-fi.)\n\ni'm looking for two things: a suggestion or pointer to an optimal pattern that i could use for my targets, and a software library and/or algorithm that can help me identify these patterns from images. i have no idea where to start with the right type of pattern so suggestions there would really help, especially if there is a project out there that does something resembling this. i've found opencv and opensift which both seem like potential candidates for software libraries, but neither seemed to have examples of doing the type of recognition i'm talking about. i'm thinking picking the right type of pattern is the big hurdle to overcome here, so any pointers to the optimal type of pattern would be great. being able to recognize the pattern from all different angles is a must.\n\nso far, my idea is to use patterns that perhaps look something like this, where the three concentric color rings are simply either red, green, or blue - allowing for up to 27 unique targets, or 81 if i use 4 rings. from about 2 feet, the capture of a 3x3 inch target (from my computer screen) looks like this which seems like it would be suitable for analysis but i feel like there should be a better type of pattern that would be more compact and easier to recognize - maybe just a plain black and white pattern of some sort with shapes on it?\n\npointers to an optimal approach for this are greatly appreciated.\n", "tags": "software computer-vision artificial-intelligence", "id": "2274", "title": "workable low-resolution object/target recognition pattern and library?"}, {"body": "i've gone through tutorials on how to build circuits and control dc, stepper, and servo motors.  i may not understand everything about them internally, but i have a good basic foundation.  \n\nnow i'm at a loss for where to go from here.  i'm more interested in learning how to make mechanical devices with them than just the electronics behind the devices.  while i know that they go hand in hand, i want to learn more about the mechanical aspects of using motors.  \n\ni have in mind several ultimate goal projects that i want to work toward, like home automation, model rc vehicles, autonomous robots, etc...  but i'm sure that there is more to mechanics that i need to learn before i can jump into a project like that.  he who will learn to fly one day must first learn to stand and walk.\n\nare there hobbyist mechanical starter kits or starter projects to learn how to make effective use of electric motors?  i don't necessarily need a specific product endorsement, but rather a general idea of what important concepts to learn and materials / projects to help me learn them.\n\nmy apologies if this question is too broad.  i can refine it if deemed necessary.\n", "tags": "motor mechanism", "id": "2277", "title": "once you understand motors, what's the next step?"}, {"body": "at the moment i am creating an android program, that will steer my simple, 3 wheel (2 motors, 1 for balance) robot to move online following the path drawn by user on his screen. the robot is operated through wifi and has 2 motors that will react on any input signals.\n\nimagine user drawing a path for this robot on smartphone screen. it has aquired all the points on xy axis, every time beginning with (0,0). still i have no idea, how to somehow \"convert\" just points, into voltage input to both motors. signals will be sent in approx. 60hz connection, so quite fast. maybe not every single axis point will be taken into consideration, there will be surely some skips, but that is irrelevant, since this path does not have to be done perfectly by the robot, just in reasonable error scale.\n\ndo you have any idea on how to make the robot follow defined axis points that overall create a path?\n\nedit 10.01:\n\nthe voltage will be computed by the robot, so input on both is between -255 and 255 and the velocity should increase or decrease lineary in those borders.\nadditionaly, i would like to solve it as if there were perfect conditions, i don't need any feedback crazy models. let's assume that all the data is true, no sensors and additional devices. just xy axis path and required input (ommit wheel slide too).\n", "tags": "wheeled-robot wifi two-wheeled", "id": "2279", "title": "2d path following robot, converting xy axis path to input on wheels"}, {"body": "orrery is a clockwork model of the solar system. i am trying to emulate one in 2d. now, to emulate, i need to know what goes on inside. can someone please explain the basic principle behind the clockwork? or direct me to a resource that will explain all the machinery inside a simple orrery.\n", "tags": "motor design", "id": "2283", "title": "emulation of an orrery"}, {"body": "my task is to apply forces to control 3-dof parallel manipulator. forces are applied to linear  actuators, friction is neglected.  end-effector of a robot is supposed to follow generated path; for this example, let it be a simple circle. so far i have made a simplified 3d model of robot and calculated inverse kinematics.\n\n\n\npromoter of my engineering work don't really know how to do this, but he said that calculating forward dynamics is too complex and i shouldn't go that way. could you tell me what will be the easiest way to go? \n", "tags": "force dynamics manipulator", "id": "2289", "title": "dynamics of parallel manipulator"}, {"body": "i'm running a kk2.0 + 4 20a multistar escs + 4 emax gf 2215-20 motors + 4 slow fly props\n\nafter about a foot off the ground, the entire quadcopter starts wobbling like crazy (no auto-level). any ideas?\n\ni'll add some video if needed.\n", "tags": "quadcopter stability", "id": "2290", "title": "kk2.0 quad stablility"}, {"body": "i am in the process of creating a power prediction model for the hubo robot.\nthe robot has 38 degrees of freedom and has a computer some sensors and motor boards. the motors are powered through motor boards. all these boards are powered through a main power board that exists at the robots chest.\n\nmy model should be able to predict the power for any trajectory of the robot. say for instance if the robot raises its hand from 0 degrees to 180 degrees my model should be able to predict the power.\n\nheres an idea i came across. my idea was to equate the electrical torque to the mechanical torque of each joint.\n\nfor instance if the right arm pitch moves from 0 to 180 degrees i can do as follows ?\n$mgsin(\\theta)= kt*i$\n\nhowever, i am not getting a proper prediction and the current value is way off than what we can read from a software installed in the robot. i know there are losses but even then its off. i was wondering if there are any other approaches or a fault in my approach.\n\nand after i do this i can add all the joint currents for a specific trajectory and then give a estimate for total power consumption.\n", "tags": "brushless-motor power inverse-kinematics motion-planning torque", "id": "2295", "title": "power model for humanoids"}, {"body": "in continuation of the question i asked here: quadcopter instability with simple takeoff in autonomous mode\n  ...i'd like to ask a few questions about implementing a basic pid for a quadrotor controlled by an apm 2.6 module. (i'm using a frame from 3drobotics)\n\ni've stripped down the entire control system to just two pid blocks, one for controlling roll and another for controlling pitch (yaw and everything else... i'd think about them later).\n\ni'm testing this setup on a rig which consists of a freely rotating beam, wherein i've tied down two of the arms of the quadrotor. the other two are free to move. so, i'm actually testing one degree of freedom (roll or pitch) at a time.\n\ncheck the image below: here a, b marks the freely rotating beam on which the setup is mounted.\n\n\nwith careful tuning of p and d parameters, i've managed to attain a sustained flight of about 30 seconds. \n\nbut by 'sustained', i simple mean a test where the drone ain't toppling over to one side. rock steady flight is still no where in sight, and more than 30 secs of flight also looks quite difficult. it wobbles from the beginning. by the time it reaches 20 - 25 seconds, it starts tilting to one side. within 30 secs, it has tilted to one side by an unacceptable margin. soon enough, i find it resting upside down \n\nas for the pid code itself, i'm calculating the proportional error from a 'complimentary filter' of gyro + accelerometer data. the integral term is set to zero. the p term comes to about 0.39 and the d term is at 0.0012. (i'm not using the arduino pid library on purpose, just want to get one of my own pids implemented here.)\n\ncheck this video, if you want to see how it works.\n\nhttp://www.youtube.com/watch?v=lpsnbl8ydba&amp;feature=youtu.be\n[yeh, the setup is pretty ancient! i agree. :)]\n\nplease let me know what could i possibly do to improve stability at this stage.\n\n@ian: of the many tests i did with my setup, i did plot graphs for some of the tests using the reading from the serial monitor. here is a sample reading of roll vs 'motor1 &amp; motor2 - pwm input' (the two motors controlling the roll):\n\n\n\nas for the input/output:\n\ninput: roll and pitch values (in degrees), as obtained by a combination of accelerometer + gyro\n\noutput: pwm values for the motors, delivered using the servo library's motor.write() function\n\n\n\nresolution\n\ni resolved the problem. here's how:\n\n\nthe crux of the issue lied in the way i implemented the arduino program. i was using the write() function to update the servo angles, which happens to accept only integer steps in the argument (or somehow responds only to integer input, 100 and 100.2 produces the same result). i changed it to writemicroseconds() and that made the copter considerably steadier.\ni was adding up rpm on one motor while keeping the other at a steady value. i changed this to increase rpm in one motor while decreasing the opposing motor. that kinda keeps the total horizontal thrust unchanged, which might help me when i'm trying to get vertical altitude hold on this thing.\ni was pushing up the rpm to the max limit, which is why the quadcopter kept losing control at full throttle. there was no room for the rpm to increase when it sensed a tilt.\ni observed that one of the motor was inherently weaker than the other one, i do not know why. i hardcoded an offset into that motors pwm input.\n\n\nthanks for all the support.\n\n\n\nsource code:\n\nif you're interested, here's the source code of my bare-bones pid implementation: pid source code\n\nplease feel free to test it in your hardware. any contributions to the project would be welcome.\n", "tags": "arduino pid quadcopter stability", "id": "2297", "title": "quadcopter pid tuning"}, {"body": "given a 12' x 12' field (4m x 4m), a reasonably cheap 3-axis gyro sensor and accelerometer, and compass, i plan to design a device capable of tracking its position to sub-centimeter accuracy for a minute of motion or so.\n\nthe device has a holonomic drive system, capable of moving any direction at a maximum of about 8mph (3.6m/s), with a maximum acceleration of about 2g's. however, there are some simplifying constraints. for one, the field is nearly flat. the floor is made of a tough foam, so there is slight sinking, but the floor is flat except for a ramp of known angle (to a few degrees). the device will, excepting collisions, not be rising above the floor.\n\naccuracy is preferred over simplicity, so any mathematics required on the software side to improve the system would be welcomed.\n\nbefore i definitively choose accelerometers as the method of position tracking, though, i would like some idea of how much accuracy i could get, and the best ways of doing it.\n", "tags": "kinematics accelerometer machine-learning", "id": "2298", "title": "how much accuracy could i get position tracking with a 3-axis accelerometer and gyro sensor, and compass, and how would i do it?"}, {"body": "i am a computer science student entering my last year of college. i'm pretty sure robotics is what i want to eventually be doing based on my interests in ai and embedded systems. i've seen a lot of topics that covers robotics such as: control theory, signal processing, kinematics, dynamics, 3d simulators, physics engines, ai, big data with machine learning. i'm hoping someone can point me in the right direction as to what i should be attempting to study in my interests of robotics. i am not sure what other topics i have not mentioned that would be relevant. i would like to deal with the software side of robotics, both ai and none ai.\n\nmy other question is about machine learning. i've seen researchers applying machine learning (deep learning/unsupervised learning specifically) to robotics but how do they do this? is information and data transferred from the internals of the robot to an external computer that does the data processing? machine learning requires a lot of data to predict. is this the only way machine learning can be used in robotics (through an external computer)?\n\ni hope someone can touch on some of the things i've mentioned, thank you.\n", "tags": "mobile-robot software artificial-intelligence programming-languages", "id": "2299", "title": "where to start for the software side of robotics?"}, {"body": "i need an app that can do live monitoring of whether each seat in an auditorium is occupied,  so visitors can load the app and see where to sit.    \n\nthe auditorium has a relatively flat ceiling 4m high, and the seats are .5m wide. \nthe hardware cost per seat needs to be $5.\n\ni'm looking for all solutions.  web cams, preasure sensors, sonars, lasers, arduino, pi, intel edison, anything. \n\nobviously there cannot be wires that people could trip over.  sensors on the ceiling could have wired networking.  sensors on the seat or floor would need to have wireless communication.  sensors on the ceiling would need to consider occlusion by people sitting in the seats (think, if there is an empty spot between 2 people, can the sensor see it as empty)\n\nin the end, the data needs to be collected as a simple list of which chairs are occupied/open\n\n\n\npossible solutions:\n\n\nrasberry pi's on the ceiling every 8 seats with a camera.   \npressure sensors under chair legs wired to pi's gpio\ndrones flying around the auditorium :)\n\n\nany ideas?\n\nupdate (more constraints):\n\n\nauditorium size is 400 seats\ninstallation costs should average 10 chairs per hour(400/10 = 40 hours)  \nas the picture shows, chairs are cushioned\nregular maintenance should take no longer than 30 min. per 2-hour event(eg, batteries)\nhardware should last 100 sessions\nfor auditorium cleaning, it should be possible to \"disconnect\" and \"reconnect\" the chairs with 4 hours of labor.\n\n", "tags": "arduino sensors raspberry-pi computer-vision", "id": "2304", "title": "system for determining occupied seats in an auditorium"}, {"body": "i'm trying to get an extended kalman filter to work. my system model is:\n\n$ x = \\begin{bmatrix}\n lat \\\\\n long \\\\\n \\theta\n\\end{bmatrix}$\n\nwhere lat and long are latitude and longitude (in degree) and $\\theta$ is the current orientation of my vehicle (also in degree).\nin my prediction step i get a reading for current speed v, yaw rate $\\omega$ and inclination angle $\\alpha$:\n\n$z = \\begin{bmatrix}\n v \\\\\n \\alpha\\\\\n \\omega \n \\end{bmatrix}$\n\ni use the standard prediction for the ekf with $f()$ being:\n\n$\n\\vec{f}(\\vec{x}_{u,t}, \\vec{z}_t) = \\vec{x}_{u,t} + \n \\begin{bmatrix}\n  \\frac{v}{f} * \\cos(\\theta) * \\cos(\\alpha) * \\frac{180 \u00b0}{\\pi * r_0} \\\\\n  \\frac{v}{f} * \\sin(\\theta) * \\cos(\\alpha) * \\frac{180 \u00b0}{\\pi * r_0} * \\frac{1}{\\cos(lat)} \\\\\n  \\frac{\\omega}{f}\n \\end{bmatrix}\n$\n\n$f$ being the prediction frequency, $r_0$ being the radius of the earth (modelling the earth as a sphere)\n\nmy jacobian matrix looks like this:\n\n$\nc = v \\cdot \\delta t \\cdot cos(\\alpha) \\cdot \\frac{180}{\\pi r_0}\n$\n\n$\nf_j =\n\\begin{pmatrix}\n  1 &amp; 0 &amp; -c \\cdot sin(\\phi) \\cdot \\frac{1}{cos(lat)} \\\\\n  -c \\cdot sin(\\phi) \\cdot \\frac{sin(lat)}{{cos(lat)}^2} &amp; 1 &amp; c \\cdot cos(\\phi) \\cdot \\frac{1}{cos(lat)}\\\\\n  0 &amp; 0 &amp; 1\n\\end{pmatrix}\n$\n\nas i have a far higher frequency on my sensors for the prediction step, i have about 10 predictions followed by one update.\n\nin the update step i get a reading for the current gps position and calculate an orientation from the current gps position and the previous one. thus my update step is just the standard ekf update with $h(x) = x$ and thus the jacobian matrix to $h()$, $h$ being the identity.\n\ntrying my implementation with testdata where the gps track is in constant northern direction and the yaw rate constantly turns west, i expect the filter to correct my position close to the track and the orientation to 355 degrees or so. what actually happens can be seen in the image attached (red: gps position measurements, green/blue: predicted positions): \n\ni have no idea what to do about this. i'm not very experienced with the kalman filter, so it might just be me misunderstanding something, but nothing i tried seemed to work\u2026\n\nwhat i think:\n\ni poked around a bit: if i set the jacobian matrix in the prediction to be the identity, it works really good. the problem seems to be that $p$ (the covariance matrix of the system model) is not zero in $p(3,1)$ and $p(3,2)$. my interpretation would be that in the prediction step the orientation depends on the position, which does not seem to make sense. this is due to $f_j(2,1)$ not being zero, which in turn makes sense.\n\ncan anyone give me a hint where the overcorrection may come from, or what i should look at / google for?\n", "tags": "kalman-filter gps sensor-fusion", "id": "2315", "title": "overcorrecting kalman filter"}, {"body": "i am using a miniature car and i want to estimate the position. we can not use gps modules and most of the tracking systems that i saw, are using imu senson with the gps module. in our car we are able to find our exact correct location with image processing but for some parts that dont have enough markings we can not do this. so we want to use the imu as backup for our positioning. so as long as the positioning is close is good for us.\n\nand we are only interested in our 2d position since the car is on a flat ground.\n\ni am using a imu 9dof sensor and i want to calculate my movement. i have seen some amazing works with imu for tracking body movements but no code or simple explanation is anywhere about it. so basically i have the reading from accelerometer, gyro and magnetometer. i also have orientation in quarternions. from the device i am getting also the linear acceleration but even when i am not moving it in any direction the values are not 0 which is really confusing.\n\ncan you please help me how to approach this?\n\nthanks in advance\n", "tags": "sensors accelerometer gyroscope", "id": "2321", "title": "tracking 2d positioning with imu sensor"}, {"body": "i want to make a quadcopter for my final year project and i am willing to use dc motors as the four rotors of the quadcopter. can any one guide me about the ratings for proper motor selection for my job.\n", "tags": "quadcopter", "id": "2322", "title": "what can be the rating and specifications of dc motor used for making a quadcopter?"}, {"body": "i have a 4 wheeled differential drive robot, like the pioneer 3-at. there are only two motors, one for left wheels and one for right wheels.\n\ni want to send velocity commands to the robot, i'm using ros and the standard commands are: [linear_velocity, angular_velocity].\n\ni need to convert them into left and right velocities, from literature if i had 2 wheels i should do this:\n\n$v_l = linear_v - \\omega * |r|$\n\n$v_r = linear_v + \\omega * |r|$\n\nwhere |r| is the absolute value of the distance from the wheels to the robot \"center\".\n\nhow should i take into account that i have 4 wheels?\n", "tags": "wheeled-robot inverse-kinematics wheel", "id": "2324", "title": "kinematics of a 4 wheeled differential drive robots"}, {"body": "i'm simulating a sensor in 3d. the sensor should determine ($p, \\theta, \\phi$) from the origin  where $\\theta$ is the rotation about z-axis and $\\phi$ is the rotation about x-axis. the sensor is given position of a point($x, y, z$). this is what i did\n\n\n\nnow i need to get the cartesian coordinates ($x',y',z'$). this is what i did\n\n\n\nthe sensor is working fine at the beginning but at a particular point it behaves strangely. i have the state vector to compare it with the measurement. i'm guessing that $\\theta$ might be the problem. \n\n\n\nedit: \n\ni'm sorry for this mistake. the aforementioned calculations based on the following picture\n\n\n\nso, the point will rotate first about z-axis ($\\theta$) and then rotate about x-axis ($\\phi$)\n", "tags": "sensors sensor-error", "id": "2326", "title": "problem with simulated sensor in matlab?"}, {"body": "i have a quadcopter equipped with px4fmu board. you may download its datasheet from here.\ni wonder whether it is possible to program the quadcopter to autonomously follow a path like circular motion without any human interference. are the built-in sensors enough for this task?\ni also wonder how accurate the built-in gps is? i read that it gives coordinates with a radius of 5m as error.\n", "tags": "quadcopter", "id": "2330", "title": "is it possible to achieve fully autonomous route following using px4fmu module?"}, {"body": "my question is very broad. however i would like a complete description to the very last detail in a way that a foreign exchange student would understand. i want to try my best to master the way the kalman filter works. please be as through as you possibly can, and more.\n", "tags": "kalman-filter", "id": "2331", "title": "what is the kalman filter in the basics of its aspects?"}, {"body": "i am an aerospace engineer (currently in grad school) and i really want to get into (embedded) electronics.\nbut i have this problem:\ni understand the theory fairly well, i took an edx course in circuits and had no problem. i can build projects from the internet. however, i have a very hard hard time connecting the theory with the practical part, understanding why projects are done the way they are done and i have a hard time to design my own projects!\n\nplease help!\n\ni'd appreciate the following:\n-general tips: how did you learn it? how is your workflow? what should i do? which steps should i take?\n-books: which hands on books and websites can you recommend? i am looking for books and website that are practical but also explain the why\n-kits: what kits can you recommend that combine the theory with the practical?\n-anything you think is important\n\nthank you for your time!\n", "tags": "electronics", "id": "2336", "title": "learning (embedded) electronics"}, {"body": "i'm implementing monte-carlo localization for my robot that is given a map of the enviroment and its starting location and orientation. mine approach is as follows:\n\n\nuniformly create 500 particles around the given position\nthen at each step:\n\nmotion update all the particles with odometry (my current approach is newx=oldx+ odometryx(1+standardgaussianrandom), etc.)\nassign weight to each particle using sonar data (formula is for each sensor probability*=gaussianpdf(realreading) where gaussian has the mean predictedreading)\nreturn the particle with biggest probability as the location at this step \nthen 9/10 of new particles are resampled from the old ones according to weights and 1/10 is uniformly sampled around the predicted position\n\n\n\nnow, i wrote a simulator for the robot's enviroment and here is how this localization behaves:\nhttp://www.youtube.com/watch?v=q7q3cqktwzi\n\ni'm very afraid that for a longer period of time the robot may get lost. if add particles to a wider area, the robot gets lost even easier. \n\ni expect a better performance. any advice?\n", "tags": "localization motion-planning sonar", "id": "2337", "title": "monte-carlo localization"}, {"body": "as far as i can tell, an ultrasonic rangefinder works by reflecting inaudible soundwaves off of objects and timing their return.  but if the object has a flat surface and is angled with respect to the line to the rangefinder, how does it detect that object?  under what circumstances might it give a false distance or otherwise fail to detect the object?\n", "tags": "sensors sonar", "id": "2341", "title": "how do ultrasonic range finders detect objects at an angle?"}, {"body": "this is actually a very simple question, but i'm lost at the moment.\n\ni was using a beaglebone black for a school project. it controls a bunch of motors and actuators,etc. we wrote everything in c++, and made libraries of functions. when a main program calls them, the functions run just fine.\n\nrecently we have been told to demo our progress so far. the main program is nowhere near done, so we were thinking of some sort of web interface that can execute the complied c++ program on command. we were hoping to get the server hosted on the board, and access it via lan from other pcs. but i've never done this before and have no idea where to start. does node.js (with the 'bonescript') going to be of any help? or is there a simpler way with basic html?\n\ni only have a few days to figure it out, so i didn't want to waste time looking at the wrong methods.\n", "tags": "embedded-systems", "id": "2345", "title": "simple web interface with beaglebone black"}, {"body": "i would like to use a kuka/abb 6 axis robot and a machine vision system to pick and place a variety of metal drill bits in size ranges from 0.5mm (ascending up 0.5mm per cylinde) to 13mm in metric and then 1/16 of an inch ascending to 9/64 of an inch. the machine would not have to differentiate between the bits, niether drill bit would weight more than 1kg.\n\ncrucially at the beginning and end of the picking and placing i would like to inspect the very tip of the cylinders to be inspected for a 118 degree chamfer on one end of the bit which should be present regardless of drill diameter of length. \n\ni am lead to believe that if the drill bits are placed end up on a conveyor belt and always the the same place its relatively low cost but crucially if the kuka 6 axis robot has to find the drill bits then the cost increases dramatically, is this true?\n", "tags": "industrial-robot", "id": "2347", "title": "the costs of using existing 6 axis kuka/abb robots and existing vision systems in picking and placing tasks"}, {"body": "i want to make a copy of this machine fisher price soothing motions\u2122 glider and i'm wondering what motor to use? simple dc motor with appropriate gearbox (slow rpm) or stepper motor?\n\nhere is another instance of this idea.\n", "tags": "motor stepper-motor mechanism", "id": "2348", "title": "what motor to use for reciprocating (reversive) movement"}, {"body": "i have very limited experience with sensors or robotic components at all, and i hope you will excuse the lack of detail in this question. \n\ni want to set up posts around my yard with electronic noses that detect dog urine. i want to use this information to make a map of my yard from a dogs perspective. is it possible with todays technology? what would it cost? there may be information that is very relevant to me, but that i'm not requesting. this is because of lacking insight. if there is something you think i should consider or research, please say so. \n", "tags": "sensors electronics", "id": "2350", "title": "electronic noses for detecting dog urine"}, {"body": "i am working on a line follower robot as part of my microelectronics project, and am confused over what sort of code to use to program the \"pic18f\" microcontroller i'm using. can someone give me source code or a layout of the code and what should be in there?\n", "tags": "c line-following", "id": "2356", "title": "line follower robot program"}, {"body": "robotics enthusiasts!\n\ni'm a member of a team which has to develop a mobile rescue robot to cooperate with firemen (e.g. on earthquake sites).\n\nthe problem we have is connection of a commander post with the robot. the robot has to enter buildings, so it is desirable that the connection can go through several decimeters of walls and have a reach 50-100 meters. on the other hand, we need to send a lot of data (camera images, point clouds, maps) which can easily eat 10 mbps or more.\n\nat this time we use 2.4 ghz wifi for this connection. as for the speed, with direct visibility it seems to be sufficient, but only when a single robot is operating (we can use up to 3 non-overlapping channels, so in theory 3 robots can work together; but usually you have the environment messed up with home routers). we need at least 5 robots to be operating simultaneousely.\n\nwe have tried 5 ghz wifi, but it has problems with penetrating walls, so it can only be used for uavs.\n\nmy idea was to use some mobile connection technology like lte. i found that lte can be run on 800 mhz, which could be great for the wall penetration performance. i also found that the lte's theoretical upload speeds (for clients) are 70 mbps, but nobody says if it is on 2.6 ghz and how would it be changed when running lte on 800 mhz.\n\nmoreover, we cannot rely on some provider's coverage. i have found that you can build your own lte transmitter for about \u20ac2000, which seems to be interesting to us. maybe it is possible to build it even cheaper. but we think both 2.6 ghz and 800 mhz are regulated frequencies. however, the cooperation with firefighters could persuade local regulators to give an exception to us to setup our own small lte base station.\n\nand now to the question: do you think such setup would give better results than using wifi? or do you know of any other technologies that would help us to either increase the bandwidth or the wall penetration performance? what are their cons and pros?\n", "tags": "wireless", "id": "2357", "title": "cons and pros of wireless technologies for rescue robot"}, {"body": "i am a beginner in robotics, and i am learning about the kalman filter. i do not seem to get it, though. i am a mathematician, and so it would be helpful if the kalman filter could be explained in a mathematical method.\n", "tags": "kalman-filter", "id": "2361", "title": "explanation of the kalman filter"}, {"body": "i have a steam radiator at home and it has a valve similar to the picture below.\n\n\n\nplease note that the valve doesn't have grooves on top to attach things to.\n\ni want to build something to turn it on and off depending on the temperature at certain points in the room.\n\ni have that taken care of but cannot find a way to attach a actuator(actuator is the right word in the context i guess?) to turn the valve in both directions.\n\nalso it is a rented apartment so i would like to avoid making any modifications to the radiator itself.\n", "tags": "actuator valve", "id": "2362", "title": "actuator to control steam valve"}, {"body": "i have zero experience with robotics, but i need to build a mobile platform for a streaming camera. the idea is that i'll plug in my android phone into the pan/tilt unit on my wheeled robot and then drive and look around via wifi. i have already solved all of the software, interface and controller issues, but i would appreciate some advice on how to build the wheeled platform.\n\nmy initial idea was to buy a cheap rc car, remove all electronics and replace them with my own. this approach almost worked. i purchased this new bright f-150 truck.  the size is good and there is plenty of storage space:\n\n\n\n\nhowever, i quickly ran into a problem with this thing. i assumed that the front wheel would be turned by some kind of servo. instead i found this nonsense:\n\n\n\nthat small gear shaft is not driven by a servo - it's a conventional motor, which spins until it is jammed at the extremes of travel. the wheels are straightened when power is removed by a small spring on the other side. this means that there is only one angle at which the wheels can be turned, and that angle is way too small for what i need. so using this rc car will not work.\n\nbefore i start buying more things, i would like to hear some opinions from more experienced people. am i on the right track? do i simply need to get a better rc car, or are they all designed like this? perhaps there are other options that would be more suitable for what i am doing?\n", "tags": "mobile-robot", "id": "2363", "title": "building a mobile camera platform"}, {"body": "i'm struggling with the concept of covariance matrix. \n$$\n\\sigma\n= \n\\begin{bmatrix}\n\\sigma_{xx} &amp; \\sigma_{xy} &amp; \\sigma_{x \\theta} \\\\\n\\sigma_{yx} &amp; \\sigma_{yy} &amp; \\sigma_{y \\theta} \\\\\n\\sigma_{\\theta x} &amp; \\sigma_{\\theta y} &amp; \\sigma_{\\theta \\theta} \\\\\n\\end{bmatrix}\n$$\nnow, my understanding for $\\sigma_{xx}$, $\\sigma_{yy}$, and $\\sigma_{\\theta \\theta}$ that they describe the uncertainty. for example, for $\\sigma_{xx}$, it describes the uncertainty of the value of x. now, my question about the rest of sigmas, what do they represent? what does it mean if they are zeros? i can interpret that if $\\sigma_{xx}$ is zero, it means i don't have uncertainty about the value of x. \n\n\n\nnote, i'm reading principles of robot motion - theory, algorithms, and implementations by howie choset et. al., which states that\n\n\n  by this definition $\\sigma_{ii}$ is the same as $\\sigma_{i}^{2}$ the variance of $x_{i}$. for $i \u2260 j$, if $\\sigma_{ij} = 0$, then $x_{i}$ and $x_{j}$ are independent of each other.\n\n\nthis may answer my question if the rest of sigmas are zeros however, i'm still confused about the relationship between these variables for example $x$ and $y$. when does this happen? i mean the correlation between them. or in other words, can i assume them to be zeros?\n\nanother book namely fastslam: a scalable method ... by michael and sebastian which states \n\n\n  the off-diagonal elements of the covariance matrix of this\n  multivariate gaussian encode the correlations between pairs of state\n  variables.\n\n\nthey don't mention when the correlation might happen and what does it mean?\n", "tags": "kalman-filter noise", "id": "2365", "title": "covariance matrix in ekf?"}, {"body": "i'm trying to make a quadcopter move laterally at a certain angle. i've been able to find the proper roll and pitch angles for this (that work with a yaw of 0\u00b0); how would i adjust these values to compensate for a different yaw?\n", "tags": "quadcopter gyroscope movement dynamics", "id": "2371", "title": "compensating for yaw in lateral quadcopter movement"}, {"body": "from each step of my vision code i am able to get around 400 coordinates of where the robot thinks the walls are\n\ni want to integrate this into monte-carlo observation step.\n\ni'm storing the map of the maze as a set of line segments. what would be a nice way to implement the sensor update, i.e. given the position (x,y) of the robot what is the probability that it is found there given the above described coordinates of the walls.\n\n\n\nthe main idea i currently have:\n\ntransform points in polar coordinates. then for each point (from vision output) compute a ray with this angle and find the first intersection with the maze. now we have the predicted distance and real distance and we can compute the probability that this measurement is right. \n\nthe main drawback is that this is slow. for each point from vision output i have to iterate over all line segments to find the one with the closest intersection. the line segments number is around 50. so it gets to o(400*50*particle number).\n", "tags": "mobile-robot localization computer-vision", "id": "2373", "title": "using vision for monte-carlo localization"}, {"body": "i want to capture two views of same scene. the scene consists of a set of objects kept on a table. from the two views, i wish to calculate homography for image matching. i want to know what is the maximum angle between the two views  such that the homography can be accurately calculated. right now, i am capturing the images at around 60 degrees of angle, but unable to construct homography accurately.\n", "tags": "kinect computer-vision stereo-vision", "id": "2374", "title": "maximum angle between the camera pose to correctly estimate homography"}, {"body": "my application is basically about sound source localization and visual servoing. i selected kinect as the main hardware.\n\ni already know the basic differences between kinect for windows and kinect for xbox. i cannot access to windows version from my country (no reseller here in turkey), but the xbox version is there at the stores. i am not sure about problem specific software selection.\n\ni found out that the latest kinect sdk supports sound source localization (and beamforming) using the built-in microphone array. can i use that sdk within the xbox version? or is there another sdk for xbox, having the same support? i am not sure because i also read that openni does not provide the best audio api.\n\ni will also apply some processing on image &amp; depth outputs, so i will be using opencv. i also want to use qt for threading, gui etc. so, another question: is it possible to use the microsoft official kinect sdk within another ide, not visual studio?\n", "tags": "kinect", "id": "2376", "title": "kinect for xbox: sdk selection"}, {"body": "just like a fish finder finds the depth of the water directly beneath it, im trying to find a sensor that i can purchase for the arduino that does the same. would like it to check up to 20 ft at least, with high accuracy, +/- 10 or 15 cm. all the threads and info i've been finding are water level sensors not for water depth. so does anyone know of a sensor like this and where i can find one?\n", "tags": "arduino microcontroller underwater", "id": "2379", "title": "water depth arduino sonar sensor"}, {"body": "we are working with an holonomic robot equipped with three (120 degree shifted) omnidirectional wheels. the relative movement is estimated by dead reckoning using wheel encoders. to improve this estimation we installed an gyroscope to measure the change in orientation. furthermore the robot has a 270 degree laser range finder. \n\nin order to solve the kidnapped robot problem we implemented a particle filter. in every step each particle is updated according to the odometry and gyroscope readings. since these readings are distorted by noise we need a motion model to include these errors. as described in probabilistic robotics by thrun (page 118 - 143) there are two commonly used motion models (velocity motion model and odometry motion model). however these models seem to describe the behavior of differential drive robots not omnidirectional robots. i base this thesis on the fact that the error in relative y-direction is proportional to the error in orientation as far as the motion models by thrun are concerned. this is appropriate for differential drive robots as the orientation and the heading of the robot are identical. for omnidirectional robots this assumption can not be made since the heading and the orientation are completely independent. even if we assume perfect information about the robots orientation we can still obtain error in relative y-direction.\n\ni would like to discuss if my assumption - that the velocity/odometry motion model fails for omnididrectional robots - is correct or not as i am not sure about that. furthermore  i am curious if there are any other motion models for omnidirectional robots that might fit better.\n", "tags": "mobile-robot localization motion particle-filter", "id": "2382", "title": "motion model for holonomic robot"}, {"body": "i have bought a really small proto x quad (it has a joystick which navigates the device) and i am looking for a way to send a signal to this thing from my computer.\n\n\n\nso can anyone point me how can i turn on one of the propellers of this quad using my laptop (i have a decent knowledge in python/matlab/c# but hardware is a completely new world to me).\n", "tags": "quadcopter programming-languages", "id": "2387", "title": "how can i start programming proto x quad"}, {"body": "i am surprised by the price range of lidar applications considering the simplicity of the design. i try to make a simple project that requires lidar for object recognitions etc. i wouldn't like to use visual recognition such as opencv. \n\nregardless of that i am trying to understand why lidar solutions are so expensive you can see\nthat this \nhttp://velodynelidar.com/lidar/hdlproducts/hdl32e.aspx\nsmall lidar sensor goes for 20,000$. \n\ni strongly believe that lidar is the next step in robotic applications but i am not sure why it is so exclusive. \n\ni have seen few projects that go for around 200$ but their performance is very bad. \n\ni hope you can answer what makes a lidar so expensive and what are some cheap systems a hobbyist can afford. \n", "tags": "arduino computer-vision lidar", "id": "2394", "title": "lidar solutions"}, {"body": "is there a generic name for the category of robots that move using two opposing wheels or tank-like treads?\n", "tags": "mobile-robot differential-drive", "id": "2396", "title": "generic name for two-motor wheeled/tracked robots?"}, {"body": "i have a couple of these dc motors\n\nhttp://www.pololu.com/product/2202\n\nwhich have an extended motor shaft that sticks out the back and is 1mm diameter. \n\ni'm having trouble trying to think of the best way to attach an encoder disk to this shaft.\n\ni thought of getting a custom wheel 3d printed and make the opening 0.9mm so it will be a tight fit. but i don't know if is just to small?\n\ni also though of taking the encoder disks from a pc mouse and drilling a 1mm / 0.9mm but its the same problem but with the added difficultly of trying to drill a small hole on a small thing.\n\nso i wondered if anyone knows a better way, or of a made disk to attach. as i just can't find anything for a 1mm shaft\n", "tags": "motor", "id": "2402", "title": "how to: attach wheel encoder to motor?"}, {"body": "i am new to robotics and planning my first purchase.\ni'm looking at the baby orangutang b-328. here is information about the microcontroller: http://www.hobbytronics.co.uk/baby-orangutan-328.\n\nthe pin headers come unmounted, so you have to do the soldering yourself. my problem is that i don't know what the pin connections are for. here is a picture of the board:\n\n.\n\ncould someone briefly tell me what the different connections are for, or link a website that does?\n", "tags": "microcontroller", "id": "2409", "title": "connections on a baby orangutang b-328 board"}, {"body": "i'm a complete newbie trying to build a simple robot that dispenses candy (m&amp;m, skittles, etc).  however, since i'm not familiar with the field, i'm having a hard time googling because i don't know the correct terms to search for.  i'm looking for a piece to build a robotic 'trap door' of sorts that will open for a specified amount of time to release candy.  what parts can i use and what is are called?  i've tried robotic lever, robotic door, etc with no luck.\n", "tags": "design", "id": "2411", "title": "robotic part to dispense candy"}, {"body": "i am studying informatics and i am interested in doing a masters in robotics and i was checking out some unis and their courses and i saw that robotics contains analysis and a lot of math.\n\nwhy is that?\n", "tags": "software", "id": "2413", "title": "why is analysis required to study robotics?"}, {"body": "why do fpv quadcopter motors (usually the more expensive ones) draw lower amps than regular motors? \n\nand why are they more squat(disk shaped) as opposed to normal motors which are about the same diameter and height?\n", "tags": "motor quadcopter", "id": "2416", "title": "low amp fpv quadcopter motors"}, {"body": "all the pro fpv builds and the more expensive quads don't seem to be using plastic props. any reason for this?\n", "tags": "quadcopter", "id": "2418", "title": "quadcopter props? wood vs plastic vs carbon fiber"}, {"body": "is there any way i can simply run a program on the nxt, but not download it?  i have all my programs already downloaded, and i am connecting with a usb cable to a macbook pro using the nxt-g interface.  is there any way i can just run programs existing on the nxt from the computer, and not download them?  it's really increasing my robot's run time.\n\n  i am competing in robocross in science olympiad, and my event is at noon.\n\nthank you.\n", "tags": "nxt usb", "id": "2421", "title": "just run program on nxt, not download?"}, {"body": "for robotic manipulator like the one on the picture:\n\n\n\nare the configuration space and joint space equivalent? i am trying to understand difference between the two...\n", "tags": "kinematics motion-planning forward-kinematics", "id": "2422", "title": "is configuration space same as joint space?"}, {"body": "i'm trying to figure out the diameter of tri-blade propellers.\n\ni found a 7x3x4.5 blade, i'm trying to understand the measurements. is the '7' the length of the blade giving the prop a 10.5\" diameter? or is the 7 the total diameter?\n", "tags": "quadcopter design", "id": "2423", "title": "what do quadcopter propeller specifications mean?"}, {"body": "for a project for a robotic lab, i'd like to build an automous quadcopter able to follow a path and land on its own. i'd like to use an onboard android phone to do the image processing and recognition part, so that i avoid to send the video stream to a control station, process it and send back the commands.\n\nas i need to use it in an indoor environment (so no gps coordinate), i need the phone to guide the quadcopter giving it relative directions like forward and after 1 sec stop. this is something a normal pilot would do via the rc radio.\n\ni already have a arducopter apm 2.5 and an arduino mega adk and i was thinking to connect the phone to the adk and then the adk to the apm to guide the copter.\n\ni think i have 2 options: either having the adk to generate ppm/pwm signals as a rc receiver would do or use the mavlink protocol.\n\nwhich is the best/easiest solution?\n\nother info:\n\n-i have already read checked some uav related websites, but i couldn't find something close to what i want. in most of them, try to build a new type of controller, or use only ab android phone + adk. i'd like to stick to something already tested and known to work (as the apm &amp; arducopter software) and i don't want to use the phone as imu as i don't trust its sensors\n\n-i already have built the quad (a hexa actually)\n\n-i have already set up the connection and protocol between the phone and the adk so i'm able to send commands like, i.e. forward, turn, hover etc...\n\n-i have already checked the andro-copter project and similar ones.\n\n-i might consider other platforms than the apm 2.5 if there's something easier to use\n\n-it'd be nice to keep the rc receiver in the loop to regain control of the quad if something goes wrong.\n", "tags": "arduino control quadcopter ardupilot", "id": "2424", "title": "android phone + adk + arducopter apm 2.5 for autonomous quadcopter"}, {"body": "i have a turnigy esc and i am controlling it from avr. now i need to calibrate it to set the range of the input.\nwith a servo tester i managed to calibrate it without any problems, more or less by following the user guide, but when i try to do the same procedure from code, the esc starts beeping in some confused pattern and then enters programming mode.\n\nmy code looks like this:\n\n\n\nwhere +servo_range_ticks is 2.2ms pulse length, -servo_range_ticks is 0.8ms pulse length and 0 is 1.5ms. the timeouts of 10s were measured during the manual calibration with a stopwatch.\n\ni have checked with an oscilloscope that the output servo signal looks the way i would expect it -- 10 seconds of 2.2ms pulses, 10 seconds of 0.8ms pulses and then 1.5ms pulses.\nedit: i made a mistake here, see my answer.\n\ndo you have any idea what to change to calibrate the esc?\n", "tags": "c calibration esc avr", "id": "2426", "title": "how to programatically calibrate turningy esc?"}, {"body": "i am planning to buy cnc mechanical skeleton without motors, spindle and controller. i will be using the cnc mainly for aluminium milling. are there any specifications for minimum torque requirement for stepper motors and spindle to perform aluminium milling ?\n", "tags": "stepper-motor cnc", "id": "2433", "title": "what is minimum torque required for cnc stepper motors and spindle for aluminium milling?"}, {"body": "is it possible to make clone of http://www.makerbeam.eu/ of some easy accessible material like:\n\n\nwood\nplywood\nosb\nmdf\nhdf\nothers\n\n\nusing any type of cnc machine to mill some holes and rails in those materials may give sufficient results e.g. to make a prototype of 3d printer of such \"beams\".\n\nof course it won't be as rigid and durable but for making prototypes it may be good idea.\n\njust for reference: when reading this http://www.lowtechmagazine.com/2012/12/how-to-make-everything-ourselves-open-modular-hardware.html i've found this http://bitbeam.org/, this https://www.google.com/search?q=grid+beam&amp;client=ubuntu&amp;hs=zar&amp;channel=fs&amp;source=lnms&amp;tbm=isch&amp;sa=x&amp;ved=0cacq_auoawovchmiilhdqf2myqivqzouch1chwv1&amp;biw=1215&amp;bih=927 and this http://www.gridbeam.com/, and this https://www.tetrixrobotics.com/.\n", "tags": "mechanism", "id": "2434", "title": "is it possible to make diy clone of makerbeam"}, {"body": "there is this project i am working on which is using a beaglebone and we need a jsp container to run on it. i was thinking of tomcat but wanted to know if tomcat is suitable for embedded systems.\n\nis it too resource-heavy? if yes, are there other lighter jsp containers? i know only of tomcat and jetty.\n", "tags": "beagle-bone", "id": "2435", "title": "jsp container for embedded systems"}, {"body": "when i buy some length of timing belt i don't know how to link ends of timing belt into loop.\n\nso far i've found one way to do that (thanks to http://www.lasersaur.com/):\nhttp://www.flickr.com/photos/stfnix/8697962319/in/set-72157624491114826\n\nany other ideas?\n", "tags": "mechanism", "id": "2443", "title": "how to link ends of timing belt into loop"}, {"body": "i'm watching this video at 36.00 min. the guy gave an example but i'm not sure what is the problem. he stated that if we want to move a robot then we should to the following \n\nfor inhomogeneous case, \n$$\nx' = rx + t \\\\\nr = \n\\begin{bmatrix}\ncos\\theta &amp; -sin\\theta \\\\\nsin\\theta  &amp; cos\\theta\n\\end{bmatrix}\n$$ \nwhere $t$ is the translation vector and $x$ is the previous position. \n\nin homogeneous case, \n$$\nx' = \n\\begin{bmatrix}\nr &amp; \\bf{t} \\\\\n\\bf{0}^{t} &amp; 1 \n\\end{bmatrix}\nx\n$$\nnow, he gave an example in which \n$$\nt = \n\\begin{bmatrix}\n1 \\\\\n0 \n\\end{bmatrix}\n, \nx = \n\\begin{bmatrix}\n0.7 \\\\\n0.5 \n\\end{bmatrix}\n$$\nmy solution is as the following in matlab\n\n\n\nfor homogeneous case\n\n\n\nboth have same result, but the guy got another result. what exactly he did is \n\n\n\nwhy he did switch $t$ and $x$? i'm aware of the issue that he is trying to calculate the velocity but in fact he is computing the position. this mistake in the notation won't affect the final result. \n\n\n\nsecond question, why he assumed that the forward movement of the robot in the above example should be\n$$\nt = \\begin{bmatrix}1\\\\0\\end{bmatrix}\n$$\n? he said that because the robot always in the forward movement move in +x axis. why this is the case? the movement in robot's frame is determined based on the direction of the robot and the distance the robot travels which is specified as hypotenuse length. \n", "tags": "mobile-robot", "id": "2445", "title": "transformation a robot in 2d?"}, {"body": "i am planing to control my rc helicopter with my computer. i have experience of programming in .net. could we use .net to control rc helicopter? \nfrom where can i start this project?\n", "tags": "control radio-control", "id": "2446", "title": "rc helicopter connect with computer"}, {"body": "i'm building my first quadrocopter, and i'm trying to come up with a parts list that is suitable for a first build. \n\ni will use this to learn how to fly a quadrocopter manually (lots of crashes!), and to do some experiments with running ai for piloting it.\n\na couple questions about the below list of parts:\n\n\nis this a good choice for a first build? are we missing any crucial parts?\ndo these components work together? is this battery strong enough to fuel all the components that need power?\n\n\nhere's the current list of parts:\n\n\nframe - 450 mm\npropellers - 10x4.5\", two pairs\nmotor (4x) - 900kv brushless outrunner motor; max current: 18a; esc: 25-30a; cell count:3s-4s lipoly\nelectronic speed controllers (4x) - 20a constant current, 25a burst current; battery: 2-4s lipoly\nbattery - 3300mah lipoly, 11.1v, 3 cell; constant discharge: 30c, peak discharge: 40c. charge plug: jst-xh. 3300 mah x 30c = 99 amps?\ncharger - lipoly, 50w, 6a, 12v power supply\npower supply - input: ac 100-240v 50/60hz; output: dc15v 5a\narduino board\ngyroscope for arduino\naccelerometer for arduino\ngps sensor for arduino?\nrc transmitter for arduino\nrc controller\n\n", "tags": "arduino motor sensors quadcopter", "id": "2453", "title": "quadrocopter first build: how to tell if components play well together?"}, {"body": "someone told me when explaining about a controller module named  that it only checks self- and moves accordingly without detecting . to me both sounds the same. how are they different? thanks.\n", "tags": "untagged", "id": "2456", "title": "how are interference avoidance and collision avoidance different?"}, {"body": "i understand this is a broad question but i'm taking the risk to ask anyway.  robotics, from what i can tell so far, is a detailed, diverse, and thorough field. however, if there are better areas of research to invest time into, what would those areas be?\n", "tags": "research", "id": "2458", "title": "what should i study if i want to get into robotics?"}, {"body": "i have a raspberry pi with this ftdi cable and a roomba 560. the roomba has an sci port to allow for control of the roomba via serial. i installed the pyserial library on the pi and send valid commands to roomba, but the roomba doesn't respond. i have the txd of the cable attached to the txd of the roomba, the rxd on the cable wired to the rxd on the roomba, and a ground on the cable wired to the ground on the roomba (everything in  it's respective port). i do not have power going from the cable to the roomba or vice-versa.\n\nwhat i can't figure out is why the commands aren't working. there's no error message upon running the python code. this is the information sheet for the roomba's sci port.\n\ncode:\n\n\n", "tags": "mobile-robot software wheeled-robot", "id": "2462", "title": "connecting a raspberry pi to a roomba via an ftdi cable"}, {"body": "i'm looking for low-cost depth cameras (less than 1000 usd) with a range of more than 3 meters.\n\ncurrently, i have found only softkinetic ds-311 that meets these requirements. \n\nhere are some other low-cost cameras that i found, but they have a short range:\n\n\npmd[vision] camboard nano\nsoftkinetic ds-325\n\n\nand others with a long range but high cost\n\n\npanasonic d-imager\npmd[vision] camcube 3.0\nswissranger sr4500\nodos imaging real.iz-1k\n\n", "tags": "sensors kinect computer-vision", "id": "2465", "title": "alternatives to primesense depth cameras?"}, {"body": "i'm multimedia developer who is searching for a way to get gps signal inside buildings/structures. is amplification a reliable way to fix this gps signal issue?\n\nwill a \"gps amplifier\" work as perfectly as using gps outside?\n", "tags": "wireless gps", "id": "2470", "title": "gps amplifier - is this reliable?"}, {"body": "i am planning on creating a quad-copter with my arduino that i have. i have created a few land robots before but no aerial vehicles, so this is all new to me. i was looking on the internet for different models, and i see that most robots have 4 propellers. i have also seen a few hexacopters (?) and octocopters but that many propellers can't get a but out of hand. does having 4 propellers the best and most efficient thrust to weight ratio, or will 3 propellers/arms work better?\n", "tags": "quadcopter", "id": "2476", "title": "benefits of the number of propellers"}, {"body": "i try to connect android device and computer over ros. \nhttp://wiki.ros.org/android_sensors_driver\n\nthis tutorial explains good. i download the application on android device and set the ros_master_uri.. when i run the application, the phone shut down and this node is not seen in 'rosnode list'\n\nis there anyone who experience similar error?\n", "tags": "ros", "id": "2478", "title": "ros ~ android sensor driver"}, {"body": "i've really tried to find something online that's suitable but what i'm after is\n\ni've two concentric circular rings, one of which has a diameter about 10mm smaller than the larger .\n\nthe rings themselves are in the region of 300mm diameter.\n\ni'm trying to find a way to connect the two together and allow the smaller to 'slide' in a circular rotational way within the larger one.\n\ni'm also trying to let the 2 rings pivot vertically in relation to each other - the intention being of producing a gyroscopic-esque motion.\n\n\n\nwhat type of bearings/tracks/spindles would suffice?\n", "tags": "tracks linear-bearing", "id": "2480", "title": "looking for a circular track and bearing with a spindle"}, {"body": "i would like to write a simple program which processes the depth feed from an asus xtion depth sensor using openni. the sensor will be fixed like a cctv camera and count multiple targets moving around. \n\nthe processing would involve some simple functions on each frame (background subtraction, level sets, connected components filter), and some multi-target tracking across frames. i have searched the web, but it is hard to see how best to get started (and i'm also quite new to programming in c). \n\ncan anyone recommend any existing code that can help to get started / any libraries which would be suitable for this real-time application? or perhaps there is some opensource code which already does such a thing? would really appreciate any pointers from anyone with experience. thanks!\n", "tags": "openni", "id": "2481", "title": "writing a simple program for processing rgbd video with openni"}, {"body": "in my bachelor, i programmed cnc machines. now, working with an industrial robot arm, i learn that their programming languages are mostly similar. val is a typical example, for instance:\n\n\n\nmost of the cases, control of a robot arm is similar to this example. cleary, move end-effector to a point with a given pose.\n\nbut... is there any way that i can control the end-effector (ee) speed? an example is \"move ee to p1 with time duration t1\", or \"move ee to p1 with velocity v1\" (i could have only seen defining for joint rotational velocity) in other way of speaking, i can command the ee to move from p0 to p1 but cannot control the duration of that traverse which is necessary in cases of ee velocity control\n\nthis is the programming manual for my robot mediafire.com/?agl76pi7t7v4hjv. the velocity control i'm talking about is not joint velocity but end-effector velocity. but ee_screw = robot_jacobian*joint_vel which means to control ee velocity, it resolves in control joint velocity. about the inverse kinematic, i've already programmed a module to solve the robot\n\nthe experienced in robotics programming and val please help! i've stuck in this problem for months\n", "tags": "control manipulator", "id": "2486", "title": "val language and velocity control of industrial robot"}, {"body": "can i assume the noise of motion model to be zero? if so, what are the consequences of doing so?\n", "tags": "kalman-filter noise ekf", "id": "2488", "title": "can motion model noise be zero?"}, {"body": "so my team made a vex robot for the toss-up competition, but we need the arm up during the autonomous. the problem is that it's too heavy to stay up on its own. i was going to use encoders to count what angle the arm is at. i was going to use this code, but i'm not sure if there's a better way.\n\n\n\nwould anyone recommend a better solution or is this the best way? this is untested by the way.\n", "tags": "arm", "id": "2489", "title": "vex - keeping arm at an angle"}, {"body": "i have to build a line following robot that will be able to detect a selected colored line on the floor and start following it.\n\nhow do color sensors do this after detecting the specific colored line?\n", "tags": "mobile-robot sensors", "id": "2491", "title": "how are color sensors used for line following?"}, {"body": "i am working on slam for autonomous car like vehicles with 2d lasers and imu (deriving odometry).\n\ni would like to know how efficient is using the existing slam algorithms (for example: gmapping in ros based on rao blackwellized particle filter).\n\ntill now i find map are high in volume and speed of vehicle is high and most importantly computational time compared to mobile robots.\n\nare there  any other important factors to consider for car like vehicles in using slam algorithm.\n\nthank you.\n", "tags": "mobile-robot localization slam", "id": "2494", "title": "slam for autonomous car"}, {"body": "\n\nwhat would the equations be for the robot's angular and linear velocity at p and also p2? i think i'm doing it wrong...\n\nwl = left wheels angular velocity\nwr = right wheels angular velocity\n\nfor p i had for example the linear velocity = (1/3)*r*wl + (2/3)*2r*wr\n\nam i on right track?\n", "tags": "kinematics wheel", "id": "2498", "title": "forward kinematics two fixed standard wheels"}, {"body": "i know that inverse kinematics ($p \\rightarrow q$, p: desired pose of the end-effector, q: joint angles) is not a function because there might be multiple joint angle vectors q that result in the same pose p.\n\nby inverse dynamics control i mean the mapping $(q, \\dot{q}, \\ddot{q}) \\rightarrow u$ (u: required torques. i am not very experienced with these kind of problems. is the mapping a function, i.e. for each triple $(q, \\dot{q}, \\ddot{q})$ there is a unique solution u? my intuition says it is. but i am not sure. if there is not, would it always be possible to obtain a solution by averaging two or more solutions?\n", "tags": "dynamics", "id": "2499", "title": "can inverse dynamics control be regarded as a function?"}, {"body": "i have big problem. i have to solve inverse kinematics for a manipulator with 6-dof using jacobian method. from what i know to do that i need to have matrix of transformation and denavit\u2013hartenberg parameters, which both i have. but i am not a mathematician, and descriptions i find on the web are not even a bit understandable to me. so i would love if you could give me an example of how to solve my problem.\n\nthe denavit-hartenberg parameters are:\n$$\n        \\begin{matrix}\n        \\alpha &amp; l &amp; \\lambda &amp; \\theta\\\\\n        90 &amp; 150 &amp; 0 &amp; var(-69) \\\\\n        0 &amp; 610 &amp; 0 &amp; var(85) \\\\\n        90 &amp; 110 &amp; 0 &amp; var(-52) \\\\\n        -90 &amp; 0 &amp; 610 &amp; var(62) \\\\\n        90 &amp; 0 &amp; 113 &amp; var(-60) \\\\\n        0 &amp; 0 &amp; 78 &amp; var(-108) \\\\\n        \\end{matrix}\n$$\n\nthe values in theta are values to get the following matrix of transformation, and values i want to get with this jacobian method. and those values are in degrees.\n\nmatrix of transformation:\n$$\n        \\begin{matrix}\n        0.7225 &amp; 0.0533 &amp; 0.6893 &amp; 199.1777\\\\\n        -0.2557 &amp; -0.9057 &amp; 0.3381 &amp; -500.4789\\\\\n        0.6423 &amp; -0.4206 &amp; -0.6408 &amp; 51.6795\\\\\n        0 &amp; 0 &amp; 0 &amp; 1 \\\\\n        \\end{matrix}\n$$\n\ni would be most greatful, if someone could walk me through how to solve it in simple language.\n", "tags": "inverse-kinematics", "id": "2504", "title": "jacobian method for inverse kinematics"}, {"body": "i want to begin robotics.so as a beginner what micro-controller would be convenient?arduino or pic?what type of robots can be built with arduino or pic? should i start from just a line-following vehicle? \n", "tags": "arduino microcontroller beginner", "id": "2507", "title": "robotics as a beginner"}, {"body": "can anyone recommend a commercial or solid, reliable diy solution for scanning cylindrical objects? i've seen a couple of simple hacks for flatbed scanners, but i'm looking for something i could make or buy for a commercial project that work reliably.\n\nmany thanks\n", "tags": "stepper-motor", "id": "2508", "title": "looking for a way to scan cylindrical objects"}, {"body": "i would like to know if there is a good source that combines slam problem with vision. from mathematical perspective, there are numerous resources that handle slam ,however, i didn't find a good source that focuses on slam and vision.  \n", "tags": "kalman-filter slam ekf", "id": "2510", "title": "slam and vision (good resources)?"}, {"body": "i am working on a micromouse and it has three sensors. call it s1,s2 and s3. for now, i have to use s1. the idea is this s1 controls the left motor and s3, the right motor. s2 will detect wall in the front. \n\nanyways, i am trying to write a code in c for the dspic30f4011 mcu which would continuously read sensor values and after reading two consecutive values, it will compare the two values. read happens every 0.1ms.\n\nthe flow of the code is as follows:\n\n\n\nso if you look at the line with the *, how do i compare two sensor values in real-time every 0.1ms ? let me know if one wants to more info!!\n", "tags": "c micromouse", "id": "2511", "title": "find the difference between two consecutive sensor readings in real-time in c"}, {"body": "motion is known to be confined in a sphere with radius of about 0.5m, and resolution doesn't have to be very high (5cm is enough). the device will actually be incorporated in a toy designed for kids.\n\ni tried implementing that with an accelerometer but the estimated displacement drifted away 100s of meters per minute.\n\nis there some other solution, maybe involving electrical or magnetic fields? it's important that the sensor costs no more than a few bucks.\n\nedit: the device should not be attached to anything mechanical and its movement is in 3d (a kid moves the toy freely).\n", "tags": "sensors imu", "id": "2519", "title": "how to measure displacement, cheaply and without using an accelerometer?"}, {"body": "i need to specify a fan motor combination and wondered if there are formulas that can work this out? the fan we are using is a crossflow fan:\n\n\n\nso i'm assuming the power required to drive it is derived from the number of blades, dimensions of blades (including angle of attack), dimension of barrel/wheel and the speed in rpm.\n\nis this possible or does it need to be worked out practically with experimental measurements etc?\n\nhopefully this is the correct stack for this question, if not then mods please feel free to edit/close. \n", "tags": "motor power", "id": "2524", "title": "how to calculate the power required to drive a fan"}, {"body": "i have been into a boggling paper research on neuromorphic engineering and its implications on robotics applications, lately. it is relatively a less applied field and full of academic papers and difficult to skim easily :)\n\nthere are many ongoing projects applying analog or digital circuitry design to implement neurosynaptic simulations of the brain. consumer oriented products like ibm synapse and qualcomm's zeroth focus on digital hardware whereas academic research like standford's neurogrid or etc zurich's human brain project focus more on actual brain study using analog hardware.\n\nif anybody is following this engineering field, can he/she spread more light on it and explain it's implications, methodologies and toolsets to the community, in detail?\n\nps : regarding toolsets, i'm talking about the most feasible engineering methodologies to commit to the field.\n", "tags": "mobile-robot artificial-intelligence research machine-learning", "id": "2526", "title": "neuromorphic engineering and robotics"}, {"body": "i have a raspberry pi hooked up to a roomba 560's serial port. while going over the spec, i noticed movement controls weren't as simple as i expected. i can't send bytes larger than 255, but, according to the spec, to go straight i have to send 8000. how does this work?\n\nedit: my solution was the following three functions:\n\n\n", "tags": "mobile-robot raspberry-pi serial irobot-create roomba", "id": "2530", "title": "help sending serial command to roomba"}, {"body": "ros is not real-time os.\nafter reading the architecture of ros, i am unable to realize why is ros not real-time? what part of the architecture or what design decision is causing that?\n", "tags": "ros", "id": "2531", "title": "why is ros not a real-time operating system?"}, {"body": "in most situations, range of motion is limited by the fact that we need to carry power or information past a joint. so, past a certain point there are either cables in the way, or the cables would stretch so much that they would either prevent further movement or break.\n\nhowever, if we situate the conductors in concentric rings around or within the rotor shaft, we can have a joint that can rotate forever while keeping in contact with any modules on the other side.\n\nwhat do we call this mechanism? does it even have a name?\n", "tags": "motor motion joint", "id": "2542", "title": "what do we call a coupling with infinite range of rotation"}, {"body": "in my project i'm aiming to control a quadruped robot from my android phone using raspberry pi as a middle device (web server). in order to make sure that the server on rpi working fine i googled and got an app that sends a specific character whenever a button is clicked and the arduino job here is simply to receive it from serial port and blink a led! (so easy huh?)\nbut the problem here is that i noticed that some leds are blinking when i click a button not assigned with them! this can be a disaster if you are controlling a robot! \ndoes anybody know the reason of this and the solution? \n", "tags": "arduino serial", "id": "2545", "title": "arduino serial mixing incoming commands"}, {"body": "i was wondering what was, in your opinion, the best way to study the different motions of a rubiks cube.\ni want to be able to recognize what face moved and in what direction. \n\nwould i be able to get the direction and the face with accelerometers/gyro and if yes how many would i need?\nif you have ever used a leapmotion or kinect, is it possible to achieve this using those?\n", "tags": "accelerometer motion", "id": "2552", "title": "best way to sense rubiks cube movements"}, {"body": "i'd like to buy a capacitive touch input robot in order to remote access my ipad but i'm having trouble describing a correct kind of robot. \n\ni would like to keep lag down to an additional 60ms so that it is still a high quality interface. \n\ni would like to have a robotic arm equipped with a capacitive pen that moves to places on the ipad screen based on the mouse or i'd like a array of capacitive pens that emulate the touch of a user. \n\ni guess i'd use squires software reflect and the mirror function but i'm open to using an shd camera with the robotic arm and a pixel sensor array with the array of capacitive pens. \n\ndoes this make sense? how could i improve the design? what materials would i need to build it myself. assuming ready built arm? how could i build an array of capacitive touch micro pens?\n", "tags": "robotic-arm", "id": "2553", "title": "capacitive touch input robot to remote access ipad"}, {"body": "i want to send image over wireless serial communication. i am planning to capture images using either raspberry pi or stm32 mcu using dcmi and then transfer image using wireless serial communication module such as xbee or 3dr radio which can provide air data rate upto 250kbps at baud rate of 115200\n\ni would like to know if there is any protocol which can send a jpeg compressed image as a wireless serial data.\n", "tags": "raspberry-pi serial communication wireless", "id": "2555", "title": "any image transfer protocol for wireless serial transfer?"}, {"body": "i am working on an ekf and have a question regarding coordinate frame conversion for covariance matrices. let's say i get some measurement $(x, y, z, roll, pitch, yaw)$ with corresponding 6x6 covariance matrix $c$. this measurement and $c$ are given in some coordinate frame $g_1$. i need to transform the measurement to another coordinate frame, $g_2$. transforming the measurement itself is trivial, but i would also need to transform its covariance, correct? the translation between $g_1$ and $g_2$ should be irrelevant, but i would still need to rotate it. if i am correct, how would i do this? for the covariances between $x$, $y$, and $z$, my first thought was to simply apply a 3d rotation matrix, but that only works for a 3x3 submatrix within the full 6x6 covariance matrix. do i need to apply the same rotation to all four blocks? \n", "tags": "kalman-filter", "id": "2556", "title": "how to rotate covariance?"}, {"body": "aren't all propellers super dangerous? how are these startups like hex and pocket drone selling drones as 'kid-friendly' to consumers? what happens if a kid puts his finger in a propeller's movement space while its flying? \n", "tags": "quadcopter", "id": "2559", "title": "are propellers dangerous?"}, {"body": "i started to follow \"get started with the wifi bee\" tutorial in wifi bee v1.0 wiki page. \ni am using xbee usb adapter v2.0,wifi bee-rn-xv,mini usb cable as listed in tools needed.\nwhen i conneted the xbee usb adapter v2 to my computer via mini usb cable wifi bee didn't light up (but usb adapter did light up). then i followed all the steps till number 4, which is \" send at command $$$ to the wifi bee and it will reply \"cmd\" to indicate that it enter the command mode properly\". when i sent command, it didn't reply anything. i typed other commands like show net and  scan, it didn't reply either.\n\nwhen i tried arduino server example in the wiki page, wifi bee lighted up. but it gave me some strange ip address with port 200, and when i entered that ip address to my browser address, browser couldn't find the page.\n\nso my question is does xbee usb adapter need some extra power sources? or it just doesn't fit with wifi bee? i don't think the problem is with usb cable, 'cause my computer found the device. \n", "tags": "wifi usb", "id": "2563", "title": "connecting wifi bee with xbee usb adapter"}, {"body": "i need to read the location of my device within a 1m radius sphere, with accuracy of 5-10cm.\n\nthe device is handheld and wireless, and currently communicates using a bluetooth v4 chip. i could add an rf transmitter on the moving part and a few stationary receivers at the base.\n\nwhat components should i look into? what would be the cheapest way to triangulate it?\n", "tags": "sensors imu", "id": "2564", "title": "how to use toa (time of arrival) to measure 3-axis location of a wireless device?"}, {"body": "i am looking at buying a servo motor for a an application that must be able to lift 4-5 lb at a rotational speed of approximately 1rpm. the servo motor listed here http://www.robotshop.com/ca/en/hitec-hs755mg-servo.html states a stalling torque of 200 oz-in. is this torque rating at the horn of the servo motor or the torque rating of the actual motor before any gear reduction is done?\n\nis this motor sufficiently strong for my application?\n", "tags": "motor servos servomotor", "id": "2570", "title": "do servo motor specifications take into account the gear ratio inside?"}, {"body": "i am making a robot that needs to continuously track the relative position of a human, up to 15 meters away and with at least 300 degrees coverage. currently i am using a hitechnic irseeker v2 sensor and made a beacon wristband with 6 tv remote ir leds. but the maximum distance i can get is around 3 meters.\n\ni ordered some 3 watts ir leds to boost the power, but the size of the wristband will be a problem because it will not run on a cr2032 battery.\n\ni also bought some ir remote receivers. but i am not sure if the reflection from the wall will give false results.\n\nis what i am trying to do possible? is a beacon 15m away feasible using this technology? \n\n\nif it is, then what do i need to modify in my current implementation?\nif not, are there any other technologies that i should be considering to track the relative position or direction of human, 15 meters away with at least 300 degrees coverage?\n\n", "tags": "sensors", "id": "2571", "title": "sensor for tracking relative position of human"}, {"body": "i'm writing a pid to control a toy car that follows a black line on a circuit. i've tuned my pid and it works at high speed for all the circuit except the winding section. for that, the error signal looks like a sine wave, and the toy car steers too much. i would like it to go close to straight, is it possible?\n\nedit: my car sees 100 grey points in a line ahead, and the difference between the darkest point and the middle of the visual range is the error signal. my output is the angle of a servo on the front wheels of the car, while the speed of the back motors is constant.\n\nthe desired performance would be to oscillate with an amplitude less than the amplitude of the winding road, and the actual performance is that the car steers close to the sine line for one period, and at the next max amplitude it under steers. sorry, i can't provide graphs right now but i'll try to add some in the next days.\n\nis there a formula for adjusting the pid constants for the desired pid bandwidth?\n", "tags": "pid", "id": "2575", "title": "pid control against sine wave error"}, {"body": "i have odometry data $(x, y, angle)$ of a real two-wheeled robot, who received control commands $(forward speed, angular speed)$.\n\nnow i want to code a motion model (in c++ (/ros)) which should follow the same trajectory, given the same control commands.\n\nnormally, the kinematics should look something like this:\n\n$$ \n\\begin{align} \nv_{fwd} &amp;= control_{fwd} \\\\\nv_{ang} &amp;= control_{ang} \\\\\nx &amp;= x_{old} + 0.5(control_{fwd} + v_{fwd,old}) * \\cos(angle) * dt \\\\\ny &amp;= y_{old} + 0.5(control_{fwd} + v_{fwd,old}) * \\sin(angle) * dt \\\\\nangle &amp;= angle_{old} + 0.5(control_{ang} + v_{ang,old}) * dt\n\\end{align} \n$$\n\nand i thought about just setting \n\n$$ \n\\begin{align} \nv_{fwd} &amp;= control_{fwd} + k_1 v_{fwd,old} + k_2 v_{fwd,old}^2 + k_3 v_{ang,old} + k_4 v_{ang,old}^2 \\\\\nv_{ang} &amp;= \\text{ ...analog...} \\\\\nx, y, angle &amp;\\text{ unchanged}\n\\end{align} \n$$\n\nand then just search the minimum of the squared distance of the computed trajetory to the real one - depending on the values of $k_i$. this would mean either a good optimization algorithm or just brute-forcing / randomly testing a lot of values.\n\nis this the way to go here? i tried the 2nd approach, but the results so far are not that good. \n\nso, as you might guess now, i'm pretty new at this, so any help is appreciated.\n", "tags": "wheeled-robot kinematics algorithm", "id": "2580", "title": "fit robot simulator to robot"}, {"body": "so i have this optical mouse with me, which has a pan3504dl-tj optical sensor. it has a usb interface and when i looked up the internet, all i could find was tutorials using a2501 or sensors in those lines and it has pins like sclk and sdi but i don't have them instead i have d+ and d-. i understand that these are the data pins so what i did was take two wires and plug them into my analog pins of my dspic30f4011 and read data from it. after setting up uart communication and transmitting data, all i get are numbers running continuously.\n\n\n\nwhat i want to do is to read coordinates over the analog pins as the mouse aka the sensor moves on a surface. i would use this for position control for my robot.\nso my question is how do i read coordinates from the optical sensor over the d+ and d- lines through my analog pins ?\n", "tags": "sensors", "id": "2587", "title": "reading data from d+ and d- pins of a usb"}, {"body": "robotics stackexchange vs. ros answer:\nwhat is better and for what purpose?\n", "tags": "ros robotc", "id": "2588", "title": "robotics stackexchange vs ros answer"}, {"body": "what are valid values for the denavit-hartenberg parameters $d$ and $a$ (sometimes called $r$) of the last 3 links of a robot with spherical wrist?\n\nfrom this reference, \"a spherical joint can be represented by three consecutive rotary joints with intersecting rotation axes.\"\n\nso the retrictions should be:\n\n$l_{n-2}$ ($d$ arbitrary, $a=0$)\n\n$l_{n-1}$ ($d = 0$, $a=0$)\n\n$l_{n}$ ($d = 0$, $a=0$)\n\nbut in this exam i have found on the internet, it says that the kuka robot has spherical wrist, and $d$ of the last joint is different to $0$. would $d\\neq0$ in the last link still yield a spherical wrist?\n", "tags": "kinematics joint", "id": "2591", "title": "denavit\u2013hartenberg parameters of a robot with spherical wrist"}, {"body": "i have been wanting to build larger robots and r/c cars for some time now, but one issue i have had is trying to find larger motors, in the range of electric wheelchair motor size. i found one set on ebay but i am trying to find a more reliable source for these.\n\nto make my question more clear, i am looking for a reliable source(s) for medium size electric motors around the size and power rating of a typical electric wheelchair motor\n", "tags": "motor", "id": "2593", "title": "sourcing motors for larger robots"}, {"body": "i have built a r/c car that runs on 2 30ah 12v dc deep cycle batteries. the motors are 24v motors that will each draw around 15a at full power. my  motor controller can handle this, as well as reclaiming braking energy. \n\nthis is my way of saying that i have a 24v power system. now my issue is that i want to run a 12v device on this 24v service. i do not want to have the hassle of another battery to maintain so i would like to power it off the main batteries. all the becs and other converters that i have found only supply around 1 amp while the device i am looking at powering will take around 4-5a 12v dc. does anyone know of a device that will do this.\n", "tags": "bec", "id": "2594", "title": "24v dc to 12vdc converter"}, {"body": "i'm using the johnny-five library to control an arduino uno running standardfirmata. i have a hc-05 bluetooth module that i want to use to wirelessly control firmata, but have yet to get it working.\n\ni used http://www.instructables.com/id/modify-the-hc-05-bluetooth-module-defaults-using-a/ to configure the board for 57600 baud rate: . i'm able to send various at commands and read back the results in my serial monitor.\n\ni followed http://www.instructables.com/id/use-your-android-phone-sensors-on-the-arduino-/?allsteps to wire up the voltage divider, to make my arduino's tx operate at 3.3 going into the hc-05's rx.\n\ni've tried running the hc-05 in master, slave, and slave-loop. it only makes a bt connection in slave, which is default.\n\nwhen i run my johnny-five script, here's the output:\n\n\n\ni've more-than-triple-checked everything. uploaded firmata many times. firmata works fine over  usb. also, i have been able to get this working in the past with an hc-06.\n\n\nam i missing something?\nwhat are some good debugging techniques to figure out why it won't connect to firmata?\n\n", "tags": "arduino troubleshooting", "id": "2595", "title": "control arduino firmata with hc-05"}, {"body": "boston dynamics keeps making great robots, however, i dont see any papers that they publish.  although now i can find papers on people using the atlas robot, i can not find an original paper detailing the robot or its mechanics designs.  is there a reference for the robot, should i use youtube videos?\n", "tags": "design mechanism humanoid", "id": "2596", "title": "atlas robot reference"}, {"body": "what software libraries are there for assisting the general problem of parsing a stream of sensor data?\n\nwe use various sensors like lidars and gpsins units that provide messages in proprietary binary formats, and have to write drivers for each one. even though there's a lot of similar concepts used in each sensor (like a general purpose datagram for all messages, consisting e.g. of start/end sentinels, length specifications and a checksum, and then a variety of well-defined message formats for the payload), it ends up being a lot of tedious work to develop a driver each time.\n\ni'd love a solution where i can write out packet/message specifications in some format, and have a library that finds &amp; extracts valid messages from a stream, and provides them in a simple data structure format.\n\ni'm not too fussed about what language, but basically want a general purpose datagram parsing library. there's a lot of customisation with sensors, maybe some odd format parsing, and probably some initial configuration to start the data stream, so this is really something i want as a library for processing the data in real-time that can be used as part of a driver/application.\n\neverything i find is either too basic (the low level tools for interpreting individual elements, so still lots of time spent extracting individual elements explicitly), or too specific (i.e. parsers written specifically for one particular protocol).\n\nas a concrete example, consider nmea messages:\n\n\nthere's a basic outer datagram (starts with  followed by message name, then comma separated data, and ends with , checksum and line terminating character)\ndata is in ascii so needs to be parsed to binary for computational use\nouter datagram allows for validation and removal of incomplete/corrupted messages\nmessage name &amp; content would be further parsed for consumption\nfield names can be specified for ease of use\n\n\na 'gpgll' message might be turned from  into a programmatic data structure containing latitude, longitude, utc timestamp and its validity.\n", "tags": "sensors software driver", "id": "2597", "title": "software libraries for parsing sensor data"}, {"body": "i just bought arduino uno and hc-06, i hooked up the connections:\n\n\n5v bluetooth &rarr; 5v arduino\ngnd bluetooth &rarr; gnd arduino\ntdx bluetooth &rarr; rx &rarr;1\nrdx bluetooth &rarr; tx &rarr; 0\n\n\nhere are the pictures:\n\n\n\n\nmy problem is that i cannot seem to search for the bluetooth connection on my laptop or on my phone.\nis there something wrong i am doing here?\n", "tags": "arduino", "id": "2600", "title": "need troubleshooting help regarding arduino uno & hc-06 bluetooth connection problem"}, {"body": "i have a robot platform with differential drive which knows it's position and orientation.\nlets say that the space through which the robot moves is known and it has only static obstacles. the task is to move the robot from point a and heading alpha (on which it currently stands) to point b and heading beta on the map.\n\nlets also say that i can obtain a reasonable trajectory (in relation to the turning abilities of the robot). as both the robot and the sensors are inert, what are some general approaches for controlling such a robot to follow the path? it should of course be kept in mind that the final task is to reach the point b without colliding with the obstacles and not the perfect trajectory following.\n\ni hope the question is not too general.\n", "tags": "control pid navigation differential-drive", "id": "2604", "title": "differential drive trajectory following control"}, {"body": "i am a beginner to robotics and embedded systems. consequently i have a lot of questions related to the toolchain and how things are going together like how to debug or how to connect a bluetooth module.\n\ni already tried http://electronics.stackexchange.com/ and it did not work out for me.\n\nany ideas where i can get help with my lpc1343 related questions?\n", "tags": "microcontroller arm embedded-systems", "id": "2606", "title": "where to ask nxp lpc1343 / arm cortex m3 related questions"}, {"body": "just wanted to clarify some pretty basic arduino concept:\n\nif i put this code into an arduino board:\n\n\n\n...i see >38000 value in my serial monitor (for the 'counter' variable).\n\nhowever, the moment i put in some heavy calculation in 'point a', the value drops to 150 - 170. this is expected, i guess.\n\nmy question is: is the only way to push up the operational frequency lies in optimising the code/calculation? or, is there some other way i can get faster execution?\n", "tags": "arduino", "id": "2609", "title": "arduino operational frequency"}, {"body": "i am trying to sync motors on a vex cortex based robot and have had mixed success using the encoders with position control.  i noticed that the motor setup directive\n\n\n\nhas a parameter \"pidcontrol\" but i cannot find any documentation as to what it actually does.  \n\ni see on the encoder documentation page here that the encoder provides velocity output, but it is not apparently built into the api.  so my question is two fold:\n\n1) what does the \"pidcontrol\" directive actually do?\n\n2) how can i use the encoder to control the speed of the motors?\n", "tags": "robotc", "id": "2610", "title": "function of pidcontrol #pragma config() directive in robotc"}, {"body": "i found a model for 2-wheeled robots here:\n\nwhat is a suitable model for two-wheeled robots?\n\nhow should i adapt it to a 4-wheeled setting?\n", "tags": "wheeled-robot kinematics", "id": "2613", "title": "what is a suitable model for four-wheeled differential drive rigid-body robots?"}, {"body": "since finding data on stalled and in use under load (not free) amp draw for servos seems impossible, i want to build/create my own servo tester.\n\nall i really want to know is how much amps the servo is drawing at idle, at movement under load, and at stalled/full position. i think that should cover all the bases relative to amp usage on the servo but if not, please let me know what i am missing.\n\nhere are my questions:\n\n\ni am going to need a power supply for an exact 4.8v and 6.0v since that seems to be the standard measurement voltages. \ni'll need some way to accurately measure the amp draw.\ni'll need some way to control the servo movement.\n\n\nis that it? what am i missing and if anyone has any suggestions please let me know and thanks for the help.  this seems to be uncharted waters for those in the rc hobby area but someone in the robotics field may have been down this path.\n", "tags": "rcservo", "id": "2619", "title": "building a servo tester to measure peak/stalled amp draw"}, {"body": "i am working on a system which is measuring a force. the specification is to have a 500hz bandwidth on the measurement. \n\nnow i was trying to explain this 500hz bandwith to my mom and i could not really explain it easily.\n\nwhat is the most easy way to explain the term bandwidth of a measurement to someone without control engineering background?\n", "tags": "control", "id": "2620", "title": "how to explain bandwidth of a measurement to a noob?"}, {"body": "i am a computer science final year undergraduate student.until now,i used to shirk away from robotics as i believed that it is more related to electrical and mechanical aspects.but my interest in robotics grew by seeing some demos and i seriously want to make a robot which involves ai by teaming up with interbranch students of college.so what is the best project i can pick up as a beginner in robotics and ai and some experience in computer science so i can apply ai machine learning concepts so that it learns something.how to start something?\n", "tags": "artificial-intelligence machine-learning", "id": "2628", "title": "project idea for a ai related project"}, {"body": "i am completely brand new to quadrocopter building. i am currently about to start building a quad. i have done a little bit of research and was thinking of buying the following parts:\n\n\nkk2.1 hobbyking flight controller\nturnigy h.a.l quadcopter frame\n4 x ntm prop drive    35-30 1100kv / 380w\nturnigy 9x 9ch\nturnigy plush 40a esc\nslow fly prop left\nslow fly prop right\nquad power dist board\nturnigy 5ah 3s25c lipo\n\n\nwhat do you think of these parts/do you have any complete builds with instructions that you would recommend instead?\n\nthanks\n", "tags": "motor sensors quadcopter multi-rotor", "id": "2631", "title": "quadrocopter build - do these parts look fine?"}, {"body": "i am using 8 brushless motors for an octocopter. each motor can be run at maximum 30a. i use 4 batteries in parallel. how high c number is needed?\n\n$$\\frac{30*8}{4*2} = 30c$$\n\nwhen running the motors at 100% load, it will draw 30c from each battery. can a 25c with max 50c be used, or will it run hot?\n\nadditionaly, how many ampere hours can be drawn from a 5000mah battery before it's empty? many 12v car batteries can only be drawn for 60% of their stated capacity before they need to be charged.\n", "tags": "battery", "id": "2635", "title": "how much can realistically be drawn from a 25c max 50c battery?"}, {"body": "when researching robots, micro mouses etc i've often come across people taking about generating \"speed profiles\" and how to calculate them. also profiles for acceleration , deceleration , turning etc. also trapezoidal profile?\n\nbut i can't seem to find exactly what is meant by this. the how or why also.\n\nso what is a \"profile\" in this sense and why you would need one?\n", "tags": "control", "id": "2637", "title": "what is meant by a speed profile?"}, {"body": "i want to create a two wheel remote controlled robot. i have worked out a lot of the logic with regards to balancing. i have started to read up on motor control and arduino vs beagleboard black vs raspberry pi.\n\nis the multitasking nature of a full linux os a problem i need to be concerned with for this application?\n\ni expect that i have to adjust the motors at least 20 times per second, but i don't think a slight variation in the update loop interval to be a problem. possibly, i will face problems if i need to do pwm myself?\n\nbasically, the way i plan to make the robot work is by using an accelerometer to have a reference to where down is. the robot will autonomously work to keep the direction of the accelerometer down. the remote control will simply adjust the readings from the accelerometer, and the balancing loop will then react as if the robot is falling and accelerate the wheels.\n", "tags": "arduino control raspberry-pi real-time", "id": "2639", "title": "raspberry pi for two wheel robot?"}, {"body": "i want to make a circuit that powers a transistor when a sound above a set threshold is reached. (trigger a flash for high speed photography.)\n\nhow long will the response time be?\n", "tags": "arduino", "id": "2641", "title": "what is the response time of an arduino nano?"}, {"body": "i am doing slam with a four wheeled (2-wheel drive) differential drive robot driving through some hall way. the hallway is not flat everywhere. and the robot turns by spinning in place, then traveling in the resulting direction. the slam algorithm does not need to run online.\n\nthe robot takes measurements from a strap down imu/gyro measuring , where  refers to acceleration the x direction and  measures angular acceleration about the x-axis. the lidar scans the hall way with a 270-degree arc and measures ranges and angles. however, so far as i know the hall way has no discernable features except when it corners\n\ni need to find the best way to fuse the proposed action measured by the encoder with imu and lidar data. it makes sense to me that i could fuse yaw from imu with encoder data to get a better sense of heading, but how should i incorporate lidar data? \n\nin essence, what is the appropriate measurement model and how should i incorporate noise into the motion model? beside just adding some gaussian noise at some ?\n\naddendum\n\nthis somewhat orthogonal to the question but just as confusing to me. currently i am using a particle filter to do slam, and i am a little confused about whether to represent uncertainty in angular acceleration in the particles themselves. i see two options:\n\n\na separate navigation filter using ekf (or anything really) to find a vector of \"best-estimate\" angular acceleration matrix first, then use this matrix as absolute truth for the particle filter. so that any drift in the particles is not from uncertainty in angular acceleration.\nincorporate the uncertainty into the particle drift themselves. this option appears more sensible but i am not sure what a principled way to do this is. \n\n", "tags": "kalman-filter slam particle-filter", "id": "2642", "title": "what is the best way to fuse measurements from imu, lidar, and encoder information in some recursive bayesian filter?"}, {"body": "most small quadcopters use rigid rotors with some fixed pitch.\nin principle, i can imagine it might be possible for such a rigid-prop quadcopter to hover upside-down,\nbut that apparently requires reversing the direction of rotation of all 4 motors.\n\n(this is very different from the way some \"standard\" single-rotor model helicopters can hover upside-down by continuing to spin that rotor in \"the same direction\", but moving the swash plate to give negative blade pitch).\n\nis it possible for a rigid-prop quadcopter to hover upside-down?\n\nwhen i build a quadcopter so it can switch from flying upright to flying upside-down and back again in mid-flight, what do i do differently than a normal quadcopter designed to always fly right-side-up?\n\n(related: \"can you run a bldc motor backwards without damage?\" )\n", "tags": "motor quadcopter multi-rotor", "id": "2643", "title": "can a rigid-prop quadcopter hover upside-down?"}, {"body": "can anyone help me out here with this dc motor, especially the encoder part? i tried searching around for its datasheet but its as short as 1 page and the only spec i get are:\n\nencoder:\n1 pulse/revolution\n\nit has 2 connection on the bottom, and i guess they are for driving the motor, but they say nothing about the 3 connections wires below.\n\n\n", "tags": "motor quadrature-encoder", "id": "2644", "title": "dc motor with encoder"}, {"body": "i'm a newbie to mircroprocessors (pcduino, for example) and i wanted to know if kinect can be integrated with the pcduino, before i go and buy the board. i know in terms of connectors etc what might be required. my concern is regarding the hardware required to run the kinect.\n\nto elaborate more, i'll explain my current system: i have a system working on my laptop that uses a kinect to extract unorganized point cloud data using \"processing\" ide which interacts with kinect using openni drivers. my matlab code then processes this information to detect obstacles and specific objects (can also be done using c++). \n\ni want to build such a system for a robot, but using pcduino as the processing module. this means that the kinect will connect to the pcduino using one of its usb ports. i'll power the kinect using battery and a converted power adapter. since pcduino can run linux (ubuntu) i (think i) can easily convert my laptop code into whatever the ubuntu requires. the only concern i have is if there were any problems associated with using depth sensors with mini pc boards in terms of hardware capabilities of mini pc boards? i know that mini pc boards are not as fast as a pc, so the processing would be slower, but i'm not concerned with the speed, atleast for the time being.\n\none problem i encountered while using kinect, even on a pc is that the point cloud drivers in openni won't initiate the point cloud data stream, unless there was a gpu in the pc; the exact same code runs perfectly on a pc with a dedicated gpu. however, i do know that pcduino has a gpu chip (opengl es2.0). would the kinect work on this?\n\ni searched online but the closest thing i could find is this which does not elaborate how the integration of raspi and asus xtion works. i'm not too picky about the boards, anything that would work with a kinect is fine with me, although i like the pcduino since it has arduino headers and built in wi-fi etc. \n\nany additional pointers can also be helpful. please let me know if i need to elaborate on anything more.\n\nthanks in advance\n", "tags": "microcontroller kinect", "id": "2649", "title": "kinect point cloud + pcduino. will it work?"}, {"body": "i'm planning to order parts for my first quadcopter build and i had a few questions. here is my parts list. i'm crossing my fingers that they are all compatible, and i'm pretty sure they are. i have two questions:\n\n\ndo i need a power distribution board and if so, what does it do?\nwhere on my flight controller do i attach my radio receiver? \n\n", "tags": "quadcopter microcontroller radio-control", "id": "2652", "title": "a few questions about my first quadcopter build"}, {"body": "i have found some load sensors (piezoelectric) that measure relatively small weights (on the order of ~ grams). that's what i need!\n\nhowever...\naround my robot, there will occasionally be bursts of extremely high pressure. these bursts do not need to be measured... they just wash over.\nthe pressure appears, to the sensor, to be a ~ 2,000+ kg \n\nquestion:\nare these sensors likely to break or fatigue? i realize piezos do not measure via deformation, but still... that's a big load!\n\nmaybe i should just order a few and try...\n", "tags": "sensors", "id": "2657", "title": "could piezoelectric sensors be crushed?"}, {"body": "i have a system that i can make a strong kinematic model for, but my sensors send readings at unpredictable times. when i say unpredictable, i am not just saying the order the readings will arrive, i also mean that sensors are able to sleep when they do not see a significant change. when an input arrives for any given sensor, that information can be used to infer the states of many other sensors based on my model.\n\nat first, it seemed like a kalman filter was exactly what i needed because i could make a prediction of all of the states of the system and then update those states when one piece of information comes in and repeat this process until a good estimate of the system as a whole was determined. however, after reading over kalman filters, it looks like they assume that every state will be updated on a regular basis. is there a way the kalman filter can be modified for when you are unsure about what input will come in next and you are also unsure how much time will elasped before the next input arrives? please note that in my case, once the information arrives, i will know the source of the input as well as the time that has elapsed since the last update, i just won't be able to predict these two things beforehand.\n", "tags": "kalman-filter", "id": "2661", "title": "kalman filter when states are not observable at the same time?"}, {"body": "my friend has acquired a (small) wine yard to discover how much work it is tending and harvesting the grapes.\n\nnow we are musing to enlist robotic help. the vine stocks are usually connected by a stiff wire, so some dangling machine might be feasible.\n\nwe are looking for references/inspirations for an agricultural robotic vine assistant. any ideas?\n", "tags": "mobile-robot robotic-arm", "id": "2667", "title": "wine yard robotics?"}, {"body": "i wrote my own quadcopter firmware which is based on some older code. this code shall stabilize the copter to be always in equilibrium. the model is behaving relatively nice. i can control it with my laptop.\nhowever i noticed, that the copter is hovering to the side (if not manually controlled), likely because of wind, not well balanced or turbulence. \n\nmy idea was maybe to fuse gps and accelerometer data to implement a function which shall help to hold the position. but this will likely only work if i have a hold altitude function, because changes in pitch or roll change the height, because the thrust is changed slightly. this is why i recently added a routine which shall allow to hold the altitude. \n\nis someone having experiences with this? i mean with avoiding side drifts of the model because of whatever by software? the problem is in my opinion, that i don't know whether the position change is wanted (by remote control) or not. additionally it is hard to localize the correct position and calculate the distance caused by drift from it (just with gps, but this is not precise).\n\n\n\ncopter control:\n\n\n", "tags": "arduino quadcopter", "id": "2668", "title": "removing quadcopter drift to the side"}, {"body": "what exactly is active compliance control in robotics joint? \nwhy is it used ? \nhow can i write a program to simulate the compliance control in matlab for a single robotic link or single robotic joint ?\ni have to develop an algorithm for torque control.\ni have to sense the torque and give feedback to bldc motor which is supposed to apply some controlled torque. \ni also have some unclear understanding of few things: lets say i have single joint two link systems, how would this system behave when i have applied the compliance control algorithm at the joint? how will i test it? i mean if i apply some external torque what should it do so that i understand that it is in compliance control mode.\nhere is a related paper.\nhttp://www.thehandembodied.eu/pdf/iccas.pdf\n", "tags": "control robotic-arm", "id": "2671", "title": "compliance control for a single link robot in matlab"}, {"body": "i'm trying to use the mcbl controller by faulhaber to control my motor. i'm trying to program some sort of driver on linux using the serial connection and libserial. but it does not seem to be working for now. \n\ni'm using the usb to rs232 converter like this one:\n\n\n\ni'm wondering if it's well supported by libserial. i've read that yes but does anyone have any experience with it?\n", "tags": "serial communication", "id": "2672", "title": "mcbl controller through rs232"}, {"body": "i am doing research of ego-motion estimation and positioning in 6dof space. and i found that apparently all systems are based on active rgb-d sensors, like kinect. i understand, that such sensors provide greater accuracy, and requires less computational resources.\n\nbut if such systems will be used, for example, for augmented reality or robot navigation, how they are going to solve the problem of the interference of signals from different systems, operating in the same space? if many people will wear ar glasses with active sensors - they will interfere with each other, aren't they?\n\nare there big commercial projects, that use passive visual odometry with multiple camera units and imu sensors? i found some good papers on this topic, but i have not found commercial application of such technology. i am going to make research of passive odometry method for ar, but is it actually a problem with active depth sensors, that i described earlier?\n\nupd:\nthe main question:\n\nis passive odometry, based on video flow analysis and imu, worth to make deep research in this topic, or active sensors - is our future, and the signal mix is not a big deal, and passive odometry is a dead end of such kind of technology? because it will be not very useful to make research in useless technology...\n", "tags": "kinect sensor-fusion odometry", "id": "2673", "title": "passive ego-motion estimation vs active"}, {"body": "i know there must be a \"tool\" that can measure oz-in of torque.  i do not want to trust what the servo manufacturers state on their site for torque values so i want to test them for myself.\n\nanyone know what tool i can use to do this?  i have used fishing scales before, but i need something more sensitive than that and my units are pretty small such as around 20 oz-in.\n\nthanks.  \n", "tags": "servos", "id": "2676", "title": "how can i measure the torque value of a servo?"}, {"body": "i am planning to build an autonomous all terrain surveillance robot using raspi, which is the better option computer vision or ultrasonic sensing to avoid obstacles?  and i want to transmit the video recording to base station.\n", "tags": "raspberry-pi computer-vision ultrasonic-sensors", "id": "2677", "title": "autonmous surveillance vehicle"}, {"body": "i am designing  a remote controlled robot which will have a base with three wheels, two of them will be simple wheels at the back of the base and the third will be a ball wheel at the front. it will have a robotic arm which will have a gripper to hold objects up to 1kg. i have designed the arm like this\n\n\n\nwhat i want to ask is how to calculate the length of the arm, the base of the  robot, the torque and also which motor to use. please suggest to me if there is a better solution for designing the robot.i am a robot enthusiast and i am designing a robot for the first time.\n", "tags": "motor control design robotic-arm", "id": "2680", "title": "how to decide about the length of a robotic arm, the base of it and the torque?"}, {"body": "i wonder currently how to implement an altitude control for a quadcopter. i have atm just a barometer/gps and an accelerometer. \n\nbarometer and gps are relatively straight forward implemented, but not very precise and slow. for the accelerometer readout, i remove the constant 9.81 m/s\u00b2 acceleration by a low pass filter. then i take this data and calculate out of it the climb-rate(in cm per s). i know the speed approximation by this way is not so great. however i don't know a better approach so far.\n\nfor the calculation of the motor speeds i use atm two pids (stab and rate). \n\ni coded the example shown below, without much testing so far. i believe it will not work out in a smooth and nice way. e. g. instead of the speed calculated of the accelerometer i could use the climb-rate of the barometer. however for low altitudes and small changes i do need very likely the accelerometer. \n\nardupilot seems to use somehow in a different way both with a third pid for the acceleration. i believe they calculate the height difference like me. then they use maybe for stab-pid the barometer climb rate (not like me the accelerometer) and calculate with acceleration data another output. unfortunately i don't know how exactly, or whether there are other methods.\n\ndoes someone know the exact layout to implement with a barometer and accelerometer an altitude hold function. i mean i am really not sure whether my ideas would be correct. maybe i can post some options later.\n\nmy pids:\n\n\n\ncode for altitude hold:\n\n\n", "tags": "sensors quadcopter accelerometer ardupilot", "id": "2683", "title": "altitude hold for quadcopter with accelerometer and barometer"}, {"body": "i'm trying to calculate the lifting capability of my four quadcopter motors. i tried using ecalc but it doesn't have battery i'm using. are there any equations to keep in mind for doing these calculations? here are some relevant details:\n\nbattery: 2200mah 3s 25~50c lipo\n\nesc: 25a\n\nmotor: 1240kv brushless\n\npropeller: 8x4\n\nany help would be much appreciated, thanks!\n", "tags": "quadcopter", "id": "2687", "title": "need help calculating the thrust on quadcopter motors"}, {"body": "i am trying to send some data over to my pc from the arduipilot, i used a normal usb connection to send over a recurring string like this:-\n\n\n\ni receive the string just fine when i open a serial monitor with baud of 38400 bits/sec. but, when i remove the usb port and plug in the 3dr radio module to the ardupilot and the pc, it gives me garbage. i know that the 3dr radios use mavlink communication protocol, but i was wondering if it's possible to change this protocol and use a normal spi so that i receive the data in the same format i receive when connected via usb.\n\nif this is not possible, is there a way to convert this garbled data from the radio module to a useful string. \nit would be greatly appreciated if someone can help me with this.\n", "tags": "ardupilot radio-control", "id": "2692", "title": "using 3dr radio to communicate ardupilot data"}, {"body": "does anybody know how to configure the node id of an ingenia pluto dc servo drive?\n\ni've got a request out to their support team, but perhaps somebody here is already familiar with these drive boards.\n\ni do have ingenia motionlab 2.7.2, but it does not ship with documentation and the motionlab user manual on the site is out of date (i had previously been looking through the hardware documentation, but it turns out the info was in motionlab documentation; although the instructions for previous versions no longer seem to apply to 2.7.2).\n", "tags": "control can", "id": "2694", "title": "set canopen node id of ingenia pluto dc servo drive"}, {"body": "i'm looking to make or buy something resembling an led, that would be thin (about 0.5mm or less) and cheap (&lt;0.1$ in mass production).\n\nany suggestions?\n", "tags": "arduino electronics", "id": "2695", "title": "what are the options for a thin light source (e.g. led)?"}, {"body": "i'm designing a simple autopilot software on top of ardupilot, my goal is to possibly interface an raspi on top of ardupilot mega (apm). i am stuck on setting up a simulation environment using either v-rep or gazebo. \nthe quadcopter will have basic sensors plus advanced sensors. basic sensors talks directly with ardupilot, while advanced sensors talks with my own autopilot software. i am trying to wrap my head around a feasible setup to test the software while using ardupilot mega in the hardware-in-the-loop. i am planning on having three stages of simulation:\n\nstage 1. simulate quadcopter physics in gazebo/v-rep, run ardupilot software and my autopilot software in a vm (not sure if it's even do-able)\n\nstage 2. simulate quadcopter physics in computer, run my autopilot software in a vm, and run apm in a hardware-in-the-loop fashion.\n\nstage 3. deploy my autopilot onto raspi and interface with apm then run both hardwares in hardware-in-the-loop fashion.\n", "tags": "quadcopter simulator", "id": "2696", "title": "an architecture for testing autonomous flight and sensors"}, {"body": "i implemented a bootstrap particle filter on c++ by reading few papers and i first implemented a 1d mouse tracker which performed really well. i used normal gaussian for weighting in this exam. \n\ni extended the algorithm to track face using 2 features of local motion and hsv 32 bin histogram. in this example my weighing function becomes the probability of motion x probability of histogram. (is this correct).\n\nincase if that is correct than i am confused on the resampling function. at the moment my resampling function is as follows:\n\nfor each particle n = 50;\n\n\ncompute cdf\ngenerate a random number (via gaussian) x\nupdate the particle at index x\nrepeat for all n particles.\n\n\nthis is my re-sampling function at the moment. note: the second step i am using a random number via gaussian distribution for get the index while my weighting function is probability of motion and histogram. \n\nmy question is: should i generate random number using the probability of motion and histogram or just the random number via gaussian is ok.\n", "tags": "mobile-robot localization particle-filter tracks", "id": "2698", "title": "random number generation for particle filter"}, {"body": "i'm working on a project where mains voltage will sometimes be disconnected, and the system will have to run on battery for as long as possible before doing a (safe) shutdown.  the desired behavior is exactly like a laptop battery system:\n\n\nwhen mains voltage is connected, charge the battery and power the system from mains\nwhen mains voltage is disconnected, power the system from the battery\nprevent the battery system from supplying current when the batteries are discharged below a certain voltage (to prevent damage).\n\n\nis there a name for this type of system, or a name for the feature(s) that i should be looking for when i look at chargers?  (if it matters, this system will be 12v, so i'm looking at 14.8v lithium battery options.)\n", "tags": "power battery", "id": "2700", "title": "battery system with and without mains voltage attached"}, {"body": "i want to implement my own gps navigation for a quad-copter.\ni can calculate and filter the gps coordinates (latitude and longitude in degrees). \n\ni believe the easiest approach for me would be, to calculate the change of the heading of the quad-copter from the current attitude to the destination point and let it fly straight on after turning. \n\nhowever i am not sure about the 2d representation of the latitude/longitude-gps coordinates (for a round earth to a 2d map system when calculating the heading change). how big is the expected error? or is there none?\n", "tags": "navigation gps", "id": "2701", "title": "2d map representation of gps coordinates in degrees"}, {"body": "a robotic usually consists of joints with sections of possibly varying width connected together. considering we know how much each is bent and the length of each section, and their location in 3d space (not local coordinates) at time zero; how do we determine how much each joint should rotate to goto position b from position a. both a and b are defined in world cartesian coordinates.\n\nnow each joint can move in terms or all at once, so should all joints move simultaneously or in turns?\n", "tags": "kinematics robotic-arm", "id": "2702", "title": "how does one calculate the angular motion of each node in a robotic arm"}, {"body": "i am trying to better understand the dynamics of forward flight in multirotors.\n\nassuming i have a quadcopter with 4 motor/propeller combinations capable (each) of a propeller pitch speed of, say, speedmax= 100 mph.\n\nin forward horizontal flight, the quadcopter will pitch down at a certain angle, let's say alphap, from horizontal. if alphap is, say, 45 degrees, and drag is neglected, wouldn't the quadcopter be capable of a max theoretical speed of sin (45)* speedmax ~ 70mph?\n\nalso, seems to me alphap cannot go all the way to 90 degree (quadcopter flying like a plane), as at that point the propellers would not produce any  upward thrust to maintain the copter aloft given there is no wing loading as available in a plane.  if drag was to be neglected, what factors would the optimum alphap be depended on, and what would that angle be, for maximum speed?\n", "tags": "quadcopter", "id": "2704", "title": "quadcopter forward speed"}, {"body": "i want to know how to write and run the correct code. i understand that i need to download opencv (which i have), but when i try to compile sample code - for example, blob detection - it doesn't compile. i am just very confused on the process of what you need to do to get something to show up on the screen. \ni know my question is really vague, but i have such a bad understanding of computer vision that i don't really know how to describe my problem. hopefully discussing more will be able to help me. \n\nplease help me! i have been searching the internet for 2 hours now and i am just lost in a sea of information...\n", "tags": "computer-vision", "id": "2706", "title": "how do i get started in computer vision?"}, {"body": "i have a system in which i have two separate subsystems for estimating robot positions.\nfirst subsystem is composed of 3 cameras which are used for detecting markers the robot is carrying and which outputs 3 estimates of the robot's position and orientation.\nthe second subsystem is a system which is located on the robot and is measuring speed on the two points of the robot. by numerically integrating those two i can get an estimate on the robot's position and orientation (because i am tracking two points at once).\n\nthe first system is less accurate but the second system drifts. first system gives output about once a second while the second one gives output much more frequently (100-200 times per second).\n\ni assume there must be a better approach than to just reset the position with the first system's estimate (as it is not 100% accurate), but to also use the accumulated position from the second sensor system and fuse that with the new data from the first system. also, there is a question how to fuse 3 estimates of the first system? there must be a better way than pure average as it might happen that the two estimates are exactly the same and the third one is completely different (meaning that it is probably more wrong)?\n\ndo you have any fusion algorithms to recommend to use in such a system? i know about kalman filter, but i am having trouble figuring out how to use it as the two systems output data at different frequencies.\n\ni hope the question is clear enough, what is the best approach to fuse the estimates into a more correct and accurate estimate?\n\nthanks\n", "tags": "sensors localization kalman-filter sensor-fusion", "id": "2708", "title": "multiple position estimates fusion"}, {"body": "i have a raspberry pi (model b) attached to a roomba. i'm using this part to bring the unregulated 17v+ power of the roomba down the 5v/1a of the pi. the problem is that the pi will randomly reboot and cause peripherals (such as the bluetooth adapter) to freak out and not work. we can sometimes drive it around for a little while before it reboots, other times it happens almost immediately.\n", "tags": "mobile-robot raspberry-pi", "id": "2712", "title": "power issues involving raspberry pi"}, {"body": "i am trying to build an autonomous multi coloured lines following robot.\n\nthe parts i have bought so far include:\n\n\narduino uno 3.\n3 colour sensors (taos tcs 230).\nl293dne motor driver.\nrobot chassis including 2 dc motors, 2 wheels and 1 caster wheel.\n\n\ni am trying to figure on how to connect all these components together (for example: arduino to the colour sensor, l293d to motor).  how do i connect it in order for the motor to rotate in both directions?  do i need to solder anything?\n", "tags": "mobile-robot", "id": "2713", "title": "how to connect arduino uno3, l293d motor driver and 3 colour sensors together?"}, {"body": "i'm considering building an absolute, indoor robot-positioning system based on ultrasound time of flight. transducers will be ordinary, narrow-band, 40 khz ones.\n\nbased on your experience, what is the best exactitude and precision we can achieve with such a system?\n\ni'm aware that the answer to such a question will depend on many factors, both hardware and software (if applicable), but i'm not asking about the performance of one solution or another, but about the intrinsic limitations of the ultrasound technology.\n", "tags": "localization ultrasonic-sensors", "id": "2714", "title": "precision we can expect of an ultrasound-based localisation system"}, {"body": "more in the line of robotics observing their environment, i'm trying to implement a proximity sensor that can sense objects in front of it to a least up to $-30^ \\circ \\space$ to   $\\space+30^ \\circ $ of it's direction of propagation.\n\nthere are only two ways i can think of\n\n\nmultiple infrareds.     con: more spacious\nfast-motor.  con: expensive in money and time-complexity wise\n\n\ni'm currently using a proximity sensor with up to 10ft distance capability\n", "tags": "sensors", "id": "2715", "title": "how to implement a distance proximity sensor with wider range"}, {"body": "i am building a 6 wheeled rover, one set of 3 wheels will work together and other set of 3 wheels will run together, so current in each side may vary from 3a-15a(blocked rotor) and 6v i want to make a h-bridge(for controlling direction and speed(i ll use pwm) so i will require two such h-bridges. what is the copper track thickness i should use in proteus for making the design or else shall i go for manual soldering replacing tracks with wires. can anybody suggest a design which is relatively easy to design with some protection circuit in it (for mcu pins isolation) or suggest any suitable motor controllers from ti or any company which can be apt to my problem\n", "tags": "motor control", "id": "2719", "title": "h bridge for rover"}, {"body": "i've been looking for articles and topics the fictitious play learning algorithms for my presentation\ni haven't found it's pros and cons\nis there a book or something that i can benefit from?\nthanks\n", "tags": "algorithm", "id": "2720", "title": "what are the pros and cons of fictitious play"}, {"body": "i'm thinking about building a scara arm to lift moderate loads(5lbs) with a high degree of accuracy. i want a relatively quick and inexpensive z axis gantry, and i was thinking about using a lead screw with dual linear rail. trouble is i'm not certain the linear velocity will be fast enough.\n\nwhat's the best method of choosing the lead screws and the associated nut, given a desired linear velocity of 10inches/second and a nema stepper motor driving it?\n", "tags": "robotic-arm mechanism", "id": "2723", "title": "scara arm lead screw choices"}, {"body": "i'm trying to get a 6dof pose solution for an object that'll be between 10 and 50 cm from a fixed point. i want to avoid putting too much special hardware on the object, but extra hardware on the fixed side is fine.\n\ni've been looking into two general methods:\n\n\nfiducial markers there are several software packages with different types of markers, but i haven't been able to find any information about them regarding precision or accuracy in short-range pose sensing.\nultrasonics i've found some commercial systems that do 6dof pose sensing (e.g. hexamite), but they're expensive and require you to put transmitters on the object.\n\n", "tags": "sensors ultrasonic-sensors pose", "id": "2724", "title": "what's a good pose estimation method for high precision (can301/402 provides max motor speed (0x6080,0x00) and max profile velocity (0x607f,0x00). in profiled motions, the maximum speed is limited to the lower of these two values. in non-profiled motions, the maximum speed is limited to max motor speed.\n\nwhat is the intended purpose of max profile velocity, rather than only providing max motor speed and using that everywhere instead?\n", "tags": "motor control can", "id": "2726", "title": "what's the difference between can's motor max velocity vs. profile max velocity?"}, {"body": "let's take a 6 dof robotic structure. it's consisting of the 3 dof global structure for the position - and the 3 dof local structure for the orientation of the endeffector.\n\nif the last 3 axis (of the local structure) are coincident in one point, the inverse kinematics can be solved analytically by decomposing it into a position- and orientation-problem. \n\nbut is it possible to solve the inverse kinematics analytically if the last 3 axis are not coincident in one point? i've read several papers that claim that due to high non-linearity of the trigonometric functions and motion complexity in 3d-space, a 6 dof serial chain cannot be solved analytically. \n\ndoes anybody know if this is right?\n", "tags": "inverse-kinematics", "id": "2729", "title": "is there an analytical solution for inverse kinematics of a 6 dof serial chain?"}, {"body": "we want to find available wifi networks near.\nso in tutorial there is command scan, which is send from coolterm program from pc.\nhttp://www.dfrobot.com/wiki/index.php/wifi_bee_v1.0_(sku:tel0067)\nnow we want to write program to arduino which will do same operation, how it can be done?\n", "tags": "arduino wifi", "id": "2730", "title": "how to use scan command in arduino wifibee"}, {"body": "probabilistic localization approaches like kalman or monte carlo benefit from an accurate prediction step. the more accurate the prediction step, the more accurate is the belief of the robots pose. in most approaches probabilistic motion models are applied, mainly because robot dynamics are more difficult to model. still some approaches rely on dynamic models in order to increase the accuracy.\n\ntherefore, i was wondering if it\u2019s reasonable to utilize a robotic simulator like v-rep or gazebo for the prediction step. the advantages i see in doing so are the following:\n\n\nthe robots kinematic is solved by default, simply through modeling it in the robotic simulator\nthe robots dynamics are taken into account\nnonlinear behaviors like slippage or collision can be modelled up to a certain extend\nthe robots workspace is taken into account, by modeling its environment (if the robot drives against a wall previous models would predict it behind the wall, which won\u2019t happen in a robotic simulator)\n\n\nwith the shown advantages i hope to achieve a more accurate prediction.\n\nhowever there might be some problems using a robotic simulator. for a start it has to ensure real time behavior and there will be delay in the prediction due to the communication with the simulator.\n\ni was looking for some papers which pick up on that idea but couldn\u2019t find any. are there any approaches similar to my idea? if not, are there any reasons why nobody is using a robotic simulator for the prediction? what are your opinions about my proposal?\n", "tags": "mobile-robot localization motion simulator", "id": "2731", "title": "using robotic simulator for prediction step in probabilistic localization approaches"}, {"body": "i'm aware of the primesense camera powering the kinect. are more advanced sensor types available now in the &lt; $500 range? for example, has there been any sort of game-changer in structured light techniques? do decent flash lidar cameras exist now?\n", "tags": "sensors kinect cameras lidar", "id": "2744", "title": "what different sensing approaches are used in the current batch of indoor 3d cameras?"}, {"body": "i have created robot using a robot chassis kit from hobbyking. at first when testing the robot connected to usb power source and the wheels lifted above the ground everything seemed to be ok. then, when i tried to power the robot with batteries i encountered a problem with starting the movement.\n\nthe robot hardly starts to move even when i power it with 100% of power - sometimes i have to push it a little bit in order to start driving.\n\nas a newbie i don't know whether it is a power source (battery) or motors problem.\n\n\nthere are 4 motors with torque of 800gf.cm min in the chassis. \nthe gear ratio is 48:1 and to power the motors \ni used two serially connected li-ion batteries and dc-dc regulator which limits the voltage output to 5v. \nthe power is regulated with dual h-bridge motor driver. \n\n\naccording to specifications, the maximum free running current for a single motor is 250ma and i have read that the stall current is 3-8x running current.\n\nanyway, the problem is that the robot has problems with starting-up driving and i don't know how whether the motors are even powerful enough to move the robot or it is a power source problem or perhaps the obstacle could be solved with appropriate power regulation (ramp). \n\nhow can i solve this problem?\n\n\n", "tags": "motor battery movement", "id": "2747", "title": "robot start-up movement problems"}, {"body": "what is the maximum distance (of say , a car ) you could measure using an ultrasonic sensor that would be compatible with arduino? is there any sensor(ultrasonic or not) that could measure the distance of a car , say upto 50 meters that can be used with arduino?\n", "tags": "arduino ultrasonic-sensors", "id": "2748", "title": "maximum distance using ultrasonic sensor arduino"}, {"body": "i'm looking for a part that will do a particular function.  it has to be able to move along one axis and tell me the force that is being exerted on it.\n\nkind of how like a piston moves inside an engine (one axis of movement) except that something will be pushing at the top of the piston and i need to know how hard its pushing.  another difference is that the piston won't be constantly moving back in forth, but needs to be able to receive commands like.   and then remain stationary at its new position.\n\ni know to make this it would involved a sensor and something that can exert force but what is the name of the described machine?\n\nedit #1 - response to matthew gordon\n\nthe piston would have to move between 0-6 centimeters.  the form factor would be small, ideally smaller than the palm of your hand.  (smaller=better) the forces it would have to deal with are comparable to the forces exerted on a bicycle by its chain.  i'm a math/cs person not engineering so i don't know technical terms for these kinds of things off the top of my head.  it would have to be real time sensor reading, but the volume of data could be processed by a phone.  would have to working in conjunction with wireless communication, probably bluetooth, but i'd have to look into the latency requirements to be sure.\n", "tags": "sensors mechanism force-sensor", "id": "2754", "title": "what is the name of the part i'm describing"}, {"body": "when one wants to model a kinematic chain and in particular define the frames attached to each body, it is common to use the denavit-hartenberg parameters.\n\nwhat are the advantages of this representation?\n\ni can understand the interest of having a normalized representation but does it impact the algorithms performance?\nthe algorithm is not trivial to implement, what gain can we expect from this instead of, for instance, just fixing reference frames by hands (i.e. arbitrarily) like this is done in many robotics formats such as urdf.\n", "tags": "kinematics", "id": "2758", "title": "what are the advantages of using the denavit-hartenberg representation?"}, {"body": "i'm trying to calculate the inverse kinematic for an 6 dof manipulator.\n\ntask:\n\na target point $p_{target} = (x,y,z)^t$ and the orientation $o_{target} = (a, b, c)^t$ are given and i want to get the angle configuration $q = (q_1, q_2, q_3, q_4, q_5, q_6)^t$ for my robot.\n\nmethod:\n\nfor that i try to implement the jacobian method (with the transposed jacobian matrix) with this guide and followed the pseudocode at slide 26. but instead using the pseudoinverse of the jacobian matrix i used the transposed one.\n\ni'll try to compute the jacobian matrix numerically and analytically, but didn't get a solution (endless loop) for any of them. here's how i retrieve the jacobian:\n\n\nnumerically: \n\n\nanalytically:\n\n\n\n\nhere is the implementation of the pseudocode:\n\n\n\nwhere $beta = 0.5$ and $epsilon = 0.0001$\n\nproblems:\n\nthe transformation matrice should be fine, because it behaves good for the forward kinematic.\nin my opinion the jacobian matrix must be somehow wrong. i'm not sure if it is correct how i put the orientation data in the numerical calculation. for the analytical computation i didn't have any clue what could be wrong.\ni would be thankful for any advice. an explicit example for calculating the jacobian would also be very helpful. \n", "tags": "inverse-kinematics manipulator", "id": "2760", "title": "computing inverse kinematic with jacobian matrices for 6 dof manipulator"}, {"body": "i am trying to test a sensor circuit i'm working on. essentially, i am using realterm to send commands to the microcontroller and it is returning the value read by the sensor. \n\nwhen logging to a file in realterm, i noticed the commands being sent were showing up as well as the data being returned. i was wondering if anyone knew a way to record only the incoming data using realterm, and not the outgoing commands. any suggestions would be greatly appreciated. unfortunately, there is no way around using realterm specifically because of a company policy. \n", "tags": "serial", "id": "2761", "title": "is it possible to record only incoming data with realterm?"}, {"body": "i'm attempting to build a segway robot using a gyrosensor and accelerometer.\n\ni'm having trouble getting the robot to remain standing, for some reason, and i can't identify the problem.\nhere's what i know:\n\nthe gyroscope api for the lejos nxt platform is here:\n\nhttp://www.lejos.org/nxt/nxj/api/\n\nby using timestamps and angular velocity, the project attempts to infer the angle of the robot.  the api suggests that in order to be accurate, it must be polled 100 times per second (or every 10ms on average).\n\nthe problem is that simply polling the gyrosensor takes 4ms.\npolling the accelerometer takes 10ms.\n\nthe dimensions of the robot:\nheight: 28cm\nwheel circumference : 13.25cm\nradius of a wheel, given the circumference:  2.1cm\n\nthe accelerometer is mounted on the top of the robot (at approximately 28cm from the ground, 26cm from axis of rotation)\n\nin order to keep the correction amount linear (as opposed to trying to correct an arbitrary angle) , i translate the angle of the robot to a distance to travel along the ground to \"right\" the robot.  this might be a bit naive, and i'm open to suggestion here.  basically it's just the horizontal distance calculated using a right-angle triangle with the angle of the robot at the top and hypotenuse of 28cm.\n\nif that's not clear, it's essentially the horizontal distance from the top of the robot and the bottom of the robot.\n\nright now my main concern is the amount of drift the gyroscope seems to be experiencing.  given the fact that with the nxt java software package, it's nearly impossible to poll 100 times per second, the amount of error accumulated by the gyroscope is fairly large.\n\nfinally, i've implemented a pid control system.  the thing i'm not clear about with respect to this system is the integral and derivative of error must be calculated given a set of values.  say, the last 20 error measurements recorded.\n\nif the amount of past errors recorded is a variable, and the pid constants are variable, and the speed of the wheels is a variable, it seems this problem begs for some kind of automated optimization.  but how to do it?  if i set the speed to 120 rpm (roughly the max of the nxt servos) and take the past 20 errors for calculating the integral and derivative of the error, will it be possible to optimize the pid constants successfully?  or must all 5 variables be tuned together?\n\nthanks ahead for any insight on the problem.\n", "tags": "accelerometer gyroscope nxt", "id": "2767", "title": "nxt segway problem. need advice/help"}, {"body": "i have a differential drive robot that works fine (good pd parameters) driving at say 1 m/s. now, if it speeds up (to 1.2 m/s) it starts wobbling again. what would be a good strategy for a controller that is able to cope with the whole speed range of 0 - 4 m/s?\n\nedit 14th of april:\n\nthe robot is a line follow robot but i do not see how this would be related to my question since a robot following a trajectory would have the same problem. \n\ni recently talked to other developers of differential drive robots and they are facing similar issues e.g. they told me that they need to adjust pid parameters once the battery is not fully charged hence the robot drives at a different speed.\n\ni do not know if you guys are into youtube, but if your are really interested in my robot this link would be helpful: https://www.youtube.com/watch?v=vmednphxleo\n\npid parameters are: p 0.31, d 0.59, i 0.00\n\npid controller programmed using c:\n\n\n", "tags": "pid differential-drive", "id": "2768", "title": "differential drive pid controller"}, {"body": "i have a four wheel dc rover with two optical wheel encoders.  i'm executing rover turns by controlling the direction of wheel motion on either side of the rover.  to turn left, all wheels on the left rotate backwards while all right wheels rotate forward.  this allows to rover to remain relatively in the same position while turning.  the directions are reversed to do a right turn.  \n\nhow can i use these two sensors to execute as close to a 90 degree turn as possible without fusing additional sensor data?\n", "tags": "sensors motion", "id": "2769", "title": "how to turn a rover 90 degrees using wheel encoders?"}, {"body": "how can i track a fixed point $p=(x_p, y_p)$  from a moving robot?\n\ncoordinates of $p$ are relative to the state/pose of the robot (x axis looks forward the robot and y axis is positive on the right of the robot).\nsuppose that the initial robot state/pose is at $s_{r}=(x_r, y_r, \\theta_r)$.\nthe next frame (namely after $\\delta t$) with the applied control $(v, \\omega)$ the robot is at state $s_{r'}=(x_{r'}, y_{r'}, \\theta_{r'})$.\n\nwhere (i set the axes as opencv):\n\n$x_{r'} = x_r + v cos(\\theta_r) \\delta t $ \n\n$y_{r'} = y_r + v sin(\\theta_r) \\delta t $ \n\n$\\theta_{r'} = \\theta_{r} + \\omega\\delta t$\n\nthe question is: which are the coordinates $(x_p', y_p')$ of the same point $p$ relative to $s_{r'}$?\n\n\n\nas visible in the picture, i know the transformation from the initial state to the next state of the robot and the coordinate of p in reference to the initial state\n\n$$\nt = \\begin{pmatrix}\ncos(\\theta_{r'}) &amp; -sin(\\theta_{r'}) &amp; x_{r'}\\\\\nsin(\\theta_{r'}) &amp; cos(\\theta_{r'}) &amp; y_{r'}\\\\\n0 &amp; 0 &amp; 1\\\\\n\\end{pmatrix}\n$$\n\nplease correct me if i made some mistakes!\n\nthank you, any help is appreciated.\n", "tags": "mobile-robot kalman-filter tracks", "id": "2771", "title": "point tracking from a mobile robot"}, {"body": "i've been writing some quad copter software and i am not sure what the best way is to map the throttle and pid inputs to esc power.\n\nmy throttle range is 0-1 and my pid outputs are 0-1. my esc's have a range of 1060us to 1860us.\n\ni have mapped the motor speeds like this:\n\n\n\nthis works but if my quad is perfectly level (i.e. the pid outputs are 0) and i apply full throttle (1.0) then map this to esc power i will only get quarter power (1260us).\n\nhow should i be doing this so that if my throttle is on max then i get max power? if my throttle is half (0.5) then i should get half power plus the pid values etc.\n\ncan anyone help me with this?\n\nthanks\njoe\n", "tags": "motor quadcopter pid pwm esc", "id": "2772", "title": "quadcopter throttle and pid mixing to motor speed"}, {"body": "can someone please post the datasheet for the colour sensor mentioned above. all i can find is for tcs3200\n", "tags": "sensors", "id": "2774", "title": "datasheet for taos tcs3200 gy-31"}, {"body": "i am preparing for my first quadcopter build and need to know how to tell what motors/esc's/propellers will work with each other. i also would like to know how to tell what the motors would be capable of carrying/how much thrust they have. i would like to put a camera on this copter. i cannot find anywhere a straight answer to this question.\n\nthe ones i currently think are the ones i want are:\n\nesc:\nhttps://www.hobbyking.com/hobbyking/store/__25365__turnigy_multistar_30_amp_multi_rotor_brushless_esc_2_4s.html\n\nmotor: https://www.hobbyking.com/hobbyking/store/__28112__turnigy_d3530_14_1100kv_brushless_outrunner_motor_us_warehouse_.html\n\npropeller: 11inch\n\nthis copter needs to be able to carry a camera (~go pro)\n\ntldr: how does one match esc's/motors/propellers, and how to tell if they can get the job done?\n\n(esc - electronic speed control)\n", "tags": "quadcopter brushless-motor esc multi-rotor", "id": "2781", "title": "how to know what motor/esc/propeller combination will work for a quadcopter?"}, {"body": "so, i'm making my second ever hobby robot(i'm 15) and am planning on soldering my own connectors for the battery, sensors, arduino, etc. it will be a small mobile robot. anyways, i was wondering what a good gauge of stranded hookup wire would be good for that purpose. thanks!!\n", "tags": "mobile-robot", "id": "2782", "title": "wire gauge for hobby robot question"}, {"body": "for a class project, i'm working with a weight stack:\n\n\n\ni'm trying to simultaneously measure:\n\n\nthe position of a moving weight stack\nthe value of the weight based on a calibrated/preloaded position in the stack, not via load sensor. (e.g. think a stack of plate weights where the sensor knows in advance that 1 plate = 10lbs, 2 plates = 20lbs, etc.)\n\n\nthe weight stack and the base camp chip/sensor/laser would be within two feet of the weight stack, so i don't need anything overly strong. my requirement is that it is small/unobtrusive and cost effective. i've looked into a few options, but i'm not an engineer so i'm not sure if i am on the right track.\n\nhow would you do this? is there any research that i could check out?\n", "tags": "sensors", "id": "2787", "title": "position and object data tracking"}, {"body": "so, i'm planning out my second hobby robot(i'm 15). i am planning on using a 7.4v lipo battery and my idea is to solder on a 3-pin header to connect to the electronics. anyways, should i solder to crimp terminals and then attach it to the pack? or should i solder directly to the battery leads keep in mind i have a decent background in hobby electronics and am just starting with robotics. my soldering skills are also decent!\n\nthanks!!\n", "tags": "battery", "id": "2794", "title": "crimp or solder to lipo battery?"}, {"body": "i'm trying to find a good beginners platform to use ros with, and i came across the irobot create. to my surprise, they do not sell in europe. why is that?\n", "tags": "mobile-robot ros irobot-create", "id": "2798", "title": "why does irobot not sell the create in europe?"}, {"body": "i'm developing/tuning a software pid for a quadcopter. for now i'm only trying to stabilise the pitch angle using the front and back motors, and i'm only looking at kp. the motors have a control resolution: input variations need to reach a threeshold to have any effect at all.\n\nthe process output does reach the setpoint, but not precisely enough for my requirements. there is no steady-state error (aka droop), the hunting range is centered on the setpoint, just too wide for my requirements. also the instability is not an oscillation, but more of a random drift which needs to be large enough before the pid attempts to correct it.\n\n\nwith a lower kp the output needs to diverge from the setpoint significantly before the error is big enough for the pid to attempt to correct it.\nwith a higher kp the pid oscillates.\n\n\ni could not find a reasonable compromise.\n\ni'm thinking about applying the cuberoot function (or similar) to the error before feeding it to the pid: that way small errors should be significant enough for the pid to attempt to correct them, and large errors would be reduced and might not trigger oscillations. i suppose someone must have been through this before: is this a good solution? are there any better alternatives?\n\nthis is not a steady-state error (aka droop) or oscillation issue as far as i can tell: please don't suggest using ki or kd\n\nedit: i have clarified the problem description and suggested using cuberoot rather than logarithm which was a bad idea indeed.\n\nthanks,\nmarc.\n", "tags": "pid", "id": "2800", "title": "pid output does not reach setpoint precisely enough"}, {"body": "most of the blogs/website say we need minimum of four channels  for a quadcopter (pitch, roll, throttle, yaw): \n\n\none channel for throttle\nsecond channel for turning right and left.\nthird channel for pitching forward and backward.\nfourth one for rolling left and right.\n\n\n\n\nbut looking at the rc transmitter , i see that at a time you can change a maximum of two sets  data ( left and right joy stick) .  even if you want to send rudder and throttle information at the same , can it not be sent in the same packet ?\n\nmy understanding is  two channels should be sufficient  to control quad copter. please provide more clarity on this.\n", "tags": "quadcopter radio-control", "id": "2801", "title": "what is the minimum number of rc channels required to control quad copter?"}, {"body": "i am wondering what technology should i use to transmit data (enough for controlling the robot and receiving video) over dozens of kilometers and mountains ?\n", "tags": "radio-control", "id": "2802", "title": "radio-control over dozens of kilometers and mountains"}, {"body": "for a robot, say path planning in particular, what are the pros and cons of choosing classical control theory or optimal control (lqr for example) ?\n", "tags": "control motion-planning", "id": "2807", "title": "robot control law - control theory vs optimal control"}, {"body": "i asked a similar kind of question some time ago (neuromorphic engineering and robotics)\n\nsince then, many things have come to the point of revelation. a road-map for neuromorphic computing was revealed recently; it proposes the analog way of computation, to solve advanced computer vision problems. ibm and qualcomm are also working on the similar project though on the digital side. memristor technology is slated to come very soon.\n\nthe question i am asking here is how is the robotics community working to adopt the technology? this question opens the domain for other pressing questions which have been answered cryptically since the 1980s.\n\nare neuromorphic computers good for mission critical precise robots, like that on mars? can we use neuromorphic systems on avionics systems? how is neuromorphic processing going to help us solve the problems on nlp, and knowledge processing? aren't quantum computers very similar to neuromorphic computers in ideology? if neuromorphic robots gain traction, will digital hardware still be required?\n\nit would be really nice if someone could explain all points, because answers in various but sparsely related research papers are very cryptic.\n", "tags": "microcontroller computer-vision machine-learning research", "id": "2812", "title": "role of neuromorphic computing and quantum computing in the field of robotics and ai"}, {"body": "i want to develop a toy project that will allow me to move object around the house. because i am interested in the programming of the robot and not actually build it, i would like to use some sort of programmable \"starter kits\" (like lego mindstorm) to get started. while i do not have everything figured out yet, here is a list of specs that i expect my ideal kit should have: \n\n\nthe ability to lift object (object as big as 4'' or 10 centimeters)\nthe ability to distinguish objects by their colors. it should have some stort of color sensors.\nobviously it should be able to move on a smooth surface.\nobstacle detection. should have sensors for obstacle detection\nextra: maybe remotely controllable.\ncan someone please suggests the cheapest kit i should use for this? \nthanks\n\n", "tags": "mobile-robot kit", "id": "2822", "title": "robot kit suggestions"}, {"body": "i am learning about kalman filters, and implementing the examples from the paper kalman filter applications - cornell university.\n\ni have implemented example 2, which models a simple water tank, filling at a constant rate. we only measure the tank level, and the kalman filter is supposed to infer the fill rate.\n\n\n\naccording to the model, the fill rate is a constant, so i assumed that over time, the kalman filter would converge more and more accurately (and with less and less noise) on the correct fill rate. however, the amount of noise in the fill rate never seems to reduce after the first few iterations:\n\n\n\nthis graph shows how the fill rate part of the state vector changes over the course of 1000 iterations of the simulation.\n\nadjusting the measurement variance matrix seems to have very little effect on the fill rate noise.\n\nalso, the kalman gain vector and state variance matrix seem to be constant throughout the simulation. i assumed that the state variance would reduce as the filter became more and more confident in its state estimate.\n\nquestions:\n- is this graph what i should expect to see?\n- should the kalman gain vector and state variance matrix change over time in this situation?\n", "tags": "kalman-filter", "id": "2826", "title": "how much should i expect a kalman filter to converge?"}, {"body": "i am using the clickbutton library from arduous and am having some problems implementing it. as it stand now the code just runs the servo clockwise and i'm not sure what i did wrong. basically i want the servo if pressed for a short period of time to move according to an exponential function, and if pressed according to a long period of time to move at a regular pace.\n\n\n", "tags": "arduino", "id": "2828", "title": "click button short vs long button presses arduino"}, {"body": "so i am building a differential drive robot and i want it to autonomously drive in a straight line on an uneven surface. i know i need a position and velocity pid. as of now, i am deciding on which sensors to buy. should i use optical encoders, accelerometers, or something else?\n\ni wanted to go with accelerometers due to the error encoders would face due to slippage, but i am not sure.\n\nsome enlightenment would help!\n", "tags": "sensors control pid differential-drive", "id": "2829", "title": "differential drive robot on uneven surfaces"}, {"body": "i'm facing a problem while building my quadruped robot which is figuring out the efficient power supply needed for the 12 servos. i'm using 12 mg995 tower pro servos powered by 2 lithium batteries 2x3.7v (about 8 volts) with 2200 ma . i really don't know if that enough for the servos or something else is needed to be added(i hardly fitted the 2 batteries into the robot's body) \nany suggestions please?\n", "tags": "power battery servomotor walking-robot", "id": "2832", "title": "servos power supply in quadruped robot"}, {"body": "recently i began to build a car-like robot and i stumbled upon dead reckoning. i use one motor for steering and one for traction. i want to be able to get the position of the robot. from what i have read 2 encoders should be used. but i am curious if you can use only one encoder on the motor shaft to get distance  and a gyro + accelerometer to get the orientation of the robot.\n", "tags": "mobile-robot motor gyroscope encoding", "id": "2834", "title": "dead reckoning on a car-like robot with a gyro and only one encoder"}, {"body": "every time i see a pid control for a motor, it involves an encoder, so the algorithm knows the real position of the motor or wheel.\nbut with the robot i have, i cannot use an encoder. i only have one optocoupler per wheel which can count how many degrees the wheel has moved. i can use it to increment a counter, but the counter always increment (if the wheel moves forward or if the wheel moves backward).\n\nthe first moment i saw it as an inconvenient was when i studied the arduino pid autotune library. in the first figure, i would not see decrements on the input. \n\nmy objective is to move a little two-wheels robot small segments driven by simple trajectories separated over time by a complete stop (straight 10 cm, stop, move right 90 degrees, stop, straight until detect obstacle...)\n\ncould you suggest me some kind of ideas?\nthe first idea i have is to transform the pid position control to a speed control (which is more convenient for the feedback loop i have) and keep a counter of the traveled distance to inform the speed control when to stop.\n", "tags": "arduino motor control pid", "id": "2837", "title": "what kind of motor control can i implement if i cannot use an encoder?"}, {"body": "first a bit of background, i am planning to make a highly maneuverable airship controlled by four thrust vectored propellers. i don't want to rely on a rudder and forward momentum for turns but instead be able to maneuver with direct prop thrust. i want to be able to point each prop anywhere within a half sphere/dome. so two axis, 360 degrees traversal for the forward/back/up/down and 180 degrees for the left/right. the nearest thing i can think of is a ball turret similar to this ball turret but instead of a gun, have a motor and propeller. the turret can rotate infinitely through 360 degrees, but the gun rotates through 90 degrees.\n\nmy first thought was for a servo for both axis, but they are limited in range and i would like the 360 axis to be able to rotate continuously. this would allow for the turret to rotate to the desired angle using the shortest path. \n\nmy question is, what do i need to be able to rotate the turret and still know what angles the turret is currently pointing?\n", "tags": "sensors servos", "id": "2841", "title": "what approaches should i consider to create rotating a turret"}, {"body": "i have not yet build this so this is basically a theoretical question. i am still wrestling with some c code to manage i2c communication etc. \n\nwhen i originally said \"i have not build this\" i meant that the robot is in what could be called a \"design phase\". for the sake of my question lets assume for a moment that the whole robot consists of just one imu sensor. it moves magically (no motors that create a lot of noise in the sensor measurements). with theoretical i mean i am interested in the mathematics and algorithms involved in solving this problem. what i call imu sensor provides raw accelerometer, gyro, and magnetometer measurements.\n\nlets say our tiny robot travels on a snooker table (3569 mm x 1778 mm). i believe this is sufficiently small to call it 2d. now, sensor fusion should be much easier (faster, consume less resources) than in 3d, right? \n\ni would like to measure the velocity and if possible the position. \nwith velocity i mean at a given point of time i need to know the current velocity of the robot moving over the snooker table. velocity is in the range of 0 - 5 m/s.\nwith position i mean at a given point of time i need to know the current position of the robot on the snooker table (x, y, heading).\n\ni hope this will be possible since the robot should be able to identify some landmarks and use this information to reduce position errors.\n\nwhen i originally said \"i hope this will be possible\" i meant to express that i already am aware of the fact that real sensor data is noisy and errors accumulate quickly. using landmarks i will / or will not be able to manage to reduce the error in the position estimates. but this is not part of my question.\n\ni am about to improve my linear algebra knowledge. so i am confident to manage some matrix multiplications, inversions and such.\n\nmy question is for some ideas, references on measuring velocity and position in 2d using an imu sensor like this one. \n\na little side question: i just figured that this question is probably too theoretical for robotics.se. if you know any forum that is more focused on mathematical / algorithmic side of robotics please let me know.\n", "tags": "localization imu sensor-fusion", "id": "2847", "title": "accelerometer, gyro, and magnetometer sensor fusion in 2d"}, {"body": "when i look at my 160a motor controller, it has a port that is called \"ctl.\"\nwhat does ctl stand for? is that a sort of protocol like rs232?\n", "tags": "motor", "id": "2848", "title": "ctl port in a motor controller"}, {"body": "i'm trying to do a quadcopter with some friends and we have a problem. it goes forward instead of hovering in place. we made a video to explain it, you can see it here.\n\nas you can see, the quadcopter flight and go forward when i don't touch the controller. i need to correct it to go backward and it goes forward again.\n\nwe use the kk2.1.5. \n\n\n  the hobbyking kk2.1.5 multi-rotor controller is a flight control board for multi-rotor aircraft (tricopters, quadcopters, hexcopters etc). its purpose is to stabilize the aircraft during flight. to do this it takes the signal from the 6050mpu gyro/acc (roll, pitch and yaw) then passes the signal to the atmega644pa ic. the atmega644pa ic unit then processes these signals according the users selected firmware and passes control signals to the installed electronic speed controllers (escs). these signals instruct the escs to make fine adjustments to the motors rotational speed which in turn stabilizes your multi-rotor craft.\n\n\nwe made some test. as you can see in the video, we placed the battery backward to be sure there is no weight against. when we check values in the debug mode, all values are at 0 when nothing is pressed.\n", "tags": "quadcopter uav multi-rotor", "id": "2849", "title": "our quadcopter goes forward instead of hovering in place. how to correct it?"}, {"body": "here is the disassembled stepper motor that i'm working with:\n\ncontains the photo of the motor, and the label that's on the bottom of the motor.\n\n\n\n\ni need to identify this stepper motor that was retrieved from scrap for a project. budget constraints force us to use the scrap motor. i tried to drive this using a l298 h bridge, but i couldn't find the right bit sequences to get this running smoothly. i also tried to search for a specifications sheet in the internet with the label, unsuccessfully.\n\ni'm using either an rpi or arduino board to run this.\n\ni just need a pin diagram and the specifications of the motor, if you guys have seen this type before.\n", "tags": "arduino raspberry-pi stepper-motor", "id": "2850", "title": "need specifications to operate this stepper motor with rpi or arduino"}, {"body": "i am planning to create a motor turret described in this question. but to simplify the problem, i'm thinking of a wind turbine with a generator in the main head that can rotate freely through 360 degrees to face the wind. how would i prevent the power and sensor wires coming from the head and down the shaft from twisting? \n", "tags": "sensors wiring", "id": "2852", "title": "how to get prevent twisting of cables"}, {"body": "how would one go about making a number of drones fly in a preset pattern or formation. for example have them rotating around a point.\n\nsomething like this.\nhttps://www.youtube.com/watch?v=shgl5rqk3ew\n", "tags": "quadcopter", "id": "2854", "title": "create set of drones to fly in patterns"}, {"body": "how long can a vex pneumatic piston be?\n", "tags": "robotic-arm arm", "id": "2860", "title": "how long can a vex pneumatic arm be?"}, {"body": "my current class assignment is to program a robot through a course that includes two moving obstacles \u2013 other robots moving at constant speed around a region the one robot must get to. since the other robots are moving at a constant pace alongside a predictable path, my robot can just stop at the border of the region, wait until the others pass by and then proceed. the robot can use a 2d laser range scanner to sense its surroundings.\n\ngiven these restrictions, what is the simplest object tracking algorithm i could use? i am thinking of something along these lines:\n\n\ncollect two laser readings (2d point clouds)  and  with a suitable time gap between them;\napply dbscan to  and , producing the cluster lists  and ; \ngenerate a list  of pair-wise matches of the clusters in  and , maybe using the hungarian algorithm;\ndiscard from  any pairings whose difference falls within a threshold;\ncalculate direction and magnitude of movements from the distance between the centers of mass of each cluster pair.\n\n\nthe reason for choosing dbscan and the hungarian algorithm is that i already have both implemented and in use elsewhere; and the difference between clusters could be measured as the distance between their centers of mass.\n\ndo you think this solution would work for my problem? do you have any suggestions on better and/or simpler ways to solve it?\n", "tags": "mobile-robot sensors algorithm rangefinder", "id": "2865", "title": "simplest way to do object tracking with a 2d range finder sensor?"}, {"body": "edited: i have a differential drive robot that needs to drive down a hall and stay in the center. i have 4 ultra sonic sensors, 2 on each side. currently, i thought of implementing pure pursuit and having a lookahead distance etc. but i am not sure if this is the correct approach. i have all the lower level pids for motor control working, i just need some help in choosing a path planning algorithm? will pure pursuit work to my needs? or do people have any suggestions.\n", "tags": "control pid differential-drive", "id": "2868", "title": "differential drive robot control"}, {"body": "i'm building a circuit to control the flow of flour. the basic idea is to open that actuator (possibly valve?) to let a specific amount go through and then close it. the tube should have a diameter of 1cm max.\n\ni wonder what is the right actuator to use? maybe a valve is the right one? any other solution? also, it would be great if you pointed out some suitable actuators that i can buy online.\n", "tags": "actuator", "id": "2870", "title": "find the right actuator to control the flow of flour"}, {"body": "i would like to build 2d ekf-slam in opengl. i've implemented the entire virtual environment in which there is a robot that moves in 2d and there are some landmarks(feature-based map). i have the motion and observation models. also, i've implemented the sensors with gaussian noise. now, i would like to use mrpt to build slam. at this point, i don't want to use data association that is the robot moves and detects its pose and landmarks and discard the previous data which means i only concern with the current state vector. my question is is it possible to build slam without data association? please suggest me some articles so that i can enrich my background about only this issue. \n", "tags": "slam ekf", "id": "2871", "title": "slam without data association?"}, {"body": "i'm building a robot with differential drive. i've reached the point when i can drive it around on remote control and i'm trying to get the localization working. now i would like to exactly measure parameters of the robot.\n\nmodel of the robot i'm using has two wheels spaced $b$ meters, each wheel has a distance per encoder tick $s_l$ and $s_r$ and variance standard deviation of the driven distance $\\sigma_l$ and $\\sigma_r$.\nwhen moving, distances are random variables from the following distributions: $d_l \\sim t_{l}s_{l}n(1, \\sigma_l^2)$ and $d_r \\sim t_{r}s_{r}n(1, \\sigma_r^2)$. later this model might expand a little bit.\n\nwhat is a good way to measure the parameters?\n\ni found a way to measure $b$, $d_l$ and $d_r$ (added that as an answer), but i have no idea how to measure the standard deviations.\n\nthe model will be used as a prediction input in mcl, so i don't need covariance matrix for localization.\n", "tags": "localization calibration differential-drive", "id": "2873", "title": "how to calibrate differential drive?"}, {"body": "i'm building the biggest robot i've ever done. the hardware i have so far is as follows:\n\n\nhcr-platform from dfrobot as a base\nx2 12v 146rpm dc motor with two phase hall encoder\nx7 sharp 2y0a21 ir sensors\nx6 urm37 ultrasonic sensor\nx4 ir ground sensor\nmicrosoft kinect\n\n\nright now i'm only using a romeo board (arduino 328 compatible) to drive the motors and process the pids for the wheels + steering and also access to all the sensors (except for the kinect). i have a beaglebone black running linux that is intended to be the main brain connected to the romeo using the rs232 port and do the processing of the kinect + wifi access.\n\ni started thinking about connecting the sensors to the beagle board directly so i don't need to waste time sending commands to the arduino board to read the sensors and that yielded the first issue, beagle board works on 3.3v instead of 5v used on the sensors. \n\nafter this i thought to create a board with voltage divisors to connect the sensors there and then using a ribbon cable to connect this new board to the beaglebone. i couldn't find any 2x23 idc male connector to create the \"interface cable\" between the two boards so the beaglebone option is out as i don't want to have tons of jumper cables all over the place.\n\nthis morning i thought about all this again and i researched about gpio boards over usb and found three from numato, only one works on ttl 5v and has 8 pins so i would need a few of them to use all the sensors so unless i design my own board this option is out too.\n\nat this point i'm quite confused in terms of what's the best hardware i could use to drive this beast. right now i think i should use a intel nuc with linux for the kinect, wifi and usb link to a custom made sensor board. this custom made board will work on ttl 5v, provide a power bus for the sensors and will interface all the \"low level\" sensors using usb as link. i also thought about an fpga for the custom board but i'm not sure if this would help me or if it's worth the effort of learning how to use it.\n\nwhat are your thoughts on this? any idea on how this issues are solved on \"complex\" robots?\n", "tags": "arduino mobile-robot sensors computer-vision beagle-bone", "id": "2875", "title": "connect sensors to beaglebone/arduino in a complex robot"}, {"body": "according not only to this link, but in my humble opinion to common sense as well, cable-based haptic devices have got lower inertia and are less complicated mechanically. i also believe that controlling them is not that big deal - the inverse kinematics should be quite straightforward. moreover, the play should be easy to compensate - if there occurs any at all, since the cables are tensioned. cables should also be easy - ? just a guess from me - to be equipped with strain gauges or to become strain gauges themselves, allowing to enhance control capabilities of a device.\n\nwhere am i wrong? why is that the links-based systems (e.g. phantom or falcon, although that latter has got cable transmission), especially with impedance control, are the only i seem to be able to buy? is it because of cable elongation (creep)? or too constrained workspace (esp. angles)?\n", "tags": "sensors mechanism", "id": "2876", "title": "why linkage-based haptic devices are much more common than cable (tension)-based ones?"}, {"body": "i want do make following installation (blowing bottle tops as music instrument):\n\n\n\nand i want to use and rc servo as electro valve (throttle) to control air flow for each bottle. is there any other way to do that?\n", "tags": "rcservo mechanism valve", "id": "2879", "title": "what to use as electrovalve?"}, {"body": "the rock framework already includes a lot of software libraries. however, i would like to add an existing external library so that i can use it for my component development. what is the preferred way of doing that?\n", "tags": "software rock", "id": "2882", "title": "how do i add an external library to the rock framework?"}, {"body": "what's the name of the \"big meccano\" used in the photo below to construct all the cabinets and racks?\n\nit appears to be an aluminium cut-to-length system of 4-way rails. i've seen it used many times and assume it has a well-known brand-name to those that know.\n\n\n\nphoto taken from theverge.com and was a feature about how audi are building a new car.\n", "tags": "frame", "id": "2885", "title": "can you identify the construction material/system used in the pic?"}, {"body": "i have a 7 dof manipulator (kuka lbr4+) and would like to calculate the joint torques needed to keep the arm in a static equilibrium. in most books the transposed jacobian is used to map the forces applying on the end effector to the joint torques.\n\n$\\tau = j^t\\cdot f$\n\nthat however doesn't take the mass of the links into account. is there a way to calculate the needed torques for a given configuration so that, assuming an ideal case, by setting these torques the arm will be in a static equilibrium?\n\ncheers\n\nedit:\n\nfor everybody interested, i found a solution to this problem in introduction to robotics - third edition by john j. craig on page 175-176. it is done with the aid of the iterative newton-euler dynamics algorithm. the actual trick is, to set all velocities and accelerations to zero except for the base acceleration. the base acceleration will be $^0 \\dot v_0 = g$, where g has the magnitude of the gravity vector but points in opposite direction. this is equivalent to saying that the base of the robot is accelerating upwards with 1 g and this upward acceleration causes exactly the same effect on the link as gravity would.\n", "tags": "torque manipulator", "id": "2889", "title": "static equilibrium for 7 dof manipulator"}, {"body": "a project has been given to me at work, with no schematics or idea of where it was going.  i need to fill 5 30ml bottles at a time with a food-grade liquid.\n\nbased on the parts i have, i think the design was going to use a air agitated pressure pot tank which is used for spraying paint, which would work if we weren't using food grade liquid, so right off the bat i cant use that.\n\nthe main parts that i can use are an allen bradley micrologix plc, 2 pneumatic cylinders, a couple solenoids, start and stop buttons. \n\nmy question is: to fill a 30ml bottle with this liquid, would a positive displacement pump with a vfd be the best way of slowing the pdp down enough to fill the 5 30ml bottle at a time?\n\ni do have a little experience with this particular plc so the ladder logic is not the issue, its the figuring out the specs for the pump and the motor.  any input would be very helpful also any links would be great. at this point im trying to determine if this is a huge waste of time and money or should i just go buy a filling machine for $3-5000.     \n", "tags": "electronics", "id": "2890", "title": "filling 30ml bottles with food-grade liquid"}, {"body": "so i need to output a varying voltage off an arduino mega in a range of 17 to 32 millivolts, which i've attempted to do by sending a pwm signal off the board into a low-pass filter which steps down the voltage.\n\nthis works, but the problem is that arduino's analogwrite function accepts a value of 0 - 255 to represent the duty cycle of the pwm which isn't precise enough. a value of 1 yields around 20 millivolts and a value of 2 yields around 40 millivolts. is there some way to have a duty cycle that is more precise than the 0 - 255 range like 0 - 1023 (i think even this isn't really precise enough)? or is there a better way to get precise voltage output?\n\nthe mega is running on and outputting a max voltage of 5 volts, and the low pass filter contains an 11 kiloohm resistor and a 1 microfarad capacitor.\n", "tags": "arduino pwm", "id": "2891", "title": "outputting a precise voltage in millivolts on arduino mega"}, {"body": "what are some good cheap, silent, motors for mannequin robots and what kind of controller should i use?\n\ni'm creating mannequin robots that require 24 motors: 2 neck, 4 shoulder, 2 elbow, 4 wrist, 2 waist, 4 hip, 2 knee and 4 ankle motors. the mannequins will be bolted via a horizontal post from the lower back to a wall. this means they can run, dance, and pose. i may also do something with the horizontal beam so the robots can do side flips, and moves like the twist, jumps and crouches. they will not stand up, but be bolted as i've described to a wall. they could even spin around completely to face the wall.\n\ni've tried using 6v hobby servos for the joins. these are too weak for lifting fibreglass mannequin body parts. they are too loud. they also make noise when poses are held, and they drop and smash when power is shut off. i have been using a handheld remote control for testing with only 5 channels. the robot must be programmable though. lets say the objective is to make a mannequin dance to 'thriller' like michael jackson. \n\ni am open to using kinnect technology as the controller (so that a dancer can simply dance in front of my robot, who can copy and remember) but i'm also open to controllers that allow me to force the mannequin into a pose at specific time codes in the song. if necessary, i am also willing to program the poses using some kind of lighting desk type controller (such as tech crews use in rock concerts to sync everything to go with the music).\n\ni have noticed that a power drill or winch is very loud whereas my fan is very quiet. i live in an apartment in which neighbours can hear footsteps from other apartments. i would not dare turn on a drill at 4am because it would wake up everyone in the whole building, but i would have no guilt in turning on a fan. i need my robot to be as quiet as a fan. the voltage does not really matter for this project. i'm happy to use up to 240v from the wall socket.\n\nplease let me know which motors and controllers are best for my mannequin robots, taking cost into account. thanks so much for any help :)\n\n\n", "tags": "microcontroller robotic-arm motion humanoid", "id": "2894", "title": "what are some good cheap, silent, motors for mannequin robots and what kind of controller should i use?"}, {"body": "i'm new to embedded devices and am trying to understand how to use i2cget (or the entire i2c protocol really).\n\ni'm using an accelerometer mma8452, and the datasheet says the slave address is 0x1d (if my sao=1, which i believe is referring to the i2c bus being on channel 1 on my raspberrypi v2).\n\nfrom the command line, i enter\n\n\n\nit returns\n\n\n\ni think that means i'm attached to the correct device.\n\nso now, i'm trying to figure out how do i get actual data back from the accelerometer?\n\nthe i2c spec says\n\n\n\nso i have tried\n\n\n\nwhere 0x01 is the out_x_msb. i'm not sure entirely what i'm expecting to get back, but i figured if i saw some data other than 0x00, i might be able to figure that out.\n\nam i using ic2get wrong? is there a better way to learn and get data from i2c?\n\nthe datasheet for my accelerometer chip is at http://dlnmh9ip6v2uc.cloudfront.net/datasheets/sensors/accelerometers/mma8452q.pdf\n", "tags": "raspberry-pi i2c", "id": "2897", "title": "how to read data from i2c using i2cget?"}, {"body": "i'm working on my first robot project. i previously used a 12v 6ah sealed lead acid battery, but recently i aquired some 15 asus li-ion battery packs, each of them 14.8v and either 2200 mah or 4400 mah. the laptops have been discarded, and some of the battery packs seem to be dead.\n\nthe battery packs have an 8 pin connector. inside, i assume there's a bunch of 18650-cells and some electronics.\n\nmy robot can handle 14.8 v directly.\n\nhow can i use these batteries? how can i charge them without the laptops? i'm a little put off by the idea of taking the 18650-cells out of the packs and rebuilding my own battery pack and charging system, but if that's what's needed i have to do it.\n\nthe packs are marked asus a41-a3 for the 2200 mah ones, and asus a42-a3 for the 4400 mah ones.\n", "tags": "battery", "id": "2899", "title": "salvaging a bunch of laptop battery packs"}, {"body": "i was wondering if you could reccomend possible solutions for locating a robot within a multilevel house.  what seems obvious to me is that need an altitude sensor to derive the story the robot, and a compass sensor to derive the heading.  however i was wondering what i could use to locate the robots xy position in the house.  if this requirement is unclear, imagine that i have to map a dot representing my robot position to an image of the current floor from the top.\nmy original idea was to use gps, however as i need submeter accuracy that would be incredibly expensive.  i also considered monti-carlo localization, however that requires no obstruction between sonar sensors and walls.  it is also a significant task programically.  i had an idea to place 3 wireless beacons of some sort on the vertexes of an equilateral triangle surrounding the house, then triangulate my position using distance from each beacon.  however, i have no idea how i would go about this hardware-wise.  do any of these ideas seem viable, and if so do you have suggestions on how to implement them? otherwise, can you reccomend an easier or cheaper alternative?  my platform is essentially an arduino hooked up to sensors and motor drivers connected to java on a laptop over serial.  thanks.\n", "tags": "arduino sensors localization gps", "id": "2902", "title": "solutions for finding position and heading in a multi-level house"}, {"body": "i want to show information about temperature over the internet using a sensor and pic.\nhow can i do using pic?  is there any way for the pic 16f628a or 18f to do it?\ndoes anyone hame some literature about it? schematics?\n", "tags": "microcontroller", "id": "2904", "title": "using pic as webserver?"}, {"body": "we have builded a quadcopter that use the flight manager kk2.1.5 with the latest firmware. when we increase the throttle, it flight. when we keep the hand on the stick we are able to maintain it but when we don't touch to the throttle, it goes up and down.\n\nyou can see an example on this video.\n\nwe have tried different values for pid but we don't know what is the best for us. we have a large quadcopter with medium propellers (may be too small). \n\ndoes the weight of the quadpcoter or the width of the propellers can be a factor? what can be the problem?\n", "tags": "quadcopter multi-rotor", "id": "2905", "title": "grasshopper effect on a quadcopter with kk2.1.5"}, {"body": "i am building a quadcopter using the sparkfun razor imu which outputs the roll, pitch and yaw axes values at 50 hz, which limits the operations of the controller(implemented on arduino imu), to 50 hz mx itslef. please tell me if flashing the escs(emax 40a) with the simon k firmware can do me any good. \n\ni'll be grateful. :)\n", "tags": "arduino quadcopter imu", "id": "2908", "title": "simon k firmware when the imu outputs at 50hz"}, {"body": "i am confused about how adding a d (which adds a zero to the complete system) decreases the speed of the system. but when we normally add a zero to the system, it causes the system to overshoot.\n\nthe same goes for the i part of the pid. normally when we add a pole to the system, it has less overshoot, but at the same time the integrator increases the overshoot!\n\nhow can i make sense out of this inverse relation?\n", "tags": "control pid", "id": "2915", "title": "effect of adding a pole and zero to pid"}, {"body": "i am working on a project where a dslr camera will be rotated on a tripod on 2 axes. i'm definitely using nema 17 motors as those are what i have. the motors will rotate 30 degrees every 5 seconds in normal usage so speed is not a requirement. the weight of the camera is 1170g and i'm using 3d printed parts for the remainder of the mount. i tried running nema 17 stepper motors off a adafruit motor shield v2 but the whole thing overheats (battery, driver, and motor). by the way, the motor will be controlled by an arduino. i need to find another motor controller to use. i looked on ebay and things like this came up for 20 dollars which seems too good to be true. \n\nmy question is what motor driver should i use for this project as i have little experience with them outside of arduino shields\n\n\n", "tags": "arduino motor cameras stepper-driver", "id": "2918", "title": "suggestions for stepper motor controllers"}, {"body": "how do i use the nicolas ziegel approach when the root locus plot of my system never becomes marginally stable , for any gain (unless it is negative).. ??\n\nhow do i estimate my ultimate gain value????\n", "tags": "control pid", "id": "2919", "title": "how do i use the nicolas ziegel approach if my system never becomes unstable?"}, {"body": "i am trying to understand the effects of p, i and d constants in a pid controller on a system.\n\nas far i've understood, p and i make the system 'faster', and d makes it 'slower'(which i read in books), but i don't actually understand what makes it go 'fast' or 'slow'.\n\nhow an integrator causes overshoot and all things like that. it makes sense that the p part causes overshoot, since it adds a gain. but what is the integrator doing? i want some kind of mathematical understanding on how all these parameters affect the system.\n\ni know how they work individually, but i'm having a hard time understanding, how it affects the system as a whole. for example, how does a zero added to the system lead to decrease in overshoot, but when normally adding a zero to a system would create more overshoot.\n", "tags": "pid", "id": "2920", "title": "understanding the pid controller"}, {"body": "many websites say that analog servo motors work on 50hz and digital servo motors work on 300hz. my question is, does this difference apply only to the inner control loop of the servo or does the user of the digital servo actually provide 300hz pwm signal? to rephrase, are all (most) servos including digital ones controlled with 50hz pwm, or are digital ones specifically controlled with 300hz pwm? thanks\n", "tags": "rcservo pwm servomotor", "id": "2921", "title": "controlling digital servos"}, {"body": "why doesn't a pid only consisting of id exist?\n", "tags": "control pid", "id": "2925", "title": "why does a id controller not exist?"}, {"body": "i have a kk2.1.5 and i fly with the self-level on. in the kk2, there are two menus. one to set pi-settings and another with selflevel-settings. both enable to set p-gain and i-gain.\n\nis it important to have good pi-settings when the self-level is on, or is setting good values in selflevel-settings sufficient?\n", "tags": "multi-rotor", "id": "2930", "title": "is it important to have good pi settings when we running in self-level with kk2?"}, {"body": "i have built a quad copter completely from scratch (electronics, mechanics and software). \ni am now at the point where all my sensor data looks correct and when i tilt the quad copter the correct motors increase and decrease.\n\ni have been trying to tune the pids for a couple of days now, in rate mode it stays level and rotates at roughly the correct degrees per second when i give it a command.\n\nin stability mode a lot of the time it just spins around the axis and when i did get it stable it kept rotating from upright to upside down and then maintaining an upside down flat position. i have come to the conclusion that i am either doing something completely wrong or i have some + - signs mixed around somewhere.\n\nwould anyone who is knowledgeable about quad copter control code be able to take a look at what i have done and how it works as i'm really struggling to work out what needs to change and what i should try next.\n\nmy flight control code is posted below, the other relevant classes are hardware.h and main.cpp\n\n\n\nthe whole program can be seen on my mbed page at http://mbed.org/users/joe4465/code/quadmk6/\n\nif you need any more info or something explaining let me know.\n\nif anyone can point me in the right direction or has any idea of what i should try next it would be very much appreciated.\n\nthanks\njoe\n", "tags": "quadcopter pid imu", "id": "2931", "title": "quad copter attitude control"}, {"body": "my frc team will recently upgrade from  compactrio  to roborio.\n\ncompactrio only supports up to java 1.4.\nwhat version of java does roborio?\n", "tags": "microcontroller", "id": "2933", "title": "does roborio support java 8?"}, {"body": "i am trying to understand how a pid controller moves the poles and zeros of an transfer function. \ni've been playing a bit with it, but aren't able to see some kind of connection. \n\ni mean that p and i rises the overshoot which would mean that the damping ratio gets smaller, thereby should  away from the real axis. \n\nand d should should do the opposite, but it doesn't seem to be true with the examples i've used.. am i doing something wrong??\n\nwell i kind of just want a general knowlegde of how it affect second order systems. \n", "tags": "control", "id": "2935", "title": "how pid affect the root locus of a close loop transfer function"}, {"body": "i'm currently working on a school project about simulating robots scanning an area, but it has been a struggle to find what strategy the robots should use.  here are the details:\n\ni am given a certain amount of robots, each with a sensing range of $r$.  they spawn one after another.  their task to scan a rectangular area.  they can only communicate with each other when they are within communication range. \n\ni am looking for the best strategy, (i.e. time efficient solution) for this. \nany reply or clue to the strategy will be appreciated. \n", "tags": "mobile-robot sensors coverage", "id": "2936", "title": "best strategy for area scanning using little sensing bots"}, {"body": "i have this close loop transfer function:\n\n\nit overshoots, but why? the poles are placed such that that the damping = 1, so why the overshoot?\n", "tags": "control", "id": "2937", "title": "how does poles and zeroes affect the step response of an transfer function?"}, {"body": "i identified my system and now i am trying to tune pi regulator since i think i do not need d.\n\ni came across this graph while matlabing and i do not know what does it mean.\n\ni am using pidtune() to get my p and i values. (i think computation is all correct, i made model in simulink to confirm). anyway see my picture and arrow is pointing at what i do not understand. why is my system going below zero first?\n\nit is supposed to be water flow regulator.\n\n\n\ntransfer function:\n$$ \\frac{-0.311s + 0.05548}{s^2 + 0.06882s + 0.0007626}$$\n\ncontinuous-time pi controller in parallel form:\n$$k_p + k_i * \\frac{1}{s}$$\n\nwith $k_p = 0.256$, $k_i = 0.000342$\n", "tags": "pid", "id": "2939", "title": "what does this \"inverse\" peak mean? (step function)"}, {"body": "i am trying to implement a controller for an inverted pendulum using lqr (with matlab command lqr(a,b,q,r)). the problem is that the motors are relatively weak, so i tried to increase r, but simulations show that the effort is still very high. how can i reduce the effort?\n", "tags": "control design", "id": "2940", "title": "lqr design with low effort"}, {"body": "from what i understand, you can create a map using sensors and storing values in an array.  for example, the array can have 0's for places not visited, and 1's for places visited, or 0's for open spaces and 1's for occupied spaces, etc.  \n\nthe map i'm thinking of making will have about 1000 x 2000 members.  this means i need an array of 2 million members.  that seems like a lot.  if i have eight attributes about each member (like temperature, light level, sound level, etc.), then i have 16 million pieces of information to capture.  \n\nis this correct, or is there a better way to create a map for a robot using software?\n", "tags": "software mapping", "id": "2945", "title": "how does a robot efficiently store a map it makes?"}, {"body": "is it possible to determine pid parameter using pole placement. \ni mean by solving the ch. eq. of close loop transfer functions  which consists of either p,pi,pd or pid controllers??\n\nbecause i've tried it, an eventhough i am getting my poles at the locations i want the systems does not act as i assumed.\n\nan example. i want my system to be overdamped and to have settling time less than 1 sec. which means that i want my poles to lie on the real axis, and to be less than -4. \n\n$$g(s) =\\frac{10.95 s + 0.9574}{s^2 + 0.09149 s +  6.263*10^{-6}}$$\n\nwith p = 0.1, i= 0.617746, d =  0.0147173\ni get a close loop system which is \n$$g_cl(s) =  \\frac{0.1612 s^4 + 1.109 s^3 + 6.86 s^2 + 0.5914 s}{  0.1612 s^4 + 2.109 s^3 + 6.952 s^2 + 0.5914s}$$\n\nbut looking at it's step response i see it creates overshoot, which i cannot justify due to an step input...\n", "tags": "control pid tuning", "id": "2948", "title": "pid tuning method based on pole placement"}, {"body": "sorry if this is out of robotics, but this seemed the closest.\na motor has to be used to rotate three cranks. they are in the same plane, but each should not run at the same time.\n\nonly one that occupies a central position, corresponding to the motor, should rotate. the discs are mounted in rectangular frame that can be slid to bring the required disc to the central position.\n\ncan anyone suggest a method to achieve this? (basically coupling and uncoupling the motor shaft from whatever disc is in the centre.)\n", "tags": "motor mechanism", "id": "2949", "title": "actuation of three reciprocating blades via conversion of rotary to linear"}, {"body": "i was recently reading that those artificial muscles had the highest power/weight ratio while electric motors only have a ratio of less than 100:1. as electrical engineer i have never worked with pneumatics before and do not have a big idea about air pumps.\nmy question is, how much will this ratio change if we consider an autonomous system. \non one side, the mckibben actuators + air pumps and valves + energy source   and on the other side, electric motors + energy source. as an example, let suppose that somebody found a way to model and control the artificial muscles as good as the electric motors and make two autonomous walking leg models, which one would have a higher resulting power/weight ratio ?\n", "tags": "control power actuator torque battery", "id": "2961", "title": "mckibben artificial muscles and the 400:1 ratio"}, {"body": "i am interested in learning how to build a dynamic quadcopter, and hope to be fairly proficient with arduino/raspberry-pi. what resources and/or practices might you recommend? \n", "tags": "arduino raspberry-pi beginner", "id": "2963", "title": "how do i learn about arduino/raspberry-pi based robotics on my own?"}, {"body": "i'm trying to develop a control system to a quadcopter and one of my options is to use a pid controller (which i think is the most used method).\n\nfrom what i've read, the commom strategy is to run a pid algorithm to each axis. my question is: how the pid output is converted to pwm signals?\n\ni'm asking that because the three axes and the four rotors depend on each other. i mean, if i slow down a couple of rotors which are opposite to each other then the quadcopter will move down in a vertical axis. but if i speed one of them and slow down the other, the quadcopter will rotate in a different axis.\n\nso we cannot isolate each axis and associate them with a single rotor or a pair of those. and because of that, how can a pid output (which is associated to an axis) can be converted to pwm signals to the rotors?\n\nis that a mimo system?\n", "tags": "quadcopter pid", "id": "2964", "title": "quadcopter pid output"}, {"body": "https://www.youtube.com/watch?v=mphhyliq294\n\ni have found this robotic arm on the internet, and i was wondering if someone can tell me what parts i need to build it or similar. the video says it can lift a small cat.\n\ni need to be able to program it, so i'll need a controller.\n\nplease recommend specific motor, controller links that you recommend. thanks :)\n", "tags": "robotic-arm", "id": "2966", "title": "what parts do i need for a strong robotic arm?"}, {"body": "would a control system consisting of 2 pid controller one plant  would be considered as an cascade controller??\n\nand how come would a proper tuning method be?\n\nas far i've googled it seems to me that only best method is to manually do it, one by one. \n\nthis is how my system looks like \nhttp://snag.gy/rjh2j.jpg\n", "tags": "control", "id": "2968", "title": "proper tuning method of an cascade controller?"}, {"body": "i'm looking to build a test rig for a robot that rotates a 1\" diameter pipe 180\u00ba (roll not yaw or pitch). i am currently testing a motor's performance when subjected to various kinds of pwm (high duty cycle, low duty cycle, etc). i would like to characterize how this performs under various loads.\n\nis there a simple mechanical mechanism i can attach to a fixture and insert into or around the pipe that lets me control how easy or difficult it is to rotate? \n\ni am thinking of something like a drill bit chuck that fits inside the pipe and expands or a circular clutch that clamps down around pipe to add resistance when one tightens a thumbscrew. i would like to go from no resistance to full stop for a 4n\u2022m motor. i would like to be able to test how 'sticky' the pipe is using a torque wrench.\n\ni imagine this would be very simple but i can't think of something that would do this!\n", "tags": "motor torque", "id": "2970", "title": "how can i dynamically control the amount of torque necessary to rotate something for a test rig?"}, {"body": "i am having a hard time grasping the concept of a dc motor  with load being unstable, and stable due to a controller \n\nmy confusions appears as i am trying to design a controller for one using z-n method, and the transfer function i've identified using matlab tells me that my dc-motor always will be stable. \n\nwhich makes sense, since feeding it constant voltages, will lead to a constant veloicty. \n\nbut to use the z-n approach the system has be able to become unstable, and since this isn't possible i am getting confused if a motor are able to become motor for which i am to design a controller for. \n\nthe question in simplicity, \nhow come can a controller make a motor, (if the motor itself cannot (due to pole zero plot)) unstable.  \n", "tags": "motor control", "id": "2972", "title": "transfer function of dc motor being unstable due to a controller?"}, {"body": "my current project calls for 6 motors (50 mechanical watts), and an assortment of sensors.  i selected the arduino mega 2560 r3 to use as the basis for the project.  i am having trouble determining the best motors and motor controllers for the project.\nthe motors need to be very precise and very controllable, meaning i need to make very small changes to the speed of the motor several times a second.  the motors will follow a position vs. time curve.  i broke up the position vs time graph into 100 hundred sections.  i found the acceleration(degrees per second^2) of each section, and i want to put it in to the array and format it as such: \n\nfloat arr[] = {position 1, acceleration between position 1 and 2, position 2, acceleration between position 2 and 3, position 3, etc...}\n\nhere is an idea of how i plan to program the motor:\n\n\n\ni am planning to run the motors in a pid loop so it will not be as cut and dry as the loop above.  \n\ni saw the spark fun autodriver stepper motor controller, i thought the accompanying library would save me a lot of work. i very quickly noticed that it had some major limitations.  first it did not allow me to very the acceleration while the motor was running, for example: if i call:\n\n\n\ni have to call softstop() or hardstop() otherwise the autodriver will keep telling me its busy, my interpretation (please correct me if i am wrong) is i cannot change any of the drive commands (i.e.) i cannot vary the acceleration.  the autodriver does have other functions but they require a target step, i do not want to give a target step because i don't want to be limited by the steps (1.8 degrees on the stepper i have) i may want to stop at 34.6 degrees, but with the auto driver i cannot.  \n\nthe auto driver is also limited by its 3a per phase current limit.\n\njust to summarize:\ni need a mega to control 6 motors with a minimum output of 50 mechanical watts, i need very precise control, i am planning on using a potentiometer or rotary encoder.  the motors will run in a pid loop with several sensors.  i need to be able to vary the speed several times a second.\n\nall 6 motors will be executing different motions and they need to be able to run very consistent over time, (i.e) when motor 1 is at 2 degrees motor 2 must be at 18 degrees.\n\ni am still open to the idea of a stepper motor i can live with having to stop on a step but i would really prefer not too.\n\nany and all help would be greatly appreciated.\n\nthanks,\njoel \n", "tags": "motor", "id": "2976", "title": "selecting precise high torque motor and motor controller for use with arduino mega 2560 r3"}, {"body": "i am bit uncertain how i should interpret the definition of an robust controller. \n\nas far i've understood, the closed loop system including the controller has to have a high gain for frequencies where disturbance appears, and decay at frequencies higher than the work area, or noise.  both of these can be determined using a bode plot, thereby determining the robustness of my closed-loop system. \n", "tags": "control", "id": "2980", "title": "definition of (or determine whether something is) a robust controller?"}, {"body": "i want to cut acrylic pieces that will be assembled into the body of a robot. what are some recommendations for acrylic/plastic cutting services? does laser cutting produce the best results?\n", "tags": "manufacturing", "id": "2981", "title": "acrylic/plastic cutting services"}, {"body": "i'm writing a c code generator geared toward robotc and complex tasks for an ftc team, and was wondering about some performance and storage concerns:\n\n\nhow much memory is available for my program's data? it'll be mostly pre-defined lookup tables, generally in the form of multidimensional arrays.\nhow much nxt memory is available for my program itself? as in, roughly how much code can i expect to fit into a single robotc compiled program?\nhow quickly do programs execute, generally? looking at disassembly most of my generated lines correspond to 2-4 opcodes.\n\n\nbased on these, i'm trying to make a decision of precomputation vs runtime pathfinding.\n\ni'm using nxt/tetrix. my major interest at this point with these questions is for pathfinding. i plan to have a 64x64 grid and be running djisktra's a* algorithm with a heuristic function that assigns a penalty to turns and is as close to consistent as possible (not sure if consistency/monotonicity is doable with the turn penalty).\n\nroughly 8 paths would be cached if i decide to use the pre-cached lookup tables.\n\ninstead of a set, i'll probably use a boolean array for the set of nodes visited. the fact that i'm working with a square layout will allow me to use a 2d array for the map needed to reconstruct the path.\n\ni'd love some feedback and answers to my question if anyone has any. thanks!\n", "tags": "nxt robotc", "id": "2982", "title": "performance/memory considerations for pathfinding lookup tables on robotc for a small set of paths"}, {"body": "i am in a situation where i need to secure a 20\" wheel made of 3/4\" thick mdf (of my own making) onto a 1-1/4\" precision-ground steel shaft, and this joint needs to be strong enough to convey a rather large amount of torque, about 575-600 in-lbs, effectively a 5hp motor driving the shaft at 500-600 rpm.\n\ni don't have a milling machine or any metalworking tools, so my preferred option of 'milling flats onto the shaft' is out. my second option was to attempt to increase the surface area of the wheel's bore and then use an appropriate adhesive such as jb weld.\n\nis jb weld a suitable solution to this problem or is there a better method of fastening that doesn't involve modifying the steel shaft at all?\n", "tags": "wheel", "id": "2983", "title": "securing a disc/wheel to a shaft"}, {"body": "i have upgraded the motors in my robotic arm to sensored, brushless rc car motors. the hope was to reuse the hall sensors to double as a rotary encoder, by tapping 2 hall sensors and treating the 2 bits as a quadrature signal (a crude quadrature since 2 of the 4 states will be longer than the other 2).\n\nthis works when none of the motor phases are powered and i just rotate the motor manually. but once the stator coils are energized, the encoder no longer counts correctly: when running at low power, the counting is correct, but when running under high power, the count is monotonic (only increases or decreases) no matter if i run in reverse or forward.\n\ni'm almost certain this is because of the stator coils overpowering the permanent magnets on the rotors. so is there still a way to use the hall sensors as an encoder?\n\nsorry if this is an obvious question. i'd love to research this problem more if i had more time.\n\nupdate:\ni've measured the wave forms with my dso quad and see the expected 120 degree separated signals (the measurement for phase c gets more inaccurate over time because i only had 2 probes, so i measured phases a &amp; b first, then a &amp; c, and then merged them.\n\nwhen esc speed is 0.1:\n\n\nwhen esc speed is 0.3:\n\n\npreviously, i was using a hardware quadrature counter (eqep module on a beaglebone). at speed=0.3, this was counting backwards no matter if i do forward or reverse!\n\ni then implemented quadrature counting on an lpc1114fn28 ucontroller. the result was still bad at high speeds (count didn't change at all). the logic was:\n\n\n\nthen i got the idea to change the code to not update prevstate until an expected state happens (to deal with glitches):\n\n\n\nnow the counting finally is correct in both directions, even at speeds higher than 0.3!\nbut are there really glitches causing this? i don't see any in the waveforms?\n", "tags": "brushless-motor encoding hall-sensor", "id": "2987", "title": "can i reuse the hall sensors in a brushless motor as an encoder?"}, {"body": "i wanted to present a voice-controlled robot in my lab's upcoming demo contest. my robot is essentially a x86 ubuntu notebook resting on top of a two-wheeled platform, so in principle any solution available on linux would do.\n\ni looked into julius, but it seems the only comprehensive acoustic model available for it is aimed at the japanese language \u2013 which coincidentally i can speak a little, but apparently not clearly enough to produce anything beyond garbled text. i also tried the google speech api, which has a decent selection of languages and worked very well, but requires internet access. finally there is cmu sphinx, which i haven't yet tested, but i'm afraid might have a problem with my accent (i'm a nativa brazilian portuguese speaker, and apparently there is no such acoustic model available for it).\n\nis that all there is to it? have i missed any additional options? as you may have guessed, my main requirement is support for my native language (brazilian portuguese), or failing that, good performance for english spoken with foreign accents. a c++ api is highly desirable, but i can do with a shell interface.\n", "tags": "mobile-robot linux speech-processing digital-audio", "id": "2988", "title": "voice control solution for linux robot?"}, {"body": "let's say we have a bunch of observations $z^{i}$ from sensor and we have a map in which we can get the predicted measurements $\\hat{z}^{i}$ for landmarks. in ekf localization in correction step, should we compare each observation $z^{i}$ with the entire predicted measurement $\\hat{z}^{i}$?, so in this case we have two loops? or we just compare each observation with each predicted measurement?, so in this case we have one loop. i assume the sensor can give all observations for all landmarks every scan.  the following picture depicts the scenario. now every time i execute the ekf-localization i get $z^{i} = \\{ z^{1}, z^{2}, z^{3}, z^{4}\\}$ and i have $m$, so i can get $\\hat{z}^{i} = \\{ \\hat{z}^{1}, \\hat{z}^{2}, \\hat{z}^{3}, \\hat{z}^{4}\\}$. to get the innovation step, this is what i did \n$$\nz^{1} = z^{1} - \\hat{z}^{1} \\\\\nz^{2} = z^{2} - \\hat{z}^{2} \\\\\nz^{3} = z^{3} - \\hat{z}^{3} \\\\\nz^{4} = z^{4} - \\hat{z}^{4} \\\\\n$$\nwhere $z$ is the innovation. for each iteration i get four innovations. is this correct? i'm using ekf-localization  in this book probabilistic robotics page 204. \n\n \n", "tags": "sensors localization ekf", "id": "2990", "title": "innovation step ekf localization?"}, {"body": "i am experimenting with using a stepper motor for a robotics project. i'd like to use microstepping to give a better resolution and smoother movement, but i have noticed that the finer the microsteps, the lower the torque from the motor. why is this?\n\nfor reference i'm using the allegro micro a4988 motor driver, and a bipolar stepper motor.\n", "tags": "stepper-motor torque stepper-driver", "id": "2998", "title": "why does microstepping give less torque?"}, {"body": "am going to be competing in robocup rescue in thailand next year - i was too busy to pull off a campaign for brazil this year :(\n\nwill be using cuda powered gpus, kinect/xtions, and ros as the primary navigation system, but i need a sensor for long range scanning - at least 25 meters. it is probably overkill for the competition, but i want it to be used in other real world applications. it will need to be very robust, fairly light, high resolution, and proven. the cheaper the better, but high quality is a must.\n\nhave read this question, but i need something that is available and proven now:\nwhat different sensing approaches are used in the current batch of indoor 3d cameras?\n\na similar question was asked before, but closed:\nlidar solutions. the suggestion was good , but i need something with a lot more range:\n\nat the moment am probably going to go with a the roboteye re05 or re08 3d-lidar:\n\nhere is a paper that descibes how this sensor can be used on a mobile robot: www.araa.asn.au/acra/acra2012/papers/pap125.pdf\n\ndoes anyone have any alternative techniques, or suggestions of a sensor that can achieve similar results?\n", "tags": "mobile-robot ros slam kinect lidar", "id": "3006", "title": "how can i achieve long distance, high quality 3d scans on a mobile robot?"}, {"body": "i had the opportunity to work for a factory/company that is in the domain space of production and they want to use a robotic arm for part of the production line.\n\nthey want basically a robotic arm with payload of about 2 kgs or more and an arm length of more than 1600mm\n\ni have researched a few companies like kuka.com but i am not sure what i should be looking for when making suggestions and researching for it.\n\nare there any suggestions you can give me on few good points to be careful about with robotics arms? any innovating companies out there i should consider? how is an installation done and if i should find a supplier for it etc. please enlighten me. \n", "tags": "robotic-arm mechanism", "id": "3007", "title": "industrial robotic arm"}, {"body": "i have built several quadcopters, hexacopters, and octacopters. this means that between the flight controller (i use 3dr apm2.6 or pixhawk) and the motors there are heavy duty power wires as well as a servo-style cable carrying a pwm control signal for the esc. three short heavy-duty wires then connect the motor to the esc, one for each phase.\n\nseveral times i've heard or read people saying that the electronic speed controllers (escs) should be mounted far away from the flight controller (fmu seems to be the abbreviation en vogue) and close to the motors. i think the idea is that this cuts down on interference (i'm not sure what sort) that could be emitted by the long esc -> motor wires that would be required if you have the escs all at the center of the aircraft. another consideration is that escs can be cooled by propellers if they are right under the rotor wash, as mine usually are.\n\nso, i've always mounted escs close to motors, but realized that design could be much simpler if escs are mounted centrally. so, my question is: what are the pros and cons of mounting escs close to the motor versus close to the fmu?\n", "tags": "brushless-motor multi-rotor esc", "id": "3013", "title": "does it matter if my electronic speed controllers are close to my brushless motors?"}, {"body": "http://pointclouds.org/documentation/tutorials/normal_distributions_transform.php#normal-distributions-transform\n\ni've used this program with the sample pcd's given and it came out correctly. this was confirmed by experienced users on here. now i'm trying to use my own pcd's. i didn't want to bother changing the program so i just changed the names to room_scan1 and room_scan2. when i attempt to use them, i get this error:\n\n\n  loaded 307200 data points from room_scan1.pcd loaded 307200 data\n  points from room_scan2.pcd filtered cloud contains 1186 data points\n  from room_scan2.pcd normal_distributions_transform:\n  /build/buildd/pcl-1.7-1.7.1/kdtree/include/pcl/kdtree/impl/kdtree_flann.hpp:172:\n  int pcl::kdtreeflann::radiussearch(const pointt&amp;, double,\n  std::vector&amp;, std::vector&amp;, unsigned int) const [with pointt =\n  pcl::pointxyz, dist = flann::l2_simple]: assertion\n  `point_representation_->isvalid (point) &amp;&amp; \"invalid (nan, inf) point\n  coordinates given to radiussearch!\"' failed. aborted (core dumped)\n\n\nthis is the program i compiled: http://robotica.unileon.es/mediawiki/index.php/pcl/openni_tutorial_1:_installing_and_testing#testing_.28openni_viewer.29\n\nbefore you suggest it, i will let you know i already changed all of the pointxyzrgba designations to just pointxyz. it threw the same error before and after doing this. the thing that confuses me is that i looked at my produced pcd files and they seem to be exactly the same as the samples given for ndt.\n\nmine:\n\n\n\nsample from ndt page:\n\n\n\ndoes anyone have any ideas?\n", "tags": "kinect computer-vision openni", "id": "3016", "title": "tried normal distributions transform with my own files (in correct pcd format) and it throws errors, why?"}, {"body": "in general, what is a good programming language for robotics? i am a starting robo nerd and don't know anyone who would know things like this.\n", "tags": "mobile-robot wheeled-robot programming-languages", "id": "3017", "title": "programming language?"}, {"body": "i want to give my linux robot the ability to locate a sound source and drive towards it. i am reading a paper on sound localization that seems to cover the theory well enough, but i'm at a loss as to how do i implement it. specifically i would like to know:\n\n\nhow do i connect two microphones to a linux pc?\nhow do i record from two microphones simultaneously?\nis there any library of sound processing algorithms (similar to how opencv is a library of computer vision algorithms) available for linux?\n\n", "tags": "mobile-robot digital-audio linux", "id": "3021", "title": "directional hearing for linux robot?"}, {"body": "hello i wanted to simulate a busy urban road,similar to darpa urban challenge for an autonomous self-driving-car. i'm in search of simulators for that.\n\ni've seen gazebo since its integration with ros is easier but editing world files or indeed creating them itself is difficult. in torcs simulator i have seen many world files but not many sensors.  i don't want much physics in my simulation. i want a light weight simulator(for checking out path planning on an urban road) and in which creating roads are easier. \n\ni've even searched for gazebo sdf files similar to urban city but in vain.\n", "tags": "simulator gazebo", "id": "3026", "title": "world files for simulating roads and tracks"}, {"body": "i am developing a quadcopter platform on which will be extended over the next year. the project can be found on github. currently, we are using an arduino uno r3 as the flight management module.\n\nat present, i am tuning the pid loops. the pid function is implemented as:\n\n\n\ni am having trouble interpreting the system response on varying the constants. i believe the problem is related to the questions below.\n\n\nhow frequently should a pid controller update the motor values? currently, my update time is about 100-110 milliseconds. \nwhat should be the maximum change that a pid update should make on the motor thrusts? currently, my maximum limit is about +-15% of the thrust range.\nat what thrust range or values, should the tuning be performed? minimum, lift off, or mid-range or is it irrelevant?\n\n", "tags": "quadcopter pid", "id": "3027", "title": "how frequently should a pid controller update?"}, {"body": "i've read a lot places that making a controller which cancels the unwanted pole or zero is not good designing practice for designing a controller.. \n\nit should make the system uncontrollable which off course isn't wanted. \n\nbut what alternatives do i have.. ??\n\nconsidering i have a system in which all poles and zeros lies on rhp. \n", "tags": "control design", "id": "3029", "title": "alternative way to perform pole zero cancellation?"}, {"body": "can someone please help me with the jacobian matrix equations for abb irb140 robot. or an easy way by which i can derive it given the dh parameters. i need it to implement some form of control that am working on. thanks\n", "tags": "control robotic-arm kinematics dh-parameters", "id": "3032", "title": "jacobian of abb irb140 robot"}, {"body": "this is quite a basic question. i'm practising robot programming with vrep. i have 2 k3 robots in the scene. one robot follows a predefined path. i want the second robot to move \"in parallel\" with the first one so they keep same orientation and same distance at all time. when there is a turn, i want the follower to slow/accelerate a little to keep the parallel.\n\nin my implementation, i use wireless communication. the first robot will periodically \"tell\" the second about its speed, orientation. the second will use these parameters to calculate two speed to its two wheel. but when i run it, it doesn't work. the orientation of the follower is wrong. the distance is not maintained. i was totally confused.\n\ni think this is quite a rudimentary task. there must be some practise to follow. can somebody help to provide some ideas, references? that will be highly appreciated! \n", "tags": "kinematics", "id": "3035", "title": "how to make one robot follow the other in parallel formation"}, {"body": "i have been reading about kinematic models for nonholonomic mobile robots such as differential wheeled robots. the texts i've found so far all give reasonably decent solutions for the forward kinematics problem; but when it comes to inverse kinematics, they weasel out of the question by arguing that for every possible destination pose there are either infinite solutions, or in cases such as $[0 \\quad 1 \\quad 0]^t$ (since the robot can't move sideways) none at all. then they advocate a method for driving the robot based on a sequence of straight forward motions alternated with in-place turns.\n\ni find this solution hardly satisfactory. it seems inefficient and inelegant to cause the robot to do a full-stop at every turning point, when a smooth turning would be just as feasible. also the assertion that some points are \"unreachable\" seems misleading; maybe there are poses a nonholonomic mobile robot can't reach by maintaining a single set of parameters for a finite time, but clearly, if we vary the parameters over time according to some procedure, and in the absence of obstacles, it should be able to reach any possible pose.\n\nso my question is: what is the inverse kinematics model for a 2-wheeled differential drive robot with shaft half-length $l$, two wheels of equal radii $r$ with adjustable velocities $v_l \\ge 0$ and $v_r \\ge 0$ (i.e. no in-place turns), and given that we want to minimize the number of changes to the velocities?\n", "tags": "mobile-robot inverse-kinematics", "id": "3040", "title": "\"smooth\" inverse kinematics model for 2-wheeled differential drive robot"}, {"body": "i am working on a quadrotor and am trying to solve the problems described here. in attempts to bring the refresh rate to 100 hz, i did an analysis of the functions  and most of the time 35+ ms is being taken by the rc receiver input function. to tackle this, i have decided on two solutions:\n\n\nuse interrupts ( library) instead of \nreduce the frequency of pilot input\n\n\nthe second solution which is much simpler is to simply read the pilot input once in $(n+1)$  pid updates. so, for $n$ times, we have a update time of $8\\;ms$, and for the $(n+1)^{th}$ time, we have an update time of t ms. $n$ will be around $10$.\n\nthis will create a system that will run on average in $(n*8 + t)/(n+1)\\; ms$.\n\nnow, how does a dual/variable frequency affect the pid system? does the system behave as if working at the effective frequency? i have been searching for some time but i cannot find anything that discusses such a situation. \n", "tags": "pid", "id": "3043", "title": "are there any problems with a variable frequency pid?"}, {"body": "i am searching for a way that allows me to wait for some conditions on ports before applying a new state.\n\nmy concrete problem:\ni want to make sure that my auv aligns to the right pipeline. therefore before starting the pipeline-tracking, i want to check for the current system heading.\n\nmy current state-machine looks like this:\n\n\n  find_pipe_back = state target_move_def(:finish_when_reached => false ,\n  :heading => 1 ...)\n  \n  pipe_detector = state pipeline_detector_def\n  \n  pipe_detector.depends_on find_pipe_back, :role => \"detector\"\n  \n  start(pipe_detector)\n  forward pipe_detector.align_auv_event, success_event\n\n\nroughly i am looking for a way to condition the last-forward.\n", "tags": "rock syskit", "id": "3049", "title": "how to define conditions for state-machines in roby?"}, {"body": "i use autodesk inventor professional 2014. i design my gears using the design accelerator. however, whenever i create gear trains, parts of certain gears become transparent. this seems completely random because sometimes if i zoom in or out or when i pan or orbit, the gears look normal again.\ni have experienced this problem using both the default and other material types.\ni also have ensured that each of these gears are enabled.\n\nhere are some example pictures\n\n\nany help or suggestions will be greatly appreciated.\n", "tags": "design errors", "id": "3052", "title": "gears in autodesk inventor are looking weird"}, {"body": "i have a riddle about ethercat in mind and i'd like to have your point of view about it...\n\nwith the rise of open platforms and hardware, and easily accessible embedded machines, it is now rather straightforward to install a rt system such as xenomai on a raspberry pi, or a beagleboard black, or whatever cheap platform you prefer...\n\nnow to connect these a rt bus would be really cool (e.g. ethercat...).\n\nhence my question: every hobbyist face the same problems with rt communication, so is there any good reason why there does not exist any open ethercat shield for raspberry pi or beagleboards? it would solve so many problems...\n\nany thoughts on why? any idea?\n", "tags": "communication", "id": "3058", "title": "why cannot we find ethercat shields?"}, {"body": "i have a standard 5v \n\n\n\ni am using the horn it came with it, i mean this piece: \n\n\n\nlike the long arm in the middle. each of its holes are 1mm diameter.\n\nthen i have a 3d printed crankshaft i did:\n\n\n\nits holes are also 1 mm. so while the servo horn is attached to the servo, i attach this crank to the horn in order to lift or lower a small scructure.\n\nwhat i am not sure is hot to connect these 2 pieces (horn and 3d printed crankshaft). so far i have been using a paper clip, and at both end of it i placed 2 blobs of tin using a soldering iron. this has worked for nearly a year, but today i failed, and i was wondering if there's something more specific for my problem, which seems something common.\n\ni have seen some people use something called dubro ez connector, but it seems an overkill, plus it won't have space for my 3d printed piece. some people seems to use a clevis pin, but i cannot find any with a diameter of less than 1.\n\nso my question is, how can i fix it? what can i put at both ends to stop if from slipping away? i have already tried simple things like simply bending it.\n", "tags": "arduino mechanism rcservo", "id": "3062", "title": "how to connect a servo motor and a crank shaft"}, {"body": "the unscented kalman filter is a variant of the extended kalman filter which uses a different linearization relying on transforming a set of \"sigma points\" instead of first-order taylor series expansion.\n\nthe ukf does not require computing jacobians, can be used with discontinuous transformation, and is, most importantly, more accurate than ekf for highly nonlinear transformations.\n\nthe only disadvantage i found is that \"the ekf is often slightly faster than the ukf\" (probablistic robotics). this seems negligible to me and their asymptotic complexity seems to be the same.\n\nso why does everybody still seem to prefer ekf over ukf? did i miss a big disadvantage of ukf? \n", "tags": "mobile-robot localization kalman-filter ekf", "id": "3063", "title": "why should i still use ekf instead of ukf?"}, {"body": "e.g. what general multicopter configurations would be generally accepted as recommendations to lift 0.5kg, 1kg, 2kg, 4kg, etc.\n\nis there any general correlation between number of motors on a similar sized frame and lift capacity?\n", "tags": "quadcopter power", "id": "3066", "title": "what are some generally accepted lift capacity guidelines for multirotors?"}, {"body": "it might be kind of a stupid question but how many degrees of freedom are there in a typical quadcopter? i say some saying 4 and some saying 6. the difference stands in translation throughout the other 2 axis (horizontal ones). being strict to what you can directly tell the quadcopter to do, only 4 movements are possible since you cannot apply a pure lateral force. but you can tilt to start a lateral movement and align the body right after and let it hover in a horizontal axis, theoretically. so, formally, how many degrees of freedom should i consider to exist?\n", "tags": "quadcopter", "id": "3069", "title": "quadcopter degrees of freedom"}, {"body": "i am implementing the atlas slam framework for a ground robot, using ekf slam for local maps and using line segment features. the line segment features can be abstracted to their respective lines  where  and  represent the distance and angle in the distance-angle representation of lines. \n\nin the given framework, there is a local map matching step where lines of the local maps will be matched, and there is a need for a distance metric between 2 lines. the mahalanobis distance is suggested in the literature, however strictly a mahalanobis distance is between a single measurement and a distribution and not between 2 distributions.\n\nhow do i find the mahalanobis distance between line 1  with covariance matrix  and line 2  with covariance matrix ?\n\nin the ekf algorithm from the book probabilistic robotics by sebastian thrun, there is a computation during the feature update step, where it looks like the covariances (of a new measurement and an existing measurement) are multiplied to give a resultant covariance matrix, and then the inverse is used in the mahalanobis distance computation. \n\nthat would be similar to \n\n\n\nis that correct?\n", "tags": "ekf mapping", "id": "3071", "title": "mahalanobis distance between 2 line features"}, {"body": "given part of the following algorithm in page 217 probabilistic robotics, this algorithm for ekf localization with unknown correspondences \n\n9. for all observed features $z^{i}  = [r^{i} \\ \\phi^{2} \\ s^{i}]^{t} $\n\n10. &nbsp; &nbsp; for all landmarks $k$ in the map $m$ do\n\n11. &nbsp; &nbsp; &nbsp; &nbsp; $q = (m_{x} - \\bar{\\mu}_{x})^{2} + (m_{y} - \\bar{\\mu}_{y})^{2}$\n\n12. &nbsp; &nbsp; &nbsp; &nbsp; $\\hat{z}^{k} = \\begin{bmatrix}\n\\sqrt{q} \\\\ \natan2(m_{y} - \\bar{\\mu}_{y}, m_{x} - \\bar{\\mu}_{x} ) - \\bar{\\mu}_{\\theta} \\\\ \nm_{s} \\\\ \\end{bmatrix}$\n\n13. &nbsp; &nbsp; &nbsp; &nbsp; $ \\hat{h}^{k} = \\begin{bmatrix} \nh_{11} &amp; h_{12} &amp; h_{13} \\\\\nh_{21} &amp; h_{22} &amp; h_{23} \\\\\nh_{31} &amp; h_{32} &amp; h_{33} \\\\ \\end{bmatrix}  $\n\n14. &nbsp; &nbsp; &nbsp; &nbsp; $\\hat{s}^{k} = h^{k} \\bar{\\sigma} [h^{k}]^{t} + q $\n\n15. &nbsp; &nbsp; endfor \n\n16. &nbsp; &nbsp; $ j(i) = \\underset{k}{\\operatorname{arg\\,max}} \\ \\ det(2 \\pi s^{k})^{-\\frac{1}{2}} \\exp\\{-\\frac{1}{2} (z^{i}-\\hat{z}^{k})^{t}[s^{k}]^{-1} (z^{i}-\\hat{z}^{k})\\} $\n\n17. &nbsp; &nbsp; $k^{i} = \\bar{\\sigma} [h^{j(i)}]^{t} [s^{j(i)}]^{-1}$\n\n18. &nbsp; &nbsp; $\\bar{\\mu} = \\bar{\\mu} + k^{i}(z^{i}-\\hat{z}^{j(i)}) $\n\n19. &nbsp; &nbsp; $\\bar{\\sigma} = (i - k^{i} h^{j(i)}) \\bar{\\sigma} $\n\n20. endfor\n\nmy question is why the second loop ends in the line 15. shouldn't it end after the line 19. i've checked the errata of this book but nothing about this issue. \n", "tags": "localization ekf data-association", "id": "3073", "title": "data association with ekf?"}, {"body": "how would one typically integrate a neural network into an online automation system?\n\nas an example, we have developed a neural network that predicts a difficult to measure variable within a reactor using multiple sensors. we then use this predicted variable to tell the automation system to, for example, increase/decrease the stirrer speed.\n\nhow would someone implement this idea into a commercial system. would they develop a function block that can simulate the neural network? would they run a software on the server that reads and writes to the plc control tags?\n", "tags": "sensors control", "id": "3079", "title": "typical method for integrating a neural net into a plc"}, {"body": "i need help on how to go about building a quadcopter software from scratch with the available tools i have with me. i don't have a transmitter radio therefore the only way i can do remote control is using an android phone with the itead studio bluetooth shield that i was recently given.\nhow can i use the existing open source software, i.e aeroquad or arducopter. the following are parts that i have:-\n\n\narduino uno \nbluetooth shield \nfour brushless motors \nq450 frame four\nesc turnigy \nmpu6050\n\n", "tags": "arduino quadcopter", "id": "3080", "title": "arduino quadcopter using bluetooth shield and android phone"}, {"body": "i'm using amcl package in ros to localize a mobile robot. i've changed  and  several times then calculated the output difference with odomotry to evaluate these parameters. the table below demonstrate results; as you see, there is no notable change in the output and if you ignore the first row of the table, output variance is small. \n\nand this is the particle filter output on the map:\n\n", "tags": "localization particle-filter", "id": "3088", "title": "different particle filter min and max particle numbers give almost the same result"}, {"body": "i'm developing a program for communicating with ardupilot using mavlink. i've generated code based on the mavlink definition for ardupilot, and i have the basic communication working.\n\nwhat i can't figure out, is how to request ardupilot to send a specific mavlink message. i'd like ardupilot to send me mavlink message attitude (#30) every second. how can i do this?\n", "tags": "ardupilot", "id": "3090", "title": "how to request a specific mavlink packet from ardupilot?"}, {"body": "i want to overwrite the git source of a package in autoproj. that package is by \ndefault on gitorious and i forked it on spacegit to apply specific patches.\naccording to the autoproj documentation (http://rock-robotics.org/stable/documentation/autoproj/customization.html), i set the new repo in the overrides.yml by:\n\n\n\nbut if i inspect the remotes of the newly checked out package, only the \nfetch url is adapted to spacegit whereas the push url still points to \nthe default gitorious repo:\n\n\n\nhow can i overwrite both the fetch and the push source of a package in the \noverrides.yml?\n", "tags": "rock", "id": "3091", "title": "how to overwrite default git source in autoproj?"}, {"body": "assume i have a rather simple system i want to control, but all sensor measurements exhibit considerable time delay, i.e.:\n\n$z_t = h(x_{(t-d)}) \\neq h(x_t)$ \n\nwith my limited knowledge about control, i could imagine the following setup:\n\n\none observer estimates the delayed state $x_{(t-d)}$ using control input and (delayed) measurements.\na second observer uses the delayed observer's estimate and predicts the current state $x_t$ using the last control inputs between delayed measurement and current time.\nthe second observer's estimate is used to control the system.\n\n\ncan i do any better than that? what is the standard approch to this problem? and is there any literature or research about this topic?\n", "tags": "sensors control sensor-fusion sensor-error", "id": "3098", "title": "controlling a system with delayed measurements"}, {"body": "i want to implement the velocity motion model in matlab. according to probabilistic robotics page 124, the model is as following \n\n\\begin{align*}\n\\hat{v}      &amp;= v + sample(\\alpha_{1} v^{2} + \\alpha_{2} w^{2}) \\\\\n\\hat{w}      &amp;= w + sample(\\alpha_{3} v^{2} + \\alpha_{4} w^{2}) \\\\\n\\hat{\\gamma} &amp;= sample(\\alpha_{5} v^{2} + \\alpha_{6} w^{2}) \\\\\nx' &amp;= x - \\frac{\\hat{v}}{\\hat{w}} sin \\theta + \\frac{\\hat{v}}{\\hat{w}} sin(\\theta + \\hat{w} \\delta{t}) \\\\\ny' &amp;= y + \\frac{\\hat{v}}{\\hat{w}} cos \\theta - \\frac{\\hat{v}}{\\hat{w}} cos(\\theta + \\hat{w} \\delta{t}) \\\\\n\\theta' &amp;= \\theta + \\hat{w} \\delta{t} + \\hat{\\gamma} \\delta{t}\n\\end{align*}\n\nwhere $sample(b^{2}) \\leftrightarrow \\mathcal{n}(0, b^{2})$. with this kind of variance $\\alpha_{1} v^{2} + \\alpha_{2} w^{2}$, the kalman gain is approaching singularity. why?  \n", "tags": "mobile-robot kinematics motion motion-planning noise", "id": "3101", "title": "velocity model motion in matlab (probabilistic robotics)"}, {"body": "we are building a 6 dof robotic arm as a college project and we've almost finished the designs. the problem is with the controls. we still havent thought on how to control the arm, as in , software gui interfaces , etc. any suggestions on this ? also, is there any simulation software for simulating and testing robotic arms ? \n", "tags": "control arm", "id": "3102", "title": "6 dof robotic arm"}, {"body": "i'm facing problems with this book and it is the only book that discusses localization in depth. the results that i'm getting makes no sense. i've read a lot of papers, majority of them copy the localization algorithm from this book. my question here is why $\\bar{\\mu}$ and $\\bar{\\sigma}$ are being changed every iteration?? i'm using them to get the predicted measurements in lines 11- 13, so they should be fixed. \n\n9. for all observed features $z^{i}  = [r^{i} \\ \\phi^{i} \\ s^{i}]^{t} $ do \n\n10. &nbsp; $j = c^{i}$ \n\n11. &nbsp; &nbsp; &nbsp; &nbsp; $q = (m_{x} - \\bar{\\mu}_{x})^{2} + (m_{y} - \\bar{\\mu}_{y})^{2}$\n\n12. &nbsp; &nbsp; &nbsp; &nbsp; $\\hat{z}^{i} = \\begin{bmatrix}\n\\sqrt{q} \\\\ \natan2(m_{y} - \\bar{\\mu}_{y}, m_{x} - \\bar{\\mu}_{x} ) - \\bar{\\mu}_{\\theta} \\\\ \nm_{s} \\\\ \\end{bmatrix}$\n\n13. &nbsp; &nbsp; &nbsp; &nbsp; $ \\hat{h}^{i} = \\begin{bmatrix} \nh_{11} &amp; h_{12} &amp; h_{13} \\\\\nh_{21} &amp; h_{22} &amp; h_{23} \\\\\nh_{31} &amp; h_{32} &amp; h_{33} \\\\ \\end{bmatrix}  $\n\n14. &nbsp; &nbsp; $\\hat{s}^{i} = h^{i} \\bar{\\sigma} [h^{i}]^{t} + q $\n\n15. &nbsp; &nbsp; $k^{i} = \\bar{\\sigma} [h^{i}]^{t} [s^{i}]^{-1}$\n\n16. &nbsp; &nbsp; $\\bar{\\mu} = \\bar{\\mu} + k^{i}(z^{i}-\\hat{z}^{i}) $\n\n17. &nbsp; &nbsp; $\\bar{\\sigma} = (i - k^{i} h^{i}) \\bar{\\sigma} $\n\n18. endfor\n\n19. $\\mu = \\bar{\\mu}$ \n\n20. $\\sigma = \\bar{\\sigma}$\n\nplease suggest me other books that discuss ekf localization in depth. \n", "tags": "localization ekf", "id": "3104", "title": "ekf localization known correspondences"}, {"body": "i'm a noobie just starting out and trying to come up what i need to build my first quadrocopter, i just wanted to run something by people with some experience before i commit to buying anything.\n\nwould this esc be fine for running this motor? as i understand it the esc should be rated for slightly above what the max amps are for the motor?\n\non top of that, should this battery be able to run all of the motors without any issue?\n", "tags": "quadcopter brushless-motor multi-rotor esc battery", "id": "3105", "title": "will a 20amp esc run a turnigy 2213-980?"}, {"body": "how might i be able to control one function (like brightness control of an led) with two different triggers (like a tactile switch and an ir remote)?\n\ni am trying to be able to control the brightness with switches as well as ir remote when desired.\n", "tags": "control", "id": "3108", "title": "how to control a function with two different inputs"}, {"body": "i'm going to build a small robot system and it seems like that ros serves a nice framework to control and program the system.\nhowever i am wondering which is the best practice to manage the components of my robot. does it make sense to put all the sensors in one node?\nshould i only put the sensors of the same type in one node or is it better so have one node for one sensor? \nis it a good practice to have some kind of handler node, which takes input from sensors and steers the corresponding actuators or should the actuator nodes and sensor nodes communicate directly?\n\n\nfused sensor nodes and actuator nodes with handler\n\nsingle sensor and actuator nodes with handler\n\ndirect communication\n\n\n\nfor me i guess the best is to have some kind of handler, which handles the communication between sensors and actuators and have one node for each element of the robot (like in fig.2). because like that the system loosely coupled and can be extended easily, but i want to know what your opinion is.\n\ngreetings\n", "tags": "control ros", "id": "3110", "title": "ros: best practices?"}, {"body": "i had just tested my first monitor, which results in the following error\nregarding the suggestion in how to define conditions for state-machines in roby?\n\nunfortunately i ran into a runtime error, i don't know whether this is a bug or if i misuse the monitor...\n\n\n\ndon't know whether this is a bug, or if i had miss-used the monitor... \nhere is the action_state_machine i'm using:\n\n\n", "tags": "rock syskit", "id": "3116", "title": "problems using syskit monitors -> failed emission of the foo event of"}, {"body": "i'm looking for a device that can push out independent pinpoints from something similar to a pin point impression toy. i'm looking to create a 3d image from for example my computer. does anybody know the name of such a device or can point me in the right direction of making one? \n\ni've been looking now for a while, but i'm having some slight problems finding a good way to describe it as a search term.\n\ni'm sorry if this is the wrong forum.\n", "tags": "3d-printing", "id": "3117", "title": "device that can push out independent pin points?"}, {"body": "let's say i have a hypothetical sensor that provides, for example, velocity estimates, and i affix that sensor at some non-zero rotational offset from the robot's base. i also have an ekf that is estimating the robot's velocity.\n\nnormally, the innovation calculation for an ekf looks like this:\n\n$$ y_k = z_k - h(x_k) $$\n\nin this case, $h$ would just be the rotation matrix of the rotational offset. what are the ramifications if instead, i pre-process the sensor measurement by rotating $z_k$ by the inverse rotation, which will put its coordinates in the frame of the robot? can i then safely just make $h$ the identity matrix $i$?\n", "tags": "kalman-filter ekf", "id": "3118", "title": "observation model jacobian for fixed transforms"}, {"body": "where can i buy multi-directional omni wheels?\n\ni'm specifically looking at something which can support in excess of 100kg/wheel, so around 400kg in total. also, a possible mission profile would include a 300 meter excursion outdoors on asphalt path, so they should be a little durable. the only ones i can find online are small ones for experimenting.\n\n\n", "tags": "wheel", "id": "3121", "title": "where can i buy heavy-duty omni wheels?"}, {"body": "it is \"good enough\" for pid output directly controls, without further modelling, the pwm duty cycle?\n\nlogic behind the question is, \n\nin case of pure resistance heater, pwm duty cycle percentage directly relates to power (on off time ratio). so, direct control is appropriate.\n\nhowever, motor has two additional effects,\n\na) with considerable inductance, initial current is smaller and ramping up over time\n\nb) as rpm gradually ramping up, after time constant of mechanical inertia etc, increasing back emf will reduce current\n\nwill it be wise to ignore the above two effects and still expect a reasonably good outcome? \n\napplication is 6 volts, 2 watt dc brushed motor, gear 1:50, 10000 rpm no load, pwm frequency 490hz, driving diy 1kg robot.\n", "tags": "motor pid pwm", "id": "3124", "title": "pwm pid control for small 2 watts brushed dc motor"}, {"body": "i\u2019m a graduate student, and we're doing a project that is going to introduce a robot arm into manufacturing. our goal is to build up an autonomous object classification system. we already have the software and hardware required for the task, but we have no idea if there is any existing manufacturing scenario where we can apply the system and really improve the efficiency or save human resources.\n\nhere is some info about the robot arm:\nfor the hardware part, the robot arm is with 7 dof and 5kg payload (the weight of the end effector is not counted). besides, the end effector is a 1.5kg 3-fingered robot hand with 2kg payload. the workspace is approximately a sphere with 0.9m diameter.\n\nfor the software part, we have programming by touch, by which human can drag the robot and record the desired pose. besides, we have pcl object recognition that can recognize the object and its pose in the scene. lastly, we have online trajectory generator and dynamic obstacle avoidance that can improve the safety when the robot corporates with human.\n\nsince we know few about manufacturing, we hope that someone can give us a hint about the scenario and an actual application where we can apply this system. \n", "tags": "robotic-arm manufacturing", "id": "3125", "title": "what is the actual application for manufactuing, with robot arm autonomous object classification system"}, {"body": "i'm getting this warning from matlab about kalman gain. \n\n\n\nthe problem is coming from high variance of the measurement model. my question is here does ekf work with high noise in sensor?\n", "tags": "localization ekf", "id": "3132", "title": "ekf localization is approaching singularity. are my sensors too noisy?"}, {"body": "following my previous question about pcduino+kinect, decided to go ahead and buy the pcduino i wish to run my robot with (kinect+pcduino+shields). however i'm having trouble getting started: i tried installing openni, nite and sensorkinect however openni installation fails (i haven't even gotten to installing nite and sensorkinect yet so no idea if that would work). i tried a bunch of pointers (here and here). for example the error i get if i follow link 1 is:\n\n\n  ubuntu@ubuntu:~/kinect/openni/platform/linux/createredist \\$ sudo ./redistmaker.arm \n  \n  target: linux-arm\n  \n  version: 1.5.7.10\n  \n  num of compile jobs: 0\n  \n  \n  building openni...\n  \n  \n  common/commondefs.mak:36: * cross-compilation error. can't find arm-j1_cxx and >arm-j1_staging.  stop.\n  \n  failed to execute: make platform=arm-j1 -c \n  \n  /home/ubuntu/kinect/openni/platform/linux/createredist/../build clean  \n  \n  /home/ubuntu/kinect/openni/platform/linux/createredist/output/buildopenni_clean.txt\n  \n  cleaning failed!\n  \n  ubuntu@ubuntu:~/kinect/openni/platform/linux/createredist$ \n\n\nafter someone suggested it, i tried removing the -mfloat-abi=softfp option but that didn't help. there seems to be some compiling/linking issue due to float types which i'm not able to figure out. in link 1 the author mentions to remove the 'calc_jobs_number()' but that does not work and i get similar error. also similar problem exists for link 2 above\n\nif i follow link 2, 'make' won't work and will give the following error:\n\n\n  /usr/bin/ld: error: ../../bin/arm-release/libopenni.so uses vfp register >arguments, ./arm-release/tinyxmlparser.o does not\n  /usr/bin/ld: failed to merge target specific data of file ./arm-release/tinyxmlparser.o\n\n\nanother approach would be to use simplecv instead of openni on pcduino as someone else claims it has worked before. however i've never used simplecv with kinect before so unless it's not radically different i prefer using openni.\n\nany suggestions as to why i might be getting these errors are appreciated. any other pointers for solving the problem of installing openni on pcduino would be welcome.\n\nplease let me know if you need more details about anything else.\nthanks in advance\n", "tags": "kinect arm openni", "id": "3133", "title": "installing openni on pcduino"}, {"body": "i installed simplecv and libfreenect on pcduino (running lbuntu). i separately verified that simplecv reads my usb webcam well and libfreenect (glview tutorial) gives me depth and rgb correctly, albeit and a pathetic framerate. what i want is to call cam = kinect() in simplecv but when i do that, i get the warning \"you dont seem to have the freenect library installed. this will make it hard to use a kinect\". although this is a warning i get an error if i then do cam.getdepth(), which says \"nameerror: global name 'freenect' is not defined\".\n\nhow do i let simplecv know that i've installed libfreenect?\n", "tags": "kinect", "id": "3134", "title": "libfreenect simplecv integration?"}, {"body": "i'm making a quadcopter. i have set up a pid loop to stabilize it to a given euler angle (pitch and roll). the problem arises when the roll approaches 90 degrees (45 degrees and up). the values don't make sense anymore, as it approaches the gimbal lock. i intend to make it do complex maneuvers like looping etc., which exceeds the 45 degree roll limit.\n\nhow can i use quaternions to overcome this problem? (i get quaternions from the mpu-9150.) i have read many articles on the matter of quaternions, but they all talk about rotations in 3d software, and tweening between two rotation points. this makes little sense as i do not know imaginary numbers and matrices.\n", "tags": "quadcopter pid stability", "id": "3137", "title": "how to use quaternions to feed a pid quadcopter stabilization loop?"}, {"body": "there are two different conventions that can determine dh parameters.   what is the difference between craig's [1, sec 3.4] convention and the spong [2, sec. 3.2] convention?\ni know that both methods must have the same response.\n\n[1]: craig, john j. introduction to robotics: mechanics and control. addison-wesley, 1989.\n\n[2]: spong, mark w., seth hutchinson, and mathukumalli vidyasagar. robot modeling and control. wiley, 2006.\n", "tags": "forward-kinematics dh-parameters", "id": "3139", "title": "denavit-hartenberg convention"}, {"body": "let me start off by saying that i am currently going to university majoring in computer engineering.  i love software/hardware and i especially love robotics and i want to apply my knowledge of software/hardware in robots.  i have never taken a formal class on robotics, so i don't really know where to start or how to approach the mathematics that robots entail.  \n\ncurrently, i am interested in calculating the inverse kinematics of a delta robot. to clarify a bit more, i am trying to determine the required joint angles that will position the end-effector of the delta robot to a specific location given some x,y,z coordinate.  the delta robot that i will be basing my design off of is shown in the image below.\n\n\n\nbased off of some research that i have been doing for the past few days, i found that the sort of mathematics involved are usually like those of denavit-hartenberg parameters, jacobian matrices, etc.  i am going to be honest, i have never encountered denavit-hartenberg parameters or jacobian matrices and i don't even know how to apply these to solve the kinematics equations and let alone find the kinematics equations.  most of the articles that i have read, mainly deal with serial manipulator robots and the mathematics in finding the kinematics equations of those serial manipulators.  i couldn't really find any good material or material that was easy to understand given my current situation on parallel manipulators.  \n\ni wanted to ask my question here in the hopes that someone in the community could direct me to where i can start on learning more on obtaining the inverse kinematics equations of parallel manipulators and solving those equations.\n\nany help will be much appreciated.\n\nthank you.\n", "tags": "kinematics inverse-kinematics", "id": "3144", "title": "inverse kinematics of parallel manipulator (delta robot)"}, {"body": "i will have a belt-driven linear actuator, consisting a gantry-plate riding on two rails. i'm thinking of using a brushed dc motor.\n\nthe gantry will move from home position to the right (outbound) at 1m/s. the mass of the gantry will vary from 3kg to 6kg. on the return home (inbound) one must avoid spillage of contents which may require soft start/stop or simply a slow return to home.\n\nin the outbound case, what i'd like to know is how,in a practical sense, do you brake the mass and bring the gantry to a stop, ensuring that the gantry plate always comes to rest to within 0.5mm of an end plate?\n\ni'm clearer how i can ensure the gantry stops to within 0.5mm of the home position, because i can use a pwm ramp to slowly decelerate.\n\ni'm wanting to avoid using an mcu. just want to use an ic with switches and potentiometers.\n\nyou can also use math if you want to explain.\n\nof course, one seeks to begin to arrest the mass as close to the end stop in the outbound case as one can without problems.\n\nthanks.\n", "tags": "motor", "id": "3145", "title": "how to brake a brushed dc motor, belt-driven linear actuator to within 0.5mm of an end stop"}, {"body": "i am using a ruby script to connent the multi layer surface map of the velodyne_slam component to the vizkit3d visualization.\n\nthe visualizazion plugin is loaded like this:\nenvireviz = vizkit.default_loader.envirevisualization\n\nit is possible to get the mlsvisualisation object from the envirevisualization in order to set visualization properties (like colors etc.) from the ruby script?\n\nrubys introspection abilities didn't help a lot here...\n", "tags": "rock", "id": "3150", "title": "rock envire - vizkit3d : change environment visualization (envire lib) from ruby script"}, {"body": "i need to search in the git history of a couple of packages to \nget back to a working state for a demo. i am searching by checking out \ncommits manually until i found the commits of all effected packages that \nwork together.\n\nby checking out commits manually, i will get into the detached head state:\n\n$ git checkout 995e018\n-> you are in 'detached head' state. [...]\n\nto save the current state of all packages, a snapshot is created:\n\n$ autoproj snapshot demo_working\n\nnow the demo_working/overrides.yml will pin the commit where the head is \npointing to (e.g. 5e2e3a259) instead of the commit that i chose manually \nfor the package (995e018).\n\nis this the desired behaviour? in my opinion a snapshot should store the \ncurrent state of all my git repositories meaning that i can also select \ncommits manually.\n", "tags": "rock", "id": "3152", "title": "autoproj snapshot with git detached head"}, {"body": "i need to get position $x$ from integrating velocity $v$. one could use 1st order euler integration as\n\n$x_{t+1} = x_t + \\delta * v_t.$\n\nhowever, doing so leads to errors proportional to sampling time $\\delta$. do you know any more accurate solution please?\n", "tags": "kinematics", "id": "3155", "title": "one of the best ways to numerically integrate the velocity?"}, {"body": "i just got an irobot icreate base and i've followed the instructions given in  ros tutorials to setup the turtlebot pc and the workstation. i could successfully ssh into username@turtlebot through workstation so i'm assuming that is all good. i had an issue with create not able to detect the usb cable which i solved using the detailed answer given for question here. this solved the problem of \"failed to open port /dev/ttyusb0\" that i was facing before. \n\nnow the next step would be to ssh into the turtlebot (which i've done) and use  to do whatever the command does (i've no idea what to expect upon launch). but apparently something's amiss since the create base chirps and then powers down after showing  as output and the log file location (see output below), but i dont see a prompt. i checked the battery and that's charged so that's not the problem. following is the terminal output.\n\n\n\nfollowing is the log file: /home/anshul/.ros/log/9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9/kinect_breaker_enabler-5*.log output:\n\n\n\nfrom the logs, i could tell something told the create to power down. and since the log is named with 'kinect', i tried minimal.launch w/ and w/o kinect attached to the turtlebot pc. it doesn't make any difference. \n\nany clue what i might be missing? or is this the way bringup works (i guess not)?\n", "tags": "ros irobot-create", "id": "3159", "title": "cannot launch irobot create. powers down upon minimal launch?"}, {"body": "is it possible to control the create without using any ros whatsoever? i know it has all these serial/digital i/o pins that connect to ros which controls it using drivers/libraries. but how hard would it be to do so using, say, a pcduino?\n\ni'm asking this because i'm having trouble launching the create using ros (question)\n", "tags": "irobot-create", "id": "3160", "title": "irobot create without ros?"}, {"body": "i have a bs2 mounted on a parallax board of education rev d.\ni was trying to use a wire to determine whether a control was pressed. \nhowever, whenever there's a wire connected the state seems to fluctuate between 1 and 0 instead of staying one or the other. when connected to the desired button it still exhibits this behavior but has the added quality of switching to zero when the button is pressed. ideally it will stay zero while the buttons pressed and 1 when it's not, but instead it flickers between 1 and 0 when unpressed. \nwhat causes this behavior and why does it occur even when the wire is not connected to anything except the bus?\nthe code used to get the state is \n\n\n", "tags": "microcontroller", "id": "3162", "title": "bs2 inconsistant pin state when connected to wire?"}, {"body": "i am using autodesk inventor 2013 and i need to round a component of a device. i want to round the green marked edges, but not the red marked. but when i click \"round\", then the bottom edge will always be added to the rounding and i cannot de-select it. any hints how to solve this problem?\n\n\n", "tags": "design", "id": "3164", "title": "autodesk inventor 2013: rounding only at specific edge"}, {"body": "this question is an extension to my previous problem (data association with ekf). my problem here is in the line 16 in the aforementioned link. \n\n16. &nbsp; &nbsp; $ j(i) = \\underset{k}{\\operatorname{arg\\,max}} \\ \\ det(2 \\pi s^{k})^{-\\frac{1}{2}} \\exp\\{-\\frac{1}{2} (z^{i}-\\hat{z}^{k})^{t}[s^{k}]^{-1} (z^{i}-\\hat{z}^{k})\\} $\n\nwhen i compute this line, i'm getting huge number . this is probability density function. why is the pdf getting bigger than 1 in a huge way?\n", "tags": "localization ekf", "id": "3166", "title": "maximum likelihood estimator (ml data association) ekf"}, {"body": "problem: the cartesian position of an end effector (no orientation) of a robot arm is recorded, say, every millisecond (the time steps can not be changed), during a motion. the robot arm is commanded the same path but with different velocities. so i get different trajectories. i want to calculate the deviation of the paths, which is the distances of equivalent points of two paths. the problem is to find equivalent points. since the two velocities are different the comparison at the same time steps of the trajectories makes no sense. i can assume that the paths underlying the trajectories to be compared are rather similar. the deviation for the ideal path being smaller than 1% of a typical length dimension of the path. i want to detect deviations of much lass than that. \n\ni have to map the timestamp of the recorded points to the path length, and make comparison of points at the same path length. but of course also the path lengths differ for different paths, so any deviation would distort the result for all later points. how can i compensate for this ? \n\nis there a reliable algorithm ? where can i find information ?\n\nnote: time warp algorithms (even memory optimized ones) are out of the game because of memory consumption. \n", "tags": "localization robotic-arm", "id": "3170", "title": "path comparison"}, {"body": "how do you make a gripper changer for a robotic arm like this? i don't see how you could connect power/control wires or what you use to hold the gripper to the arm.\n", "tags": "robotic-arm", "id": "3172", "title": "making a gripper changer for a robotic arm"}, {"body": "i am working on a robot that has an accelerometer. this accelerometer measures the vibration of the robot. when the robot hits a certain vibration, i would like it to slow down in order to reduce the vibration. i thought about a pid controller, but i don't think it would work. does anybody have some input on different types of controllers i can use? \n\n\nmechaman\n\n", "tags": "pid accelerometer navigation", "id": "3173", "title": "velocity control via vibration"}, {"body": "i'm controlling the angular position of a pendulum using a dc motor with a worm gearbox. mechanically, worm gears are impossible to backdrive.\n\nusing a pid controller on a pendulum system with a regular dc motor (no worm gear), the integrator would help the motor find the appropriate constant power setting to overcome gravity so the pendulum can hold any arbitrary position. with the worm gear, however, there is no need to apply constant power to the motor once the desired position is achieved. power to the motor can be cut off and the worm gear will resist gravity's force to backdrive the pendulum to the lowest gravity potential.\n\nit seems to me, then, that the integrator of the pid algorithm will cause large overshoots once the desired position is achieved. i want the integrator initially to help control the pendulum to the desired position. but once the position is achieved, i'd need the integrator to turn off.\n\nthe only solution i can come up with is to test for a special condition in the pid algorithm that checks if the position has been reached and the angular speed is small, then instantaneously reset the integrator to zero. is there a better way to handle the integrator in a system that resists backdrive?\n\n** edit *\n\nwhen i originally worded my question, i was mostly just interested in the academic approach of backdrive resistance in a pid loop. but it'll help if i explain the actual mechanism i'm building. the device is a robotic arm that rotates on a car window motor. it will also occasionally pick up and drop small weights at the end of the arm. manufacturing variability in motors and the difference in drive torque when picking up the small weights led to me consider a pid loop.\n", "tags": "pid", "id": "3178", "title": "controlling a system with pid that resists backdrive"}, {"body": "i want to calculate humanoid robot hand position with given shoulder roll, pitch angles and elbow roll angle.\n\ni'm able to calculate elbow position using rotation matrix which includes shoulder angles.\n\nbut i dont know how to calculate hand position using elbow position and elbow roll angle.\n\ncan you propose a method to calculate hand position? \n", "tags": "robotic-arm forward-kinematics", "id": "3181", "title": "how to calculate robot hand positions using roll, pitch angles"}, {"body": "is it possible to strengthen permanent magnet dc motors by simply attaching extra magnets on the outside of the motor casing - adding to the magnetic field?\n\nif this is not possible, the question becomes; what happens if i replace the magnets inside the motor with better magnets?\n\ni know that the coils will not handle more current than they currently do, so what will the net effect on the motor be?\n", "tags": "motor", "id": "3183", "title": "adding external magnets to a dc motor"}, {"body": "i am trying to build a 2ft square an xy plotter. i have seen three designs so far: 1)rack and pinion 2)threaded screw 3) belt-driven. all these use a stepper motor to drive the system.\n\neach one has their obvious pros and cons but correct me if i am wrong, i believe the rack and pinion system is the most sturdy and easiest to put together.\n\ni googled for rack and pinion but all i get is industrial websites. is there any place that sells cheaper rack and pinion sets for hobbyists? the payload of the xy table is an eletro-magnet that isn't extremely heavy (maybe a half kilogram at most).\n\nso obviously the motor must be strong enough to move anothe rack which will be significantly heavier than the payload.\n\nthis is my first real robotics project so i am new to all this.\n", "tags": "stepper-motor motion actuator", "id": "3188", "title": "where to go to purchase parts for xy plotter"}, {"body": "i want to implement a manipulator link using a physic library. i can only apply some torque to the centre of mass, but the torque should be applied at the beginning of the link.\n\nshifting a reference frame from the centre of mass and recalculating inertia tensor in the new frame is not a problem, neither is recalculating a new torque, based on the change of distance, but i think it is not the correct solution.\n\nin short, how can i scale a torque of a control signal applied at the beginning of the link to a torque of a physic simulation applied to the centre of mass. thanks.\n", "tags": "simulator torque", "id": "3200", "title": "manipulator link applied torque"}, {"body": "i have been working on a robot project for a while. now i am tired of finding parts that just does the job, so it is time to do create parts.\n\na 3d printer will do the trick for many parts, but 3d printers share a lot with a cnc mill in terms of control and parts. so my question is this:\n\ni am building a reprap style printer, but i will use more heavy duty parts and motors, hoping to make a aluminum capable 3 axis mill later. i found some bipolar nema 23 stepper motors at 1.9 nm and 3 amps per coil. according to the reprap.org website, they recommend nema 17 and low voltage. seems to me that they use voltage to limit the current.\n\ncan i build a reprap, and use current limiting stepper drivers with an arduino and some software i find online, and get away with these large stepper motors? or am i in for a lot of trouble?\n", "tags": "arduino stepper-motor stepper-driver cnc reprap", "id": "3206", "title": "current-limiting stepper motors for reprap"}, {"body": "i made several tests with different setups in order to achieve an acceptable speech recognition quality. it works well when i push a button to activate it but now i want it to be automatically activated when a user speaks. this is a big problem, especially when i only use the energy of the audio signal to guess when the user is speaking. that is why i thought about using a headset and not a distant microphone. in a headset the microphone is very close to the users mouth and it is easier to make correct guesses about when the user is speaking. now my question is if bluetooth sets used with mobile phones also have such a property. they are not long enough and their microphone is not positioned exactly in front of the mouth. is there a possibility that such devices can also capture some speech/noise from a distant user? is there a significant difference in the signal energy coming from the user's speech and a 1 meter distant person's speech?\n", "tags": "digital-audio speech-processing", "id": "3207", "title": "best microphone for speech recognition tasks"}, {"body": "i am a programmer who has never worked with electronics before. i am learning the concepts and hoping to build a quadcopter, with the control software entirely written by me. motor control seems to be the most important part.\n\nis it true that the typical brushless dc motor and esc (electronic speed control) can only approximately control the speed?  that's because the esc seems to have only a very approximate idea how fast the motor is revolving. this still works for a pid (proportional integral derivative) controller because it gets indirect feedback from say a gyroscope whether the motor is going fast enough and so it can tell the esc to make it revolve \"even faster\" or \"even slower\", and that's good enough.\n\nis my understanding in the above paragraph correct?\n\nif so, i wonder whether a servo motor that can inform about its current rate of rotation could help do away with the esc entirely? i feel that if the microcontroller can receive an input about motor speeds and send an output requesting a certain speed, it would not need the esc. but i am not sure how servo motors work -- what happens immediately after you request 100rpm when say they were at 80rpm?\n\nsince they cannot adjust the immediately, should the microcontroller immediately adjust other motors to account for the fact that not all motors are at 100rpm yet?  does that imply that the microcontroller should only request very small deltas from the currently measured speed, so that the period of deviation from desired state is negligible?\n\nin the latter model, of requesting only very small deltas from currently measured speed, the algorithm seems like it would not really be pid since there is no way to control the acceleration? but may be requesting the servo to go from 80rpm to 100rpm causes it to reach 81rpm much faster than requesting it to go from 80rpm to 81rpm?\n\ni feel i know so little i cannot put my finger on it more precisely, but i hope this gives an idea of the concepts i am struggling to absorb.\n\nto summarize, the questions are:\n\n\ncan a servo (brushless dc) motor allow doing away with esc?\ndoes a servo motor accept control inputs such as \"revolve at 100rpm\"?\ndoes a servo motor offer an output saying \"i am at 80rpm now\"?\ndoes a servo motor at 80rpm go to 81rpm faster if it is requested to revolve at 100rpm versus at 81rpm?\nthe less precise questions implicit in the text above.\n\n\n(crossposted from electronics.stackexchange)\n", "tags": "motor pid brushless-motor esc servomotor", "id": "3208", "title": "electronic speed control concepts"}, {"body": "i'd like to drive the position of various components within a virtual assembly based on sensor data being collected in real time from an external device. does inventor support such a setup?\n\nthe goal is to match the relative movements of the components on screen to the real-world counterpart. for example, a absolute rotary encoder records the current angle of a physical joint and the virtual joint is rotated to match. is this feasible?\n\nmy past searches for information on this have turned up empty; perhaps because i'm using the wrong search terms. most results point to irrelevant mechanical stress simulations.\n", "tags": "sensors kinematics", "id": "3209", "title": "can active sensor data be fed into an autodesk inventor simulation?"}, {"body": "i'm thinking of starting my adventure in area of professional manufacturing. when i started to look onto machines i figured out that they are build somehow like in the 70s: huge footprint, big 3kw electric motors etc.\n\nis there any explanation why they are build in that way?\n\nthe only one i can think of is: they were developed long time ago and if it worked, it stays as it is.\n\nbtw: if you know other place where to ask this question please let me know!\n", "tags": "manufacturing", "id": "3210", "title": "why production lines are so huge and power hungry?"}, {"body": "i came up with an idea and am working with a mechanical engineer to design and prototype the idea but i keep sketching out my own ideas in the process and i just came up with this.. i'm quite sure this is not an idea he'll go with but i'm just kinda curious whether or not this would actually be feasible.  or for all i know it's already common place, or totally stupid...  i dunno.\n\nwhat do you think?\n\n\n", "tags": "mechanism movement", "id": "3216", "title": "is this gear design feasible?"}, {"body": "i have a chinese cnc mill (cnc3020t, though several different devices go under this name), and its z axis was very imprecise, often being randomly off position by as much as 0.5mm. i've disassembled the linear actuator and discovered several problems with it.\n\nfirst problem is that they apparently forgot to lubricate the linear ball bearings. i make this conclusion because the rails have a set of grooves ground into them, and after wiping the rails with a tissue the only thing that is left is the finely powdered metal, with no traces of oil or other lubricant.\n\n\n\n\nsecond problem is the nut. i expected to see a ballnut, but in reality it is just a piece of threaded ptfe! the leadscrew rotates smoothly in it, but there is quite some lateral movement, i.e. i can tilt it slightly without any opposing force.\n\n\n\nthird problem is the overall mounting. in the picture below, the top left screw has been sheared in the factory and then they hid their mistake by tapping a larger thread and putting in a shorter screw that doesn't actually hold anything in the top plate.  so the whole assembly was fixed in three, rather than four, points. however, the remaining screw was quite tight.\n\n\n\nso my closely related questions are:\n\n\nis the assembly even salvageable? how do i verify that linear ball bearings, the ptfe nut are relatively undamaged?\ncan i just rotate the rails by 45\u00b0 to get smooth surface again?\nwhat do i lubricate the linear bearings with? do i clean them before lubrication? i have an ultrasonic cleaner.\nany other advice on maintenance of the whole assembly? there may be something that i missed.\n\n", "tags": "actuator linear-bearing", "id": "3218", "title": "repairing non-lubricated linear actuator"}, {"body": "suppose we have a moving object (a horizontal projectile motion as one of the most basic examples). is there any way to predict where it will hit finally? please note that i'm looking for a machine learning method not a closed form solution.\n\nalthough we can track the motion, using kalman filter, that is only applicable when we want to predict the new future(as far as i'm considered). but i need to predict the ultimate goal of a moving object.\n\nto better express the problem let see the following example:\n\nsuppose a goalkeeper robot that of course uses filtering methods to smooth the ball motion. it needs to predict if the ball is going to enter the goal or not, before it decide to catch the ball or neglect it to go out.\n\ninput data is a time series of location and velocity [x,y,z,v].\n", "tags": "machine-learning", "id": "3222", "title": "predicting the impact point of a moving object"}, {"body": "i want to make a simple device that causes my cellphone to vibrate for 30 seconds when my phone is 10 feet away from it. how would i go about doing that. how small could i make the device?\n", "tags": "arduino", "id": "3225", "title": "android vibrating based on arduino devices"}, {"body": "i'm researching potential actuators i can use on a project i'm doing. i'm designing a creeper (platform for rolling under vehicles) that can lift you up just like the operation of those hospital beds. the creeper will have a joystick that will control up and down motion as well as the option to drive it forward,backwards,left and right.\n\ni need an actuator that will support an average weight of 250lbs that would be able to lift a body of that weight. i was thinking of a hydraulic actuator but i'm not sure if these exist. i can very well have two actuators to share the load also. however, i need to control these actuators through a micro-controller unit. i'm planning on using a raspberry pi because i have an abundance of them mainly, but i'll be researching other potential units. \n\ntherefore, my main question is where can i find an actuator that would be a good fit for this type of project that can be integrated with a micro-controller unit? does anyone have experience with this type of project or any important details i need to take into consideration that i'm not thinking of?   \n", "tags": "microcontroller actuator", "id": "3227", "title": "finding a hydraulic actuator to be controlled through a mcu"}, {"body": "i'm working on an application where i need to apply a linear or angular force to operate a linkage mechanism, but i don't (yet) know what amount of force i will need. i anticipate that it will be less than 4.5 kg (44 n). the travel distance on the linkage input should be less than 15 cm.\n\nas i look through available servos, they seem to exist firmly in the scale-model realm of remote control vehicles, and as such i am uncertain if any will be suitable for my application. for example, one of futaba's digital servos, the mega-high torque s9152, is listed at 20 kg/cm.\n\nfrom what i understand, this means that at 1 cm from the center of the servo shaft, i can expect approximately 20 kg force. if i wanted 15 cm of travel distance i would need roughly a 10.6 cm radius, which would diminish the applied force to 20 / 10.6 = 1.9 kg, well below the 4.5 that might be required.\n\nquestion:\n\nis my understanding and calculation even remotely accurate? should i be looking at other types of actuators instead of servos? they seem to become prohibitively expensive above 20 kg/cm torque. (for the purposes of this project, the budget for the actuator is less than $250 us.)\n\nfor my application, i'd like to have reasonable control over intermediate positions across the travel range, good holding power, and fairly fast operation. for this reason i have dismissed the idea of using a linear actuator driven by a gearmotor and worm drive.\n\ni am relatively new to robotics in the usage of motorized actuators, but i've used pneumatic cylinders for many years. for this application, i can't use pneumatics.\n\nedit:\n\nper comments, some additional constraints that are important:\n\n\nlinkage details: the linkage is a planar, one degree-of-freedom, part of a portable system (similar to a scissor lift mechanism). it is for a theatrical effect where the motion is amplified and force reduced (speed ratio and mechanical advantage are &lt; 1).\npower: it will be carried by a person. as such, the actuation needs to be battery-operated, as no tubing or wiring can tether the person. tubing or wiring that is self-contained is okay. because this is a portable system, battery-power will be used. the control system will be designed specifically for an appropriate actuator. rechargeable batteries up to 12v will most likely be employed. actuators could operate on as high as 24v. ideally a motor would not exceed 1-2 amperes draw, but as it is not in continuous operation, this is not a hard limit.\nnot pneumatic: i've considered pneumatic actuation, using co2 cartridges, for example, but the client would prefer not to use pneumatics. also, the ability to stop/hold at intermediate points in the motion range is desirable, and somewhat more complicated to do with pneumatic actuators.\nspeed: an ideal actuator will be able to move the input coupling 15 cm in 1-2 seconds.\nweight: weight constraints are not well-defined. as it will be carried by a person, it should be moderately lightweight. the actuator itself should probably be less than 1kg, but certainly this can vary. (the rest of the mechanism will probably be 6-8 kg.)\nsize: the primary size constraint is that everything must fit within a space measuring no more than 500 x 500 x 120 mm (h x w x d). the linkage mechanism extends from and collapses outside the enclosure, parallel to the width.\nnoise: the quieter the better, but noise is the least priority.\n\n\nservos seemed like the best choice for the job, but they don't seem to be available with the sort of torque i need.\n", "tags": "servomotor", "id": "3231", "title": "up to what force is a servo motor a reasonable choice as an actuator?"}, {"body": "i'm trying to increase the torque on the output shaft of my robot's gearbox.  i have a motor with a pinion attached to it with 8 teeth.  i want to create a gear with 33 teeth that will mesh with the pinion that i currently have.  i've got access to a 3d printer to make the gear, but i don't know how to design the second gear so that it will mesh properly.\n\nwhat parameters do i need to know about the first gear (8 teeth) to ensure that the second gear (33 teeth) will mesh correctly?  how do i translate these parameters into the design of the second gear?\n", "tags": "motor differential-drive", "id": "3233", "title": "designing compatible spur gears for a robot gearbox"}, {"body": "is there any well documented robot interaction language? i would imagine something like taking a user's speech in english, parsing it using some natural language processing like nltk or stanford nlp and then building a new sentence understandable by the robot. does something like this already exists?\n\ni recently found roila http://roila.org/language-guide/ but it seems like it is a whole different language and not just a reformulation of sentences using english words with less grammatical complexity.\n", "tags": "speech-processing", "id": "3234", "title": "robot interaction language"}, {"body": "i am in charge of studying passage of different species of fish (six species) between lakes in patagonian andean range. we've been thinking of deploying video cameras underwater, but we'd need software that would control the cameras and record images only when the video adequately changes so as to avoid having to continuously check the video.\n\nif the software is also capable of recognizing the species that would even be better.\n", "tags": "software", "id": "4238", "title": "i need a software that will help me track passage and identify fish in clear water"}, {"body": "given a pid controller with an anti-windup, what are some practical ways to retune the controller once oscillation has been caused and detected? i have access to the magnitude and period of the oscillation.\n\ni don't want to use the ziegler-nichols method; rather i'd like a method that allows me to specify a phase/gain margin as i am returning the system. \n\ncould someone recommend me towards a book/article or theory?\n", "tags": "control pid", "id": "4239", "title": "good method for retuning a pid after detecting oscillation"}, {"body": "the propellers of a multicopter produce thrust. unfortunately the thrust is the smaller, the more the copter is tilted. i was currently wondering whether there is an established method to calculate how much the overall thrust has to be modified to hold the current altitude, based on the current attitude.\n\nthis is the way a calculate the motor output so far. rol/pit/yaw-output already ran through the pids. \n\n\n", "tags": "quadcopter", "id": "4247", "title": "tilt-compensated motor output to keep altitude for quadcopter"}, {"body": "is there any software where i can simulate production line elements (joints, motors, springs, actuators, movement)? for example i want to simulate mechanism to unwind paper from big roll to weld it later with bubble foil and finally make bubble foil envelope, mechanism will look like this:\n\n\n\ni need it as simple as possible and preferably free.\n", "tags": "mechanism simulator", "id": "4255", "title": "software to simulate mechanics of production line"}, {"body": "i have a differential drive robot whose motors are virtually quiet while driving on a completely flat surface, but the motors make a lot of noise when on a incline. this is likely due to the correction required to maintain speed with the high inertial load where the robot cannot accelerate fast enough for the pid to keep up.\n\nbut i noticed that some of the noise is related to acceleration, and the higher the acceleration, the smaller the amount of noise i hear, or the smaller the time the same level of noise lasts (up to a certain acceleration limit, otherwise the motors get really noisy again).\n\ni am trying to find out of how to use an imu that i have a available in order to change the acceleration based on how steep the path's incline is.\n\nany documentation (papers, tutorials, etc) about motion planning related to this topic that you can point me to?\n", "tags": "ros imu differential-drive noise", "id": "4256", "title": "imu based acceleration parameters for differential drive robot"}, {"body": "i am doing stereo camera calibration as described in this blog post. i wonder i do not need to input camera baseline for the calibration. the fact probably goes back to some very basic mathematics of triangulation. can someone explain?\n", "tags": "computer-vision calibration stereo-vision", "id": "4261", "title": "stereo camera baseline not needed for calibration?"}, {"body": "i'm searching for a python toolbox/library to do visibility graph based motion planning. i have searched on the internet, but couldn't find anything. i'm probably missing out...\n\nis there any package, you can recommend me?\n", "tags": "motion-planning python", "id": "4263", "title": "visibility graph toolbox for python"}, {"body": "how to determine the limit range of end effector orientation (roll-pitch-yaw) at one specific point(xyz)?i had derived forward/inverse kinematic. i'm making a program for 6dof articulated robot arm so that the user can know the limit of tool rotation in global axis(roll-pitch-yaw) at a certain point.\n", "tags": "robotic-arm", "id": "4265", "title": "determing limits of rotation in a robot workspace"}, {"body": "is there a standard format of how stereo calibration data (various matrices, usually saved in xml) are stored? can i load calibration data generated say from a opencv script in c to another opencv script say in c++ or to completely different software where i create disparity image?\n", "tags": "computer-vision calibration stereo-vision", "id": "4266", "title": "are stereo camera calibration data standardized?"}, {"body": "i've been looking for ideas on how to launch a ping pong ball a small distance (&lt; 1 metre) for a game. solenoids look like they might be useful but i'm not 100% on what force/type i need. i can mount it under a base and have the balls roll over it, with a pin pushing the ball up a ramp to it's target.\n\nas it's only a ping pong ball, it should be light. i was considering something like this: http://www.adafruit.com/product/412\n\nam i along the right lines? or should i go back to the drawing board.\n", "tags": "motor", "id": "4270", "title": "solenoid to launch a ping pong ball"}, {"body": "i am currently building a line-following mobile robot. i've done all my image processing work in c#, and now i am in the control phase. i am looking for a pd controller program written in c# to start with. i've searched a lot but without success. my robot is not an arduino based, it has a motherboard with a core i3 cpu, and i am using a camera not an ldr sensor.\n", "tags": "control algorithm", "id": "4271", "title": "pd controller in c#"}, {"body": "a screw is defined by a six dimensional vector of forces and torques. it can represent any spatial movement of a rigid body (as written here). but i don't get the following distinction between screw and wrench: \n\n\n  the force and torque vectors that arise in applying newton's laws to a rigid body can be assembled into a screw called a wrench. \n\n\nit seems to be some kind of contextualisation but in what way?\n", "tags": "dynamics theory", "id": "4277", "title": "what is the difference between screw and wrench in rigid body motion?"}, {"body": "\n\nheld and rotated by the knurled ends, one in each hand, the silver spokes rise and fall in order for the assembly to rotate. what is it, some companies' salesmen show tool? found in an old building, unit has no markings.\n", "tags": "design joint", "id": "4281", "title": "what type of mechanism is this?"}, {"body": "i've successfully done with ekf localization algorithm with known and unknown correspondences that are stated in \"probabilistic robotics\". the results make perfect sense,so i can estimate the position of a robot without using gps or odometry. now i've moved to ekf-slam with known correspondences in the same book. i don't understand this matrix \n\n$$\nf_{x,j} = \n\\begin{bmatrix}\n1  &amp; 0  &amp; 0 &amp; 0  \\cdots  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\cdots 0 \\\\\n0  &amp; 1  &amp; 0 &amp; 0  \\cdots  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\cdots 0 \\\\\n0  &amp; 0  &amp; 1 &amp; 0  \\cdots  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\cdots 0 \\\\\n0  &amp; 0  &amp; 0 &amp; 0  \\cdots  0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\cdots 0 \\\\\n0  &amp; 0  &amp; 0 &amp; 0  \\cdots  0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\cdots 0 \\\\\n0  &amp; 0  &amp; 0 &amp; \\underbrace{0  \\cdots  0}_{3j-3} &amp; 0 &amp; 0 &amp; 1 &amp;  \\underbrace{0 \\cdots 0}_{3n-3j} \\\\\n\\end{bmatrix}\n$$\nwhat is exactly the bottom of this matrix? the following\n$$\nf_{x,j} = \n\\begin{bmatrix}\n0  \\cdots  0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\cdots 0 \\\\\n0  \\cdots  0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\cdots 0 \\\\\n\\underbrace{0  \\cdots  0}_{3j-3} &amp; 0 &amp; 0 &amp; 1 &amp;  \\underbrace{0 \\cdots 0}_{3n-3j} \\\\\n\\end{bmatrix}\n$$\nis it as following (assuming n = 3)\n$$\nf_{x,j} = \n\\begin{bmatrix}\n1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\\n0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1\\\\\n\\end{bmatrix}\n$$\nor \n$$\nf_{x,j} = \n\\begin{bmatrix}\n0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\\n\\end{bmatrix}\n$$\nwhere ones' represent a specific landmark. \n", "tags": "slam ekf", "id": "4285", "title": "sparse matrix in ekf slam"}, {"body": "i am working on estimating a robots pose using odometry and gps.\n\nmy first problem is that all kinematic model i have seen for a differential drive robot proposes using the displacement of the left and right wheels to evaluate the robots next pose. however, in my situation the robot i have only spits out current x and y pose relative to the starting point of the movement. can i use this as my state estimate\np = [x,y]t\n\np = [x0,y0] + [dx,dy] where dx and dy are change in respective coordinates gotten from the robots odometry.\n\nif the above is posible how do i calculate the state covariance q of the filter.\n\nfor gps, how do i evaluate the covariance r; i have tried to collect multiple reading of latitude and longitude from a fixed point but i dont know if this is righ and i just dont get evaluate the covariance from these data (feeling dumb).\n\nthank you in anticipation.\n", "tags": "mobile-robot kalman-filter gps odometry", "id": "4287", "title": "kalman filter issue - gps odometry fusion"}, {"body": "i'm a bit at my wits' end here - i'm trying to build a tilt compensated compass for my autonomous sailboat (ardusailor!). i'm using an invensense mpu9150. originally, i used the built-in fusion support on the sensor to get a quaternion, pull the yaw/pitch/roll angles from that, and then use this formula to do the tilt compensation:\n\n\n\nwhere the various s_angle is sin(angle) and c_angle is cos(angle). that didn't work. i tried using a vector-based approach stolen from here. that didn't work. then, i took away the tilt compensation, and just did an uncompensated atan2(yh,xh), and that produced very strange result as well. \n\nbasically, as i rotate the sensor about the z axis, the value rotates between 70 and -10 degrees, completing a full circle (i.e. as i make a 360 degree rotation, it starts at 70, gets to -10, and then back up to 70). 70 is at about 0* magnetic, 10 is at about 180, 0 is at about 70-80.\n\ni see the same behavior from an hmc5883l magnetometer chip as well. the thing is, looking at raw values, i get magnetic values that seem fine, and hard and soft iron offsets are in place:\n\n\n\ntop row is corrected for offsets (using an ellipsoid fit method), bottom is raw. the numbers may look skewed, but they aren't - the scales aren't all the same. graphs are, in order, x:y, y:z, x:z\n\nwhat could this be?\n", "tags": "compass magnetometer", "id": "4288", "title": "tilt-compensated compass - at my wits' end"}, {"body": "i'm facing a real weird problem with ekf localization. the filer gives me wrong error every time the robot is in parallel with a landmark. i've debugged the code many times but failed to solve the problem however i found out where is exactly the problem occurs. the following picture shows the scenario. the robot moves in a circular motion. there are four landmarks. i have indicted in the picture where the filer gives me wrong angle for the estimated state.  as you see, when the robot is in parallel with all landmarks, i got a wrong angle for the estimated robot's pose. \n\n\n\nthis is another picture shows how the estimated angle is wrong where the red circle is the estimated robot's pose and the blue one is the actual robot's pose. \n\n\n\ni did also track the problem numerically. what i found out is that the estimated measurement of landmark # 4 is in the opposite direction of the actual measurement of landmark # 4. \n\n\n\nand this is how i computed the angles. \n\nfor the actual measurements,\n\n\n\nfor the predicted measurements\n\n\n", "tags": "localization ekf", "id": "4295", "title": "ekf localization when robot is in parallel with a landmark."}, {"body": "firstly i'm unsure whether this question belongs here or on another se site (but i'll wing it for now).\n\ni've recently been given the job of connecting up a 'smart camera' to a setup where a robotic arm will pick and place objects from point a to point b. the real application for the camera is to check if the objects are out of alignment to their supposed positions.\n\nhowever i am curious to see if there is any way i can calculate the distance of an object given that i already know the objects actual size. naturally the camera will see the object as bigger when closer and smaller when farther away but how can i turn this information into depth/distance from the camera?\n\ni have not yet started using the camera. for now it is just an idea. i will assume that i can calculate what percentage of the view frame is taken up by the object.\n\nfor example if i have an object of uniform shape, i know that from dist1 it takes up 75% of the view frame and from dist2 it takes up 45% of the view frame.\n\n\nshould this prove to be possible i imagine that it could have a number of different applications. /anyway any feedback is appreciated. thanks! ( :\n", "tags": "localization calibration cameras", "id": "4296", "title": "calculate object distance with camera"}, {"body": "i am very new two robotics, however i have a working stereo algorithm which i want to combine with a slam algorithm. i am developing this system for an other application but i decided integrating it on a robot first and testing it might be a good way to get used to the system and test its behaviour in a realistic environment. (rather than testing it in some kind of software simulator only) however, i want the system to be autonomous and running on-board of the rover. \n\nthe system i am talking about will consist of:\n\na stereo camera\na rover with wheels with one motor each \npossibly some kind of sensor that \"measures\" the movement, e.g. how much the wheels turned\nmaybe some distance sensor \n\nappart from this it's only software\nthe stereo software is already developed, the slam algorithm not. therefore it is currently impossible to say how much ram it needs. i am currently running the stereo-vision only on an i7 in approx. 1s.\n\nnow my question:\nas mentioned i have no idea about robotics, and also my electronics knowledge is limited, so i have no idea what i need for this robot when it comes to the processor and the electronics. \n\ni read some stuff about the raspberry pi and arduino boards but i have no idea what to make from this. i am afraid that a arduino will not be able to handle the computational load of the stereo vision and the slam algorithm but i read that raspberry pis are not the first choice when interfacing with sensors is needed (in this case my stereo cameras). also i found the leika kit which is a robotics kit for the raspberry pi. maybe this would be a good option for me?\n\nmaybe an entirely different system would be even more advisable? \n\npossibly someone else build an equally complex system before and can give me some advise form his/her experience? \n", "tags": "arduino mobile-robot raspberry-pi slam stereo-vision", "id": "4297", "title": "which micro-controler/processor to be used for autonomous stereo vision robot system?"}, {"body": "i am using the l298n motor driver to drive two had1 linear actuators (12v each and a no-load drive current of ~950ma each)\n\nlinear actuator: http://www.alibaba.com/showroom/mini-linear-actuator-had1.html\n\nmotor driver: is a l298n dual-h-bridge motor-driver controller board-module for arduino robot\n\ni am also using a current sensor per motor to get feedback of what the motor is doing (only sensors i have available, but i can detect of the motors are moving or stopped). i am using two acs714 current sensors. the supply voltage for each is 4.5v to 5.5v and supply current is 10ma to 13ma:\n\ncurrent sensor: is an acs714 current sensor.\n\nand here is the circuit diagram that i made for my actual setup (an arduino uno, two current sensors, to linear actuators, and one motor drive):\n\ncircuit diagram: \n\n\nwill this setup work? will i have enough current/power coming out of the 5v of the arduino to power both the l298n logic and the two acs714 sensors?\n", "tags": "arduino control actuator current circuit", "id": "4299", "title": "arduino with two linear actuators, two acs714 current sensors, and an l298n motor driver setup"}, {"body": "is it possible to build a quadcopter which can detect obstacles and thereby avoiding them in order to reach its destination?\nif so,how could it avoid the obstacles and how can the destination be set\n", "tags": "quadcopter", "id": "4303", "title": "autonomous obstacle detecting quadcopter"}, {"body": "i always wanted to have a cnc to make pcb quickly at home. \n\nfinally, i got a 7x7 kit from zentools recently and put it together. i attached a battery powered screw driver to 2nd shaft of the stepper and moved the each axis all the way back and forward before wiring. all 3 axis moves smoothly, i can turn the steppers even by hand. every piece works smoothly, no mechanical jam.\n\ni decided to use grbl as controller software. tested the software without the shield or stepper (qv: testing grbl in arduino board without the steppers) i use universal gcode sender to communicate with grbl.\n\ni got an arduino cnc shield for arduino uno, put it together, attached to arduino uno, re-tested grbl, it worked. \n\ni used reprep's stepper wiring article to connect stepper to the driver, wired 1 stepper to the stepper driver (x axis). powered the shield with 20v 17.5amp (350w) dc regulated power supply. (it was the power adaptor for an old 17\" notebook. notebook died, i kept the adaptor)\n\nwhen the move 5 steps command (g1 x5) was sent, stepper makes a small move in the direction and then makes a grinding noise. (can be seen on youtube) \n\ni tried switching 1st pair's cables, using another stepper driver (3 drivers), turning the potentiometer to increase the current, but still no luck.\n\ni attached 2 photos of the cnc and the controller and controller unit.\n\ni tried everything i can think of, any suggestions?\n\n\n\n", "tags": "stepper-motor cnc", "id": "4307", "title": "stepper does not turn"}, {"body": "while looking at mecanum wheels, i noticed that there are two different designs that are popular.\n\none type holds the rollers in between the wheels frame, and the other holds the rollers from the center.\n\nis there a significant advantage to using one over the other?\n", "tags": "wheel", "id": "4308", "title": "what is the difference between the two different types of mecanum wheels?"}, {"body": "i am looking for a cheapest possible gps setup with a centimeter precision without much hw hacking. i am not able to produce my pcb or do any soldering (though i would do that if there is no other way) so a kind of a easy-to-assemble setup would be welcome. i know about the $900 piksi thing but that is still too expensive for me. it seems like cm precision should be possible for much less - like employing a 50 usd raw gps sensor with an antenna and ordinary pc with rtklib software.\n\ni am not sure if it is better to use two gps sensor setup for rtk (one base station and one for rover) or whether i can get the corrective dgps data elsewhere (my region is czech republic - there seems to be national grid here allowing to stream correction data for reasonable cost).\n\nmy application will be in a passenger car so i will not be limited with power source - no low power needed although that would be nice. i will be using the position readings within opencv - so i need to get the data into c/c++ code. the application is data collection so i can use raw gps post-processing.\n", "tags": "gps", "id": "4312", "title": "low-cost centimeter accurate satellite positioning (gnss/gps)"}, {"body": "is integration over not constant dt (\u2206time) a possible thing? let's say you have a pid loop with differentiating frequency, can the integral part of it still work? (assuming you know the dt from the last iteration)\n\ncould i just use a variable dt (\u2206time) in my calculation and the pid principle would still function correctly?\n", "tags": "quadcopter pid real-time", "id": "4313", "title": "pid integration over not constant dt (\u2206time)"}, {"body": "i want to build a small cylindrical arm, with a main 360\u00ba angular servo on the longitudinal axis, and a secondary angular servo with variable speed in a trasversal axis that rotates with the main angular one. the secondary needs to receive data and power from a slip ring across the main servo, since it must be able to rotate freely and must not be binded by wirings. the width of the cylindrical arm must be below 0.4 cm\n\ni've reviewed the market for off-the-shelf servos and there are a few that could fit the bill for the main servo, i know where to obtain the slip ring required, but it beats me where to obtain the secondary servo, since the space limitations demand that it is really small (&lt; 0.2 cm) and the smallest i've been able to find on internet are 0.5 cm\n\nany suggestions are greatly welcome!\n", "tags": "servomotor", "id": "4316", "title": "compact design - building from off-the-shelf components"}, {"body": "i'm trying to build a robot with a differential drive powered by two dc motors. first i implemented a pid controller to control the velocity of each motor independently. estimated the tf using the matlab's system identification toolbox, of the open loop system by the acquiring the velocity of each wheels encoder in function of the pwm signal applied by an arduino microcontroller. all went well and i successfully dimensioned the pid gains for this controller.\n\nwhat i'm trying to accomplish now is to control the exact (angular) position of the dc motor. i thought in cascading a pid controller in the input of the other already implemented. so this way, i can give a position to the first controller, which will be capable of generate an output reference to the second (velocity) controller so it generates the appropriate pwm value signal to drive the dc motor accordingly.\n\nwill it work? is that a good approach? or should i try to implement a different controller which outputs the pwm signal in response to a position reference signal?\n\nmany thanks for your attention and i hope somebody can help me with these doubts.\n", "tags": "arduino control microcontroller pid wheeled-robot", "id": "4318", "title": "cascading pid dc motor position & velocity controllers"}, {"body": "i am currently working on an exoskeleton.  the exoskeleton is going to help kids with cerebral palsy learn to walk 4 years sooner than traditional therapy.  currently we are using 2 ame 226-3003 with the roboclaw 2x60a motor controller controlled by an arduino mega.  the ame 226-3003 motors are not powerful enough.  in addition the ame 226-3003 has a worm gear thus the motor cannot be moved when the motor is turned off.  our position feedback system is a gear attached to the shaft of the motor which spins a gear on a potentiometer.  the two gears have a 1:1 ratio.  \n\nin order to better understand the project, please see the video: \nhttps://www.youtube.com/watch?v=nl_acwjsrie&amp;feature=youtu.be\n\nthe ame 226-3003 catalog page: \nhttp://www.amequipment.com/wp-content/uploads/2013/02/801-1071-web.pdf\n\nwe need a new drive system:\n\n\nmore powerful than the ame 226-3003 motor.  we do not have an exact torque spec but we believe any drive system that is 70-100% more powerful than the ame 226 - 3003.  \nwe like the rpm range of the ame 226-3003.\nthe drive system must be able to spin freely when the motor is not in use.  \nwe need a way to get position feedback, the potentiometer system we are using seems to work, however it adds to much extra hardware(more stuff to break), (ie) the gear on the potentiometer and the gear on the shaft have to mesh constantly and we have to zero the potentiometer every time we put the leg together so the potentiometer doesn't over spin.  * we would prefer to have an optical encoder inside the motor.\nwe need to have the drive system be at a right angle.\n\n\ni need help designing a drive system that will meet the requirements.  \n\ni think i might have found a motor that will work:\n\nthe amp flow g43-500\n\nhttp://www.ampflow.com/standard_motors.htm\n\ni like the g43-500 because it can run at 24 v, thus it will take less amps than 12v. will that motor get the job done?\n\ni need to gear this down to around 80rpm.  what type of gear box would work best?\n", "tags": "motor control microcontroller motion-planning encoding", "id": "4323", "title": "exoskeleton drive system help"}, {"body": "when installing a servo or other actuator, i measure the force needed to perform whatever action is needed and find an actuator that can generate more than the necessary force. however, i recently wondered if there's a rule of thumb or guideline for how much overhead is useful, before it becomes a waste.\n\nfor a (perhaps oversimplified) example, say i have a lever to lift something, and the force needed is 100 newtons. an actuator that can manage 100 n maximum will have problems with this and stall, with any sort of friction or other imperfections. i would use an actuator that can produce 150 or 200 n - whatever is available and fits the design and budget. after testing, it may become apparent that 200 is overkill, 120 is sluggish, but 150 is good. other than trial and error, is there a way to measure this, or a rule of approximation?\n\ni realize that variables in the mechanics and construction can significantly alter what force might is needed to be considered ideal, but is there a commonly accepted value for simple applications? something like \"if you need x force, install an actuator with x + 20% force.\"\n", "tags": "actuator force", "id": "4326", "title": "is there a rule of thumb for actuator torque overhead?"}, {"body": "i want to build a closet with ejectable drawers. on the top should be 4 buttons, each eject opening one of the four drawers of the closet. \n\ni am looking for ideas on how to accomplish this. what kind of springs, slider mechanisms perhaps, and other materials to use? \n\nany examples?\n", "tags": "motion", "id": "4327", "title": "best technique to built an ejectable drawer?"}, {"body": "for my robotic projects i need some aluminium parts. currently i am looking for a way to build a chassis including simple gear box. so i need relatively high precision. which options do i have to machine aluminium without investing in expensive tools?\n\nthis is what i could think of so far.\n\n\ndesign parts in cad and send them to a third party company for fabrication. the problem with this is that hobby projects almost never need large quantities and piece production can be still expensive.\nbuy cheap tools to work aluminium by hand. i don't know which tools would fit this task best. moreover, the results might be inaccurate, which is a problem for designs with moving parts.\nfind someone with a cnc who let's me machine my parts. this would most likely result in very slow prototyping cycles though.\n\n\na method that i can do at home with not too expensive tools would be perfect, but i'm looking forward to every solution.\n", "tags": "cnc chassis", "id": "4329", "title": "how to machine aluminium on a low budget?"}, {"body": "i want to create a robot that will navigate on a desired path!\nthat path can be a straight line or a circular path with a given radius.\n\ni will use 3 or 4 omni wheel drive platform and for positioning,\ni am using this research paper which perform dead-reckoning using mouse sensors.\n\ndead-reckoning using mouse sensors\n\ni've understood that i will get x, y and \u03b8 positions, which are actual positions of robot.\nthese can be used to calculate the error and then using pid to compensate the error.\n\nbut, to find the error, i must have the desired position of the robot at that moment!\n\nfor example, the robot is at (0,0) and it needs to move in a circular path of equation\n\n$$ x^2 + y^2 - 10y = 0 $$\n\nnow, i want to calculate the position at t = 2 sec, how to do that?\n\nif someone has already done similar stuff, please post the link. i am not able to find any resource on web!\n", "tags": "mobile-robot pid kinematics wheeled-robot motion", "id": "4336", "title": "position control of an omni wheel drive robot"}, {"body": "the matlab code is used to detect red colored object, but i want to control a bot to move towards the detected object. just need a simple algorithm or idea, controlling the servo i will be able to do it.    \n\n\n\n\n", "tags": "mobile-robot microcontroller wheeled-robot robotc", "id": "4340", "title": "matlab for moving a robot towards the detected block"}, {"body": "\n\ni am able to locate centroids of each blocks, but i am unable to join two blocks with a line segment by avoiding the obstacle as shown in the figure. please need help how do i achieve this using matlab.\n", "tags": "computer-vision motion-planning navigation matlab", "id": "4346", "title": "how to plot a line between two centroids in matlab"}, {"body": "there seems to be consensus here that the beaglebone black has 1ms+ latency while toggling gpio pins due to the fact that gpio is handled outside of the cpu. are the uart/i2c/spi lines equaly slow, or are they significantly faster?\n\ni've seen references to people talking to the gpio more directly. could this decrease uart/i2c/spi latencies as well?\n", "tags": "i2c beagle-bone", "id": "4348", "title": "beaglebone uart/spi/i2c latency"}, {"body": "i'm looking for some direction on how to create a device that does the following:\nimagine you have a yoga mat, i want a device that can roll it up and unroll it without a human intervening in the rolling process.\ni reliable this is a robotics forums but there doesn't appear to be a section for mechanical engineering so i'm posting my question here.\n", "tags": "motor mechanism", "id": "4357", "title": "device to roll a mat"}, {"body": "need help to choose desk top, low cost, diy/high school grade laser cutter for making base plate for diy robots, about maximum a4 paper size, as this photo.  idea, comment, advises, even if only partially cover some and not all questions, are welcome.\n\n\n\n\nwhat power is needed to cut acrylic 3 to 5mm thick. many sellers at 40 to 60 watts range. what can these do?\nhow cut thickness depends on cut speed? to what extend can i choose slow speed to cut thicker sheet.\ndoes cut thickness depends on color/clear acrylic? it is co2 laser. \nsome units have options, like air blower and honeycomb bottom plate. what are their functions? what options are useful for this case.\nwhich cad 2d drawing software is best supported by these range of products?\napart from main function of cutting flat acrylic plate, some has additional z axis motor to rise/lower work piece for engrave/photo/line_letter marking on 3d objects. what software is needed to support these 3d operation.\n\n", "tags": "laser", "id": "4359", "title": "diy laser cutter for small acrylic robot baseplate"}, {"body": "i am currently trying to parametrize the low-level gains of a robotic arm. this arm uses a classical pid for each joint.\n\ni am trying to use a method based on computation rather than a trial-and-error/tweaking approach. the method i use considers each joint independently and assumes the system driven by the pid is linear. hence i infer a transfer function, a characteristic polynomial, poles and this gives me gains $k_p$, $k_i$, and $k_d$ for each joint.\n\nnow, computed as i did, these gains depend on the natural angular-frequency. for example:\n$$\nk_p = 3 a w^2\n$$\nwhere $a$ is the inertia and $w$ is the natural angular-frequency.\n\nhence my question: how shall i compute $w$, the natural angular-frequency for my system? is this an enormous computation involving the geometry and other complex characteristics of the robot, and that only a computer can do or are there simple assumptions to be made which can already give a rough result for $w$?\n\ni guess this is a complex computation and this is one of the reasons why pid gains are most often found by trial-and-error rather than from computation. though i am looking for some more details on the subject to help me understand what is possible and what is not.\n\nkind regards,\n\nantoine\n", "tags": "control pid robotic-arm", "id": "4360", "title": "natural frequency computation (for pid gains computations)"}, {"body": "i have decided to pursue a career in automation and robotic. at the moment, i am being torn between mechanical and electrical engineering. i know that both of them relate to my choices of career, and at the moment, i think that i like them equally. i hope you guys can help me solve my dilemma by using your insights/experiences to assist me with the following questions:   \n\n1/ from your experiences and opinions, which of the two engineering fields is generally more crucial and challenging, especially in an automation/robotics project?\n2/ which will see an increase in demand and importance in the near future? which of them might become outdated/obsolete or at least develop at a slower rate compare to the other?(i have a feeling that ee has a slight edge over this matter; however, i am not so sure)\n3/ which of the fields is more versatile? which is more physical demanding (i am actually quite frail)\n4/ which is generally easier to self-study? robotics is obviously an incredibly broad and complex field and i have prepared to step outside of my comfort zone and do lots of studying by myself to achieve my goals and passion.  \n\ni could probably come up with a few more questions; however, i am sure that you guys got the gist of my puzzle. thank you very much and i apologize if there is any grammatical error.\n", "tags": "beginner automatic", "id": "4362", "title": "mechanical or electrical engineering for robotic and automation?"}, {"body": "i'm going through the textbook robot modeling and control, learning about the dh convention and working through some examples. \n\ni have an issue with the following example. given below is an image of the problem, and the link parameter table which i tried filling out myself. i got the same answers, except i believe there should be a parameter d1 representing the link offset between frames 1 and 2. this would be analogous to the d4 parameter. \n\n\n\nif anyone could explain why i might be wrong, or confirm that i have it right, that would be great. i hate it when it's me against the textbook lol. \n\ncheers.\n", "tags": "forward-kinematics dh-parameters", "id": "4364", "title": "denavit-hartenberg parameters for scara manipulator"}, {"body": "i've seen in a lot of places some methods of tuning a pid controller. most of them will say that one should apply a step input to the system and based on that response you can tune the pid parameters following some rule of thumb. but what about a system which one of its pole is at origin? in other words, a step response on a system like that will have an infinitely increasing ramp in the output (theoretically).\n\nan example:\n\nlet's say we have a spinning wheel (fixed at center) and all we can control is the amount of torque applied to make it spin. if we can read its position (angle) and we want to design a pid controller to set its position (more or less like a step-motor). how can that be done? note that a step input in this case will be a constant torque and this will make the wheel spin faster and faster. how should one proceed?\n", "tags": "pid", "id": "4366", "title": "pid in a system with pole at origin"}, {"body": "i've been working on my two-wheeled mobile robot i've been trying to perfect my obstacle avoidance algorithm which is artificial potential field method . also i use arduino uno kit . the basic concept of the potential field approach is to compute a artificial potential field in which the robot is attracted to the target and repulsed from the obstacles. the artificial potential field is used due to its computational simplicity. the mobile robot applies a force generated by the artificial potential field as the control input to its driving system . the artificial potential field method in its computations depends on the distance between robot and goal or target and the distance between robot and obstacles that effected the robot (which could easily get for ultrasonic senors)\n\ni applied the artificial potential field method in matlab environment / simulation and it is done successfully , really what i need in the simulation is to get the current position of mobile robot and position of goal as x, y coordinates (to compute the distance between robot and goal) and the obstacles positions.\n\nthe output of the artificial potential field is the desired angle to avoid obstacle and reach to the goal , the method give the robot the angle the pointed to the goal then the robot goes toward that angle and if the robot face an obstacle in his way (got from sensor reading) the artificial potential field will update the angle to avoid the obstacle and then re-give the robot the angle that pointed to the goal and so on.\n\nthe question is how could i apply the artificial potential field method in real would? what should i get? is it easy to do that or it is impossible? \n\ni had rover 5 with two normal dc motors and two encoders (incremental rotary encoder) per each wheel.\n\nany help or suggestion on the topic will be highly appreciated please.\n\n\n\nedit: based on the response from shahbaz.\n\nthe case is very simple, but first, there is something to know that i constrained with some limitations that i couldn't overstep them, one of them is that the real world should be exactly as simulation for example in the simulation i consisted that robot started with (0,0) on coordinates  x, y axis and i should put the goal point for example (10,20)  and feed this point in the artificial potential field method and then compute distance between  robot and goal (so i don't need any technique to determine the position of goal) and i don't know if i could applied that.\n\nthe second constraint is that i should use the encoders of wheels to determine the current position of mobile robot and its orientation depending on a calculation formula (something like this here) even if that will be inaccurate.\n\ni had a rover 5 with two normal dc motors and two encoders (incremental rotary encoder) per each wheel, each encoder has four wires i don't know how to deal with them yet, and how could i translate the pulses of encoders or how to work out the x.y position of your robot based on the shaft encoder data.\n\ni am still searching for \u2026.\n", "tags": "mobile-robot", "id": "4369", "title": "artificial potential field navigation"}, {"body": "i've been going through syskit tutorials at rock-robotics.org. in the tutorials e.g. first composition, there are two different components declared with:\n\n\n\ni was wondering how could i add an additional  into the composition, so that the instantiation would then create two separate instances of the same component?\n\ni've tried something like\n\n\n\nbut this apparently isn't the way to go.  command shows only one instance of rocktutorialcontrol, but gives two roles to it (rock and foo). what is the meaning of \"role\" in this context?\n\ni've noticed that the tutorial explains how to make multiple instances of the same component when we're declaring our components as devices. but how to do this with components that should not be concerned as devices?\n\nbr,\nmathias\n\nedit:\n\nthis was my first question to stackexchange, and i don't know what's the policy for adding additional information to the original question, but here we go:\n\nit seems that both the deployment and configuration need to be different when there are two instances of the same component. i did a small scale testing with two components:\n\n\n\nwhere  has an input &amp; output port of /std/string.  has corresponding i&amp;o ports.  has also two configurations called 'default' and 'other'. it also has two deployments 'foo_depl' and 'bar_depl'.\n\nin order to create a \"pipeline\" where data flows producer ==> proxy ==> consumer, i made define line:\n\n\n\nand then instanciated the network with \n\n\n\nthe component instanciation failed if either  or  clause was left out. in both cases the produced error was \"cannot deploy the following tasks... ...multiple possible deployments, choose one with #prefer_deployed_tasks\". \n", "tags": "rock syskit", "id": "4370", "title": "rock/syskit: how to add multiple instances of same component into a network"}, {"body": "i am willing to use a universal robot arm (ur10) in a path following mode. i.e. i have a desired trajectory for the robot's effector and i would like the effector to follow it as close as possible.\n\nthe specs here give a repeatability of +-0.1mm. this is not written but i guess this is the static precision (after the robot had enough time to converge to the position). now what about the dynamic precision (i.e. max position error while performing the desired trajectory)?\n\ndoes anyone know more than me on this matter?\n\nkind regards,\n\nantoine.\n", "tags": "dynamics errors", "id": "4371", "title": "dynamic tracking precision of ur5/10"}, {"body": "i am trying to control the servo motor operation by torque control by interfacing the sensor to an avr , which will continuously monitor the torque value from the sensor and control the torque according to the given set point .is it possible to make such a setup? if yes how?\n\nthanks.\n", "tags": "torque servomotor avr", "id": "4379", "title": "torque control and monitoring of servo"}, {"body": "i need to pop out a needle like object(toothpick,matchstick,etc) from a hole in a\nsurface and push it back in automatically.i need to make a array of such needles in which each needle's position can be controlled individually.the objects aren't supposed to be oscillated continuously, instead they are to be locked in one of the two positions-either above the surface or inside it.\ni am trying to search a mechanism to achieve this.this can be easily done with a simple dc servo motor, but the problem is i have to do this in very limited space-about 6 such objects in base area of 3 cm x 3 cm.moreover the power source would be dc +5 v\n\nso far i have thought of creating small electromagnets with springs,but still not sure about it.any inputs will be appreciated.\n\n\n", "tags": "mechanism", "id": "4380", "title": "mechanism to oscillate a needle like object in vertical motion"}, {"body": "i am designing a robot in real world and i want to plot everything in x,y (cartesian) coordinates \n\ni just want to use the encoders of wheels to determine the current position of mobile robot and its orientation depending on a specific calculation formula (like this http://rossum.sourceforge.net/papers/diffsteer/ ) even if that will lead to inaccurate calculations .\n\nactually , i found out this formula below to compute x, y coordinates from encoder data but i still confused in some sides of this formula \n\n\n\n\ni had a rover 5 chassis form dagu with two normal dc motors and two encoders (incremental rotary encoder) per each wheel, how could i translate the pulses of encoders or how to work out the x.y position of the robot based on the shaft encoder data.\n\ni deduced some of values from rover 5 chassis :\n\ncm = conversion factor that translates encoder pulses into linear wheel displacement\n\ndn = nominal wheel diameter (in mm)\n:\nabout 20 cm\n\nce = encoder resolution (in pulses per revolution)\n:\nencoder resolution: 1000 state changes per 3 wheel rotations\n\nn = gear ratio of the reduction gear between the motor (where the encoder is attached) and the\ndrive wheel.\n:\ngearbox ratio: 86.8:1\n\nin rover 5 chassis there are 4 small wires with female headers. red is +5v for the encoder , black is 0v (ground) , white is signal a , yellow is signal b . the important wires in each encoder are signal a and signal b ,so \n\nhow to get values of nl , nr  in the formula above from signal a &amp; signal b ?\n\nis the value of nl is the direct value from  wire signal a or signal b ? the same question for nr . \n\nthanks a lot\n", "tags": "wheeled-robot quadrature-encoder", "id": "4384", "title": "translation the shaft encoder data"}, {"body": "i wonder if this would be a competitive robot compared with one made with a traditional approach using a microcontroller and infrared sensors. i suppose that raspberry can perform an edge detection to tell the dynamic of the line far away, much more that the infrared sensor, but how fast can the raspberry do this process? should be a relative simple process in terms of computational requirements , an edge detection in a high contrast arena. probably the bigger issue would be get the relative position of the robot respect to the line, may be a combination of the camera with some infrared sensors would work better, and what about the size? the robot will be significantly bigger when is used a camera and a raspberry for the control.\n", "tags": "raspberry-pi line-following", "id": "4385", "title": "it's worth to make a line follower using a raspberry pi and a web cam?"}, {"body": "so far i have done ekf localization (known and unknown correspondences) and ekf slam for only known correspondences that are stated in probabilistic robotics. now i moved to ekf slam with unknown correspondences. in the algorithm in page 322, \n\n\n  16. &nbsp; &nbsp; $\\psi_{k} = h^{k} \\bar{\\sigma}[h^{k}]^{t} + q$\n  \n  17. &nbsp; &nbsp; $\\pi_{k} = (z^{i} - \\hat{z}^{k})^{t} \\psi^{-1}_{k}(z^{i} - \\hat{z}^{k})$\n  \n  18. &nbsp; &nbsp; $endfor$\n  \n  19. &nbsp; &nbsp; $\\pi_{n_{t+1}} = \\alpha$\n  \n  20. &nbsp; &nbsp; $j(i) = \\underset{k}{argmin} \\ \\ \\pi_{k}$\n  \n  21. &nbsp; &nbsp; $n_{t} = max\\{n_{t}, j(i)\\}$\n\n\ni don't understand the line 19. in the book page 323, the authors state \n\n\n  line 19 sets the threshold for the creation of a new landmark: a new landmark is created if the mahalanobis distance to all existing landmarks in the map exceeds the value $\\alpha$. the ml correspondence is then selected in line 20. \n\n\nwhat is $\\alpha$ in line 19 and how is it computed? also, what is the mahalanobis distance? i did research about mahalanobis distance but still i can't understand its role in ekf slam. \n\n\n\nedit: \ni found another book in my university's library robotic navigation and mapping with radar the authors state \n\n\n  the mahalanobis distance measure in slam is define as\n  $d^{2}_{m}(z^{j}_{k}, \\hat{z}^{i}_{k})$, which provides a measure on\n  the spatial difference between measurement $z^{j}_{k}$ and predicted\n  feature measurement $\\hat{z}^{i}_{k}$, given by  $$ d^{2}_{m}(z^{j}_{k}, \\hat{z}^{i}_{k}) = (z^{j}_{k} - \\hat{z}^{i}_{k})^{t} s^{-1}_{k}(z^{j}_{k}, \\hat{z}^{i}_{k}) $$ this\n  value has to be calculated for all possible $(z^{j}_{k}, \\hat{z}^{i}_{k})$ combinations, for which  $$ d_{m}(z^{j}_{k},\\hat{z}^{i}_{k}) \\leq \\alpha $$ often referred to as a validation gate.\n\n\nleave me to the same question what is $\\alpha$?\n", "tags": "slam ekf mapping", "id": "4392", "title": "ekf slam and mahalanobis distance?"}, {"body": "regarding my project work, i have to write an algorithm for mobile robot planning. for that, i have chosen genetic algorithm. is it good for mobile robot path planning? if it is, then where can i start from and get some guidelines?\n", "tags": "mobile-robot navigation algorithm", "id": "4393", "title": "is a genetic alogorithm suitable for mobile robot path planning?"}, {"body": "i am trying to make a line follower robot and i need help regarding the type of dc motor to use.\nso we have a single shaft bo motor and a double shaft bo motor. can anyone help me understand what is the difference between the two?\nhere's the link for \nsingle shaft bo motor:\nhttp://www.evelta.com/industrial-control/motors-and-accessories/100-rpm-l-type-single-shaft-bo-motor\n\ndouble shaft bo motor:\nhttp://www.evelta.com/industrial-control/motors-and-accessories/100-rpm-l-type-double-shaft-bo-motor\n", "tags": "motor line-following", "id": "4394", "title": "single-shaft vs double-shaft motors"}, {"body": "simply, i had rover 5 with 2 dc motors and 2 quadrature encoders, i just want to use encoders to measure the distance of travelling for each wheel.\n\nto start with, i just want to determine the total counts per revolution. i read the article about quadratic encoder from this broken link.\n\nin rover 5, each encoder has four wires: red (5v or 3.3v), black(ground), yellow (signal 1) and white (signal 2). i connected each wire in its right place on arduino uno board, using the circuit:\n\n\nrotary encoder channela attached to pin 2 \nrotary encoder channelb attached to pin 3\nrotary encoder 5v attached to 5v\nrotary encoder ground attached to ground \n\n\nfor one encoder, i test the code below to determine the total counts or ticks per revolution, the first program by using loop and second by using an interrupt.\n\nunfortunately while i run each program separately, rotating the wheel 360 degree by hand, the outputs of these two programs was just \"gibberish\" and i don't know where is the problem . could anyone help?\n\narduino programs posted below.\n\nfirst program:\n\n\n\nthe second program (with interrupt)\n\n\n", "tags": "mobile-robot quadrature-encoder", "id": "4395", "title": "counts of quadrature encoder"}, {"body": "i am migrating from a differential drive design to a skid steering design for my robot, and i want to know how easy would it be to use the navstack with skid steering. would there be any problems in terms of localization and things like that?\n\nif i let two wheels on the same side of my robot (two on left side and two on the right side) maintain same velocity and acceleration, would the unicycle model of a differential drive robot still apply for skid steering?\n", "tags": "ros navigation differential-drive driver", "id": "4396", "title": "ros navstack with skid steering robots"}, {"body": "i'm trying to understand the core differences between the two topics.  is one simply a newer term?  connotations of automobile vs automation?  something with a screen vs without?\n\ni've only ever heard the term  (tagged).\n", "tags": "computer-vision", "id": "4398", "title": "machine vision vs computer vision?"}, {"body": "need to buy a diy/high school grade laser cutter/engraver  \n\nhow much laser power is needed for wood, acrylic (3 to 6mm thick), cutting and decorative engraving? \n\nwhat parameters i need to take care in selecting suitable machines?\n\n\n\n\n", "tags": "laser", "id": "4399", "title": "what laser power for cutting and engraving wood and acrylic robot baseplates?"}, {"body": "while i am reading and collecting information about rotary encoders , i faced some troubles about the meaning of some expressions concerned with encoder ,which make me to be confused and stray,  these expressions or words are :\n\n-count per revolution (rotation) \n\n-pulse per revolution\n\n-tick per revolution\n\n-transitions per revolutions \n\n-number of transitions\n\n-number of state changes\n\ni thought the transition is same as state changes which means change from high to low or low to high , but what about the others what is the diffenece among them (count , tick ,pulse ,transition .... etc)? and what the relationship between transitions and pulse ? could anyone clarify that , please\n", "tags": "mobile-robot", "id": "4405", "title": "expression used with rotary encoders"}, {"body": "i recently spent some work on my quadcopter firmware.\nthe model is stabilizing its attitude relatively well now.\nhowever i noticed, that it is changing its altitude sometimes (maybe pressure changes, wind or turbulence).\nnow i want to get rid of these altitude drops and found not much literature.\nmy approach is using the accelerometer:\n\n\ncalculates the current g-force of the z-axis\nif the g-force is > 0.25 g and longer than 25 ms, then i feed the accelerometer term (cm per s\u00b2) into the pid\nthe output is sent to the motors\n\n\nthe model now reacts when it is falling down with an up-regulation of the motors.\nhowever, i am not sure, whether it is smart to feed the current acceleration into the regulator and i currently wonder, whether there is a smarter method to deal with sudden and smaller changes in altitude.\n\ncurrent code: \n\n\n\n\n", "tags": "quadcopter multi-rotor", "id": "4409", "title": "quadcopter: stabilization along the z-axis (for holding altitude)"}, {"body": "i have started in the programming stage of my project , and my first step is to made and test the odometry of my rover 5 robot on arduino uno by using encoders to determine position and orientation .\n\ni wrote this code and i don\u2019t know if that code right or there are some mistakes,  because i am novice to arduino and robotic field so i need for some suggestions and corrections if  there were . \n\nthanks a lot\n\narduino codes posted below.\n\n\n\n\n", "tags": "mobile-robot", "id": "4410", "title": "programming the odometry of rover 5"}, {"body": "i want to power my arduino uno and i know i can do that either by connecting it with usb to pc or with dc power supply.but i want to connect it with a battery source(kindly see the image below) and i know its a silly question but how do i do it? the battery connector is not the regular dc jack but the one that's found in rc toys. so how do i power my arduino with that battery? \nand also how do i connect it with a dc power supply adapter to charge it once its discharged? please also mention the specifications of the dc power supply adapter that is to be used while charging this battery.\n\n", "tags": "arduino battery", "id": "4413", "title": "arduino power supply"}, {"body": "i'm new to robotics. i would like to know if 56 output lines can be taken from an arduino or raspberry pi?\n", "tags": "arduino raspberry-pi", "id": "4414", "title": "motor control using arduino/raspberry pi"}, {"body": "does anyone know of a robotics developer environment ideal for testing ai programs for drones (e.g. quadrocopters, planes, helicopters, etc.)? i would like something like microsoft robotics developer studio that includes a virtual environment (such as an outdoor environment with gravity, wind, etc.) to test out flight dynamics. i would like the options to add sensors to the virtual drone, such as gps, altimeter, gyros, etc. that the ai program can then use to steer the drone.\n", "tags": "quadcopter artificial-intelligence machine-learning", "id": "4415", "title": "virtual testing environment for drones"}, {"body": "sebastian thrun says in his paper on particle filters that - no model however detailed fails to represent the complexity of even the simplest of robotic environment. what does he means by this? can someone please elaborate?\n", "tags": "localization theory", "id": "4420", "title": "why models are not perfect to represent robotic environments?"}, {"body": "i'm looking for a research paper or series of papers that compare the performance of various simultaneous localization and mapping algorithms for rovers in a variety of real world environments.  in particular, i'm looking for computational speed, accuracy (compared to the real world environment) and memory &amp; power efficiency metrics.  is there a journal that regularly publishes experimental performance comparisons?\n", "tags": "slam reference-request", "id": "4424", "title": "comprehensive comparison of slam algorithms"}, {"body": "i'm working on a quadcopter. i'm reading the accelerometer and gyro data out from the mpu6050 and using complementary filter to calculate the roll and pitch values. when the quad is on the floor, and the motors are turned on the roll values are: \n\n\n\nit is very messy. after minus five there is plus seven. i would like to filter out this too high/low values programmatically but i have no idea how to do it.\n\nedit:\nat this moment i think the solution is the low-pass filter. i'll let you know if it is successful or not.\n", "tags": "quadcopter accelerometer gyroscope", "id": "4426", "title": "how to filter vibration programatically?"}, {"body": "what free of charge robot magazine, journal, newsletter or similar publication are available?  \n\neither geared toward technical professionals or the general public.\n", "tags": "untagged", "id": "4427", "title": "free of charge robot magazine, journal, newsletter or similar"}, {"body": "the aim is to guide a bot from source  to goal  while passing through all the checkpoints  (in any order).  \n\n\n\none way to solve it would be to select one checkpoint as goal from current state and then guide the bot to it. then select the next checkpoint as goal and current checkpoint as source and guide the bot to its new goal. eventually guide it to the state  from the last checkpoint.but this technique relies heavily on the order of checkpoints traversed. i would like to know if a good heuristic can be found to decide which checkpoint to go to next?\n", "tags": "artificial-intelligence", "id": "4430", "title": "what would be a good heuristic to solving this?"}, {"body": "i am currently studying the freak descriptor and i read the article published by its designers. it states that the aim was to mimic the retinal topology, and one of the advantages that could be gained is the fact that retinal receptive fields overlap, which increases the performance. \n\ni thought about it a lot, and the only explanation i was able to come up with is the fact that, looking at this problem from an implementation point of view, a receptive field is the ensemble of an image patch centred around a pixel, plus the standard deviation of the gaussian filter applied to this patch. the size of the receptive field represents the value of the standard variation. the bigger the size is, the more pixels will be taken into consideration when gaussian filtering, and so we \"mix\" more information in a single value. \n\nbut  this guess of mine is very amateurish, i would appreciate it if someone could give an explanation from what goes on in the field of image processing-computer vision-neuroscience.\n", "tags": "computer-vision", "id": "4435", "title": "computer vision using the freak local features descriptor - why overlapping fields?"}, {"body": "i'm usig rpi and servoblaster to control servos. i've set the , but i'd like to decrease it to 1us. i've tried to set the , but the servoblaster displays: invalid step-size specified. \n\ni've also tried to set the pulse width in micoseconds like . it works, but it's unpredictabe (step size is set to 2us):\n\n\n\nmotor: turnigy aerodrive 2830-11, esc: turnigy multistar 30a\n\nany idea?\n", "tags": "raspberry-pi servos", "id": "4439", "title": "raspberry pi finer servo control"}, {"body": "i have known map of the environment (2d occupancy grid map). i am trying to find if anything changed in environment using 2d laser while navigating by using maximum likelihood of laser with known map. \n\nmy question is how to know which measurements are corresponding to changes. my environment is not static and has some changes which is differs from known map. now i am trying to find which objects newly came into the environment or moved out of the environment using laser.\n", "tags": "slam mapping laser occupancygrid", "id": "4440", "title": "finding changes in environment using 2d laser"}, {"body": "i have a robotic simulator that enables a 6 wheel rover to perform spot turn.\nto prepare the rover to spot turn, i have to arrange/align the wheels in such a fashion:\n\n\n\nwhat is the technical name of it? circular wheel arrangement? circular alignment? \n", "tags": "mobile-robot wheeled-robot wheel", "id": "4442", "title": "what is the technical name when robot wheels are aligned to perform spot turn?"}, {"body": "i've seen several examples of slam algorithms (ekf slam, graph slam, seif slam) written in terms of the velocity motion model.  i have yet to see an example of any slam algorithm utilizing the odometry motion model.  i wonder if there is an inherent advantage to using the velocity motion model over the odometry model for this problem.  does it have something to do with the fact that odometry sensor information comes after the motion has already taken place, whereas velocity control commands are executed before motion?\n", "tags": "slam motion", "id": "4447", "title": "is there any advantage to velocity motion models over odometry motion models for slam?"}, {"body": "i'm trying to understand the role of landmarks in slam algorithms.  i've glanced over a few books concerning landmark based slam algorithms and i've come up with a rudimentary understanding which i believe is flawed.\n\nhow i think slam works:\n\nas i understand it, landmarks are a set of points in a map whose locations are known a priori.   furthermore, the number of landmarks in a map is fixed.  the number of landmarks detected at any one time may change, but the number of landmarks that exist in the map remains static at all times.  \n\nmy understanding is that slam algorithms exploit the fact that these points are both uniquely identifiable and known a priori.   that is, when a robot senses a landmark, it knows exactly which landmark it detected and thus knows the exact location of that landmark.  thus, a slam algorithm uses the (noisy) distance to the detected landmarks (with known location) to estimate its position and map.  \n\nwhy i think i'm wrong  \n\nin my naive understanding, the usefulness of slam would be limited to controlled environments (i.e. with known landmarkds) and completely useless in unknown environments with no a priori known landmarks.  i would presume that some sort of feature detection algorithm would have to dynamically add landmarks as they were detected.  however, this fundamentally changes the assumption that the number of given landmarks must be static at all times.  \n\ni know i'm wrong in my understanding of feature based slam, but i'm not sure which of my assumptions is wrong:  \n\ndo feature based slam algorithms assume a static number of landmarks?  \n\ndo the landmarks need to be known a priori?  can they be detected dynamically?  and if so, does this fundamentally change the algorithm itself?\n\nare there special kinds of slam algorithms to deal with unknown environments with an unknown total number of landmarks in it?\n", "tags": "slam", "id": "4449", "title": "assumptions about the nature of landmarks in slam algorithms"}, {"body": "how are several channels multiplexed down to a single physical wire? if two channels are transmitting the same value in the same frame, wont there be an overlap of the pulses?\n", "tags": "rcservo pwm", "id": "4456", "title": "pulse position modulation as used in rc controls"}, {"body": "i'm a newbie in rc field..i am planning to construct my first tricopter ever.  can anyone help me to find the power rating to select the motor for a tricopter?\n\ni am at the beginning stage of construction.  arm length of frame: 50cm each.  i need a thrust of about 2kg -- nearly 666 gms for each motor.\n", "tags": "motor", "id": "4463", "title": "choosing motor for a tricopter"}, {"body": "i have a big miss conception between yaw and attitude ? \n\nisn't both represent \"how far is the quad from earth ?\" \n\nalso if you could post how to calculate them from imu (gyro +accele + magent ) \n", "tags": "imu", "id": "4465", "title": "what's the difference between yaw and attitude in quad rotor"}, {"body": "it's been while since i started reading about ins, orientation and so for quadrotors . \n\ni faced the following terms : ahrs - attitude - yaw,pitch and roll - marg sensors\n\ni know for example how to calculate yaw,pitch and roll , but does it related to attitude ? \n\nwhat's attitude  any way and how it get calculated ?\n\nahrs \"attitude and heading reference system\" does it formed from yaw,pitch and roll ? \n\nmarg(magnetic, angular rate, and gravity) ? how it's related to other terms ? \n\nwhat about ins ( inertial navigation systems ) ? \n\nmy questions here are about these concepts, and there meaning , how they cooperate with each other , how they got calculated and which sensors suits for what ?\n", "tags": "navigation", "id": "4467", "title": "need to clear some concepts: ahrs - attitude - yaw,pitch and roll - marg sensors -ins"}, {"body": "i read somewhere that in the case of photoshop for example, the size refers to the number of pixels an image contains, but resolution involves the pixel's size, i don't know whether this definition goes for all the other fields. in computer vision, what's the difference between image size and image resolution?\n", "tags": "computer-vision", "id": "4469", "title": "image size vs image resolution"}, {"body": "i am new to embedded, starting with avr programming using c. i am working on mac os 10.9.4, so far i am using avrdude and xcode as ide. it works very well, for now i am testing my code using proteus.\n\nbut now i want to burn my .hex to avr atmega16 board. i have usbasp, which i am able to connect and it lights up the board. now after searching on the internet, i think mac is not detecting my board. i have checked /dev directory, but no usb device found. \n\nso i am not sure what to next, how to make mac detect my board and burn my .hex on it. i've found this: http://www.fischl.de/usbasp/ but no idea how to use this or its required or not.\n\nso question stand is: how to make mac detect avr board using usbasp and burn program to it?\n\nfyi: i've installed crosspack on mac. \n", "tags": "usb embedded-systems avr", "id": "4472", "title": "how to make mac detect avr board using usbasp and burn program to it?"}, {"body": "i am trying to build a servo-controlled water valve.  max pressure 150 psi , valve size 1/2\".\n\ncan anyone recommend a suitable 1/4-turn valve, either ceramic, ball valve, or anything else that is easy to turn, even under pressure? it must require very little torque to turn, so a standard servo can rotate it with a small lever attached.\n", "tags": "servos valve", "id": "4474", "title": "servo controlled valve"}, {"body": "i have just built my first quadcopter, and have run into a bit of a snag. when i plug in the power, i only get one beep and a red blink from the flight control board, and nothing else happens. when i turn on the controller, however, a red light turns on on the reciever. otherwise, nothing else happens. from what i can tell, i have plugged in everything correctly, and am not sure how to proceed.\n\n\nflight control board\nflight control board manual (pdf)\nesc's\n\n\ni do not have a connection from the power distribution board to the flight control board, because i am assuming that it gets its power from the esc's.\n\nhere is the video i used to figure out how to build a quad.  (side- note about the video: i have not cut the esc's cords as done in the guide, seemed like a silly step, also i have seen other applications where they were not cut)\n\ni have not updated the firmware on the board, i have put it in out of the box\n\nhere is the board's user manual (pdf)\n", "tags": "control quadcopter", "id": "4476", "title": "quadcopter one beep and blink problem"}, {"body": "how to calculate attitude from imu ? \n\nfor example, mathematical equations \n", "tags": "imu", "id": "4477", "title": "how to calculate altitude from imu?"}, {"body": "i am trying to create a simulation of a robot with ackerman steering (the same as a car). for now i'm assuming that it's actually a 3-wheeled robot, with two wheels at the back, and one steering wheel at the front:\n\n\n\nknowing the wheel velocity, and the steering angle a, i need to be able to update the robot's current position and velocity with the new values at time t+1.\n\nthe obvious way to do this would be to calculate the position of the centre of rotation, where the axles of the wheels would meet, however, this leads to an undefined centre of rotation when a = 0. this means that the model doesn't work for the normal case of the robot just driving in a straight line.\n\nis there some other model of ackerman steering which works over a reasonable range of a?\n", "tags": "mobile-robot simulation", "id": "4486", "title": "ackerman steering model"}, {"body": "im trying to develop a system that autonomously navigates a large outside space, where accuracy is vital (gps is too inaccurate). there are a number of options but have largely been used inside, has anyone tried these or used anything else?\n\nwifi triangulation,\ndead reckoning,\nrfid landmarks\n", "tags": "navigation", "id": "4487", "title": "relative navigation systems"}, {"body": "i'm currency a web programmer and i'm very passionate by robotics and specialty for artificial intelligence. \n\ni have already make some c++ program for microship and arduino for little robots and other lisp codes (example for labyrinth path search) but i think it's not really applicable for projects further. \n\ni have read a lots for artificial neural network to create artificial mind, but it's very theoretical and i have no idea to reproduce that on code.\n\nsomeone have a idea to help me, a specific language, or just a c++ library ? \nif you have some links, articles, or other tutorials i take it.\n\nthank a lots !\n", "tags": "artificial-intelligence programming-languages", "id": "4488", "title": "what's the most adapted programming language for robotic and principally ai?"}, {"body": "simply , how can i calibrate imu unit ? \n\ni read some papers about this topic and was wondering if there are any standard methods.\n", "tags": "imu", "id": "4489", "title": "how to calibrate an imu unit?"}, {"body": "i am having some trouble understanding how to practically use the speed-torque curve of a dc motor.\n\ni understand that the gradient of the speed-torque curve is defined by the design of the motor, the exact position of the curve depending on the voltage applied. so if the voltage is changed the speed-torque curve is also changed but remains parallel to the initial curve before the voltage was changed. see figure below.\n\n\n\nso my intuitive guess is that when using the motor at a given desired operation point (desired speed and desired torque), the corresponding speed-torque curve cd has a gradient specified in the data sheet of the motor and passes through the operation point. this curve cd is obtained at a corresponding voltage vd. see diagram below.\n\n\n\nso my next guess is that in order to have the motor operate at this desired operation point, you have to set the voltage applied to the motor to vd, and apply a current id (computed using the torque and the torque constant).\n\nnow from what i read this is not what is done in dc motor controllers. these seem to only drive the motor using current and some sort of pwm magic as is shown in the following diagram by maxon.\n\n\n\nanyone knows why voltage is not used in dc motor control and only current is? i do not understand how you can set the speed if you do not modify the voltage? and what is pwm useful for?\n\ni have looked for hours over the internet and could not find anything relevant.\n\nthanks,\n\nantoine.\n", "tags": "motor control", "id": "4492", "title": "dc motor control - speed-torque curve"}, {"body": "i have a 7 dof arm that i am controlling with joint velocities computed from the jacobian in the standard way.  for example:\n$$\n{\\large j} = \\begin{bmatrix} j_p \\\\j_o\n\\end{bmatrix}\n$$\n$$\nj^{\\dagger} = j^t(jj^t)^{-1}\n$$\n$$\n\\dot{q}_{trans} = j^{\\dagger}_p v_{e_{trans}}\n$$\n$$\n\\dot{q}_{rot} = j^{\\dagger}_o v_{e_{rot}}\n$$\n$$\n\\dot{q} = \\dot{q}_{trans} + \\dot{q}_{rot}\n$$\n\nhowever, when specifying only translational velocities, the end-end effector also rotates.  i realized that i might be able to compute how much the end-effector would rotate from the instantaneous $\\dot{q}$, then put this through the jacobian and subtract out its joint velocities.  \n\nso i would do this instead of using the passed in $v_{e_{rot}}$:\n\n$$\nv_{e_{rot}} =  r(q) - r(q+\\dot{q}_{trans})\n$$\n\nwhere $r(q)$ computes the end-effector rotation for those joint angles.  \n\nis this ok to do, or am i way off base?  is there a simpler way?  \n\ni am aware that i could also just compute the ik for a point a small distance from the end-effector with no rotation, then pull the joint velocities from the delta joint angles.  and that this will be more exact.  however, i wanted to go the jacobian route for now because i think it will fail more gracefully.\n\na side question, how do i compute $r(q) - r(q+\\dot{q}_{trans})$ to get global end-effector angular velocity?  my attempts at converting a delta rotation matrix to euler angles yield wrong results.  i did some quick tests and implemented the above procedure to achieve pure end-effector rotation while maintaining global position.  (this is easier because $t(q) - t(q+\\dot{q}_{rot})$ is vector subtraction.)  and it did kind of work.\n", "tags": "kinematics robotic-arm jacobian", "id": "4496", "title": "how to get pure end-effector translation through jacobian?"}, {"body": "we are working on a project where we want to sound an alarm if somebody is messing around with our robot (e.g., the robot is being shaken abruptly or the cameras/lidars are blocked).\ni am using \"loud speakers\" (4.1 x 3 inch 10 watts 8 ohm speakers), but they are not loud enough.\n\nare there any small speakers or alarm systems small enough, but loud enough (closed to a car alarm) that you would recommend? \nideally something that i can just plug into the robots computer, or interface with through a microcontroller. either one would be fine.\n", "tags": "microcontroller ros navigation", "id": "4498", "title": "robot loud alarm"}, {"body": "i am bulding a quadcopter using these compenents:\n\n\nmicrocontroller: tiva c lanchpad (arm cortex m4 - 80 mhz), but\nrunning at 40mhz in my code\nmpu 9150 - sensorhub ti\nesc - hobbywing skywalker 40a\n\n\ni use the sample project comp_dcm from tivaware and use that angles for my pid which running at 100hz\n\ni test pid control on 2 motors, but the motors oscillate as in the video i found on youtube from one guy!\n\nquadcopter  unbalance\n", "tags": "quadcopter", "id": "4504", "title": "quadcopter cannot balance!"}, {"body": "what are the parameters which should be selected to choose camera for lane detection system.what parameters should be kept in mind (like picture quality,frame rate,cost e.t.c). which camera will suit best to my application.\n", "tags": "cameras", "id": "4510", "title": "which type of camera should be used for detecting road lanes for good processing in matlab"}, {"body": "using the adafruit 9dof module i need to convert the accel + magneto + gyro into euler angles for a motion capture application. any hints on where to start?\n\nmanaged to get x,y,z when the imu is facing upward but when that orientation changes the axes dont behave normally that is because i am not using euler angles. so any hints to any reference where to start?\n\nthe euler compass app is an example of what i am trying to get to.\nget pitch,yaw, roll for the imu module irrespective to how its kept.\n", "tags": "imu", "id": "4511", "title": "euler angles from 9dof imu"}, {"body": "i am trying to implement a particle filter for a robot in java. this robot is having a range sensor. the world has 6 obstacles - 3 in the top and 3 in bottom. i am calculating the distance of the robot from each obstacle's center and then performing the same activity for each particle. then, i calculate the difference between the robot and particles. the particles for which the difference with robot measured distance are small, i give them higher probability in resampling.\n\nbut, the problem with this approach as told by my friend is that i am assuming that i already know the locations of the obstacles, which make this all process useless. how should i approach it rather in a sense that i don't know the obstacles. how can the particle filter be implemented then? how will the particle filter work in case i don't know the obstacles location? an example of process would be great help. thanks\n\n\n", "tags": "localization particle-filter", "id": "4513", "title": "whats the logic to implement a particle filter for a robot with range sensor?"}, {"body": "i saw this maze, and tried to apply pledge algorithm on it. but i am not able to solve this maze using this algorithm. what am i missing?\ni want to ask, what i am doing wrong?\n\npledge algorithm:\nin both cases we don't get to exit.\n\n\nyou can read about these algorithms at:\n\nhttp://en.wikipedia.org/wiki/maze_solving_algorithm\n\nhttp://www.astrolog.org/labyrnth/algrithm.htm\n", "tags": "mobile-robot", "id": "4514", "title": "pledge algorithm for maze solving robots"}, {"body": "what are the most basic skills and components needed for creating a robot which gets two \"yes\" or \"no\" inputs with two push buttons, and goes down the defined flowchart and plays the relevant audio file each time it gets an input.\n\na flowchart like this:\n\n\n", "tags": "microcontroller", "id": "4516", "title": "how can one create a robot which respondes to input following a flowchart?"}, {"body": "i am trying to build low cost and precise outdoor positioning. i explored ns-raw with rtklib - this would be doable but probably will need either a base station to get the correction data for rover or external correction data which may be a hassle. the action radius with own base station is quite limited too. this solution is not really straightforward while you have to deal with either in-house or streamed correction data.\n\ni am wondering whether one would be able to substantially improve the accuracy of an ordinary (uncorrected) gps+glonass device (maybe one found in a common smartphone) with stereo visual odometry. today's consumer gnss chips seem to have reasonably stable accuracy in the 5m range. the viso 2 library has a translation error of about 3% on 500m distance. the idea is to use the visual odometry for \"smoothing\" the rough gps track. \n\nthe question is how this can be technically done in terms of sw. the input would be two tracks - one from gps device and the other viso 2 library. i think i need a kind of filter that will fuse the sensor data to get greater precision.\n", "tags": "computer-vision gps sensor-fusion odometry", "id": "4520", "title": "correcting gps track with visual odometry (sensor fusion)"}, {"body": "i know that there is an extended kalman filter approach to simultaneous localization and mapping.  i'm curious if there is a slam algorithm that exploits the ensemble kalman filter.  a citation would be great, if at all possible.\n", "tags": "kalman-filter slam reference-request", "id": "4523", "title": "ensemble kalman filter slam"}, {"body": "i am tuning pid for quadcopter, the problem i have is that with different base throttle, i seems that i have to adjust different pid gains in order for the quadcopter to balance! \n", "tags": "pid", "id": "4526", "title": "pid tuning quadcopter problem"}, {"body": "i am planning to buy an esc for my tricopter setup. what is the purpose of programming an esc? i am cost effective and is it really necessary that i should necessarily buy a programming card to program my esc for my model?\n", "tags": "esc", "id": "4531", "title": "purpose of programming an esc"}, {"body": "i am looking for a 12v dual motor controller that can supply at least 5a per channel for two 12v motors, and that can be used with an arduino. do you know of any product with those specs?\n\nthanks\n", "tags": "arduino motor ros h-bridge", "id": "4533", "title": "12v arduino dual bridge to supply at least 5a"}, {"body": "i'm building a control system with a parrot ar 2.0 drone where i have access to thrust controls for up/down (z), left/right (y), forward/backwards (x), turn left and turn right (yaw) through a ruby library on my computer.\n\nthe goal of the system is to keep the drone a particular distance from and parallel to a wall while moving in the up/down and left/right directions. we have added two sonar distance sensors to the left and right forward props. \n\nthe main problem i am having is figuring out how the two distance sensors equal a yaw reading (\u03c8) so i can feed that into the pid and then take action on thrust to the turn left or right for correction.\n\nmaybe just getting some help with the conversion from two distances to the yaw angle would be a big help, but any thoughts on the pid are greatly appreciated too since it is my first time working with it.\n", "tags": "sensors quadcopter pid", "id": "4536", "title": "yaw angle calculation for drone pid from two distance sensors"}, {"body": "i have an rsl line sensor which is designed to distinguish black and white lines. it detects white surface and gives me digital 1 as output, with 0 in case of black, but the surface needs to be close to it.\n\nas it uses infra-red-sensors, i wanted to use this sensor as a proximity sensor, to tell me if there is a white surface near it. is it possible to do this?\n\ni think the only problem here is that we need to increase it's range of giving 1. currently, it gives 1 only when white surface is too close to the sensors. i want 1 even if the white surface is there at a bit more distance.\n\nalso there is an adjustable screw there to adjust something, under which pot is written. i am working with an arduino.\n", "tags": "arduino mobile-robot sensors", "id": "4540", "title": "can we use this line sensor as a proximity sensor?"}, {"body": "how to compute the angular and linear velocities quaternions? i am new to this area and although i have studied the algebra i am unable to understand how to compute the velocities.\n", "tags": "kinematics", "id": "4542", "title": "velocity derivatives using quaternions"}, {"body": "i'm currently building a hexapod bot, composed with an arduino mega board and an usb ssc-32 (from lynxmotion).\nbut now i want add a ps3 wireless controller to move my hexapod, i have made some search but nothing realy interesting. maybe the servoshock module but it seems works only with the servoshockshield, a kind of arduino card with servo output. \n\n\ncan i use the servoshock module alone ? \ncan i connect it with rx/tx port of the arduino mega board ?\ndo you have other solution for me ? board with documentation and sources codes ?\n\n\nthank you all\n", "tags": "arduino wireless", "id": "4545", "title": "how to control an arduino board with wireless ps3 controler?"}, {"body": "i am designing a badminton robot but i am very confused about mechanisms needed for a badminton robot and various calculations needed for millisecond response.i am also confused about calculations needed about the forces needed and efficient angles needed for hitting the shuttlecock.please suggest me some ideas or suggestions needed for construction of badminton robot.\n", "tags": "sensors design mechanism actuator servomotor", "id": "4546", "title": "what are the calculations for a badminton robot and mechanisms?"}, {"body": "i have to use kinect for an application. however,  the final work must be mobile: it means no computer. consequently, i thought using a microcontroller to handle data from kinect. but is it possible? my job is mesuring some points of a body (axis x, y, z) and get back these coordinates. i don't know if i'm enough accurate. \n", "tags": "microcontroller kinect", "id": "4554", "title": "using kinect for medical application but without computer. is it possible?"}, {"body": "i have made a rc robot from a wheelchair and i'm planning to attach a snow plow. i'm wondering if there is any mechanism that would be able to lift the plow when reversing. \ni have only 2 channel transmitter so i can't control the plow's movement through it so i was thinking of some mechanical lift that triggers when reversing.\n\ndo you guys know about something i could use for it?\nthanks.\n", "tags": "wheeled-robot mechanism", "id": "4555", "title": "reverse lift mechanism"}, {"body": "i have a big problem trying to stabilize a quadrotor with a pd controller.\nthe model and the program has been written in c++ and the model dynamic has been taken from this source in internet: \n\nwell, in my code i wrote the model like in the eq. system ( see eq. 3.30 on page 21):\n\n\n\nwhere  and  are structures defined in a class to store position, velocities and accelerations of the model given the 4 motor velocities about all 3 axis.\n\n$$\n\\large \\cases{ \n \\ddot x = ( \\sin{\\psi} \\sin{\\phi} + \\cos{\\psi} \\sin{\\theta} \\cos{\\phi}) \\frac{u_1}{m}  \\cr\n \\ddot y = (-\\cos{\\psi} \\sin{\\phi} + \\sin{\\psi} \\sin{\\theta} \\cos{\\phi}) \\frac{u_1}{m} \\cr\n \\ddot z = (-g + (\\cos{\\theta} \\cos{\\phi}) \\frac{u_1}{m} \\cr\n \\dot p  = \\frac{i_{yy} - i_{zz}}{i_{xx}}qr - \\frac{j_{tp}}{i_{xx}} q \\omega + \\frac{u_2}{i_{xx}} \\cr\n \\dot q  = \\frac{i_{zz} - i_{xx}}{i_{yy}}pr - \\frac{j_{tp}}{i_{yy}} p \\omega + \\frac{u_3}{i_{yy}} \\cr\n \\dot r  = \\frac{i_{xx} - i_{yy}}{i_{zz}}pq - \\frac{u_4}{i_{zz}}\n}\n$$\n\nonce i get the accelerations above i m going to integrate them to get velocities and positions as well:\n\n\n\nthe model seems to work well but, as like reported in many papers, is very unstable and needs some controls.\n\nthe first approach for me was to create a controller (pd) to keep the height constant without moving the quadcopter, but just putting a value (for example 3 meter) and see how it reacts.\n\nhere the small code i tried:\n\n\n\nthe problem, as you can see here in this video, is that the copter starts falling down into the ground and not reaching the desired altitude (3.0 meters).\nthen it comes back again again like a spring, which is not damped.\ni tried already many different value for the pd controller but it seems that it doesn't affect the dynamic of the model.\n\nanother strange thing is that it goes always to a negative point under the ground, even if i change the desired height (negative or positive).\n\nwhat s wrong in my code? \ncould you me please point to some documents or code which is understandable and well documented to start?\n\nthanks\n\nedit:\nmany thanks to your suggestion.\nhi was really surprise to know, that my code had lots of potential problems and was not very efficient. so i elaborate the code as your explanation and i implementers a rk4 for the integration. after i ve read those articles: here and here i got an idea about rk and its vantage to use it in simulations and graphics pc.\nas an example i rewrote again the whole code:\n\n\n\nwhich is much more clear and easy to debug. here some useful functions i implemented:\n\n\n\nnow i m really lost because...because the simulated qudrotor has the same behavior as before. nevertheless i ve implemented the same pd algorithm as discussed in the paper, it stabilize on z (height) but it get really crazy due to unstable behavior. \nso... i dunno what is wrong in my code and my implementation. and above all i cannot find any source in internet with a good self explaned dynamic model for a quadrotor.\n\nregards\n", "tags": "control quadcopter", "id": "4558", "title": "pd algorithm for a quadrotor [simulation]"}, {"body": "while reading the paper \"multirotor aerial vehicles: modeling, estimation, and control of quadrotor\" by mahony, kumar and corke, i stumbled across the following equations for a non-linear attitude observer, which i would like to implement, but i believe there is something wrong.\n\n$\\dot{\\hat{r}} := \\hat{r} \\left( \\omega_{imu} - \\hat{b} \\right)_\\times - \\alpha \\\\\n\\dot{\\hat{b}} := k_b \\alpha \\\\\n\\alpha := \\left( \\frac{k_a}{g^2}((\\hat{r}^t \\vec z) \\times a_{imu}) + \\frac{k_m}{|^am|^2} ((\\hat{r}^t {^am}) \\times m_{imu}) \\right)_\\times + k_e \\mathbb{p}_{so(3)} (\\hat{r} r_e^t)$\n\nwhere $\\hat{r}$ and $\\hat{b}$ are etimates of orientation and gyroscope bias, $\\omega_{imu}, a_{imu}, m_{imu}, r_e^t$ are measurements and $k_x$ are scalar gains, which may be set to 0 for measurements that are not evailable.\n\nnow $\\dot{\\hat{r}}$ and $\\alpha$ need to be matrices $\\in \\mathbb{r}^{3\\times 3}$ due to their definitions. $\\hat{b}$ and thus $\\dot{\\hat{b}}$ need to be vectors $\\in \\mathbb{r}^3$. but then what is the correct version of the second equation $\\dot{\\hat{b}} := k_b \\alpha$?\n", "tags": "control quadcopter", "id": "4562", "title": "non-linear complementary filter on so3: corrected equations?"}, {"body": "i'm trying to make a quadcopter from scratch, i have a fair amount of experience with adruinos, and i'm trying to understand how to necessary systems work, and i can't seem to figure out what pid means, is it a method of regulating pitch and roll? like a stabilizer? i think from what i've read that its a system that detects orientation of the craft and tries to correct it\n", "tags": "arduino quadcopter microcontroller pid beginner", "id": "4568", "title": "what is a pid as is related to quadcopters"}, {"body": "i'd like to slice and dice floor tile into pieces so i can arrange it in geometric patterns. i have cad designs for the parts. would any consumer grade cnc machine be capable of doing the job?\n", "tags": "cnc", "id": "4569", "title": "are consumer grade cnc machines capable of cutting tile?"}, {"body": "i am trying to calculate likelihood of laser scan($z$) at give pose($x$) with known map ($m$) using beam based model\n\n$p\\left(z_t|x_t,m \\right)=\\prod_{i=1}^{n}p'\\left(z_i|x_t,m \\right)$    \n\nmy scan has 360 rays i.e $n=360$, when i calculate $p\\left(z_t|x_t,m \\right)$ it becomes zero as multiplication all propabilities $&lt;1.$ \n\nin ros amcl they are using ad-hoc which works better like\n\n$p\\left(z_t|x_t,m \\right)+=\\sum_{i=1}^{n}p'\\left(z_i|x_t,m \\right)*p'\\left(z_i|x_t,m \\right)*p'\\left(z_i|x_t,m \\right)$\n\nlater they normalise it with number of particle to get weight of each particle.\n\nmy query is how to get probability normalised and not zero with single calculation (i.e image in case of single particle)\n\nthanks.\n", "tags": "mobile-robot slam laser probability", "id": "4573", "title": "laser beam based model probability in case of single particle"}, {"body": "i would like to create an infinite-horizon, continuous-time lqr with a cost functional defined as\n\n$$j = \\int_{0}^\\infty \\left( e^t q e + u^t r u \\right) dt$$\n\nwhere e is the states' error $x-x_d$, but i have trouble concluding to the appropriate ricatti equation since $x_d$ is a function of time therefore leading to a term of $\\dot x_d$ . is this problem solvable? any ideas?\n", "tags": "control", "id": "4575", "title": "use linear quadratic regulator to minimize output error"}, {"body": "i purchased a pololu dual mc33926 motor driver shield for arduino, and for some reason i cannot read current from the motor controller. on the serial.println() it just prints weird data (garbage), and when i use ros (robot operating system) i only see -0.0 (minus zero) value for both motors.\n\nall i've done is plug the shield on my arduino uno r3 model, and run the demo that comes with the sample library --\nhttp://github.com/pololu/dual-mc33926-motor-shield .\n\nhow can i fix this issue?\n", "tags": "arduino ros actuator stepper-driver current", "id": "4577", "title": "can't read current on pololu dual mc33926 motor driver shield for arduino"}, {"body": "some friends and i are interested in working on a robot. i know little to nothing about robotics, but do have a lot of experience with programming. before we start, i am hoping that i can find some development kits or libraries that will help aid the goals of the robot. which are:\n\n\nrobot needs to move from point a to point b. while moving, it needs to detect rocks (approx. 1 foot diameter) on ground. it needs to detect rocks that are big enough to stop it, turn away from them, and proceed. \n\n\nin theory, we will want to detect the kinect's angle via the accelerometers, and use that data to obtain cartesian coordinates of the ground from the kinect's sensors. later, we will want a way to assemble a 'map' in the robot's memory so that it can find better paths from a to b.\n\nright now we aren't concerned with the motors on the robot - only the vision element. ie, i am not really interested in software that interfaces with the motors of the robot, only only something that interfaces with the kinect. \n", "tags": "computer-vision kinect", "id": "4578", "title": "kinect - development kit to aid obstacle-avoiding robot?"}, {"body": "actually , i have been since two weeks looking for convinced and final solution for my problem , actually i am completely lost , i am working on mobile robot (rover 5) with 2 motors , 2 encoders . the controller that designed to the robot needs to know the odometery of mobile robot (x ,y, heading angle ) , actually i am trying to function the encoders for this purpose , getting x ,y, heading angle by measuring the traveled distance by each wheel , so to get the x ,y, heading angle values , i should compute a accurate readings without missing any counts or ticks as could as possible .\n\nthe problem now is :\n\nin the code in the attachment , while i am testing the encoders counts , i noticed that there is a difference between counts of encoders even when they spin in the same constant speed (pmw) , the difference increases as the two motors continue . so i thought that is the main cause of inaccurate odometery results .\n\nin the output of the code (in the attachment also) the first two columns are right and left motors speed , the third &amp; forth columns are right and left encoder counts , the fifth column is the difference between two encoders count , as you could see ,that even when the speed of two motors are approximately the same (each motor feed up with 100 pwm) there is a difference in the encoder counts and as you could see that the difference become big and big as the motors continuing spin .\n\none thing i thought that sending the same pwm value to two different motors will almost never produce the exact same speed , so i think that i should detect the absolute motion of the motors and adjust the power to get the speed/distance , but when i test the speed of motors after feed them with 100 pwm at same time , the two speeds were almost identical , but i noticed that there is a difference between counts of two encoders even when the motors spin in the same constant speed .\n\nactually , i don't know where is the problem , is it in the code ? is it in the hardware ? or what ? i am completely lost , i need for patient someone to help.\n\n\n\nthe result:\n\n\n", "tags": "mobile-robot wheeled-robot quadrature-encoder", "id": "4579", "title": "quadrature encoder counts"}, {"body": "i'm currently designing a linear camera slider, that will be used to hold camera equipment weighing just about 15 kgs including all of the lenses and monitors and everything else. \n\nfor those who don't know what a camera slider is, it's a linear slider on top of which a camera is mounted and then the camera is slided slowly to create some nice footage like this.\n\nthe problem\n\nnow, looking at the commercially available camera sliders out there, there seems to be two ways in which the motor maybe mounted on these sliders:\n\n\nmotor mounted on the side:\n\n\nmotor mounted directly on the carriage:\n\n\n\n\ni would like to know which option would be optimal - performance-wise (this slider maybe used vertically too, to create bottom to top slide shots), efficiency-wise and \nwhich one of these two will be resistant to motor vibration (these motors vibrate a lot, the effects of which may sometimes leak into the produced footage).\n\nadditional questions\n\n\nmotor mounted on the carriage directly maybe, just maybe more efficient, but it also has to carry it's own weight in addition to the 15kg camera load?\npulling force is greater than pushing force (i have no idea why, would be great if someone explained why, atleast in this case?), so a motor mounted in the end should be able to lift vertically with ease?\ndoes a belt setup as shown in the first figure above really dampen the motor vibrations? will/won't the motor vibrating on the end get amplified (because, the whole setup will be attached to a single tripod in the exact center of the slider)\nwhich design will be less stressful for the motor, taking inertia into consideration for both cases?\nwhich one of these designs will be best suitable for vertical pulling of load against gravity?\n\n\nmanufacturers use both designs interchangeably, so it's hard to predict which design is better than which.\n\nany help would be much appreciated!\n\nplease note, this question has been migrated from the stackexchange physics (and electrical) forum by me because the mods thought it would be appropriate here.\n", "tags": "control design stepper-motor motion", "id": "4580", "title": "linear slider motor mount location - pros/cons"}, {"body": "it need not be as effective as lidar or it may have some disadvantages when compared with lidar.   what are the probable alternatives?\n\nedit:\ni'm intending to use it outdoors for navigation of autonomous vehicle. is there any low cost lidar or is there any alternative sensor for obstacle detection?\n", "tags": "sensors navigation cameras sonar lidar", "id": "4583", "title": "what are some low cost alternatives for lidar?"}, {"body": "i need two state linear actuator. you can have a look at the picture to understand what i mean. \n\n\n\ndon't care about the hand !\n\n\n\ni need to electrically move the things like this squares up and down. bidirectional linear actuators are needed.\nwhat is the cheapest and tiniest actuator (or sth else) that i can use to move this squares up and down. there are just two states ('up','down'). don't care how much higher a square rises, when it is up.\n", "tags": "actuator", "id": "4587", "title": "two state linear actuator"}, {"body": "i am implementing a particle filter in java. the problem with my particle filter implementation is that the particles suddenly go away from the robot i.e the resampling process is choosing particles which are away from robot more than those which are near.it is like particles chase the robot, but always remain behind it. i am trying to find the root cause, but to no luck. can anyone please help me where i am going wrong?\n\ni am adding all the imp. code snippets and also some screenshots in consecutive order to make it more clear. \n\n\n\n\n\n\n\ndetails:\n\ni am using a range sensor which only works in one direction i.e. its fixed and tells the distance from the obstacle in front. if there is no obstacle in its line of vision, then it tells the distance to boundary wall. \n\ncode:\ncalculating range\n\n\n\ncalculating weights\n\n\n\nresampling\n\n\n", "tags": "localization particle-filter", "id": "4589", "title": "particles not behaving correctly in the implementation of particle filter"}, {"body": "i am trying to write a simple program where the robot(lego nxt2) will follow a blue line.\n\n\n\ni am using an nxt color sensor and the problem is that only 1 motor is moving. i know that none of the motors are broken either because i tested them out.\ncan somebody help me diagnose my problem?\n", "tags": "sensors robotc", "id": "4591", "title": "robotc color sensor error"}, {"body": "simply, when to use brushless dc motor and when to use servo motor ? \n\nwhat are the differences , specially when adding an encoder to the dc motor you can have the position and it will be similar to servo motor ? \n", "tags": "motor brushless-motor servomotor", "id": "4592", "title": "for robot wheel control : brushless dc motor or servo motor?"}, {"body": "i am on a robotics team that plans to compete in a competition where one of the rules is that no sort of sonic sensor is allowed to be used. i guess that limits it to some sort of em frequency right?\n\nideally, my team is looking for a simple beacon system, where beacon a  would be attached to the robot, while beacon b would be attached to a known point on the competition space. then, beacon a can give information about how far away b is. after some searching, i could only turn up laser rangefinders that required pointing at the target. i am a cs student, so i'm not familiar with the terminology to aid searches.\n\nanother nice property would be if the beacons also gave the angle of beacon a in beacon b's field of view, although this is not necessary, since multiple beacons could be used to obtain this information.\n\nwe have an xbox 360 kinect working, and able to track things and give distances, but it looses accuracy over distance quickly (the arena is about 6 meters long), and this beacon should be as simple as possible. we only need it for a relative position of our robot.\n\nalternate solution:\nanother way to solve this would be for an omni-directional beacon to only give angle information, two of these could be used to triangulate, and do the job just as well.\n", "tags": "localization electronics laser rangefinder", "id": "4595", "title": "do simple, non-sonic, omni-directional rangefinding beacons exist?"}, {"body": "in ekf-slam (based-feature map) once the robot senses a new landmark, it is augmented to state vector. as a result, the size of the state vector and the covariance matrix are expanded. my question is about the uncertainty of the new landmark and its correlation with other pairs of the covariance matrix. how should i assign them? when i assign them to be zero, the error of the estimation this landmark won't change as time goes. if i assign them with very large value, the estimation is getting better every time the robot reobserves this landmark however, the error approaches to fixed value not to zero. i assume the problem id with assigning the uncertainty. any suggestions?\n", "tags": "slam ekf errors mapping", "id": "4599", "title": "the uncertainty of initializing new landmark in ekf-slam"}, {"body": "for my quadcopter, i turn on the quadcopter while letting it stable on the ground. but i see that the roll, pitch fluctuate with the max difference being 15 degree. when i protect the sensor with soft material, then i observe the max difference is around 6 degree. is this fluctuation for the quadcopter? by the way, i use complementary filter and dcm with scaling factor being 0.8 gyro and 0.2 accel\nthanks in advance!\n", "tags": "quadcopter", "id": "4607", "title": "quadcopter roll, pitch fluctuation"}, {"body": "i'm trying to control a plane via roll  using pid controller , \n\ni had a problem finding the transfer function thus i used the following method :- \n\n\n  fix the plane in an air tunnel \n  \n  change the motor that controls the roll in fixed steps and check the\n  roll \n  \n  thus i will have a table of roll/motor degree \n  \n  next is to deduce the nonlinear function using wolfram alpha or\n  approximation neural network  .\n\n\nis this a correct method or should i try another method ? \n", "tags": "pid", "id": "4608", "title": "deducing single wing plane transfer function aka transfer function estimation through set of points"}, {"body": "i've been using mpu6050 imu unit ( gyro + accelerometer  ) \n\ni found that i can set acc range to +/- 2g or 4g till 16 g \n\nand same for gyro +/- 250 deg/sec , 500 deg/sec and so \n\ni know that they are low cost and full noise , so which settings to the range are best to ensure higher accuracy ? \n", "tags": "sensors imu accelerometer gyroscope", "id": "4609", "title": "is increasing gyro , accelerometer sensor range is good or bad ? how does it affect the accuracy"}, {"body": "i am trying to compute forward kinematics of the kuka youbot using dh convention: \n\nhttp://www.youbot-store.com/youbot-developers/software/simulation/kuka-youbot-kinematics-dynamics-and-3d-model\n\nthe arm joint 1 and arm joint 5 are revolute and rotate about the world z-axis (pointing to the sky)\n\nbut the other 3 joints are all revolute and rotate about x-axis, let's say (points horizontally)\n\ndh convention says the \"joint distance\" is along the \"common normal\". but unless i am mistaken, the only common normal is the y-axis, and that is also horizontal, meaning there is no joint distance.\n\ni was thinking i would use link offset for joint1 - joint2, but then i ran into a problem with joint4 - joint5. link offset is supposed to be along the previous z-axis, and in that case it would point horizontally out to nowhere. but link distance still doesn't work either, because that is the common normal distance, and as established the common normal is x-axis, also horizontal. so now i feel very screwed. i am sure there is a simple solution but i can't see it. \n\nso i guess the question is, how do i use the dh convention for the links between 1-2 and 4-5, when the joint rotational axes are perpendicular?\n", "tags": "kinematics forward-kinematics dh-parameters", "id": "4610", "title": "forward kinematics/d-h parameters for perpendicular joint axes"}, {"body": "i am designing a experiment of controlling 6 small wind turbines wirelessly. for each wind turbine, i need to measure power time series (or voltage or current time series) from the generator, and control blade pitch angle, yaw angle, and generator load (using variable resistance). the control input will be all pwm signal.\n\ni am planning to put an arduino uno with a zigbee wireless module to each wind turbine, making it measure the power time series and transmit to the central node, as well as receive the control input from the central node and command the control input to servo motors. the central node will be additional arduino uno.\n\nhere are my questions:\n\nis it possible for each arduino to send time series signal to central node wirelessly without interference with other arduino? (6 wind turbines transmitting time series to a central server). if it is possible, how can i implement such network ? recommending a source for learning would be also greatly helpful.\n\ninterface between the central node and the computer software: the algorithm in the computer need to process the received power time series and determine the optimum control input for 6 wind turbines. then these control input should be transmitted to wirelessly to 6 wind turbines. in such case, what is the good option to interface the algorithm and the arduino connected to the computer? currently the algorithm is written in matlab. i heard there is the sketch interfacing arduino and matlab, is it efficient enough for such project?\n", "tags": "arduino", "id": "4612", "title": "controlling multiple arduinos wirelessly"}, {"body": "i was playing the old \"confuse the cat with a flash-light\" game, when i thought that i might like to program a confuse-a-cat robot.\n\nsomething, probably with tracks, which can right itself if he flips it over, and which i can program to move randomly around a room, turning at walls, making an occasional sound or flashing a light.\n\nsince i am on a very tight budget, i wondered if there is some cheap kit which i can program ...\n\narduino, raspberry pi, any platform, so long as it is programmable.\n\nthanks in advance for your help\n", "tags": "mobile-robot", "id": "4625", "title": "seeking dirt cheap, wheeled, programmable robot"}, {"body": "i had a doyusha nano spider r/c mini-copter, it's controlled by a 4ch joystick 2.4 ghz.\n\ni look for a low cost method to control it from the computer. the software is not a problem, but how can i transform the wifi or the bluetooth signal of the computer to an r/c signal compatible with the mini-copter receptor?\n\nor is there another solution that is low cost?\n", "tags": "control quadcopter wireless", "id": "4627", "title": "control a 2.4 ghz ar drone from the computer"}, {"body": "sorry i am asking a mechanical question here, but, after all, where else people have experience with using motors? if there is a better forum for this, please do guide me.\n\neverywhere i've seen online, the stepper motor dyj48 is used in tutorials, to rotate on its own, or, at most, to spin a clothes pin attached to it. i am trying to get arduino to work for my 10 year old kid. he's got the motor rotating, now what? how does he attach anything to it?\n\ndon't laugh, i made him a wheel out of a raw potato. he is happy with it now. where can i find any guidance as to what to do next?\n", "tags": "arduino motor", "id": "4632", "title": "what mechanical parts can be attached to dyj48 stepper motor?"}, {"body": "i am trying to make a simple robot with few functionality for someone, one of these functionality is inflating a balloon inside the robot, i know how to control a compressor using arduino but the problem is that the requested task is bit different here:\nthere must be an air exit and it must be controllable through arduino, so he can inflate the balloon to a certain pressure, and depress the air from another exit if needed (i don't know if it is possible to have a depression through the same pressure-in valvle.\n\ni think that it can be done somehow using a solenoid 3/2 valve or something but i am bit unfocused these days and i need some hints.\n\nany thoughts?\n", "tags": "arduino wheeled-robot mechanism industrial-robot valve", "id": "4635", "title": "12v compressor and air pressure control"}, {"body": "i recently built a self-driving vehicle-type robot for a competition, and am looking to sell sensors (gps, ins, etc.) used in order to have money for the next project. is ebay where people tend to go looking for used sensors and hardware?\n", "tags": "sensors servos", "id": "4639", "title": "where do roboticists look for used sensors/hardware?"}, {"body": "i have a project that requires me to be able to accurately and repeatedly rotate an object 120 degrees.\n\nthe object is small and lightweight (let's say several grams). the axis does not necessarily have to always spin the same direction. it simply needs to be able to stop reliably at 0, +/- 120, and +/-240 degrees from the origin.\n\ni have very limited experience with motors and robotics, but my understanding is that a servo motor will be my best bet for accuracy (if that incorrect, please let me know).\n\nsince i know next to nothing about these motors, the spec sheets list a lot of specs which don't mean all that much to me. i'm hoping to learn, but in the mean time, what specifications do i need to be focusing on for these requirements?\n\nit doesn't need to be high speed. when i say accurate, it doesn't have to be absolutely perfect to the micrometer, but i would like it to be able to run through a loop of stopping at 0, 120, and 240 hundreds of times without visually noticeable variance - the more precise the better though.\n\nto be more specific about the accuracy. let's say the object being rotated will have a flat surface on the top at each of those 3 stopping points.  upon inspection the surface needs to appear level each and every time through hundreds of cycles.\n\ncould these requirements be met by a servo that might be used in building a quadricopter, or am i going to be looking for something higher grade than that?\n", "tags": "servomotor", "id": "4642", "title": "finding a light load high precision servo motor"}, {"body": "i am planning to control multiple dynamixel servos (mx28t or mx-64t) wirelessly using arduino mega. since this servo uses serial communication, i need an additional serial port to interface with xbee module. although it seems to be very common application controlling these servos wirelessly based on arduino, i could't find any of them in web. i found the two very well constructed libraries.\n\nhttps://code.google.com/p/slide-33/downloads/list. this library is for mx28t servo, which is the same servo i am trying to use, but it uses uno;therefore, i cannot interface with xbee.\n\nhttp://www.pablogindel.com/informacion/the-arduinodynamixel-resource-page/. this library use uart1 (serial1) to interface with servo (ax-12) motors. therefore, i can connect xbee module to uart0. but, the problem is that this library is outdated and not compatible with mx64-t servo anymore.\n\nso my question is here:\n\nis there any one who has experience in controlling dynamixel mx24t, mx64t servo series using xbee module simultaneously? if you have experience, please share with me.\n\nis it possible for arduino mega can interface with xbee module using serial1 (i.e., rx18 tx19)? if it can, i might be able to use the library1 without any modification.\n", "tags": "serial", "id": "4648", "title": "controlling dynamixel servo wirelessly using arduino mega"}, {"body": "i'm currently programming an app for a robot and i'd like to make him map a zone and then make him move autonomously from one point to another.\n\ni have to solve a slam problem, but the biggest matter is that i can't use landmarks to find myself in the environment. the robot just has the abilities to move, and to make distance measurements over -120/+120 degrees using a sonar.\n\ni can't find any simply explained algorithm that permits me to solve this slam problem with the no-landmark limitation.\n\nhave you any idea ?\n", "tags": "slam sonar", "id": "4650", "title": "slam without landmarks using sonar"}, {"body": "this is for a battle robot in the hobby-weight class (5.44 kg max)\n\ni want to drive the robot using  2 cordless drill motors rated at 14.4 volts. i have 4s lipos which means i have 4 x 3.7 volts or 14.8 volts. so far so good. \n\nthe problem is that i bought 2 escs and only afterwards noticed that they are rated for 2-3s (or max of 11.1 volts).\n\nso my question is am i likely to damage the esc if i use my 4s lipos instead of 3s lipos?\n\nor should i just buy 3s lipos and live with the reduced performance?\n", "tags": "motor esc", "id": "4656", "title": "over-voltage on a brushed electronic speed controller"}, {"body": "this is for a hobby-weight (5.44 kg) battle robot.\n\ni bought two escs for my drive motors but the escs do not have a reverse function (or brake for that matter).\n\nis there any simple way i can achieve this through maybe either: \n\n\nthe r/c settings (setting middle position of joystick as stopped, top-wards as forward and bottom-wards as reverse?) \nor could i maybe achieve this using arduino? i have a card with relay switches that i can use with the arduino so am not worried about high voltage or current but i am worrying it could get messy..\n\n\ni could just buy two new escs with the above features but they cost quite a bit more than the ones i already have so i would prefer to try a few tricks first - if there are any!\n", "tags": "motor esc", "id": "4657", "title": "adding reverse function to a brushed motor electronic speed controller"}, {"body": "newbie to robotics here! \n\ni bought a 5s lipo but now realise that it is overkill. and these things are expensive!\n\nso, given that (as far as i know) the pack is apparently made up of individual cells of 3.7 volts each, is there any way in which i could somehow (safely) separate out the cells to get a 3s and a 2s or even single 1s cells?\n", "tags": "battery", "id": "4658", "title": "can a 5s lipo battery be changed to a 3s and a 2s?"}, {"body": "how can i periodically estimate the states of a discrete linear time-invariant system in the form $$\\dot{\\vec{x}}=\\textbf{a}\\vec{x}+\\textbf{b}\\vec{u}$$ \n$$\\vec{y}=\\textbf{c}\\vec{x}+\\textbf{d}\\vec{u} $$if the measurements of its output $y$ are performed in irregular intervals? (suppose the input can always be measured).\n\n\n\nmy initial approach was to design a luenberger observer using estimates $\\hat{\\textbf{a}}$, $\\hat{\\textbf{b}}$, $\\hat{\\textbf{c}}$ and $\\hat{\\textbf{d}}$ of the abovementioned matrices, and then update it periodically every $t_s$ seconds according the following rule:\n\n\n  if there has been a measurement of $y$ since the last update: $$\\dot{\\hat{x}}=\\hat{\\textbf{a}}\\hat{x}+\\hat{\\textbf{b}}\\hat{u}+\\textbf{l}(y_{measured}-\\hat{\\textbf{c}}\\hat{x})$$\n  if not:\n  $$\\dot{x}=\\hat{\\textbf{a}}\\hat{x}+\\hat{\\textbf{b}}\\hat{u}$$\n\n\n(i have omitted the superscript arrows for clarity)\n\ni believe that there may be a better way to do this, since i'm updating the observer using an outdated measurement of $y$ (which is outdated by $t_s$ seconds in the worst case).\n", "tags": "control sensor-fusion", "id": "4663", "title": "how to periodically estimate states of a lti if the output is measured irregularly?"}, {"body": "how could i compute the shortest path between point a and b using wave planner?\n\ni don't see how using the wave planner would give me the shortest; it would just give me a path! as far as i can tell, i would only be able to give a random path to the destination, but nothing else than that.\n", "tags": "theory mapping", "id": "4672", "title": "shortest path using wave planner?"}, {"body": "i know that some dc motors produce a lot of torque but only actually move at a very slow rate, while others do the exact opposite. i know that i need some sort of balance between torque and the rpm's of the motor for use in a underwater thruster, but i am not sure what i should favor more, torque or rpm's? also, it would be great if someone could suggest a motor at or below the $300 range for a urov.\n", "tags": "motor torque", "id": "4674", "title": "what should i be looking for in a dc motor that will be used in a urov thruster?"}, {"body": "i have a question regarding the implementation of a quadrotor's position controller. \nin my matlab model the quadrotor takes 4 inputs: a desired altitude ($z_{des}$) and desired attitude angles($\\phi_{des}$, $\\theta_{des}$, $\\psi_{des}$) which reflects the motion described by the differential equations of the model (see last picture). \n\n\n\nhere an insight into the implemented matlab dynamic model. as you can see it has a structure like an inner loop controler:\n\n\n\nanyway...it \"hovers\" perfectly on the starting point. (perfect graphs :) )\nnow i just need to go over and implement a sort of position controller to let the quadrotor to get from a start to a goal point, defined as usual through 3 coordinates $[x_d, y_d, z_d]$. \n\nthat's tricky because i don't have the same space state variables as input and output of the system. so the controller must take a vector of three coordinates and be able to output 3 different angles to get there. the only exception is the height because it will be simply bypassed by the controller and doesn't need another calculation loop. a different story is for the three angles... \n\nmy first idea was to simply create a feedback between the position given at the output of the simulated system and the desired position as in the figure above.\nbut that rises another question: my quadrotor model solves the following equation system:\n\n$$\n\\large \\cases{ \n \\ddot x = ( \\sin{\\psi} \\sin{\\phi} + \\cos{\\psi} \\sin{\\theta} \\cos{\\phi}) \\frac{u_1}{m}  \\cr\n \\ddot y = (-\\cos{\\psi} \\sin{\\phi} + \\sin{\\psi} \\sin{\\theta} \\cos{\\phi}) \\frac{u_1}{m} \\cr\n \\ddot z = (-g + (\\cos{\\theta} \\cos{\\phi}) \\frac{u_1}{m} \\cr\n \\dot p  = \\frac{i_{yy} - i_{zz}}{i_{xx}}qr - \\frac{j_{tp}}{i_{xx}} q \\omega + \\frac{u_2}{i_{xx}} \\cr\n \\dot q  = \\frac{i_{zz} - i_{xx}}{i_{yy}}pr - \\frac{j_{tp}}{i_{yy}} p \\omega + \\frac{u_3}{i_{yy}} \\cr\n \\dot r  = \\frac{i_{xx} - i_{yy}}{i_{zz}}pq - \\frac{u_4}{i_{zz}}\n}\n$$\n\nthat means that they expect (as in the matlab model above) the desired angles and height. \nbut now i need right the inverse: given a desired position calculate the right angles!!! \nfor the direction is the solution really simple, since i can write something like:\n\n\n\nwhere y and x lies on the horizontal plane. this is not so simple for the other two angles. so what can i do at this point? just \"invert\" the given equations to get the desired angles?\n\nanother idea could be to implement a simple pd or pid controller. this is much more easier given the fact that i can experiment very quickly using simulink and get very good results. but the problem is here again: how get i the desired angles from a desired position?\n", "tags": "pid quadcopter", "id": "4675", "title": "position controller for a quadrotor"}, {"body": "i would like to estimate the yaw angle from accelerometer and gyroscope data. for roll and pitch estimate i've used the following trigonometric equations:\n\n\n\nand a simpified version of the kalman filter to consider also angular rates. the roll and pitch estimates are accurate (accelerometer values need to be filtered in presence of chassis vibrations).\n\n\n\nin order to get the yaw angle i'm using the following equation:\n\n\n\nbut the it doesn't work. do you have any advice?\n", "tags": "sensors quadcopter kalman-filter imu accelerometer", "id": "4677", "title": "how to estimate yaw angle from tri-axis accelerometer and gyroscope"}, {"body": "i use mpu9150, also use dcm and complimentary filter to compute roll, pitch and yaw. however, my yaw is not so smooth. how can i solve that problem?\n\ni looked at the datasheet of mpu9150, but i didn't see anything related to sampling frequency of magnetometer like gyro and accel.\n", "tags": "quadcopter", "id": "4679", "title": "mpu9150 - yaw angle drift"}, {"body": "i'm building an autonomous sail boat (ripped out the guts of an rc sail boat and replaced with my own mainboard etc.)\n\nthe controller board i have can accommodate both an mpu9150 and an hmc5883. is there any advantage is using both magnetometers for a tilt-compensated heading? i'm thinking that i could compute the unit vector with soft/hard iron offsets removed for both, and then average  the two vectors to get one slightly better one?\n\nnot sure if it would yield a better result though.\n", "tags": "sensors sensor-fusion", "id": "4682", "title": "is there an advantage to multiple magnetometers for heading computation"}, {"body": "i am looking to write and test my own control algorithms for tricopter flight. i am looking for a simulator that can simulate a tricopter but at the level of receiving simulated pwm and returning simulated gyro, compass and other sensor readings. ideally it would also have graphics for visualization (need not be fancy). ultimately, i want to port this to a real tricopter but at the moment i would just like to simulate it. any suggestions for free simulators that are low level as i described? \n", "tags": "control quadcopter simulator", "id": "4687", "title": "tricopter simulation to test control algorithms"}, {"body": "i have the formulas to derive the rpm's of each wheel from the robot's linear velocity.\nnow, i am trying to do the same thing for the acceleration (mainly angular acceleration).\nfor linear acceleration i am always assuming that the linear velocity of the wheels is the same as the robots when the robot is moving on a straight line...according to physics. am i right?\n\nbut angular acceleration seems more complicated, specially when the robot is following a curved path (not necessarily turning in place).\n\nany readings or ros packages that deal with this acceleration issue?\n\nthanks\n", "tags": "ros wheeled-robot differential-drive", "id": "4688", "title": "acceleration formula for a differential steering robot"}, {"body": "i'm looking for a way to transport balls (diameter 50mm) 220 mm up with over a slope with a length of 120 mm. currently i'm considering the usage of a belt system but i cannot seem to find a good belt system.\n\nbecause of space constraints within my robot, normally i would probably take a nylon belt and jam nails trough it to make little slots and then use that. however this would result in considerable reduction in available space as it means that i have to also take into account the extra space required for the nails on the way back. this means that ideally there would be a way to reduce the space used by the nails on the way back.\n\ndoes anybody have a good solution for this?\n", "tags": "kinematics manipulator", "id": "4689", "title": "short distance ball transport"}, {"body": "i'd like to track my run in an indoor tennis court. gps won't be available so i was thinking researching for other solutions:\n\naccelerometer: i concluded it's a no go because while playing tennis the player makes a lot of movements that include spinning his body that can alter the data.\n\nthen i thought that a 3/4 point ir system might help but again from what i've understood it's hard for the ir system to track the movement since they won't be able to focus on the player.\n\nso my final thought went to radio systems but i couldn't find any info and it's also hard for me to see a theoretical solution at least on how i can mesure the movement/speed of the player.\n\nso here is my question: is there any existing system that is able to track random movement of an object (athlete) and give info like speed and distance? is there anywhere resources about how such a system might be achieved or at least the exact technology used for it?\n\nany suggestions and ideas are greatly appreaciated.\n", "tags": "arduino kinematics movement", "id": "4693", "title": "is it possible to track movement on a tennis court?"}, {"body": "i want to communicate the tiva c arm cortex m4 to sensorhub from ti which has multiple sensors with different i2c addresses such as mpu9150, bmp180, temperature sensors...\nwith a single i2c slave, i can communicate to it successfully, but if my project involves interface microcontroller with both mpu9150 and bmp180, then i get stuck.\nanybody suggest me the process of commnunication in this case?\n", "tags": "i2c", "id": "4698", "title": "interface multiple i2c slaves to microcontroller"}, {"body": "i want to build a simple obstacle avoider robot, but this time i want it to be self-recharging so i am building a dock for this purpose, so i want it to be able to locate the dock and go for it when battery voltage is lower than a fixed value.\n\ni am having trouble to chose the right components for locating the dock, i think i am going to use an ir emitter on the dock so the robot can head toward it when battery is low (let's forget about the orientation problem for the moment, but if you have any thoughts about it that will be helpful) but i am not sure if the robot is able to detect the ir led (or whatever) from a long distance (over 10 meter)\n\nis it possible to use this solution for this distance? if not, what do you suggest?\n\n(if there is a simple ready solution to buy that's ok, let's say i have no budget limit)\n", "tags": "sensors localization wheeled-robot battery wireless", "id": "4699", "title": "robot docking for self-recharging"}, {"body": "i have a basic question because i'm trying to understand right now a concept that i thought it was obvious.\nlooking at this video he is going to feedback the variable state x with the input of the system, which is a force f. \n\nnow, if i'm correct it is only possibile to feedback variables which share the same units, so i expect to drive a meter through an input variable which is a meter and the difference will be then feed into the pid. is the example in the video just to show up how to use simulink?\nor i m wrong? \n", "tags": "pid", "id": "4701", "title": "a general question about pid controller"}, {"body": "i am trying to make a nerf sentry gun to shoot my co workers. i am building it more or less from scratch and have come to the part where i need to come up with plans to assemble it. i am looking for advice on how to mount the mg995 servos to allow them to tilt and pan. i originally thought about having a base with a metal rod through the middle and use a gear to control the pan functionality. the idea is it would mimic a skateboard truck with a gear that would turn the rod through the middle and pivot the shooting mechanism. another idea was to have the metal plate sit on top of the servo and use one of the attachments to attach it to the top plate. the problems i see behind this is the attachment is just a small piece of plastic and over a short period of time i could see this wearing out especially if the shooting mechanism is not centered perfectly. i also need to come up with a solution to make it tilt but i think i have an idea for this to simply use a rod with a gear to turn the pvc pipe barrel. \n\nhere are the servo's i am using\n\nsorry if this is the wrong forum for the question but i was unsure where else to look for some expert advice. \n\nedit 1\n\nfor anyone intersted i found an example of someone doing almost exactly the same thing with blueprints. i am going a slightly simpler / cheaper route and mounting the servo to the bottom of the spinning plate between the lazy susan plate i ordered. this way i don't have to buy the gears which are rather expensive and without the gears it may reduce some of the torque.\n\nhttp://projectsentrygun.freeforums.org/build-progress-gladiator-ii-paintball-sentry-t130.html\n", "tags": "mechanism servos servomotor", "id": "4702", "title": "advice on mounting a servo for a nerf sentry gun"}, {"body": "i have this idea or a very curious question in my mind. i am no where near professional though, but i would like it to be answered. \n\nwe all know how wind turbines can be used to generate electricity. so is it possible to create a quadcoptor that will start with some minimal power by small battery but in time will sustain and keep its system on by self generating electricity and keep on rotating its rotors on its own without other external supply? \n", "tags": "quadcopter", "id": "4704", "title": "self powered quadcoptor"}, {"body": "i am currently building a hobby-weight (5.44kg) robot and will be using 2 x 14.4 cordless driller motors for my wheels. \n\nthe thing is i keep reading about high amperages when working with r/c models such as quadcopters but when i connect my cordless driller motor to my bench power supply and monitor current draw it never rises above 3.2 amps even when i try to stop the motor by hand. \n\nof course in the arena in the event of a stand off i have plastic wheels which will slip so i am not too concerned about stall currents. \n\ni am now left wondering whether i have mis-calculated or whether people make a lot of fuss about high currents for nothing. or do these currents only perhaps really apply to brush-less motors?\n", "tags": "motor", "id": "4705", "title": "amperage on brushed motors"}, {"body": "i am currently building a hobby-weight robot (5.44kg) and will be using 2 x 14.4v cordless drill brushed motors to drive my wheels. \n\ni have read somewhere that due to \"induced currents\" when i turn the motor off (or reverse it presumably?) i should protect it by using a diode or a capacitor across the terminals. \n\nwhich should i use (capacitor or diode) and what are the parameters i need to consider for these components (voltage or current)? \n\nsome answers to a similar question discussed capacitors but not diodes. are diodes relevant?\n\nwould i seriously damage the cordless drill (presumably quite tough) motor if i did nothing?\n\nand don't motor controllers have any form of inbuilt protection for the motors anyway?\n", "tags": "motor", "id": "4706", "title": "diode or capacitor across terminals of brushed motor"}, {"body": "i am building a hobby-weight robot and my weapon of choice is a spinning disk at the front.\n\nas regards the disk i was thinking of buying commercial (grinder-type) disks and change type of disk depending on the \"enemy's\" chassis construction material. so for instance i would have an aluminum cutting disk if the enemy's chassis is made of aluminum and so on.\n\nfirst question is therefore; do such disks do the job in practise (or break, fail to cut?)\n\nsecondly, should i use a brushed or brush-less motor for the disk? i actually have escs for both but sort of feel a brushed motor will give me more torque while a brush-less motor might give me more speed. so which is more important speed or torque? \n\ni do know - from my uncle who uses metal lathes - that machines that cut metal usually spin at a slower speed (drills, cutting wheels etc)- indeed he likes to say that metal working machines are safer than wood-working ones partially for this reason. \n\nbut i am a newbie and really would like to have an effective weapon if possible and breaking or not-cutting disks do not make such a weapon!\n\nalso is it normal practise to use one battery for everything (drive and weapon) or have two separate batteries?\n", "tags": "motor battle-bot", "id": "4707", "title": "spinning disk weapon"}, {"body": "in a lab build i'm doing, i'm stuck at this problem, so i am fishing for suggestions.\n\ni'm creating a turn-table type setup where i need to make readings (with a nanotube-tip probe i've already designed, similar to an afm probe) on the very edge/circumference of a 10 cm radius disk (substrate).\n\nthe current hurdle is: i need to get the substrate disk to move circularly in steps of 0.1 mm displacement -- meaning, i occasionally need to stop at certain 0.1mm-increment positions.\n\nwhat would be a way i can achieve this, assuming an accurate feedback system (with accuracy of say ~0.1 mm, e.g., with quadrature optical encoders) is available if needed for closed-loop control?\n\nspecs of commonly sold steppers don't seem to allow this kind of control. i'm at the moment trying to study how, e.g. hard disks achieve extreme accuracies (granted they don't have such large disks).\n\ncertainly, direct-drive like i'm currently building (see below image) probably doesn't help!\n\n\n", "tags": "motor stepper-motor", "id": "4713", "title": "rotate (and stop) a large disk in very tiny increments"}, {"body": "i am new to morse and robotics.\n\nthis code control the robot by giving the linear and angular velocity.\n\nthis is the scene description\n\n\n\nand this is the control script\n\n\n\nthe robot moves well. but when i remove the semantic cameras from the scene description the robot do not move. i am confused, they are just sensor, why the robot don't move ?\n", "tags": "mobile-robot sensors control", "id": "4715", "title": "control a robot with pymorse on morse simulator question"}, {"body": "i am currently building a hobby-weight (5.44kg) robot. the weapon will be a vertical spinning disk at the front. it will probably be a commercial one from the hardware store or i could maybe get one made.\n\ni have 2 cordless drill motors to drive my wheels so i should be ok there, but i am still lost where it comes to what motor i should get for my weapon. i am now inclined to think it should be brush-less although i am still open to other opinions.\n\ncan anyone please recommend a good motor (in-line brush-less) or brushed motor that will give me the speed and strength i need for the weapon?\n", "tags": "motor battle-bot", "id": "4716", "title": "motor for weapon for hobbyweight"}, {"body": "i want to analyze a traffic scene. my source data is a point cloud like this one (see images at the bottom of that post). i want to be able to detect objects that are on the road (cars, cyclists etc.). so first of all i need know where the road surface is so that i can remove or ignore these points or simply just run a detection above the surface level.\n\nwhat are the ways to detect such road surface? the easiest scenario is a straight and flat road - i guess i could try to registrate a simple plane to the approximate position of the surface (i quite surely know it begins just in front of the car) and because the road surface is not a perfect plane i have to allow some tolerance around the plane.\n\nmore difficult scenario would be a curvy and wavy (undulated?) road surface that would form some kind of a 3d curve... i will appreciate any inputs.\n", "tags": "mobile-robot wheeled-robot computer-vision algorithm stereo-vision", "id": "4717", "title": "detect road surface in a traffic scene point cloud"}, {"body": "i am building an autonomous robot using pid control algorithm. so, far i have implemented pid using online resources/references. i am testing for stabilizing an axis of the quad copter. however, i am not successful to stabilize even one axis.\ndescription: my input for the pid is an angle value i.e the orientation of the quad copter measured by ahrs (a gyroscope that measures angles) and the motors take integer values as speeds. what i am doing is,\n\n\n\nwhere ajusted_value is a buffer that accumulates or subtracts the pid output value based on either pid output is +ve or -ve.\n\ni also tried,\n\n\n\nboth don't seem to be working.\n\ni have tested using a wide range of p gain values (from very small to very large), but the quad copter only oscillates; it does not self-correct. your help with suggestions would be greatly appreciated. thanks!\n", "tags": "quadcopter", "id": "4721", "title": "how to convert pid outputs to appropriate motor speeds for a quad copter"}, {"body": "i am looking for some figures surrounding the specs of brushless motors and their relative efficiency (in power usage terms) for multi-copter use. \n\nthere are 4 basic specs for motors themselves:\n - motor width (eg 28mm)\n - motor height (eg 30mm)\n - \"kv\" - rpm per volt supplied (eg 800kv)\n - wattage (eg 300w)\n\nthis would then be a 28-30 800kv 300w motor. \n\nwhat i am looking for is a chart containing:\n - motor spec\n - pack voltage (eg 14.8v)\n - amps drawn @ various % throttle (10% to 100% say)\n - static thrust from various propellers (11x5, 12x6 etc etc)\n\ndoes such information exist?\ni know its a bit subjective as prop and motor designs vary slightly, but a baseline would be a start.\n", "tags": "brushless-motor", "id": "4722", "title": "brush-less motor specs vs efficiency for multi-copters"}, {"body": "i've got my hands on this laser range scanner but seem to have some problem receiving any output from it.\n\ni can't find any guide on  how to set it up on the internet, so i was wondering if it is even possible to set it up for mac or shall i do it using linux and ros ?\n", "tags": "communication laser rangefinder", "id": "4723", "title": "hokuyo urg-04lx-ug01 and mac compatibility issues"}, {"body": "i am working on a project but i lack advanced programming knowledge, especially about genetic algorithms. i am developing a prototype using webots 7.4.3 for the simulation. the project is to use genetic algorithms to evolve the gait of a biped robot. i have developed a physical model, but i am still uncertain about the motor choice. for the algorithm part, i find it hard to understand how to set the algorithm parameters and how to determine the fitness function. could you please suggest a fitness function?\n\nthank you for your help and efforts.\n", "tags": "algorithm machine-learning legged", "id": "4724", "title": "biped walking using genetic algorithm"}, {"body": "i have been going through a code base for multi agent motion planning. and i came across a recursive tree building algorithm for the agents. i haven't been able to figure out the algorithm. does anyone know what it is called? or any other similar kinds of algorithms so i could read more about it?\n\nhere is what i got from the code: \nthe node of the tree is as follows - \n\n\n\neach node has a max and min value for x and y. and also a begin, end, left and right value. \n\nthen the tree is split either horizontally or vertically based on which limits are longer (x or y). and an optimal value is found and agents are split. \n\nthen these split agents are recursively build again.\n\nthank you very much. \n", "tags": "multi-agent", "id": "4725", "title": "recursive tree representation for multi agent robots?"}, {"body": "i have the following code for the ros turtlesim: \n\n\n\nthe idea behind this code is to introduce a random error to then practice error recovery in my code but this node does not appear to do anything at all. i do know that my other nodes are running but this one just doesn't appear to do anything, it doesn't exit it just hangs. anybody know how to fix this?\n", "tags": "ros", "id": "4726", "title": "ros node does not seem to do anything"}, {"body": "it would be easy to understand if you imagine a robotic vacuum cleaner. (for some models) it goes back to a specific place automatically to recharge. like this, i want to make a robot which automatically goes to the place where a specific signal(like infrared ray) is emitting. \nfollowing is the scenario that i've imagined.\n\n1.set the ir emitter in a specific place of a room. it always emits infrared ray.\n\n2.i connect 4 ir receiver to my 4wd robot car - front, left, right, and back side.\n\n3.they receive ir from the emitter. i earn the distance from the emitter to each receiver with the intensity of ir.\n\n4.with these values, arduino find out which receiver is closest from the emitter and choose the direction to go.\n\nbut i could't know this will be possible. because ir is a kind of light ray, so i can't get the distance with the difference of arrival time(like ultrasonic). i searched several kinds of ir sensors, but they were only for sensing the possibility of collision. \n\nso my question is these..\n\n\ncan i get the distance and the direction from ir emitter to my arduino device with an ir receiver? \nif i can, then how many ir receivers do i need? and if i can't, what can i use to substitute ir emitters and receivers?\ni guess ir can be interrupted because of sunlight or other light. so i guess i need some daylight filter. do you think it's essential?? \n\n", "tags": "arduino sensors wheeled-robot", "id": "4734", "title": "can i make an automatically-parking robot car with ir sensor?"}, {"body": "i am simulating a wheeled robot of six-wheels and can be independently steered, like mer-opportunity.\nthe wheeled robot can perform throttling forward, \n\n\n\ncrab-motion, \n\n\n\nand turning on the spot.\n\n\n\nmy question is: is it correct to say that i have 2 motion primitives? throttling forward is basically crab-motion with heading zero.\n", "tags": "wheeled-robot motion", "id": "4738", "title": "wheeled robot motion primitives: is throttling forward and crab motion considered as one?"}, {"body": "i am writing a kinematics library in go as part of my final year project. i am working with product of exponentials method and have successfully implemented the forward kinematics part of this. i need help with the inverse kinematics. \n\ni understand the theoretical aspect of it. i would like a numerical example where actual numbers are used for the paden-kahan subproblems as the ones dealt in \"a mathematical introduction to robotic manipulation - murray,li and sastry\" [freely-available online pdf]. \n\ni specifically need help with knowing what should p,q be when trying to solve the inverse kinematics. the book just says given, a point p,q around the axis of rotation of the joint. but how do you know these points in practice, like when the robot is actually moving, how do you keep track of these points? for these reasons i need a numerical example to understand it. \n", "tags": "inverse-kinematics product-of-exponentials", "id": "4741", "title": "numerical-example for paden-kahan subproblems?"}, {"body": "i've just started taking a robotics course and i am having a little problem.\ni need to rotate the $o_i-1$ coordinate system into a position, where $x_i-1$ will be parallel with $x_i$. \n\nthe transformation matrix is given, but i have no idea how i can figure out this transformation matrix from the picture that can be found below.\n\nactually, i know why the last vector is [0 0 0 1] and the previous vector is [0 0 1 0], but i can't figure out why the first vector is [$\\cos q_i$ $\\sin q_i$ 0 0] and the second [$-\\sin q_i$ $\\cos q_i$ 0 0].\n\n\n", "tags": "robotic-arm industrial-robot", "id": "4745", "title": "how do i get this transformation matrix?"}, {"body": "i was wondering if there was a good book or paper that surveys current techniques in local navigation? the earliest one i could find was from 2005 and i was hoping to find something more recent. \n\ni have worked with certain approaches such as the dynamic window approach and the velocity obstacles approach. i'm hoping for some book or paper to give me a broader perspective to the problem of local navigation which i believe has been fairly robustly solved by a number of autonomous driving companies. \n\nthank you. \n", "tags": "navigation motion-planning", "id": "4749", "title": "survey for local navigation"}, {"body": "im currently designing a robot for my undergraduate project. one of the task of this robot is to follow the wall. for the purpose i'm using a pid control system, where the reference is given from a ultrasonic sensor. so my problem here is im having a hard time tuning the pid. i know i can find the p coefficient pretty easily by plotting the desired set point range vs desired motor output speed. even then the robot is not so stable, so i though of adding di part of pid. but how do find out roughly the values of these coefficients without just trying out random values (manual tuning)? thank you so much. much appreciated.\n", "tags": "motor control pid", "id": "4750", "title": "pid control tuning"}, {"body": "i've build a simple wheeled robot based on two continuous servos, controlled by raspberry pi running ros-groovy, with a smart phone mounted on top to provide additional sensors.  i'd like to situate the bot in a room and have it move to various points on command.  i don't have laser ranger finder but do have a good ultrasonic ranger finder and kinect sensors.\n\nwhat are the typical ros setup for this?\n\nthe idea i'm thinking is to personally (e.g. manually) map my room using kinect and use this map using only the ultrasonic range finder sensors and imu in the lightweight robot.  would this be possible?\n", "tags": "localization ros slam raspberry-pi ultrasonic-sensors", "id": "4754", "title": "build a ros robot with slam without laser"}, {"body": "by watching this video which explains how to calculate the classic denavit\u2013hartenberg parameters of a kinematic chain, i was left with the impression that the parameter $r_i$ (or $a_i$) will always be positive. \n\nis this true? if not, could you give examples where it could be negative? \n", "tags": "forward-kinematics dh-parameters", "id": "4755", "title": "parameter $r$ of denavit-hartenberg"}, {"body": "so, i am designing a rover that will navigate to a rock, and then calculate the height of the rock. currently, my team's design involves using an ultrasonic rangefinder and lots of math. i was interested in what sensors you would use to solve this problem, or how you would go about it? assume that the rover has already located the rock. \n\nadditional info: we are using an arduino uno to control our rover. it is completely autonomous.\n", "tags": "arduino wheeled-robot algorithm", "id": "4759", "title": "how to find the height of a rock with a rover?"}, {"body": "i'm building camera device which is able to take pictures of paragliders in mid air.\n\nto let the camera know where the glider is i thought about using gps data from the pilot's smartphone.\n\nmy question is: what are possible ways to transmit the gps data to the groundstation and which can be considered a good solution?\n\ni thought about sending the data to a server via some mobile network, but a direct communication solution would be preferable.\n\nthe pilot has mid-air pretty good mobile reception and the maximum distance between pilot and ground station is around 3km.\n", "tags": "gps communication wireless", "id": "4761", "title": "sending a smartphone's gps wireless"}, {"body": "i am a web developer. i am fascinated by quadrocopters and i am trying to learn how to build one and basically i am trying to jump into robotics fields. i don't have much electric circuit and electronics knowledge so i did some research on how to build and what type of knowledge you would require to develop such flying machine. so i started learning basics of electronics from lessons in electric circuits by tony r. kuphaldt\n\nthe books are very interesting but i could not find a technique so that i can implement what i learn from the books. basically i am just going through the stuffs, and understanding them little by little. what i want to know is that what is the right way and effective way to learn electronics and electric circuit from your experience and i should i do now so that i can increase my learning speed so that i can achieve my goal.\n\nwhile i was researching i came across topics such as mathematical modelling and modelling the quadrocopters first and them implementing them on real. how can i gain such knowledge to model something mathematically and implement such in real life? how much math and what areas of mathematics do i need to learn and how can i learn such? \n\nnow you have idea what i want to learn and achieve. can you please suggest me a road map or steps i need to take to gain such knowledge and skill to develop myself, so that in near future i would be able to build such flying machines on my own. \n", "tags": "quadcopter", "id": "4762", "title": "learning materials for beginners in robotics and quadrocopters"}, {"body": "instantaneous rate of change of displacement is given by,\n\n\n\nwhile average rate of change of displacement is given by, \n\n\n\nthe first one gives the slope or derivative of a displacement function at a particular instant of time and thus varies with time. i was wondering how is it going to help me calculate the velocity of my robot's end effector, which is the foot of the leg of the robot(bipedal). if i make a reading of the position of the robot every 1ms to keep the approximation as accurate as possible, my instantaneous velocity would be zero wouldn't it? since my robot wouldn't have moved anywhere in 1ms time. agreed, 't' would increment as t+dt, dt == 0.001s. then v(t) would be v(0.001) = s(0.002) - s(0.001) which is zero, because there is no displacement in that small time frame, right? am i doing something wrong here? or on the other hand, do i just use average rate of change?\ni have this question, since, if there is a manipulator, in my case the foot of my robot, and it's trajectory is given by a 3x3 homogenous matrix,\n\n\n\nif on paper, this is differentiated, this would give me a spatial/body velocity matrix as \n\n\n\nso how do i compute this differentiation in code. i just need something of the sort of a pseudocode.\n", "tags": "mobile-robot", "id": "4764", "title": "which is more useful? instantaneous rate of change of displacement or average rate of change of displacement?"}, {"body": "i'm building a robot that uses a beaglebone black, however i have several different usb devices that i want to connect to it (microphone, usb sound device and some other things). now i have heard that the usb output of the beaglebone doesn't power more then 0.1a. so the combined draw of these usb devices is likely to exceed this by a fair margin. so i started looking for powered usb hubs to use instead. however these tend to be powered by 220v and my robot currently only has a 12v power supply and a converter to 5v for the beaglebone. which given the size expense and inefficiency of converting power to 220 from 12v and then back again doesn't seem very good. is there a good method for fixing this? \n", "tags": "power usb beagle-bone", "id": "4766", "title": "how to power extra usb devices for beaglebone black"}, {"body": "i'm trying to track a simple robot (e.g. arduino, raspberry pi, even toys) in a room using fixed location kinect sensor(s) and cameras at different parts of the room.  how might one usually use to do this?\n\nedit 1:  more specifically, i want to know the position (and if possible, orientation) of an moving object in the room using one or more cameras or depth sensors.  i'm new to the area, but one idea might be to use blob or haar to detect the moving object and get its location from kinect depth-map, and i'm trying to find what package i can use for that end.  but for navigation to work i'd have to pre-map the room manually or with kinect.  i can put some sensors on this tracked moving object, e.g. imu, sonar, but not a kinect.  i am allowed full pcs running ros/opencv/kinect sdk in the environment, and i can wirelessly communicate with the tracked object (which is presently a raspberry pi running ros groovy on wheels)\n", "tags": "arduino computer-vision kinect", "id": "4771", "title": "tracking robot in a room"}, {"body": "we are working on a project which requires us to detect and hit a ball. we are trying to accomplish the task by detecting the position of ball by processing the input from a camera. the problem is that we are required to do this in very bright lights. the bright lights are making it difficult to detect the white colored ball.\n\n\nis there a way we can write the code such that it automatically reduces the intensity of lights in the image?\nis there an efficient way to extract only the v component from the hsv image?\n\n\nwe have very limited experience with image processing so any alternative approach to detecting the object will also be helpful\n", "tags": "computer-vision", "id": "4775", "title": "image processing in bright lights"}, {"body": "i am constructing a 5.44kg hobby-weight battle robot and one of the safety rules is that the robot must have a power switch that turns off power to the motors (and weapon).\n\nthe robot has three sub-systems; the drive motors (one battery), the weapon (another battery) and some lighting (a small 9 volt battery).\n\ni have read that since all these will be connected to the same receiver it is important to have all the electronics sharing a common ground for everything to work properly.\n\nnow i know that usually it is the \"live\" wire that is connected to the switch, but i was thinking of hitting two birds with one stone and connecting all the ground wires (rather than the live wires) to the switch. in this way i still turn off power and also have a common ground. in terms of safety (shorts) etc i am not too concerned because i am using xt 60 connectors and have been careful to use only female plugs for the power leads (so no prongs are visible).\n\nit seems to me that it should work and still be safe enough especially since i am not dealing with mains voltage levels here, but on the other hand i don't want to look stupid.\n\ndoes this way of connecting to the switch make sense or am i violating some unwritten law? is this normal practice? would it effect the circuitry in any way to have the grounds connected together?\n\ni was also thinking of using a switch from a pc power supply; as far as i know this is rated for reasonably high currents. in my case i will have 3 cordless motors, each of which might be drawing up to 5 amps when under load, so say 15 amps in all. has anyone out there ever used such switches or did you buy high current ones? in that case what should i ask for?\n\nthanks.\n", "tags": "control power battle-bot", "id": "4782", "title": "power switch and common ground on a battle robot"}, {"body": "i'm trying to import the tutorial robot given at this link\n\nhowever this gives the following error: \n\n\n\nthis sugest something is wrong with parsing but does not actually point towards any line of my code (the example is only 103 lines long). \n\n\n\nthis is on ubuntu 14.04. is there any hint at what i'm doing wrong or what information can i provide to better come to a solution \n", "tags": "simulator gazebo simulation", "id": "4784", "title": "gazebo import robot gives error"}, {"body": "i'm currently developing a slam software on a robot, and i tried the scan matching algorithm to solve the odometry problem.\n\ni read this article :\nmetric-based iterative closest point scan matching\nfor sensor displacement estimation\n\ni found it really well explained, and i strictly followed the formulas given in the article to implement the algorithm.\n\nyou can see my implementation in python there :\nscanmatching.py\n\nthe problem i have is that, during my tests, the right rotation was found, but the translation was totally false. the values of translation are extremely high.\n\ndo you have guys any idea of what can be the problem in my code ?\n\notherwise, should i post my question on stackoverflow or on the mathematics stack exchange ?\n\nthe icp part should be correct, as i tested it many times, but the least square minimization doesn't seem to give good results.\n\nthe parts that might be problematic are the function getaxx() to getbx() (starting at line 91).\n\nas you noticed, i used many decimal.decimal values, cause sometimes the max float was not big enough to contain some values.\n", "tags": "slam", "id": "4787", "title": "scan matching finds right rotation but false translation"}, {"body": "i have been working with the velocity obstacles concept. recently, i came across a probabilistic extension of this and couldn't understand the inner workings. \n\nsource: recursive probabilistic velocity obstacles for reflective navigation http://www.morpha.de/download/publications/faw_aser03_kluge.pdf\n\n\n\nwhat does the equation at the bottom and the top mean? vij is the relative velocity of agent i to agent j. ri &amp; ci and rj &amp; cj are their respective radius and centers.\n\nupdate:\nwhat does inf(ri + rj) and sup(ri + rj) mean? does it mean that i should define a function that goes from 1 to 0 from inf to sup? and if not, then how do i calculate the value of pcc at any given point?\n", "tags": "probability", "id": "4789", "title": "probabilistic velocity obstacles"}, {"body": "i'm reading this pdf. the dynamic equation of one arm is provided which is \n\n$$\nl \\ddot{\\theta} + d \\dot{\\theta} + mgl sin(\\theta) = \\tau\n$$\n\nwhere \n\n$\\theta$ : joint variable. \n\n$\\tau$ : joint torque\n\n$m$ : mass\n\n$l$ : distance between centre mass and joint. \n\n$d$ : viscous friction coefficient\n\n$l$ : inertia seen at the rotation axis. \n\n\n\ni would like to use p (proportional) controller for now. \n\n$$\n\\tau = -k_{p} (\\theta - \\theta_{d})\n$$\n\nmy matlab code is \n\n\n\nfor solving the differential equation \n\n\n\nthe error is \n\n\n\nmy question is why the error is not approaching zero as time goes? the problem is a regulation track, so the error must approach zero. \n", "tags": "control dynamics manipulator", "id": "4793", "title": "proportional controller error doesn't approach zero"}, {"body": "i bought 2 brushed motor controllers from china to use with my hobby-weight battle robot  (http://www.banggood.com/esc-brushed-speed-controller-for-rc-car-truck-boat-320a-7_2v-16v-p-915276.html). \n\nthese are intended for use with my 2 cordless drill motors which will be driving the left and right wheel respectively. the robot will therefore be steered in \"tank mode\" by varying the speed and direction of rotation of the 2 motors using the two joysticks on my turnigy 9x transmitter.\n\nmy question is: i have seen videos on youtube where people calibrate brushless motor controllers (escs) using some system of pushing the joystick on a standard transmitter forward and listening to tones and then doing the same for reverse and so on. \n\nhowever when i asked the suppliers about a similar procedure for these brushed controllers, all they could say is that they did not need calibration. the exact words were \"it seems that you're talking about transmitter for copters,but this esc is for rc car or boat. you pull the trigger, it goes forward, you push the trigger, it reverse. and you don't need to calibrate it, just plug it, then it can work.\"  \n\nmy transmitter is not one of those gun shaped ones used for cars. so am i in trouble with these controllers or should they work correctly out of the box as the supplier seems to be implying?  \n\nyou may fairly ask why have i not just tried this out and the simple answer is that my lipo charger has not yet arrived and i therefore cannot power anything up as yet. \n", "tags": "motor", "id": "4794", "title": "motor controller calibration"}, {"body": "i bought 2 brushed motor controllers from china to use within my hobby-weight battle robot (http://www.banggood.com/esc-brushed-speed-controller-for-rc-car-truck-boat-320a-7_2v-16v-p-915276.html). \n\nthese are intended for use with my 2 cordless drill motors which will be driving \nthe left and right wheel respectively. the robot will therefore be steered in \n\"tank mode\" by varying the speed and direction of rotation of the 2 motors using \nthe two joysticks on my turnigy 9x transmitter.\n\ni am seeking to refine the model and make it easier to operate so does anyone know of a way in which i can somehow synchronize the motors to get a single joystick steering system?  my transmitter has 9 available channels so if this is part of a solution then i am fine with it. i also have an arduino available if needs be. \n", "tags": "motor differential-drive", "id": "4796", "title": "changing tank drive (differential) mode to single joystick drive mode"}, {"body": "i bought 2 brushed motor controllers from china for my hobby-weight battle robot (http://www.banggood.com/esc-brushed-speed-controller-for-rc-car-truck-boat-320a-7_2v-16v-p-915276.html). \n\nthese are intended for use with my 2 cordless drill 14.4v motors which will be driving the left and right wheel respectively.\n\ni will be using 4s lipos which (when fully charged) have a voltage of 16.8v. can someone put my mind at rest that the .8 volt excess is unlikely to damage the controller (which is rated for 7.2v - 16v)?\n\nalso is the fact that the motor controllers are rated for 320amp likely to damage my motors? \n\ni am to be honest not very clear on current and how this is drawn from a lipo battery. for instance would connecting a lipo directly to my motor result in a massive discharge or does the motor just \"take what it needs\" in terms of current? can someone maybe kindly point me to an article which casts some light on the subject or even more kindly explain it to me here?\n", "tags": "motor battery", "id": "4797", "title": "risk of overloading motor controller"}, {"body": "i am building a drone using the raspberry pi and i am using 6*pid controllers to control the speed and the value for each angle, can i use a recurrent neural network (rnn) or other neural network to stabilize the angles. if so what can the training data be? what type of neural network (nn) is best suited for this kind of application?\n", "tags": "pid raspberry-pi artificial-intelligence", "id": "4798", "title": "rnn instead of a pid controller"}, {"body": "as far as i understand, ahrs use orientation reference vectors to detect orientation error. and we can use magnetometer to correct yaw drift. but i see from my ak895 magnetometer that the data is not so stable, it kind of fluctuates continuously.\n\nhow can we use this data for ahrs algorithm?\n", "tags": "quadcopter", "id": "4804", "title": "ahrs algorithm question"}, {"body": "based on the wiki page of esc, the esc generally accepts a nominal 50 hz pwm servo input signal whose pulse width varies from 1 ms to 2 ms\n\nhttp://en.wikipedia.org/wiki/electronic_speed_control\n\nfor our project, we integrate a flight controller for our uav, naza m-lite and we want to implement position control. we already have localization and we can control the quadrotor by applying servo width to roll, pitch, yaw and thrust throttle. since the esc only accepts 50 hz, will the pid controller work at only 50 hz?\n", "tags": "pid", "id": "4808", "title": "is the input of esc really limited at 50 hz and will the pid controllers work properly?"}, {"body": "the beaglebone black which i work on has only 2 i2c busses. say for some crazy reason, i need to use 8 i2c busses. i dont want to chain any of the devices. the idea is to have every device's sda line separated and use a shared scl line so i can use the same clock and have as many sda lines as i want. since the scl clock is hardware controlled there wont be any major issues here. the gpio can do around 2.5mhz of switching so i am happy with that. \n\nif that works out, i can spawn 8 threads to talk on 8 i2c lines making my solution faster!\n\ndo you think its doable? i would like to hear from you guys as this idea of using 1scl and gpio as sda just popped in my head and i thought of sharing it with you guys. \n\ncheers!\n", "tags": "i2c", "id": "4815", "title": "using i2c's scl and gpio pins as sda"}, {"body": "i have just sized the dc motors i want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). now i need to actually choose the exact motors i want to buy from the manufacturer (i am targeting maxon motors as i am not an expert and want no problem). i have a few down to earth questions about linking the mechanical needs to the electrical characteristics:\n\n\nmaxon states a \"nominal voltage\" in the characteristic sheets. is that the voltage you should apply to the motor? this may be a dumb question but i have followed the full maxon e-learning course and read about other tutorials on the web and i could not find this information anywhere. can anyone who knows about motors confirm?\nas far as i understand, the nominal torque corresponds to the maximum torque the motor can sustain continuously. so i guess, as a rule of thumb, i should find a motor with a nominal torque = my max torque (after reduction), or around. right?\nalso i chose a motor reference (310005 found here) which has a stated power of 60w, as the nominal voltage is 12v, i was expecting to have a nominal current of 5a, but it states 4a. where am i wrong?\nthe motor i chose has nominal speed = 7630rpm - nominal torque = 51.6mnm. my needs are max speed = 50.42rpm / max torque = 10620 mnm. this means a reduction factor of 151 for speed and 206 for torque. should i choose a gear closer to 151 or 206?\nwhat is the \"rated torque\" mentioned when choosing a gear? i know my input torque (torque on the motor side) and my output torque (torque on the system side), does that correspond to any of these two?\n\n\ni have followed some theoretical and practical courses on the web but i find it hard to find answers to my down to earth question...\n\nthanks,\n\nantoine.\n", "tags": "motor", "id": "4820", "title": "choosing motor characteristics"}, {"body": "we are using naza-m-lite for our flight controller without gps. the localization is obtained through our rgb-d camera sensor. we are able to teleoperate and even implement pid controllers for roll, pitch, yaw and throttle channels for our quadrotor. however, we do not know the plant model because what we are inputting from arduino to the naza-m-lite are servo pwm ranging from 1000 to 2000. \n\n\n  for throttle:   1500 altitude hold, 2000 maximum throttle, 1000\n  minimum throttle\n  \n  for pitch, roll, yaw: 1500 maintain 0 angle, 2000 and 1000 moves the\n  quadrotor towards its respective axes.\n\n\nhowever, even at 1500 on every channel, the quadrotor drifts, maybe due to flying indoors and the wind pushes the quadrotor. once it gains momentum, it drifts. we are having trouble tuning this because we do not know the relationship of the output is to the position. if the output were velocity, it would have been easier. but as in our case, it is not. is there a way to find the plant model of the naza-m-lite and how can we tune this?\n", "tags": "localization pid quadcopter uav", "id": "4824", "title": "implementing a position control for uav through a flight controller. plant model is unknown"}, {"body": "i am trying to create a model for the nao [robot]'s motors. the figure below shows the step response for the knee motor. afaik the nao internally uses a pid controller to control the motor. i have no control over the pid or it's parameters. thus i would like to treat the motor including pid as a black box. theoretically it should be possible to model pid+motor as a $pt_2$ system, i.e. a second order lti system.\na $pt_2$ system is defined by the following differential equation:\n\n$$t^2\\ddot{y}(t) + 2dt\\dot{y}(t)+y(t) = ku(t)$$.\n\ni tried fitting a $pt_2$ model but was unable to find good parameters.\n\nany idea what model to use for this kind of step response?\n\nedit:\ni tried modifying the equation to add a maximum joint velocity like this:\n\n$$t^2\\ddot{y}(t) + (\\frac{2dt\\dot{y}(t) + m - |2dt\\dot{y}(t) - m|}{2})+y(t) = ku(t)$$ \nwhere $m$ is the maximum velocity. the fraction should be equivalent to $min(2dt\\dot{y}(t), m)$.\n\nhowever i am not sure if this is the correct way to introduce a maximum joint velocity. the optimizer is unable to find good parameters for the limited velocity formula. i am guessing that is because the min() introduces an area where parameter changes do not cause any optimization error changes.\n\n\n", "tags": "motor", "id": "4827", "title": "nao motor model identification"}, {"body": "i have just sized the dc motors i want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). now i need to actually choose the exact motors i want to buy from the manufacturer (i am targeting maxon motors as i am not an expert and want no problem). i have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them:\n\nquestion #1:\n\nmaxon (or the other manufacturers) states a \"nominal voltage\" in the characteristic sheets. is that the voltage you should apply to the motor? this may be a dumb question but i have followed the full maxon e-learning course and read about other tutorials on the web and i could not find this information anywhere. can anyone who knows about motors confirm?\n\ni have followed some theoretical and practical courses on the web but i find it hard to find answers to my down to earth question...\n", "tags": "motor brushless-motor", "id": "4828", "title": "is the nominal voltage of a motor the voltage to apply to the motor?"}, {"body": "i have just sized the dc motors i want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). now i need to actually choose the exact motors i want to buy from the manufacturer (i am targeting maxon motors as i am not an expert and want no problem). i have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them:\n\nquestion #2:\n\nas far as i understand, the nominal torque corresponds to the maximum torque the motor can sustain continuously. so i guess, as a rule of thumb, i should find a motor with a nominal torque = my max needed torque (after reduction), or around. right?\n", "tags": "motor brushless-motor servos servomotor", "id": "4829", "title": "choosing dc motor: max needed torque vs nominal torque"}, {"body": "i have just sized the dc motors i want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). now i need to actually choose the exact motors i want to buy from the manufacturer (i am targeting maxon motors as i am not an expert and want no problem). i have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them:\n\nquestion #3:\n\ni chose a motor reference (310005 maxon reference found here) which has a stated power of 60w, as the nominal voltage is 12v, i was expecting to have a nominal current of 5a, but it states 4a. where am i wrong? \n", "tags": "motor servos servomotor", "id": "4830", "title": "stated power for a motor does not equal nominal voltage x current?"}, {"body": "i have just sized the dc motors i want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). now i need to actually choose the exact motors i want to buy from the manufacturer (i am targeting maxon motors as i am not an expert and want no problem). i have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them:\n\nquestion #4:\n\nthe motor i chose (maxon brushed dc: 310005 found here) has nominal speed = 7630rpm - nominal torque = 51.6mnm. my needs are max speed = 50.42rpm / max torque = 10620 mnm. this means a reduction factor of 151 for speed and 206 for torque. should i choose a gear closer to 151 or 206?\n", "tags": "motor brushless-motor servos servomotor", "id": "4831", "title": "selecting a gear reduction: torque vs speed"}, {"body": "i have just sized the dc motors i want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). now i need to actually choose the exact motors i want to buy from the manufacturer (i am targeting maxon motors as i am not an expert and want no problem). i have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them: \n\nquestion #5:\n\nwhat is the \"rated torque\" mentioned when choosing a gear? i guess it is related to the maximum torque the gear can support... but now, i know my input torque (torque on the motor side) and my output torque (torque on the system side), does that correspond to any of these two?\n", "tags": "motor brushless-motor servos servomotor", "id": "4832", "title": "how does the \"rated torque\" for a gear relate to the maximum torque?"}, {"body": "my yaw angle varies from -180 degree to 180 degree. \n\n\n\nif my current heading is about 170 degree, then the wind makes it rotate to the left at about -170 degree, then how can pid control it to make it rotate back to the right at 170 degree.\n\nsince, for pid   \nin my case, , and , the the .\nso instead of moving to the right and apply , it rotate to the left and apply  and come back to the desired position, which is 170 degree?\n", "tags": "pid", "id": "4836", "title": "how to control pid yaw"}, {"body": "i'm working on a robotics platform and we need an on-board ubuntu machine to run ros image recognition.\n\ndoes anyone know of a good set of computer hardware that has\n\n\nno screen\nno keyboard\nbuilt-in battery (for charging separate from the robot)\nquite a bit of compute power (i5+, 4+ gb ram)\n\n\ni thought about using a laptop, but the keyboard and screen are a lot of extra weight/volume i don't want to carry around. something like an intel nuc is appealing, but has no battery.\n", "tags": "driver", "id": "4840", "title": "a robotics computer with graphics card, lots of computation power, battery, no screen, no keyboard?"}, {"body": "i am trying to localize an object in a point cloud using ros, pcl. for that i capture the scene and model using asus xtion pro sensor. i use rgbdslamv2 for capturing the model. \n\nthen i use icp (nonlinear version) to find the transform from the model to each cluster of the cloud. the cluster with the lowest score is chosen as the best matching cluster. \n\npseudocode:\n\n\n\nhowever, i am not able to find the correct transformation.\n\nhere are the screenshots of the results i got:\n\n\n\n \n\nthe red colored object is the transformed model overlayed onto the scene. the yellow object represents the original model in the coordinate system of the scene.\n\nnow, my concern is why there is no proper transformation? am i missing something? \n\nsecond, i see that the object model and scene are in different coordinate system. so the model appears inverted when presented in the scene's coordinate system. is there a way in which i can transform the model upright before running icp?\n\nthanks :)\n", "tags": "computer-vision kinect", "id": "4843", "title": "how to change the orientation of an object w.r.t a scene?"}, {"body": "i'm really willing to understand and implement such a controller (sliding mode) for a quadrotor.\ni've found this interesting document explaining that topic.\nif you scroll down until page 381 (don't be scared, the document is just 6-7 pages) you can find the following height control law (equation .19):\n\n$$\nu_1 = \\frac{m}{\\cos{\\phi}\\cos{\\theta}}[c_1(\\dot z_r - \\dot z) + \\ddot z_r + \\epsilon_1  sgn(s_1) + k_1 s_1 + g]\n$$\n\nthe explanation of most of the term should be quite easy, but let's focus on the variable z, the height (or altitude if absolute) of the quadrotor. anyway the control law \"pretends\" not only the goal height z (through $s_{1}$) but even the vertical speed $\\dot z_{r}$ and vertical acceleration $\\ddot z_{r}$ (r means here reference).\n\nnow...to me is not clear whether those variables the setpoints are, that must be reached once the quadrotor reaches its predefined height or they just symbolize an abstract mathematical formalism but are going to be most of the time zero (because i want to reach the target height with $z = z_{r}$ but $\\dot z_{r} = \\ddot z_{r} = 0$)\n\n?!?!?\n\ni hope my question is clear. even if this i put in the title \"sliding control\" i think it may be helpful for other type of controllers.\nregards\n", "tags": "quadcopter", "id": "4847", "title": "understanding a sliding mode controller for quadrotors"}, {"body": "i want to build a robotic vacuum. i have a 400w 24v vacuum motor that i want to switch on automatically at a set time every night. the batteries i will be using will be 2x12v 80ah deep cycle gel batteries connected in series. i want the arduino to switch the motor on and off. so my first real question i guess is will the 5v supplied from the arduino be able to switch on a motor that big? the second question is a mosfet the answer? my apologies i'm pretty new to all this but love it..\n\ncan i control a 400w motor with 24v 16a batteries with an arduino board and a mosfet? what type of mosfet would i use?\n", "tags": "arduino", "id": "4853", "title": "controlling a 400w motor with 24v 16a batteries with an arduino board"}, {"body": "i am trying to implement a monte carlo localization/particle filter localization with a simple range sensor. the range sensor only sees in the direction the robot is heading and returns back any obstacle in its line of sight. if there is no obstacle, then the sensor returns back the distance to the boundary wall i.e. there is no maximum range for sensor.\n\nbut, the problem is that i am not able to locate the robot's position. now, i am feeling is it cause the sensor is not powerful enough. is it feasible to do localization with such a sensor or should i change the sensor type?\n\nplease tell me what you guys think?\n", "tags": "localization particle-filter", "id": "4855", "title": "is a simple range sensor described below sufficient to implement particle filter localization?"}, {"body": "a motor needs to spin n*360 degrees. on top of the motor there are distance sensor which scan the room. i.e. lidar. what options do i have for implementing a continous rotation while having cables which are in the way?\n", "tags": "motor chassis", "id": "4857", "title": "continous rotation with cables"}, {"body": "i have robot vision system which consists of conveyor with encoder, two cameras (gigabit eth and usb) and simple illuminator.\n\ni need to trigger cameras and illuminator when encoder reaches position interval.\n\ni'm considering using real time operating system for this task:\nencoder, illuminator and cameras connected to pc and vision system application runing on it.\n\nwhich real-time solution you can reccomend for this problem? \n\ni'm considering using beckhoff twincat software which turns normal operating system into rt. \n", "tags": "computer-vision real-time", "id": "4863", "title": "real time operating system for robotics vision"}, {"body": "ok apologies for those who think my questions are not direct enough as i got warned about this. i am really new to this and i will try to keep this one direct enough for the forum. \n\nfor obvious reasons i cannot test this out without damaging something so would prefer to learn from the experience of others.\n\ni have a \"turnigy trackstar 1/10 17.0t 2400kv brushless\" motor which i will be using for my weapon (spinning disk). \n\nrelevant specs of the motor are:\nkv: 2400\nmax voltage: 21v\nmax current:24amps\nwatts: 550\nresistance: 0.0442ohms\nmax rpm: 50000\n\ni will use this with an esc with the following specs:\n\nconstant current: 30a\nburst current: 40a\nbattery: 2-4s lipoly / 5-12s nixx\nbec: 5v / 3a\nmotor type: sensorless brushless\nsize: 54 x 26 11mm\nweight: 32g\nprogramming functions:\nbattery type: lipo /nixx\nbrake: on / off\nvoltage protection: low / mid / high\nprotection mode: reduce power / cut off power\ntiming: auto / high / low\nstartup: fast / normal / soft\npwm frequency: 8k / 16k\nhelicopter mode: off / 5sec / 15sec (start up delay)\n\nif the motor stalls, i know the current draw will increase drastically. so my questions are:\n\n\nin the case that the motor stalls (my disk gets stuck in the opponent etc), then what gets damaged? the motor, the esc, both? and how long before this happens? \nwould i have time to turn the r/c switch off before irrevocable damage occurs (once i am obviously observing the action?). notes. i will be using an on/off switch on the r/c to just turn the motor on and off (so no proportional speed increase etc), plus i will be using an 11.1 volt battery even though the motor is rated for a 21 volt maximum.\n\n\nthanks.\n", "tags": "brushless-motor esc", "id": "4866", "title": "what happens when a brush-less motor stalls?"}, {"body": "my problem is that when i hold my sensors (mpu9150) so that +y axis is downward, and y axis is on the horizontal plane, i expect that pitch = 90 degree,  and roll = 0 degree, but actually pitch = 90 degree, and roll = 160 degree. however, when roll = 90 degree and pitch = 0 degree (that is what i expect). do you know what cause my problem?\nthanks\n", "tags": "quadcopter", "id": "4872", "title": "roll, pitch calculation problem!"}, {"body": "which of the following simulators is the best choice for simulating a swarm of auvs working together to perform a mission? please clarify your reason and if you know any better choice, i would greatly appreciate it if you kindly help me. please consider the need for doing hardware-in-the-loop(hil) simulation. \n\n\nwebots\nv-rep\nauv workbench\ngazebo\nuwsim\nswarmsimx \n\n\nin addition, notice that capability to connect to the middle-wares like ros is really important. \n\nthe other option is using a game engine like blender but i think it needs a lot of developing effort and is time-consuming! would you recommend this approach be used? if not, why not? and what would you recommend instead?\n", "tags": "mobile-robot ros simulator auv", "id": "4873", "title": "choosing suitable simulator for a swarm of auvs"}, {"body": "working with a swarm of robots, collaboration between the nodes is really important(either for the goal of simulation or real-word operation). middle-wares are the frameworks for this special purpose. i know some of the relevant middle-wares like ros(general-purpose but popular) or umvs(that is basically design for auvs). now, i have two questions:\n\n\ndo you know any other choice for the above mentioned purpose? \nwhat criteria should i consider for choosing a middle-ware suitable for my purpose?\n\n\nthank you.\n", "tags": "mobile-robot ros", "id": "4874", "title": "what middle-ware do you recommend for a swarm of auvs?"}, {"body": "i have a gy-31 light sensor and i am trying to make a skittle sorting machine. there is a little wheel skittles can drop into then they are turn to 90 degrees read by the light sensor then dropped out the bottom. the problem is the code seems to get stuck after reading the light sensor (it still flows through but won't execute the moving of the servo). as soon as i take out the readsensor line the servo works like normal. do i have to dispose of the color sensor some how?\n\n\n", "tags": "arduino sensors rcservo", "id": "4875", "title": "arduino lightsensor blocking code"}, {"body": "i want programme my very own g-code generator for my final year electrical engineering project. i know that there are many open source g-code generators out there, but i need a g-code generator which generates a g-codes for custom circuit designs drawn by the user and pass the g-code serially to my 2 axis cnc machine. so currently i'm working on a qt based gui where i draw .dxf format circuit diagrams and electrical components (like resistors, capacitors) and when i press a \"generate g-code\" push button i should generate a text file with nice set of g-codes for my designed diagram.\n\nso the problem here is, how do i generate the g-code? is there any specific algorithm to follow or adapt? i tried googling for g-code generator algorithm but i couldn't find any helpful stuff.\n", "tags": "cnc circuit", "id": "4880", "title": "programming a g-code interpreter"}, {"body": "i\n suppose who know ros and how it works (at least most of you)\ni have some question regarding the implementation of a quadrotor in that\n framework.\n\n\n3d movements: a quadrotor has 6dof and moves in a 3d \nenvironment. looking at the various ros packages i could not find any \npackage that allows to drive a \"robot\" in the 3d space. the package \n/move_base for instance allows only 2d. make sense to use this \npackage for such a project? i thought to use 2d navigation projecting \nthe \"shadow\" of a quadrotor on the ground...\nmoveit: it seems a real interesting and promising package, but i\nread that it is for robotic arms and not expressly indicate for \nquadrotor. maybe one can use the possibility to create a virtual \nfloating joints in moveit to let the quadrotor any movement in a 3d \nenvironment...that's ok, but i cannot understand whether is \"too much\" \nand not useful for a flying robot.\ntrajectories: the possibility to create a 3d trajectory in the \nspace seems to be not a standard package of ros. i found octomap \nwhich allows the creation of 3d maps from sensor datas. very interesting\nand for sure very useful. but...i don't think it could be useful for \ncreating 3d trajectories.  should i in that case create an extra package\nto compute 3d trajectories to be feed into the quadrotor? or there \nalready something like that?\n\n\nthere is already an existing project hector_quadrotor which seems to \nacclaim a good success ans it is very considered in the field. most \npeople refer to that project when speaking or answering question \nregarding quadrotors in ros. i saw many times that project...since \nweeks. and due to the total lack of documentation i didn't try anymore \nto understand how it works. really too difficult. \nanother interesting project, ardrone, has comments in the source \ncode...in russian!!! @_@\n\ncould you me give any good suggestions? or point me in the right direction \nplease?\nit would help me to understand how to focus my searches and which \npackage i can/cannot use.\n\nupdate: my goal is to let the quadrotor flying and using gmapping to localize itself. i've heard and read al lot of stuff about that but i found all this tutorials very hard to understand. i cannot get a global vision of the software and sometime i run in problems like: \"is there a package for this task, or should i invent it from scratch?\"\n\nthanks!\n", "tags": "ros quadcopter", "id": "4885", "title": "developing a quadrotor using ros"}, {"body": "the robot should go around 2 boxes and stop at the starting point after tracing a 8 figure pattern. with micro-controllers, i guess it can be easily done using sensors or navigation algorithms. please suggest how can be one made without them.\n", "tags": "mobile-robot", "id": "4888", "title": "how to make a robot following a virtual eight figure pattern without using microcontrollers?"}, {"body": "i'm reading a book about a hypothetical economy in which robots work for us because they eventually became able to do everything we do (\"our work here is done, visions of a robot economy\" by nesta).\n\ni was wondering though: is it theoretically possible for a human brain (an extremely complex  artificial intelligence the way i and many others see it) to comprehend itself in its deepest details and produce an artifical intelligence which is exactly identical? it sounds unlikely. if not then, how close can we get?\n\nthis is not a philosophic question, the ideal answer would be a rigorous demonstration based on metrics simplifying the problem to be able to answer it, however objective answers with valid arguments are always interesting, too.\n", "tags": "artificial-intelligence", "id": "4890", "title": "will ais ever be as advanced as the human brain?"}, {"body": "my quadcopter can lift off the ground, but it kinds of circles around. here is my video\nhttps://www.youtube.com/watch?v=rxjpwhbgiw0\n\nanyone helps me?\n", "tags": "quadcopter", "id": "4891", "title": "quadcopter hovering problem"}, {"body": "i have a project in mind for a robot which is able to recognize surfaces, and thought about including the following sensors:\n\n\na temperature sensor\na colour sensor, or a complex of electronic components to determine a colour\nand a texture sensor, or, as above, a complex of components to fulfill the purpose\n\n\nnow, i did some research on finding a(preferably small) texture sensor for soldering into an electronic circuit, similar to those little temperature sensors one can buy.\ni already thought that \"small\" would probably turn out to be not small at all before i searched.\nbut my research has been fruitless.\nnot just fruitless like in \"i can't find exactly what i want.\" but fruitless like in \"i cannot find anything similar to what i want.\"...\n\nmost things that turned up either were scientific papers about whole devices, or whole devices for purchase.\nsome company even choose \"structure sensor\" as the name of their ipad-compatible 3d scanner, which made the search utterly depressing as every second article i found is about buying some pre-built ipad device.\n\nall i need is the electronic component, nothing else.\nso hope that any of you people can spare me some research time and recommend me a company/site/whatever which sells such texture sensors.\n\n(btw., i do know that surface sensors are probably a bit way more complex than temperature sensors, and my hope for getting what i want is low, but just because i cannot find something, ot does not mean that it doesn't exist.)\n", "tags": "sensors", "id": "4897", "title": "are there surface texture sensors for integration in circuits?"}, {"body": "i have a simple rrr manipulator where one motor controls the base rotation, and the other two allow movement in a plane extending forward from the base and upwards/downwards. are there any standard ways to ensure the angle of the end effector remains constant?\n\nmy current solution uses explicit trigonometric expressions based on distance between joints, but if there is a better way to solve it to include restraints i'd be open to suggestions.\n\nedit\n\nthe manipulator is essentially like the image below, but with an additional base rotation. this allowed for the inverse kinematics to be simplified. as a reference here is the site http://www.hessmer.org/uploads/robotarm/inverse%2520kinematics%2520for%2520robot%2520arm.pdf\n\n\n", "tags": "inverse-kinematics", "id": "4900", "title": "inverse kinematics constant end effector angle"}, {"body": "i previously used the ziegler method to tune the parameters of my pid controller to control my robot's position. i then implemented fuzzy logic for self-tuning the parameters. \ni have two inputs to the fuzzy logic controller; one is the position error and the error rate.\n\ni know that my problem might be due to not understanding the effect of each parameter very well.\n\nthe problem is that i am confused in setting up the fuzzy rules. when do i need to use high and low values for kp, kd and ki to achieve the best tuning? is it that kp must be very low when the error is almost zero (hence, the robot is at the desired position)? the same question applies for all of the three parameters.\n", "tags": "control pid tuning", "id": "4901", "title": "how to tune the pid parameters using fuzzy logic?"}, {"body": "i am making a self balancing scooter which runs off 2 x 12v sla batteries connected in series to make 24v. everything works as expected except for the power supply which makes me pull most of the hair in my head for 2 weeks now. hope someone could help.\n\nthe 2 24v motors run off the batteries directly. now for my scooter, i need a +12v line for the half bridge drivers, and a +5v line for the signal part. for +12v i am using a lm2576-12 hooked to the batteries (+24) and for the +5v signal i am using a lm2596 adj, also hooked directly to the batteries (or is it supposed to be hooked to the output of lm2576-12 for better performance??).\n\nthe problem is that, when the motors are under load this power supply system makes the microcontroller hang (or reset i am not too sure, since everytime i have to try to turn off the power switch immediately, because the motor keeps running with whatever value they are fed with right before this happens), usually within 1 minute of riding, which is very dangerous when someone is onboard.\n\ni have read and re-read the datasheet of lm2576 and lm2596 many times, and have tried many settings, from recommended to different values of capacitor and inductor. for the diode, i am using the ss34.\n\ni guess its not because of electromagnetic interference, since while i do have the pcb located near the motors, the pcb is actually put inside a homemade faraday cage which is grounded (battery -), and the motor cases are also grounded. plus that the microcontroller only hangs when motors are under load (i.e. me on board), especially when i go from forward to backward.\n\nthe motor controller is self made, using 8 x auirf2804s mosfets. i also put 4 x 1000uf caps between the motors and the +24v.\n\nwould anyone be so kind to throw some light. what would a power supply for this kind of application is supposed to be?\n", "tags": "motor", "id": "4902", "title": "lm2576 and lm2596 regulator make microcontroller hang"}, {"body": "i am planning to control my bicycle derailleur with an electronic circuit, the advantages being multiple but not topic of this question.\n\nthe actuation would be performed by placing the assembly very close to the derailleur (but not on it) and by winding the usual steel cable around a spool placed on axis of a gear, and by using a motor to turn the gear. \n\nthis question concerns the alternatives for the spool self-locking mechanism (and eventually the kind of motor to use).\n\nin the literature i found http://nabilt.github.io/diy_eshift/ and other similar ones that directly modify the derailleur with a stepper motor, and then are forced to keep the stepper motor powered up all the time to keep some torque on the shaft. i consider this inefficient, therefore i require a self-locking system to be able to remove power.\n\ni come up with two ideas:\n\n\nworm gear operated by a dc motor, where the steel cable is wound around the gear. this system is self-locking or almost self-locking, according to the gear ratio: the gear cannot (easily) drive the worm.\na motor driving normal gears with an appropriate reduction factor, but with an additional friction element, whose friction force is greater than the strength of the spring mounted on the derailleur (sorry if i mixed the technical terms). this is what normal bicycles already have: the friction along the cable and in the element placed n the handle is high and keeps the derailleur in place.\n\n\nboth system would be assisted by a position-sensitive element (a trimmer?) to detect the actual position of the gear and/or of the derailleur, all configured in a closed feedback loop.\n\ni don't consider additional options for the gear such as this one: http://www.agma.org/assets/uploads/publications/10ftm17_kapelevich.pdf that consists of parallel-axis gears, whose teeth are however shaped in a manner to achieve self-locking without the need of a low-efficiency worm system.\n\nfrom my point of view, i cannot see any clear advantage of worm gear vs friction except for:\n\n\nthe worm gear may allow me to build a more compact assembly, thanks to the two axes being perpendicular\nspeed vs torque: worm gears reduce the torque requirements, but the motor has to spin a lot and i cannot wait 3 seconds for each gear change.\n\n\nconcerning the choice of the motor type (this is not the main question though), i think that:\n\n\na worm gear allows me to easily use a dc motor, since torque requirements are low and i don't need to detect the position of the shaft. moreover, dc motors increase torque with decreasing speed, while stepper motors have the maximum torque defined by the stepping frequency.\ndc are more compact and cheaper, important if i decide to offer this assembly as a kit and not unique, personal prototype.\ni am working with 5v supply and i fear most easy-to get dc motors (old printers, scrap electronics) work on 12v, with a significant reduction of the available torque when operated at 5v.\n\n\ni was looking for a \"mechanics\" section in stack exchange but i couldn't find it, so i opted for robotics instead of electronics.\n", "tags": "brushless-motor stepper-motor", "id": "4903", "title": "self-locking actuator: friction versus worm gear"}, {"body": "i am new to robotics, and would like to build a smaller robotic arm than in manufacturing facilities.\ni want a small robotic material handlers that can pick up or handle small objects around 12\"x12\"x12\" objects. essentially a small robotic assembly line in my garage.\n\nare there any kits i can purchase that deals with robotic assembly lines?\ni was wondering has anyone dealt with this before any suggestions on this?\n", "tags": "robotic-arm", "id": "4904", "title": "is there build your own garage robotic assembly lines out there?"}, {"body": "i am a software engineer and also a poultry farmer.\n\ni periodically have to manipulate my poultry in such a way as to grab their head and hold them for a brief period, approximately 10 - 30 seconds.  this is an extremely labor-intensive process and it occurred to me that i might be able to use robotics to do the same task.  i am a software engineer so i know very little about robotics and am hoping that someone can point me in the right direction.  can someone please refer me to companies and/or robotic systems that might be able to help me with this task?\n\ni currently load the poultry into cages specifically designed for this process.  i am thinking that these cages could still be used as they keep the birds from running and make it much easier to capture their heads.\n\ni recently read about a raspberry pi that had a port of deep belief image recognition sdk and thought this might be a promising start.\n", "tags": "robotic-arm", "id": "4911", "title": "robot to manipulate poultry"}, {"body": "i am using pid controller to stabilize quadcopter. its working well on the jig. but when i removed jig it went up and hit ceiling. i analyzed my data and from which i come to the conclusion that at static poisition my gyro sensor is outputing +-6deg/sec but when i start motors (without being control in) gyro rate jupms to +-30deg/sec. this increase in rate due to vibrational noise is causing quadcopter to liftup without beign my intension. any suggestions to get rid from this vibrational noise? \n", "tags": "sensors", "id": "4913", "title": "gyro rate gets increase problem"}, {"body": "i am a complete beginner at this.\n\ni have an arbitrary map as a png file (black and white, only). and i'm supposed to draw a robot at a position(x,y) and then i'm supposed to simulate the robot taking a laser-scan measurement for an opening angle of 130 degrees, separation of each scan-line is 2 degrees. so, obviously: each laser scanline is supposed to be obstructed by black pixels in the png map, while the other lines just keep on going for an n distance.\n\ni've attempted drawing scanlines around the object, but i'm running into issues when it comes to getting each line to be obstructed by the black-lines. to my mind, it requires a completely different approach.\n\nany helpful advice on how to get started would be greatly appreciated.\n", "tags": "python mapping simulation", "id": "4914", "title": "robotics simulation from png map"}, {"body": "my and a friend are hacking together a nespresso coffee pod dispenser. we have our heart set on a particular design after thinking up countless ways of dispensing a single pod. \n\nthe design has a single flavour of pods in a vertical tube so they tend to fall down. one or more latches around the base of the tube stop any pods from falling out. releasing the latch for 45ms will allow the pod through (10mm fall, well past the lip of the pod) while catching the next one.\n\nthe latch is the problem component. i haven't yet found a suitable product off the shelf. ideally, the solution would be compact and cheap (&lt; $5). \n\nhere are some of the latch ideas to date (most of which are considered linear in motion):\n\n\nsolenoid - seems over-kill and tend to be > 5 dollars each\nultrasonic motor - can't find any\nlinear actuator - usually around 50 dollars and quite bulky\npiezoelectric actuator - mostly tuned for nm scale precision, and are hard to come by.\nrotating disk with release notch, driven by stepper motor - still > $5 and moderately bulky.\nrotating latch with string attached to rack and pinion powered by electric motor - don't think it's a simple enough solution.\nrotating cam - how a gumball machine works (i suspect). (this was also suggested in an answer, but would involve both a mechanical and electronic motor component, not as simple as option [5])\n\n\ni have a 3d printer, so i am open to mechanical solutions - a custom latch with crude electromagnet for example.\n\n\n\nnote the desired size of the latch (yellow), holding pods (orange) in a tube (black). yes, motors can work, but they would be quite bulky. i'm not after the obvious solution, but a clever one, or one which finds the suitable product.\n\n(i understand that with only one latch on one side, the pods will not sit perfectly vertical, and the latch would need to be higher up.)\n", "tags": "mechanism", "id": "4928", "title": "simplest and cheapest way to create a spring back latch"}, {"body": "i have sorted out all the internals for my robot (drive systems and weaponry) and now i need to put it all together in a chassis which will be about 40 cm wide by 35 cm long by 7 cm high. i have examined different options, including perspex, acrylic and polycarbonate as well as aluminum, in a number of thicknesses. however i have excluded perspex and acrylic because, unlike polycarbonate, they tend to shatter if bent.\n\nso now it is down to polycarbonate and/or aluminum.\n\nso here is the problem, up for discussion or a solution.. but first i must point out (a) that the overall weight limitation in turn imposes chassis weight limitations (b) that this is my first ever robot wars entry, and (c) i am likely to be up against cutting and tearing devices. \n\ni already have the weights of the different materials in hand, so all of the following options are possible in terms of weight.\n\noption 1: do it all in 6 mm polycarbonate.\n\noption 2: combine a thin (2 mm aluminum) outer shell with an underlying 3 mm polycarbonate one to get a good mix of the properties of both (rigid and hard, thin and heavy + flexible and strong, thick and light. \n\noption 3: as option 2 but the other way round - 3mm polycarbonate on the outside and 2 mm aluminum on the inside.\n\nshould i go with option 1, 2, 3 or something else altogether that maybe i am not seeing? (note: having it all in 3 mm aluminum is not possible as it will be too heavy - i checked)\n\nshould i have the aluminum on the outside as a heavy duty shell or on the inside as a \"last resort\" layer? (note: in my mind these layers would be held together with nuts and bolts with washers to spread any impact loads; but even here should the nuts and bolts be tightened for rigidity or left slightly loose for impact absorption?)\n\nany advice, especially from people seasoned in the art of robot warfare please?\n", "tags": "wheeled-robot chassis", "id": "4930", "title": "chassis materials for hobby-weight (5.44kg) battle robot"}, {"body": "i scavenged a 4 terminal power switch (legion eps 21) from some electronic device (don't remember what it was) and it has the following markings on it:\n\nlegion eps21\n10a 250vac tv-5\n8a/128a 250v &micro;t85 \n\n\n\ni would like to use this as the main power switch for my robot which will have nothing higher than 12 volts and a \"normal\" total amperage (i.e. with all the motors in it running) of around 25 amps, but of course if a motor stalls then the current will rise much higher. \n\nfirst of all i cannot understand how the same switch can be rated for different amperages. i cannot find any datasheets for this switch that might help nor any reference to the standards (tv-5 and &micro;t85). so i would like to know if this can handle 128 amps at 12 volts. if it helps at all, the wires currently connected to the terminals are quite thick and have \"18 awg 600 volts\" written on them.\n\nsecondly i would like to ask whether i need to cater for normal running amperage levels or for stall current levels, which are obviously much higher. although people talk about stall currents running over 100 amps in some cases - which quite frankly cause me some concern - i cannot seem to be able to find any such switches on any robot component websites, so i am starting to think that it is the \"normal\" level of current i should plan for and not the \"stall\" one. am i right in this?\n", "tags": "current", "id": "4933", "title": "power switch standards and suitability for purpose"}, {"body": "after seeing the movie, i, robot, i got this question.\n\nif asimov's 3 laws (actually implementing law 1 automatically implements the other 2) are perfectly implemented on a quantum computer that controls an army of humanoid robots, and it  decides that taking complete control over the politics and economics via revolution is the best way to ensure human happiness, shouldn't it be allowed to proceed peacefully to ensure minimal loss of life? isn't the hero's decision to destroy the computer fundamentally wrong? \n", "tags": "theory", "id": "4934", "title": "perfect implementation of asimov's 3 laws"}, {"body": "qt has native bluetooth support, but can it be used to communicate with the lego nxt robot?\n", "tags": "nxt robotc", "id": "4937", "title": "can i use qt to communicate with a lego nxt robot?"}, {"body": "i'm working on a diy quadcopter build from scratch and have bought a 4pack esc from castel creations.while i currently have my quad up and running(sort of), from what i've read on the various sources and forums on the internet, i am not able to/ not recommended to use different escs together on the same quad.\n\nas i bought my escs together as a 4 pack, and am not able to buy any replacements unless i were to switch out all 4 of them, this has me worried in the eventual case of a spoilt esc in the future.\n\nfrom what i can gleam from various posts on the internet, it seems to have something to do with the rate at which escs communicate with the flight controller.if so, can i not simply buy a esc programmer and program all of them to communicate at the same rate?\n\ni've asked the dude at my local hobby shop, and he said that i cannot/should not be using different escs from different brands or even the same brand but different models( i.e 35v &amp; 20v ) escs together.\n\ni would really appreciate it if someone were to clarify what exactly is the issue with using different escs together on the same quadcopter.\n\np.s if it helps, i'm currently using the apm 2.6 as my flight controller on a wfly transmitter and a f450 frame.\n", "tags": "quadcopter microcontroller electronics esc multi-rotor", "id": "4939", "title": "why can't i use different escs together on a multirotor?"}, {"body": "well, i wanted to use some very small servos for a project and the smallest i could find there these: \nhttps://www.hobbyking.com/hobbyking/store/uh_viewitem.asp?idproduct=33401\n\nbut danny choo (a japanese blogger) started a business with robotic dolls some time ago and i remember him mentioning somewhere on his site that he uses servos in his dolls.\n\n\n(also this pic, containing doll nudity: https://farm4.staticflickr.com/3889/14502508165_fde682636b_o.jpg)\n\nthis is about 60cm in height and therefore the servo in my first link is obviously too big, to e.g. fit inside the arm.\n\ni was wondering what kind of servo(or motor in general if it's not a servo in the end) he is using that is so tiny it can fit in there.\n\ndoes anyone here have any idea?\n", "tags": "motor servomotor", "id": "4943", "title": "what servos does this robot use?"}, {"body": "i'm stumped. i've been looking through all of the qt classes and i'm so completely and utterly lost. there are only three examples of bluetooth use in qt, but none of them work for me. i just need a program that can talk with nxt and analyze an image from a webcam.\n\nhas anybody gotten this to work before?\n", "tags": "nxt troubleshooting", "id": "4944", "title": "does anyone have a working example of using qt to communicate with nxt?"}, {"body": "i have a camera mounted on a rotational joint. i need to calibrate the extrinsics of this camera. i can fix the camera at an estimated angle (facing the ceiling). then i want to get the real angle.\n\nfor that i track key-points in the ceiling while moving my robot forward. supposing that odometry is perfect, i will see a difference between real key-points shift and estimated shift from the odometry.\n\ni thought about using levenberg marquardt to find the optimal solution which is the angle and of my camera in the robot frame but what would my equation look like?\n", "tags": "cameras odometry joint", "id": "4946", "title": "joint angle correction using lm"}, {"body": "how does the submarine prevent water flow through the screw mechanism? the mechanism rotates the screw so there should be a little gap, how come the water doesn't come through this gap in to the submarine?\n", "tags": "motor design", "id": "4948", "title": "submarine screw and the isolation from the water"}, {"body": "i've been reading about hierarchical reinforcement learning (hrl) and it's applications. a well-written literature review on the subject can be found here. however, i was wondering if research has ever been done where an hrl system has been implemented on an individual robot? this paper seems to imply that it has been, by saying that the delivery task that it models \"is commonly used in hrl, both in computational and experimental settings\". however, my google scholar searches haven't turned up any fruit as to what this real-world experimental setting might be. help would be appreciated for finding either model-based or model-free implementation of hierarchical reinforcement learning in a robot.\n", "tags": "machine-learning reinforcement-learning reference-request", "id": "4949", "title": "has hierarchical learning been embodied in a robot before?"}, {"body": "how would i go about measuring and quantifying the performance of an esc? i am looking to measure the response rate(in hz) of an esc, as well as it's performance(i.e how fast it starts and stops, as well as how much it increases/decreases the motor's speed per tick).\n\ni'm assuming that i can manually program an esc and it's response rate with a programming card/module that is compatible with said esc, but i would still not be able to measure the exact performance of the esc.\n\ni would appreciate any and all inputs and suggestions.\n\np.s this question is linked/asked in conjunction with a previous question of mine here on the robotics stackexchange here   why can&#39;t i use different escs together on a multirotor?\n", "tags": "sensors quadcopter electronics esc multi-rotor", "id": "4953", "title": "measuring the performance and response rate of escs"}, {"body": "i wish to start my vending business, but none of the existing vending machines fit my needs.\nso, i need to choose the \"brains\" to my vending machine under current development.\n\nthe user experience of my vending machine will be:\n\n\n  user change their products on touchscreen display (firegox open rails application running in the \"brains\"), insert moneys, after that products will be returned to the user and notification (json query) will be send to it saas.\n\n\nthere are requirements:\n\n\npopular (i want to use a widely used computer for better support)\ndebian-like or centos like system (easy to develop rails apps on them)\nbig count of gpios\nworking with touch-screen and large display (at least 15\")\nworking with mdb protocol (for currency detector needs)\n\n\nso, i need your hints. it seems that beaglebone is more powerful then raspberry pi, but there is one problem: it doesn't support many of the video outputs. is there any solution to make good video output on beaglebone? do other such computers exist?\n", "tags": "raspberry-pi beagle-bone", "id": "4954", "title": "raspberry pi vs beaglebone black rev c on vending machine"}, {"body": "i am using the wheels and motors of an rc toy car as a simple robotics platform. the car has 2 motors, one drives the back wheels, the other steers the front wheels. the steering motor is stalled by design when steering, it is blocked at a fixed angle by the plastic chassis. it draws 0.85 a when stalled (i.e. anytime when steering).  \n\ndue to this marvel of toy engineering i have to use an oversized motor driver ic (l293b \u2013 1a continuous) and this motor draws about 3w of power (0.85a x 3.6v). i\u2019m using this ic to control the other (\u201cnormally rotating\u201d) motor as well, which appears to be the same type: 0.85a stall current, around 100ma no-load, and 250-400ma at normal loads. \n\ntesting with various series resistors i have found out that 0.3a are sufficient to turn the steering wheels and keep them in position. using a resistor might allow me to use a driver ic with a lower amp rating (l293d \u2013 0.6 a), however the same energy is still wasted, only as heat. while this is not a serious issue with this toy setup, i am planning to build bigger robots with significantly more power, so energy conservation and current control will be important in the long run, and motors may also stall accidentally.\n\nlooking into dc motor current limiting, i\u2019ve found the following approaches:\n\n\nseries resistor \u2013 simple, cheap, bidirectional, wastes energy, dissipates heat\ncurrent source with 2-3 transistors and sensing resistor \u2013 relatively simple, however i\u2019ve only found unidirectional circuits, which would get shorted when switching motor direction. is there a way to use this method bidirectionally? (and/or with a 2-channel h-bridge ic?  - i cannot place it before the ics common supply, because the 2 motors draw different currents). \nchopper circuits/pwm \u2013 will this reliably protect the ic from overload? is it energy-efficient? \nare there other other methods i am unaware of? something on the principles of switching supplies?\nwould it be simpler in my application to use 2 separate drivers/h-bridges and place a voltage divider between them, so that a lower voltage is provided for the inefficient stalling motor and more to the one that moves the robot?\n\n\nso how do the above  methods compare in terms of efficiency and simplicity of design? what is the preferred method in robotics/other dc motor applications?  also, is it standard practice to limit dc motor current, or a motor is most efficient if allowed to draw as much current as it needs? is it acceptable to use a dc motor that is mechanically stalled by design, or is this only used in cheap crap toy cars?\n", "tags": "motor power driver current h-bridge", "id": "4955", "title": "comparison of the efficiency of dc motor current limiting / control methods?"}, {"body": "i've read about various robots using a 2d lidar system for slam ( such as at igcv, http://www.igvc.org/ ) but i'm wondering how good exactly does the sensor have to be? specifically: \n\nwhat accuracy is necessary? \n\nwhat field of view is necessary? is it enough just to have lidar scanning forward in a 90 degree sweep?\n\nwhat angular resolution is needed?\n\ni realize that probably with super clever software you could probably do slam with a couple ultrasonic sensors, but using standard packages for software navigation what's a reasonable minimum value for these parameters? (and any other important ones i've forgotten) \n", "tags": "slam navigation lidar", "id": "4956", "title": "what kind of lidar is necessary for slam?"}, {"body": "in my possession i have an inovatic usb interface. (in detail: ui-8x8 v1.1 2009) i would like to program it to do some simple stuff and things. i am familiar with c# programming but from what i have heard its not possible to program this interface with c#.\n\nwhat it looks like:\n\n\n\n\n\n\nwhere can i find the drivers for this interface?  i have checked the inovatic website but they only have the v1.0 version of the drivers and i'm pretty sure that i need v1.1 !\nhow can i program it? what language do i use?\n\n", "tags": "microcontroller", "id": "4958", "title": "how to program an inovatic usb interface?"}, {"body": "i am computer programmer and it's really been long since i have done electronics. i need help on connecting my servos to an arduino to power my robotic arm. this is the robotic arm that i am trying to build.\n\ni have come up with the connections as shown in the below diagram with my basic knowledge and browsing the internet. i have omitted the signal wires for clarity.\n\n\n\nwhat i would like to know is \n\n\nwill this work?\nis this a good/decent design? i think it isn't as i have 4 battery packs.\n\ni would like to have a single power source that would save me the trouble of maintaining so many batteries. to do this i have thought of using a voltage regulator but i am concerned about how this would perform if one servo starts drawing too much load. it might suck up all most the power leaving very little for the other servos.\n\n\nany suggestions would be greatly helpful.\n", "tags": "arduino robotic-arm servos", "id": "4959", "title": "connecting multiple servos to a robotic arm"}, {"body": "when i put stereo camera on a moving platform and sync the cameras, what kind of delay between the cameras is tolerable? this would probably depend on the motion, but are there some guidelines?\n\nare there any practitioner guides on how to build a moving stereo system?\n", "tags": "stereo-vision", "id": "4960", "title": "stereo vision on a moving vehicle"}, {"body": "i am new to robotics. i want to make a robot using arduino uno r3. i need to use gear motor 9 for that here is the link. \n\nthe problem is that motor needs 50ma current. but arduino only outputs only 40ma current. \n\ni want to supply the motors with another power source and use a switch to connect both the circuits. can you please tell me what type of switch i can use.\n\nthanks in advance. \np.s. sorry if i used any wrong technical terms\n", "tags": "arduino motor", "id": "4965", "title": "how to use gear motor 9 with arduino"}, {"body": "i have a two-wheeled (two dc motors) robot that needs to follow the wall beside the robot.\n\nthe issue is that the dc motors spin at different rates (because they are not identical, of course), so the robot does not go straight when the same voltage is provided to each motor.\n\nhow can i use ir distance sensors (and op-amps) to keep the distance from the wall constant?\n\n(the robot must travel parallel to the wall)\n", "tags": "two-wheeled", "id": "4966", "title": "keeping two wheeled wall following robot straight"}, {"body": "i have a two-wheeled (two dc motors) robot that needs to follow the wall beside the robot.\n\nthe issue is that the dc motors spin at different rates (because they are not identical, of course), so the robot does not go straight when the same voltage is provided to each motor.\n\nhow can i use ir distance sensors (and op-amps) to keep the distance from the wall constant?\n\n(the robot must travel parallel to the wall)\n", "tags": "motor", "id": "4967", "title": "keeping two wheeled wall following robot straight"}, {"body": "to determine an outdoor location i think that i need to measure the angles between at least 3 beacons and take into account the order i sweep the beacons.  is this a workable solution to get positional accuracy of about 30cm in a house block sized area?\n\nrewrite of question, note no distance measurement is suggested only angle measurements.\ni am proposing that it might be possible to have a minimum of 3 local rf beacons and a robot device that sweeps an antenna through a circle identifying the angles between these beacons and to use this information and the order that the beacons are seen to find an absolute location.  i tried to prove this geometrically and it seems that with the 3 beacons there is 2 unique solutions without knowing the order and 1 solution if the order is known.  there would (i believe) be no need to try to find the distance to the beacons.  my question is, could this be implemented for a reasonable cost with some nrf24l01 based transcievers and some sort of rotating antenna?\n", "tags": "localization", "id": "4972", "title": "can triangulation by measuring angles to 3 beacons to find location work over a large outdoor area"}, {"body": "this is all hypothetical. if it was possible, it would have been done by now.\n\ni realise that this area has been touched upon in many sci-fi movies but i wondered that if it was even feasible, how could it be achieved?\n\ni know that it would raise a lot of ethical questions, i don't doubt that but i'm interested in the science.\n\nwhat would a robot's brain have to be like to function like a human brain? for example, for it to have emotion (e.g. love, empathy), learn new things and remember them, make all those connections that a human brain does?\n\nthanks to all who reply!\n", "tags": "mobile-robot humanoid", "id": "4980", "title": "could a robot be programmed to be human?"}, {"body": "i am a newbie in this drone field. i am curious to know what type of rotation and translation a dualcopter can achieve ? by rotation and translation i mean can it be able to roll, pitch and yaw like quadcopters?\nif not, in any copter what makes them to roll pitch and yaw? furthermore are there any dualcopter design that have movable wings that will rotate the rotors itself or do up and down motion while flying?\n", "tags": "quadcopter multi-rotor", "id": "4983", "title": "dualcopter degree of freedom"}, {"body": "given the following differential equation 2\u00b0ode in the following form:\n\n$\\ddot{z}=-g + ( cos(\\phi) cos(\\theta))u_{1}/m $\n\nfound in many papers (example) and describing the dynamic model of a quadrotor (in this case i'm interested as an example only for the vertical axis $z$) , i get the movement about $z$ after integrating the variable $\\ddot{z}$ two times. as control input i can control $u_{1}$, which represents the sum of all forces of the rotors.\n\na backstepping integrator (as in many of papers already implemented) defines a tracking error for the height $e_{z} = z_{desired} - z_{real}$ and for the velocity $\\dot{e}_{z} = \\dot{z}_{desired} - \\dot{z}_{real}$ to build virtual controls.\n\nthrough the virtual controls one can find the needed valueof $u_{1}$ to drive the quadrotor to the desired height (see the solution later on)\n\nbut wait...as said above i need to track both: position error and velocity error.\n\nnow i asked myself, how can i transform such equation and the corresponding virtual controls to track only the velocity??\n\nin my code i need to develop an interface to another package which accepts only velocity inputs and not position information.\ni should be able to drive my quadrotor to the desired position using only velocity informations, tracking the error for the z displacement it not allowed.\n\nthe solution for the more general case looks like:\n\n$u_{1}=(m/(cos(\\phi)cos(\\theta))*(e_{z} + \\ddot{z}_{desired} + \\alpha_{1}^{2}\\dot{e}_{z} - \\alpha_{1}^{2}e_{z} + g + \\alpha_{2}\\dot{e}_{z})$\n\nfor  $\\alpha_{1}, \\alpha_{2} &gt; 0$\n\ni could simply put brutal the $\\alpha_{1} = 0$ for not tracking the position on z but i think that is not the correct way.\n\nmaybe could you please point me in the right direction?\n\nregards\n", "tags": "control quadcopter", "id": "4986", "title": "backstepping integrator: changing the virtual control"}, {"body": "i have a robot simulation that simulates mars exploration rover with six steerable wheels.\n\nin case of the following steering configuration\n\n\n\ni'd say the heading of the rover with respect of the rover's body is about 45 to the right.\nmy question is what is the right approach of calculating heading with respect of the rover body? \n\ndo i simply sum the steering angles of steering actuators and divide it by the total number of the steering actuators? \n\nadditional note:\nassuming no slippage on a perfectly flat plane.\n", "tags": "wheeled-robot", "id": "4987", "title": "how do i determine the heading of a six wheeled robot?"}, {"body": "i am looking for open source implementations of an ekf for 6d pose estimation (inertial navigation system) using at minimum an imu (accelerometer, gyroscope) + absolute position (or pose) sensor.\n\nthis seems to be such a recurring and important problem in robotics that i am surprised i cannot find a few reference implementations. does everyone just quickly hack together his own ekf and move on to more interesting things? is that not rather error-prone?\n\ni would ideally like a well-tested implementation that can serve as a reference for fair evaluation of possible improvements. \n", "tags": "kalman-filter ekf pose", "id": "4990", "title": "open source implementations of ekf for 6d pose esimation"}, {"body": "with the lego nxt mindstorm kit i would like to have a rotating carousel that has \"perfect\" movement. this carousel has baskets and therefore it has quite a bit of inertia. i would like to find a method to calculate the perfect time to slow it down--taking into account motor friction, and momentum etc.\n\nhere is some data i've collected:\n\n\n\nthe motor power is the power to the motor. the break time was the time it took to stop from the time that the motor power was set to 0. the over-turn dist was amount of rotation in degrees that the motor continued to rotate after the power was set to 0.\n\nis there a specific method or approach to optimize the motors movement so movement can be precisely rotated to x degrees?\n", "tags": "motor motion motion-planning", "id": "4991", "title": "seamless motor movement"}, {"body": "my lab is interested in a good all-terrain ugv that can also be used indoors. we are particularly interested in the clearpath husky, clearpath jackal, and the robotnik summit xl (or xl hl), though we would welcome any other suggestions. does anyone happen to have experience with more than one of these, and can speak to their pros and cons?\n", "tags": "ugv platform", "id": "4994", "title": "best ugv platform?"}, {"body": "i am looking into building a nano quadcopter, but as i watch more resources and videos i get more confused, regarding some of the things that i hope would be answered here. i am in very basic level of expertise here, i haven't built any robots or quadcopters to be exact.\n\nwhat i want to know is, when i program a quadcopter say using intel edison chip, how do i power the quadcpter? i could not find that small size battery to move the propellers and start the chip.\n\nfurther more what is the procedure i should follow while developing a nano or small quadcopter, i saw a link on instructable that uses python on raspberry pi and then that raspberry pi control the arduino to control the robot. can it be done only by using raspberry pi itself? \n\ni am getting confused and i would like to know if i have to make small or nano quadcopter what should i be doing to get started?\n\nmost of the latest chip support linux and high level programming language like python, so i hope i can go about programming the entire quadcopter using python or similar high level language and i don't suppose i have to stick with c langauge now. if i am wrong please help me understand the matter, there is high chance that i could be wrong.\n", "tags": "arduino quadcopter raspberry-pi python", "id": "4996", "title": "nano quadcopters microcontroller and battery"}, {"body": "what's an appropriate tool for simulating a car driving in a simple closed-loop racetrack? i'm trying to implement the control logic for an autonomous vehicle, and i'd like to be able to first simulate the behaviour before testing on a physical platform. the target environment is mostly 2d, but there are some 3d obstacle like small ramps and arches, so i can't use a strictly 2d simulator.\n\ni've looked at some robotics simulators, as listed here, but they seem like overkill and none of them seemed designed to model outdoor environments. i've done a little work with gazebo, and i can't find any guide of texturing the ground/sky/background.\n\nall i really need is some way apply a texture map to the ground and sky, create a handful of obstacles, and then to calculate a camera feed as a simple two-wheeled chassis moves along a mostly 2d course. however, i need the video input to be as realistic as possible because i don't have access to the real world racetrack. i need to be able to test and train the control logic in the simulator, and then load that logic onto the real mobile platform and have it navigate the course.\n", "tags": "navigation simulator cameras", "id": "4998", "title": "good 3d simulator for outdoor autonomous navigation"}, {"body": "when i look at my toy gyroscope (i have never seen the inside of a motorized gyroscope), the central flywheel is suspended within the various gymbals and needs a lot of freedom of movement.  it's hard to see how an electric motor in the flywheel hub could be supplied with power.\n\nhow do \"real\" gyroscopes maintain angular velocity in their flywheel?\n", "tags": "gyroscope", "id": "5001", "title": "how does power get to the flywheel in a motorized gyroscope?"}, {"body": "i have a position level forward and inverse kinematics blocks that i built on simulink by using s-function. i need to obtain the motion level fk and ik as well. \n\nfk input is two motor angles and output is planar x,y coordinates and ik is the other way around.\n\nnow i wonder if i simply put a derivative block at the output of each block, would it work ? i tried and this and cascaded the blocks to see if the input overlaps with output but it didn't, so apparently my idea is wrong ?\n\ncan someone explain why it is ?\n", "tags": "inverse-kinematics forward-kinematics", "id": "5006", "title": "turning position level fk to motion level fk"}, {"body": "how can i write a linear state space model for a 4 wheel mobile robot with ackerman steering in terms of error. i want the robot to follow a line. the robot is rear wheel drive \n", "tags": "mobile-robot line-following", "id": "5011", "title": "linear state space model for mobile robot"}, {"body": "my friend and i are building the upper body of a humanoid robot for our m.sc thesis project. there are 24 dc motors in this robot. the thing i want to know is what is the best way to command these motors simultaneously?\n\nthe design i had in mind is for each motor to have its own micro for position and velocity control and then one master micro to command and control the slave micros. if this is the best way to go how does the master micro command slave ones simultaneously?\n\nanother question i have is what is the best micro for the robot to go with between arm and pic? i want the master micro to receive its command from a pc. any help would be appreciated.\n", "tags": "control microcontroller communication", "id": "5014", "title": "control circuit of a humanoid robot (something like icub or asimo)"}, {"body": "i am looking to buy a laser distance meter and to connect it to a motor and a 3g cellular to control both the motor and to mesure the distance. i will appriciate your advice on how to do so. thanks\n", "tags": "laser", "id": "5016", "title": "remote control laser meter"}, {"body": "please give me guidance how should i proceed to know about slam algorithm? i am following some youtube videos but those are not so much helpful for me..... \n", "tags": "computer-vision", "id": "5020", "title": "i am new in robotics....i want to know about slam algorithm....how should i proceed?"}, {"body": "we estimate position of robot in localization and slam. my intuition says we get better position estimation in localisation than in slam because we have better sensor model likelihoods in localization because of given complete environment than in slam.\n\ni would like to know the difference in accuracy of estimated position in localization and slam.\n", "tags": "localization slam particle-filter", "id": "5023", "title": "is the accuracy of estimated position in localization better than estimated position in slam?"}, {"body": "i'm trying to devise a system to lift a 10kg weight a distance of 1.4m vertically, and allow it to move in/out a distance of 30cm. i'd like the motions to be able to occur simultaneously if possible. \n\ni'm thinking for the vertical motion i can use a suspended climber system. however i am unsure as to how i devise a system for the horizontal motion (in the horizontal plane i need nothing to protrude - only when the device is told to extend, so a horizontal suspended climber system isn't a possible solution.\n\ni'm thinking i will need to use 2 electric motors.\n\nalso - i'd like to mount it to the side of a car - so lightweight and low power draw is a must.\n\ndoes anyone know if there is anything available that will do this? or suggest how i could combine a couple of systems to make this work?\n\nany information is appreciated.\n", "tags": "mechanism", "id": "5028", "title": "trying to design a mechanical system with vertical and horizontal movement"}, {"body": "i'm building a quadcopter using raspberry pi. there is the pi camera connected to the raspberry pi which is streaming the captured video. i can connect to this stream via wi-fi on my notebook (linux) by using the console command \"nc\" and then show it by \"mplayer\".\n\nwhat i want to do though is avoid the console commands and connect to this stream directly through my java application. the reason is i want to do some image processing operations with this video so i need to have it in my application.\n\nis there anyone able to help me?\n", "tags": "quadcopter raspberry-pi cameras linux", "id": "5031", "title": "connect to video stream with java app instead of console and mplayer"}, {"body": "i am building the upper body of a humanoid robot for my m.sc thesis project with 24 dc motors and multiple sensors (something like i-cub or nao). i have basic knowledge of communication protocols and i have worked with micros before but i have no knowledge and experience on working with ros. the question i have is whether or not it is worthy and practical for me to learn ros and use this for my robot or should i stick with what i already know.\n", "tags": "microcontroller ros humanoid", "id": "5034", "title": "control circuit of humanoid robot: is it worth to learn and use ros?"}, {"body": "my native language is not english, so i don't know all the specific terms you may expect me to use. i apologize for that.\n\n\n\nanyway, i have a motor and three connecting rods (in french, bielles). so point c will have a circular trajectory and a, thanks to the sliding pivot (pivot glissant, i really hope i am using the right translations), should have a perfectly vertical trajectory.\n\nmy question is, how could i calculate the force f? i need this to emboss a piece of paper.\n\nthanks a lot for your attention!\n", "tags": "force", "id": "5036", "title": "calculating the force of this system"}, {"body": "i am using l3gd20 and i am trying to implement a kalman filter for it on the stm32f3 discovery board. i have though a few questions about that:\n\n\nafter the filter gave me the values, do i have to make an average between them and those of the original model or should i use them as they are?\naccording to this document, we don't use the original state space vectors in the filter, so how could we have \"correct\" space state estimated values?\n\n", "tags": "kalman-filter gyroscope", "id": "5037", "title": "kalman filter model values or state space original value? which values to use?"}, {"body": "i am making a robot that is supposed to roam inside my house and pick up trash using opencv. i plan to send information from my arduino mega to my arduino nano connected to window pc using radio transceivers. i also plan to send video feed from raspberry pi camera over wifi to the windows pc. the windows pc then uses opencv and processes other information from the sensors and sends command back to arduino mega.\n\ni have right now:\n\n\narduino mega\nraspberry pi + usb camera + wifi dongle\nxbox 360 kinect \nwheel encoders\nsonar distance sensor\narduino nano\nwindows pc\n\n\ni want to know how to keep track of the robot like the room it is. i think what i am trying to do is slam, but i just need to make the map once because the rooms don't change much. i am open to ideas.  cost is a factor.\n", "tags": "arduino mobile-robot localization", "id": "5039", "title": "best localization method?"}, {"body": "i am trying to integrate bluetooth in a project with msp430 so to be able to communicate between it and my pc. doing a search on ebay i found the following item:\n\nhc-05 06 transceiver bluetooth module backboard interface base board serial\n\n\n\nthere are also a lot of other bluetooth modules that appear to be a lot more expensive and their boards are populated with ic's that this one doesn't have. so i am wondering if this is what i need or it has another use.\n", "tags": "arduino", "id": "5041", "title": "bluetooth integration with msp430"}, {"body": "is there a \"cape\" to make wiring sensors into a beaglebone black easier?\n\nwhenever i read some guide for wiring a sensor into the beaglebone (like this one) it always recommends attaching wires directly to gnd, +v and signal pins, which is horribly messy and unmaintainable. even for small projects, you end up having several wires connected to the same gnd/5v+ pins, so if you need to replace or repair something, you end up disrupting the wiring for every other component in your project.\n\nmost arduino guides assume this bad practice too, but at least i've found various \"gvs\" shields to help organize groups of gnd/5v/signal pins so i can attach individual sensor cables.\n\nis there anything similar for the beaglebone? i can't find anything appropriate googling \"breakout\" or \"io\" cape. i could only find one gvs cape, but it's less than ideal, since it only exposes 5 5v gpio pins, and everything else it exposes as 3.3v or 1.8v which are incompatible with most peripherals.\n", "tags": "beagle-bone", "id": "5044", "title": "wiring 5v sensors to beaglebone black"}, {"body": "i am using this gyroscope in order to measure the rotation of my robot around the z axis.\ni want to implement a kalman filter in order to improve the values.\nwhat i came with since now is this space model:\n\n$$\n \u03b8(k+1)=\u03b8(k)+dt*\u03b8'(k)+w(k) \n$$\n$$\n y(k)=\u03b8(k)+z(k)\n$$\nwhere $\u03b8$ is the angle, $\u03b8'$ is the angular rate given by the gyro and $w$ is the noise. (i hold up my gyro and measured 50 values while it was steady and find out that the variance is equal to 0.0002).\nwhat i want to ask:\n\n\nis what i did is correct?\nhow can i find out $z(k)$? .according to the data sheet noise density is equal to 0.03 dps/sqrt(hz),how can i use this information to find out $z(k)$ and correct $w(k)$ if it is wrong.\n\n", "tags": "kalman-filter gyroscope noise", "id": "5045", "title": "how to choose the state space model for 1 axis gyroscope to implemnt a good kalman filter"}, {"body": "i originally asked this here but i am struggling to take the concepts, particularly \"circumcenter\", discussed there and apply it in my context. so here it is in my context...\n\ni have a small robot that can \"see\" (via a camera) a partial arc (from a birds-eye view)\n\ntherefore i know the height and the width of the square and therefore the width and height of the arc, i am sure there must be a way of approximating the circles centre?\n\nwhat would be the quickest (not necessarily the most accurate) way of helping my robot find the centre.\n\nin my head these are the steps of the problem:\n\n\nbreak up the arc into evenly spaced vectors ?\nwork out the angle between the vectors\nuse this information to complete the circle ?\nwork out radius of circle \nfind the center of the circle\n\n\nbasically i would love some input on this problem because i think i know\n\n\n?\ndot product\n?\nadd up the lengths of all the vectors to get circumference and then divide by pi, then divide by 2 (or divide by tau : ) ) \ni think this is where circumcentre comes in\n\n\nbasically i feel i have some pieces of the puzzle but i am not sure how they fit together. \n\ni am currently using python with opencv and you may have guessed, i am not great at understanding math unless its expressed in algebraic formula or code. \n\nhere are some illustrive pictures to reference:\n\n", "tags": "computer-vision navigation", "id": "5047", "title": "find centre of circle, when robot can \"see\" a partial arc"}, {"body": "i was looking at the pixhawk specs and noticed that it has 2 different imus- invensense and stm. is it for redundancy or does it have any other higher purpose?\n", "tags": "uav", "id": "5053", "title": "why does the pixhawk have 2 imus"}, {"body": "suppose i have a robot arm with $n$ linkages of fixed length and equal density whose motion constrained within a 2d plane.  i want the end effector to reach a particular pose $(x^*,y^*,\\theta^*)$.  \n\ni know that in general, there can be multiple solutions that can reach this pose.  for my particular application, i'm interested in a solution that minimizes the maximum torque exerted over any joint under the influence of the weights of all the linkages, combined.  \n\nis there a way i can reformulate the inverse kinematics problem as a minimization problem over the joint loads?  can i formulate my objective function to be differentiable (i.e. so that i can use traditional optimization techniques)?  would this yield an unique solution (in a least squares sense) for the 2d planar motion problem?\n", "tags": "inverse-kinematics", "id": "5058", "title": "2d robot arm inverse kinematics with minimum joint loads"}, {"body": "i am working with a position-controlled manipulator. however, i want to implement a torque-controlled method on this robot. is there any way to convert a torque command to a position command?\n\ni try to find research papers on this but i have no idea where i should start or which keywords i should use in searching. do you have any suggestion?\n", "tags": "robotic-arm industrial-robot manipulator", "id": "5059", "title": "implementing a torque-controlled method on a position-controlled robot"}, {"body": "i am new to robotics, and ime looking for a &lt;12v motor that can be used to power a car-like robot.i will have two of these, so i can turn on-spot. furthermore i want them not to require a gearbox, so i can just attach them to the wheels. i don't really know where to start looking for one. \n\ni have heard servos have built in gearboxes, but don't they only have 180 degree rotation?\n\nso does any body know a motor like i described in paragraph 1, or at least point me in the right direction? \n", "tags": "motor", "id": "5062", "title": "what motors should i use that do not require gearboxes? this is a car like robot"}, {"body": "i am looking to build an adaptable robotic arm with under-actuated three (or four) fingered hands. before i start shelling out money, i want to test my prototypes in a simulator which would ideally\n\n\nallow me to try out various actuators, and also possibly a tactile sensor (like a pressure sensitive resistor or a pressure sensitive conductive sheet). \nsimulate different environments and tasks like gripping various shapes, sizes, weights etc.\nbe able to talk to an external learning/inference programs for the adaptive part (which, i think, goes under the name 'dexterous manipulation planning' tasks), with sensory feedback from tactile sensors within the simulator and also camera input from a separate module.\n\n\n\n  what are my options for such a simulator, including those that only\n  partially address my requirements above?\n\n\na bit about my background: i dont have any recent experience in building electronics hardware projects, although i have experienced it as part of my labs during electronics engineering under-graduation, a field that i have left a long time back. i am just a wannabe hobbyist now.\n", "tags": "robotic-arm motion-planning simulator planning", "id": "5068", "title": "simulator for an adaptive, under-actuated robotic gripper"}, {"body": "sharp ir range finders are pretty popular sensors, but i usually see them externally mounted and directly exposed to the environment which makes them prone to being damaged or getting crufty.\n\ni have a few that i'd like to use on an outdoor rover, and i'd like to cover them with some sort of transparent case to protect them dirt and impacts in the environment. what type of plastics would be completely transparent to these sensors, and where would i buy simple sheets of it?\n", "tags": "sensors rangefinder", "id": "5071", "title": "plastic that's transparent to ir range sensors"}, {"body": "i'm trying to pull analog input from a beaglebone black using this tutorial. however when i go to  there is no . i have spoken with several other programmers and one of them suggested that the cape-bone does not work with the newer versions of linux. however downgrading could have negative impact on the rest of the project. is there any other solution?\n", "tags": "beagle-bone linux", "id": "5074", "title": "ubuntu arm lacking /sys/devices/cape-bone-iio"}, {"body": "let's first start of by explaining that i do not have a decent background in electronics and interfacing with them, so maybe this is a stupid question. i am currently trying to connect an old harvard 33 syringe pump (website, pdf manual) to a computer, with the goal of controlling things like pump rates and direction. for this purpose, i connected the instrument to my computer using a d-sub/usb conversion dongle. i then connected to the dongle with pyserial without issues. however, whenever i try to send commands or request the instrument's output, for example , the instrument does not do anything at all. requesting data () returns only a couple of . i suspect i am communicating with the dongle itself rather than the pump. when the pump is turned off or unplugged, i get exactly the same results!\n\ncould anyone explain to me why my method does not work?\n\nmy python code for reference:\n\n\n\nadditional observations: when the instrument is plugged in but the above code is not running, the pump does all sorts of things at random (move one way, stop, move the other way, etc.). this behaviour is much less pronounce but still present when the code runs (and 'locks' the channel or something?). this seems to suggest that the reference voltages (logical high and low) are not properly set at 2-5v and 0-0.5v respectively\n", "tags": "control usb rs232", "id": "5079", "title": "communicating with syringe pump using pyserial"}, {"body": "i'm currently flying a f450 quadcopter using a apm 2.6 flight controller.while i am able to get the quad off the ground and relatively steady horizontally(via the use of trims).however, i am unable to get the quad to hover no matter what i do.i've tried using trims on throttle,but i am still unable to get it hover.on my transmitter (wfly wft06ii), where the throttle has \"ticks\", i am currently stuck between too little lift and too much lift, where i push the throttle up by 1 \"tick\" and the quad goes from slowly decending to ascending, and vice versa.\n\nis there any way i can get my quad to hover ( with my hands off the throttle if possible), as currently, evern with me trying to fly it, i can never get it to hover vertically as it alternates between ascending and descending whenever i fiddle with the throttle.\n", "tags": "quadcopter multi-rotor radio-control", "id": "5082", "title": "unable to hover my quadcopter"}, {"body": "we want our wheeled robot to follow a (rather short) trajectory. we wrote an lqr controller, which works well in simulation. however, our robot offers two problems:\n\n1.) the reported state information does not seem to be very accurate.\n\n2.) its motion seems to underly some random deviations. we did not succeed in establishing a good model to predict the robots motion with a given control input.\n\nis it possible to manage these problems with the lqr controller? if yes, how?\n", "tags": "control wheeled-robot kinematics", "id": "5095", "title": "following a trajectory with lqr controller"}, {"body": "suppose we need to detect the occurrence of a special event in a large area(e.g. a city). if we need that each point being visited every h hours, how could we find the optimal number of robots for cooperative surveillance? \n\nnotice that there is no control center!\n", "tags": "multi-agent swarm", "id": "5097", "title": "optimal number of robots for cooperative surveillance"}, {"body": "i am looking for possible iso standards for robot safety specifically for software.\n\ni have come across this presentation that mentions many iso standards and it's not very clear which exactly applies to software. the most probable ones are:\n\n\niso 10218-1\niso 13849-1 / iec 62061\niec 61508\niso/ts 15066\n\n\nthe safety related to software seems to be categorized as level 4 and level 6 in the presentation above.\n\ni would appreciate if anyone with knowledge in this area could point me to the right standard. they are quite expensive so i could simply go through them all to see which one applies.\n\nas a side note, some standards like c have their standard \"draft\" freely available. could there be free copies of drafts for those standards too?\n", "tags": "software", "id": "5099", "title": "robot safety standards for software"}, {"body": "i'm connecting an arduino to a lcd screen using the following library. for the display code i have written a simple piece of code that should display 2 picture os eyes (1 angry one friendly) and switching at regular intervals, however the display keeps showing me weird pixels around the borders that shouldn't be there, by making it show the same eyes twice this can be fixed however as long as i have both eyes being used it runs into trouble. here is my code: \n\n\n\neach time i re upload the image it changes the way the noise patterns look suggesting this is some kind of overflow problem. however changing the last byte of the bitmaps creates lines at the bottom of the screen, right where the noise is on one of the images. note that with different images this noise can very much \"cut\" into the images even creating not active pixels (0's) rather then just set ones. suggesting that the images themselves to at least fit the display. \n", "tags": "arduino", "id": "5103", "title": "arduino -lcd screen has weird noise with multiple pictures"}, {"body": "i need to simulate robotic cell where cartesian robot trims a pcb arriving on conveyor, picks it up with vacuum cup and and places in another device. after receiving signal from device the robot would pick it up and place on another belt. i want to make the cartesian robot myself using servomotors and control cell using a plc. would there be software that can simulate all this? i would also need to integrate sensors and possibly machine vision.\n", "tags": "simulation", "id": "5104", "title": "robotic cell simulation software plc"}, {"body": "the rubber hand illusion (wikipedia) involves touching both a fake arm and a subject's real arm simultaneously. this causes the subject to feel that the fake arm belongs to him. normally a human delivers both touches, so the timing is approximate. i want to vary the latency between the fake touch and real touch precisely (~5 ms at minimum) to probe how close they need to be to create the illusion. what can i use to touch a human and fake hand lightly at variable but precise times?\n", "tags": "research", "id": "5106", "title": "how do i set up a rubber hand experiment with precise latency?"}, {"body": "i'm looking to buy a micromouse (i.e. a small single-board unit with wheels and ir sensors that can move around freely). i've done a lot of searching but have only found resources relating to building one from components bought separately. however, i'm more of a programmer than an electrician so i fear i would struggle with this.\n\nanybody know where to buy one of these in the uk? (picaxe does some suitable stuff but they're basic only unfortunately). my budget is about \u00a360.\n", "tags": "arduino mobile-robot wheeled-robot micromouse", "id": "5108", "title": "looking for a cheap(ish) micromouse that i can program with c/c++"}, {"body": "i have a requirement to transmit some sensor data through wireless to a distance of 2 kilometers. i am a newbie to these technologies and concepts. can anyone help me by providing some pointers to start with this. \n", "tags": "wireless", "id": "5109", "title": "regarding long distance wireless communication"}, {"body": "i want to calibrate my compass, which is installed on a board which inherits a gps module. because the gps antenna is up-side-down the compass is 180\u00b0 inverted. the easiest way to correct the problem would be to invert the rotation matrix 180\u00b0. \n\nhowever i got interested how a general approach to calibrate a compass would look like. i found some approaches like this. they seem to collect magnetometer readings an project them on a sphere. but what is actually the point in this? \n\ndoes someone know how a general calibration algorithm of a 3d magnetometer looks like?\n", "tags": "calibration compass magnetometer", "id": "5110", "title": "3d magnetometer calibration algorithm"}, {"body": "i have a 7dof robotic arm and a set of end effector trajectories in cartesian space i need it to follow. \n\nhow do i deal with the redundancy in the arm when planning to follow these trajectories both with and without obstacle avoidance?\n", "tags": "robotic-arm motion-planning c++ planning", "id": "5111", "title": "redundant arm path planning and trajectory following"}, {"body": "i've built a quadcopter and a rig to safely test it on. i'm working on the pid for controlling the roll pitch and yaw. i understand how a pid works on a more simple plant like say a robot with wheels and i'm just really in the dark ( i believe ) with controlling and stabilizing a quad.\n\nmy question, how do i make these sensor readings effectively alter the motors' throttle? \n\nfirstly, my approach is based on this model,  \n\n\n\nmy imu calculates the roll and pitch as a value between +-1.0 where being perfect balance will read as 0.0.  now a degree of +-1.0 means approximately 90 degrees from the original axis.\na normal input to the pitch to go forward would be something like 0.33, meaning tilt 30 degrees forward.\n\nnow my motors take some value between 0 and 100. originally i thought this would mean i would have to modify my motor values like so. \n\n\n\nfinally, i'm taking those floating point numbers, from the imu and computing them like with this method, which appears to be the normal way as far as i've found.\n\n\n\ni don't know where to go from here? also i have angular rate calculated from my imu i just haven't encountered a solution that called for it. \n\nedit. below is a graph of roughly 300 readings (20ms apart) so roughly six seconds where i hold it in one hand and roll it roughly 45degrees right. with kp=1 ki=0 kd=0\n  \n", "tags": "quadcopter pid design", "id": "5116", "title": "how to find a solution for quadcopter pid control"}, {"body": "i am building a collision avoidance system for my robot. as a part of this system i am using a pan and tilt kit \n\n(http://www.robotshop.com/en/lynxmotion-pan-and-tilt-kit-aluminium2.html)\n\nmy aim is to pan the sensor attached to this kit, and thus plan the route the robot should take.\n\nin order to pan this sensor, i need to know the angle the kit has panned, and need to be able to call upon that angle at point in time. \n\nbasically the sensor keeps panning, and at a point in time when certain conditions are met, it stops panning. when those conditions are met, i need to be able to extract the angle the sensor is panned at.\n\nthe servo being used on is: http://www.robotshop.com/en/hitec-hs422-servo-motor.html\n\nif someone could help me find a way to extract this information that would be helpful. i did read somewhere that the servo could be hacked and changed into a closed loop system where the effective angle would be shown, but that option is not viable.\n\nthanks\n", "tags": "mobile-robot sensors servos", "id": "5118", "title": "finding the position of a servo"}, {"body": "i can't open parameter list with apm planner.\n\nmoreover i can't find anyone with same problem.\ni run it on ubuntu/trusty 14.04\n\nit does not see files with any extensions including  and  downloaded from internet or created with my version of apm planner.\n\nany ideas how can i fix it?\n\n\n\nps  from terminal\n\n\n\ni have ,  files in downloads folder also.\n\nps\n\n\n", "tags": "ardupilot", "id": "5126", "title": "apm planner on linux/ubuntu \"open from file\" not working"}, {"body": "as already mentioned the pid output values that correspond to the error from the desirable error and current error has no units. let's say we are using only the proportional part of the pid. is it better to map the output of the pid values to the corresponding thrust values on each motor, or is it better to increase the proportional coefficient  until the output values correspond to the proper thrust value to the motors? \n\nfor example if my desired angle is 0 and the angle that the sensor is reading is 40 degrees the difference is multiplied by  and the output is added or subtracted from the current thrust depending on the motor.\n\nif i increase  too much, then the quadcopter is oscillating and not listening to the controller command that i am sending for the desired degrees from the joystick. if i map the values then it is listening to the joystick commands and not oscillating so much. why is this happening? isn't mapping the pid output values to bigger values the same as increasing ?\n", "tags": "arduino pid", "id": "5130", "title": "pid for quadcopters"}, {"body": "i'm attempting to build a heavy platform on the create 2 but am worried about weight on the platform.  what is the maximum weight for the platform and is there an optimum?\n\ni have an old create and want to know if any of my existing cables and accessories can be used with the new create 2? \n", "tags": "mobile-robot irobot-create", "id": "5132", "title": "what is the maximum payload weight for create 2/can i use old create accessories with the create 2?"}, {"body": "what aged children is the create 2 appropriate for?  what is prerequisite knowledge?  is this an appropriate first robot kit for a child?\n", "tags": "irobot-create", "id": "5133", "title": "what is recommended prerequisite knowledge to get kid started with create 2?"}, {"body": "i have an arduino uno and a beaglebone black and would like them to talk with eachother. both the beaglebone and the arduino have their own 5v power supplies so no need for power to be transferred over usb. the used communication line we would like to use is the direct serial line. preferably in such a way that the arduino can just call serial.read() in order to get the bytes it needs (there won't be many). how do i get this to work on the beagleboneblack? i suspect we can call the serial.write somewhere inside the bbb(we mainly program in c++). but how do we achieve this? \n", "tags": "arduino serial usb c++ beagle-bone", "id": "5138", "title": "connecting an arduino uno with a beaglebone black over usb"}, {"body": "what is the best yet simple to use angular position sensor? i am building a robotic hand and i want to implement this sensor at the joints of the fingers. i don't need a module, just an analogue output.\nthank you.\n", "tags": "sensors hall-sensor", "id": "5140", "title": "simple yet effective angular position sensor to be used in robotic hand"}, {"body": "i am new to robotics. i want to understand how gears state is preserved as the gears turned to same positions repetitively. \ni have a bevel gear and a step motor connected to one gear. this gear will turn 45*n = degrees. that is to say there are 8 states gears will stay in. the problem here is there will be force on the gear, which is not connected to motor, in any direction. that force must not change worm position even in micrometers. i think there should be a locking mechanism. is there any applications of that you can give example of?\n", "tags": "stepper-motor stability", "id": "5148", "title": "step motor and bevel gear calibration"}, {"body": "i' m looking for a trajectory generator (the algorithm doesn't matter, since i m going to write it using c++ ) that generates a trajectory (a parametric curve in space) defined point by point which is going lately to be feed into my quadrotor drivers.\n\ni'm honest: i don't know where to start.\n\n\nreading the following interesting answer. but the problem here is: the trajectory has a pd controller with it. my quadrotor should take just a parametric curve as input.\nompl: this library seems very powerful and interesting. it let the user to define a planner which many different algorithms. the problem is that it is not well documented, good examples and explanations are missing and till now i could'nt find anything related to quadrotors, which does use that library. there is an example for a quadrotor, which doesn't find my expectations and i cannot figure out, how to implement it in my package. i don't want just copy and paste code that i don't understand.\nb-spline and bezier curve...and the whole family of parametric curves. i found very interesting libraries in internet that implement those algorithm directly c++. the problem here is: i can define some points in space, generate a spline that contains them and interpolate points for the pid controller in the quadrotor. the basic idea is like a dog chasing a rabbit. a point is generated from the start point of the spline and regularly sent to the quadrotor. the latter flies behind the point, trying to reach it until a goal point has been reached. what is the problem here?!? in this case i can only generate a curve based on geometric properties and not considering the dynamic and the kinematic of the quadrotor (which i would consider for a future project). the rabbit runs and has a tighter curve radius than the dog, which could result in a strange behavior of the quadrotor.\n\n\ni'd like same good tips to point to the right direction.\n\nwhich kind of trajectory planner are usually developed fr such an application?\n\nthanks!\nregards\n", "tags": "mobile-robot ros quadcopter movement", "id": "5149", "title": "defining a trajectory for a quadrotor"}, {"body": "after a lot of learning, i'm launching a reballing business and i feel the need to have a realtime plot of the temperatures involved (ideally 3 or 4) and i have an arduino uno and a few k type thermocouples, i was researching the subject and saw a lot of different approachs, most of them use arduinos to send serial data to a pc port, then from there they process it with phyton, other guys matlab, some use ms excel plus a free add on in vb for apps. etcetera, and now after some reading i feel overwhelmed by all the different methods, so i wonder, perhaps i'm already losing perspective here? may be there is a simple method i can use and kiss way of get it done? thank you.\n", "tags": "arduino sensors microcontroller electronics", "id": "5150", "title": "what is the easiest method to plot a temperature in my pc?"}, {"body": "i am trying to control an omni wheel robot which has 4 motors using 2 joysticks, plus there are some actuation switches which i want to control too. i am using arduino mega and a pair of bluetooth wireless module(hc-05).\n\nthis bluetooth modules works on serial communication. how should i program arduino to send both the analog values provided by the joystick and the input from the switch continuously?\n", "tags": "arduino serial communication", "id": "5152", "title": "to control an omni wheel robot wirelessly using bluetooth and arduino"}, {"body": "for a school project i am looking to track the sun with +/- 0.1 degree accuracy for use with a parabolic dish reflector. say we need a final output torque of about 20nm, what kind of gearing/motors and feedback would you guys use to accomplish this? the sun position will be found with a solar positioning algorithm.\n\ni am pretty new to this all but from my research stepper motors seem the easiest but brushless dc motors, from what i have read, can yield better results with smoother tracking. i am confused how you even use regular brushless dc motors to achieve precision positioning. i am aware of encoders but i don't really understand why the bldc are preferred for this particular application, and how one would implement them.. any ideas that can help kick start my researching?\n", "tags": "motor brushless-motor stepper-motor servomotor", "id": "5156", "title": "sun tracking with +/- 0.1degree accuracy?"}, {"body": "i'm trying to add a new message to the mavlink interface. following this page, there are the steps i took:\n\n\nadded the message to ardupilotmega.xml. right at the end of the file:\n\n\nregenerated the mavlink messages headers using . it worked okay and the new headers appeared.\nthen i added a function to the gcs class to make sure i'm sending on the right channel:\n\n\nnow it's time to send the message, i added my own function to the scheduler (on last priority). i made sure the function is called by sending text first and seeing it on the mission planner console. here is the function i added:\n\n\n\n\nmy message, however, isn't received on the mission planner end. it might have been received and ignored by the mission planner, but anyway it doesn't appear on the console window (even with 'mavlink message debug' on)\n\nis there configuration to be made to the mission planner for it to receive new messages? or am i sending the message wrong?\n\nalso, is there a way to filter out messages from the console when using 'mavlink debug mode'?\n\ni'm using sitl for testing\n\n(i don't have enough reputation - but this should be under the tag 'mavlink')\n", "tags": "ardupilot mavlink", "id": "5164", "title": "how to send a new mavlink message from ardupilot?"}, {"body": "i am reading the following research paper regarding trajectory tracking of mobile robots.\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5415188\n\nthere are two things at the start of the paper that i do not understand.\n\n1) the author derives equation(14) as the state space model of the system in which he considers the error as the state. can anyone please elaborate on why he is using the error as the state space model of the system and not the vx, vy, and w(omega, angular speed) of the robot.\n\n2) why does the author linearize the system around the reference trajectory?\n", "tags": "mobile-robot wheeled-robot differential-drive", "id": "5169", "title": "error as a state space"}, {"body": "i'm interested in quadcopters/multi-rotors and want to eventually code my own flight controller ala an apm and/or pixhawk. i've got a little experience in programming (i.e i know about if/else/else if conditionals), and have done a little programming with php, though it was procedural code.\n\ni currently have a quadcopter that i built/assembled myself that is running on a f450 frame, using a apm 2.6 flight controller,so i have a reasonable grasp of how a quad works, and i would like to take it a step further and make my own adjustments to the code base, with the eventual aim of coding my own flight controller.\n\ni've had a look at the code base, but am still unable to get a grasp of what the code is actually doing....yet. how would i go about learning how to code a flight controller?\n\ni'm thinking that i would have to learn c++ &amp; oop first, but how familiar/proficient would i have to be with c++ before i can reasonably attempt to edit the code base?also, what else would i need to learn apart from c++ &amp; oop?i am looking at setting a 6 month timeframe/deadline for me to do this, would it be possible?\n", "tags": "arduino quadcopter microcontroller multi-rotor", "id": "5171", "title": "how would i go about learning to code a flight controller?"}, {"body": "i've got an industrial sewing machine (think \"can sew with thread that looks like string, and has no trouble pounding through 20 layers of sunbrella fabric\"); it's got a 1 hp motor to power it. (i've got a smaller machine as well, w/ a 1/2 or 3/4 hp motor, which i might work on first.) the motor is a \"clutch motor\" which is always on, and a foot-pedal engages a clutch, so unless you \"slip the clutch\", you're basically always sewing fast or stopped. i'd like better control. in particular, i'd like to \n\n\nbe able to stop with the needle \"up\"\nbe able to stop with the needle \"buried\" (i.e., most of the way down)\nbe able to press a button to move forward exactly one stitch\nbe able to adjust -- probably with a knob -- the top speed of the motor\nhave the motor off when i'm not actually sewing\n\n\nthe 1 hp motor is probably overkill for what i'm doing. i don't suppose i've ever used more than about 1/4 hp even on the toughest jobs. \n\ni'd appreciate any comments on my thinking so far: \n\nfrom what i've read, it looks as if a dc motor is the way to go (max torque at zero speed, which is nice for that initial \"punch through the material\" thing, and the ability to \"brake\" by shorting the + and - terminals). brushless would be nice...but expensive. and i have a nice dc treadmill motor, and if i drive it at about 12-24v, it'll give me more or less the right speed; adjusting pulleys will do the rest. such dc motors are powered (in electric lawnmowers, for instance) by running ac through a diode bridge rectifier to produce pulsating dc, and i've got such a bridge rectifier handy. i also have an old autotransformer that i can use to get 24vac pretty easily. thus i can get 24v pulsating dc to drive the thing ... but that may or may not be a good idea. \n\ni've also got an arduino and the skills to program it, and several years of electronics tinkering, and some rc experience...but no experience handling larger dc motors like this one. i've been told the magic words \"h-bridge\", and found this motor driver which certainly seems as if it'll allow me to turn on/off the motor, and regulate the voltage going to the motor. i don't know whether, when presented with pulsating dc at the input, it'll still work. any thoughts on this? \n\ni also can't tell -- there doesn't seem to be a handy datasheet or instruction page -- whether this thing can do braking. \n\nfor position sensing, there are lots of places i can get information -- either from the needle baror the handwheel of the sewing machine, so i'm not too concerned about that part. to give a sense of the problem, a typical stitching speed is something like 1000 stiches per minute, so if i'm just sensing whether the needle is in the top 1/4 of its travels or the bottom quarter, we're talking about something on the order of 10-50hz, which doesn't sound like a challenging control scenario. \n\ni guess my questions are these:\n\n\nwill pulsating dc work with a controller like the one i've cited? \nwould i be better off with an rc motor-controller? i can't seem to find one designed for the 24v range that can handle 50 amps, unless it's for a brushless motor, which i don't have. and i think that i want one that has braking ability as well. and i worry that with an rc controller, the software in the controller may prevent me from making the motor stop at a particular position. \n\n\nany comments/suggestions appreciated. (btw, i'm happy with mathematics and with reading data sheets, and i've read (a few years ago) about half of \"the art of electronics\", just so you have an idea of the level of answer that's appropriate.) \n\nto answer @jwpat's questions:\n\n\ni got my voltage value by saying that the motor is rated for (i think) 130v, and is 2.5hp (yikes), but turns at something like 6700 rpm. (here's one that looks just like mine).  dividing by 5 or 6, i got \"about 24 v\" to give me about 1400 rpm. (i'm at the office; the motor's at home, so i can't tell you the exact part number, alas.) i honestly don't think that the no-load vs load condition is a big deal, because i can wangle a factor of 2 with pulleys. \nthe sewing machine is a juki 562 \ncurrent motor/clutch are similar to this\n\n\nsorry for the lack of detail here, \n", "tags": "motor control power", "id": "5178", "title": "i want to control a sewing machine motor; need help with choices"}, {"body": "i've just made a radio frequency remote control using pic microcontroller and i want to do something useful with it. i am thinking of a robot that gets things for you while you are at bed but here comes the question: how am i going to have the pic determine the location of the remote control calling for it? it can't really be done using a gps module because it will all be in the house.\n\nwhat options do i have?\n", "tags": "microcontroller localization", "id": "5180", "title": "making a robot that knows your location"}, {"body": "i have followed the tutorial for razor imu and it worked perfectly when the imu is directly connected to the pc. currently, i am trying to interface the 9 dof razor imu with the arduino uno by simply connect the rx to tx and tx to rx. sadly, it doesn't work! so, i am just wondering, has anyone done this before? or can anybody give me some hints? much appreciated! \n", "tags": "arduino imu", "id": "5182", "title": "interfacing arduino uno with 9 dof razor imu"}, {"body": "are any commercially available compound vision sensors available?\nnot a simple 8 sensor system using photo-diodes but a genuine sensor that can provide a >32x32 compound matrix. would some form of reduction in the granularity of a megapixel camera be a better option? the real purpose is to reduce processing time to a minimum, while extracting the maximum basic information. \n", "tags": "computer-vision", "id": "5188", "title": "compound vision system or megapixel camera reduction"}, {"body": "i'm looking for an arduino compatible depth sensor not for water. what i need is a sensor very similar to an xbox kinect (but much smaller) that will tell me what is in front of the sensor and also the shape of the object. for example, if i place a cylindrical water bottle in front of the sensor i would like to be able to figure out how far away the bottle is and also the shape of the object (in 2d, i don't need to know whether it is actually a cylinder only the general shape). the sensor only needs to be accurate at most 1 meter away. does this exist and if so where can i purchase one. if it does not exist wholly what pieces do i need to buy to put it together? thanks.\n", "tags": "arduino sensors kinect", "id": "5189", "title": "arduino depth sensor"}, {"body": "when i use a standard manual vacuum, i often notice that i have to pass over a spot several times because a single pass does not necessarily catch all the dirt.  my eyes/brain can easily perceive this information visually, but i don't know how an autonomous robot vacuum can detect whether a pass over a patch of dirt was successful or not.  what kind of sensor/action can i use to determine if the robot vacuum successfully picked up the dirt from a particular patch?  \n\ni would prefer to avoid a visual camera if at all possible because it would necessarily have to be mounted above the robot and thereby limit the range of reachable locations.  is there some other low-cost sensor that can accomplish the same task that can be placed low to the ground?\n", "tags": "sensors", "id": "5192", "title": "alternatives to cameras for a dirt sensor for an autonomous vacuum robot?"}, {"body": "i know that occupancy grid mapping requires the assumption that the robots' pose is always known when generating a map.  however, i also know that in reality, position and orientation usually treated as uncertain in practical applications.  assuming that my target mapping environment is inside a home, is there a way i can overcome inherent robot pose uncertainty in a real world application of occupancy grid mapping without resorting to implementing a slam?  that is, what is a low-cost way to increase the certainty about my pose?  \n\nor is occupancy grid mapping only useful in theory and not in practice?  \n\nupdate:\n\nit is clear to me, from the responses given, that occupancy grid mapping is just one possible way to represent a map, not a method in and of itself.  the heart of what i really want to know is:  can mapping be done without also solving the localization problem at the same time (i.e. slam) in real life applications?   \n", "tags": "slam mapping occupancygrid", "id": "5193", "title": "can mapping be done in real life applications without also solving the localization problem at the same time (i.e. slam)?"}, {"body": "i want to use brushless for my line follower.\nthe problem is most escs don't accept more than 400-500 updates/s due to the characteristic of steering signal.\nis there a way to overcome this with a custom flash or am i out of luck?\n", "tags": "brushless-motor esc line-following", "id": "5196", "title": "is there a brushless motor controller accepting over 500 updates/s?"}, {"body": "i am struggling to find good links to the use of goal babbling in slam applications. has this technique been used as a method for optimizing movement in a slam environment?\n", "tags": "localization slam motion-planning mapping", "id": "5199", "title": "slam goal babbling"}, {"body": "how can i power a wheel but let it spin freely when not under power?\n\ni saw the question how can i modify a low cost hobby servo to run &#39;freely&#39;? but i'm more interested in knowing if there is some sort of gearbox that disengages ( moves to 'neutral' ) when no torque is being applied to it.\n\ntwo ideas that come to mind are:\n\n\na drive gear on a spring loaded arm with a nominal amount of resistance before engaging. perhaps when under power it would first use power to move in one direction, then it would engage with another gear, but without power the spring would return it to a non-engaged position\na centrifugal clutch - although i'd like something that works at low rpms as well\n\n\nthe idea is to create a small bot that can move up and down a track, but if someone interacts with it when not under power it will just roll and won't damage the gearbox.\n", "tags": "motor power rcservo radio-control", "id": "5201", "title": "how can i power a wheel but let it spin freely when not under power?"}, {"body": "i have the quadcopter in the photo below. it has rotate  degrees about the  axis. i want to get the  and  components in the local frame for the weight  which always points along the vertical downward.\n\n\n\nwe simply have:\n\n\n\nsuppose that  and , then:\n\n\n\nthe negative sign in the angle was put because the rotation is about the  axis (counterclockwise).\n\n seems correct as it is pointing towards the negative local  axis but  is 2 which seems wrong because according to the diagram it is supposed to be -2 indicating that it point towards the negative local  axis.\n\nwhat's wrong with my simple calculation?\n\n\n\nusing rotation matrices, we have the following rotation matrix when pitching (rotating about ):\n\n\n\nthis matrix is used to transform vectors from inertial frame  to local frame . to find the components of the weight , we can multiply this matrix by . doing so, we get the same result:\n\n\n", "tags": "quadcopter frame", "id": "5203", "title": "simple vector problem, weight vector components & sine and cosine of rotation?"}, {"body": "i have found a continous control in the following form:\n\n$$\nu(s) = \\left( k_{p} + \\frac{k_{i}}{s} + k_{d} \\frac{n}{1 + \\frac{n}{s}} \\right)e(s)\n$$\n\nbut since i need it to \"convert\" in a digital control i need something like:\n\n$$\ny_{k} = y_{k-1} + q_{0}e_{k-1} + q_{2}e_{k-2}\n$$\n\nor everything that i can use in a digital way.\nis there any algorithm to achieve such transformation?\nactually the problem is the term $n$ in the equation. at first i thought that it was a simply pid controller but the n term is far from my understanding\n\nthank you very much and happy christmas!!\n", "tags": "control pid", "id": "5208", "title": "the algorithm for the following analog controller => digital controller?"}, {"body": "i want to make a robot arm where a joint is powerful enough to lift 8 kg up at a distance of 1 meter. \n\nthis requires torque $tau = r*f = r*m*g$ = about 80 nm.\n\nso now i am trying to find the requisite parts to put this together, i.e. motor + gears and then controller. \n\nit seems that i will require a very high gear ratio to make this happen. for example this motor: http://www.robotshop.com/ca/en/banebots-rs-550-motor-12v-19300rpm.html has stats:\n\nstall torque = 70.55 oz-in = 0.498 nm\nno load roation speed = 19300 rpm\n\nto get my required torque, i need a gear ratio of 80/0.498 = about 161:1 (and the max speed drops to about 120 rpm).\n\nmy questions:\n\n1) do my calculations so far seem correct? it seems a bit suspect to me that an $8 motor with some gears can cause a 17.5lbs dumbbell to rotate about a circle of radius 1m twice a second (i'm barely that strong). this type of torque would be awesome for my application, but perhaps i'm missing something in the calculations and being too optimistic (e.g. efficiency).\n\n2) is it safe to operate a motor at such a high gear ratio? gears are small, and i'm worried they'll easily crack/break/wear down quickly over time. does anyone know of example gears that i should consider for this?\n\nthank you very much for any help, cheers.\n", "tags": "motor robotic-arm gearing", "id": "5209", "title": "how high of a gear ratio can a motor have?"}, {"body": "first of all please see this video : http://www.youtube.com/watch?v=n0dkn4ziqvg\n\ni think there is ony one stepper motor -or servo- working in the mechanisim. but as you can see each flip counter works alone and separately.\n\nit is not like classical counter mechanism like this : http://www.youtube.com/watch?v=rjwfiiaofr4\n\nhow does it works?\n", "tags": "mechanism stepper-motor", "id": "5214", "title": "how does fliike smiirl counter mechanism work"}, {"body": "i will be beginner need a help-i want to gain knowledge about robotics so it need a basic theoretical knowledge what is the best way to start?\n", "tags": "beginner", "id": "5218", "title": "i want to be a guy of robotics"}, {"body": "help! i have recently installed lejos nxj on to my nxt brick, and soon after my batteries died. i inserted new ones, and now i cant start my brick up. when i press the startup button(orange) it makes a clicking sound and when i let go it stops. i have tried reflashing the brick with both lejos nxj and the nxt software and both programs say something along the lines of \"unable to locate brick.\" any suggetions?\n", "tags": "mindstorms", "id": "5220", "title": "clicking nxt brick"}, {"body": "as far as i can tell, the markov assumption is quite ubiquitous in probabilistic methods for robotics and i can see why.  the notion that you can summarize all of your robot's previous poses with its current pose makes many methods computationally tractable.\n\ni'm just wondering if there are any classic examples of problems in robotics where the markov assumption cannot be used at all.  under what circumstances is the future state of the robot necessarily dependent on the current and at least some past states?   in such non-markovian cases, what can be done to alleviate the computational expense?  is there a way to minimize the dependence on previous states to the previous $k$ states, where $k$ can be chosen as small as desired?\n", "tags": "probability", "id": "5223", "title": "non-markovian problems/approaches in robotics"}, {"body": "what is a good website for buying 1.5v continuous motors? i'm looking to build a clockwork robot but i cannot find a motor small enough to fit inside my power box and i don't want to mount it outside of the box. i have a 1\" by .75\" space that the motor needs to fit into. i have found a few websites but they look sketchy and none of them have good reviews.\n", "tags": "motor brushless-motor motion", "id": "5224", "title": "nano sized electric motors"}, {"body": "i know that this isn't a programming question, but it is robotics so i thought you could all be flexible since it's my first question?\n\nanyway. i love making robots using robot kits that come with instructions. it's always fun to use afterwards because of the controllers i build with it.\n\nthe problem is that i can't find anymore robots. they are all either too expensive, not what i'm looking for, or both.\n\ncan anybody give me links to some good robot kits?\n\nmy price limit is \u00a330 - \u00a340.\n\nhere are the three robot kits i have built. i need kits that are like these:\n\nrobot arm: http://www.amazon.co.uk/gp/aw/d/b002hxtonc/ref=mp_s_a_1_9?qid=1419721030&amp;sr=8-9&amp;pi=ac_sx110_sy165\n\nremote control robot beetle: i can't post more than two links. go to maplins and type in the name of the robot and you'll find it. it's a rover version of the robot arm.\n\n3-in-1 all terrain robot kit: http://www.maplin.co.uk/p/3-in-1-atr-all-terrain-robot-n12dp\n\ni don't want to program this robot. i want it to be like it is in the examples above. buy the kit, read the instructions, then build it. \n\nthank you all in advance!\n\nps. any further information will be given if asked for.\n", "tags": "control", "id": "5226", "title": "i need more robots"}, {"body": "i am reading some theories related to rigid body motion from the book \"a mathematical introduction to robotic manipulation\" by prof. richard murray. \n\ni am focusing on chapter 2, sec 4 to derive some formulation. according to his introduction of chapter \"we present a modern approach treatment of the theory of screws based on linear algebra and matrix groups\". i myself feel rather understandable and comprehensive explanation from this approach. \n\nhowever, his scope in this chapter is limited in inertia coordinate frame where he refers to as spatial frame and moving frame as body frame. is there any other references that treat the topic in the reversed order? spatial as moving/non-inertia frame and the other one is inertia frame?\n\nthank you!\n", "tags": "control", "id": "5227", "title": "theory on rigid body motion in robotics book"}, {"body": "in the dynamic model of the robot, it is obvious that we found the torques as functions of the angular acceleration of the joint as well as the linear acceleration of the link center of mass along the three axies.\n\nmy question is regarding the values of these accelerations. in general, step motors specifications do not give the acceleration.\n\nthank you\n", "tags": "torque", "id": "5229", "title": "how can i replace acceleration in the dynamic model of the robot?"}, {"body": "i would like to implement the joint compatibility branch and bound technique in this link as a method to carry out data association. i've read the paper but still confused about this function $f_{h_{i}} (x,y)$. i don't know exactly what they are trying to do. they compared their approach with the individual compatibility nearest neighbor (icnn). in the aforementioned method we have this function $f_{ij_{i}} (x,y)$. this function simply the inverse measurement function or what they call it in their paper the implicit measurement function. in laser sensor, given the observations in the polar coordinates, we seek via the inverse measurement function to acquire their cartesian coordinates. in icnn, every thing is clear because we have  this function $f_{ij_{i}} (x,y)$, so it is easily to acquire the jacobian $h_{ij_{i}}$ which is \n\n$$\nh_{ij_{i}} = \\frac{\\partial f_{ij_{i}}}{\\partial \\textbf{x}}\n$$ \n\nfor example in 2d case and 2d laser sensor,  $\\textbf{x} = [x \\ y \\ \\theta]$ and the inverse measurement function is \n$$\nm_{x} = x + rcos(\\phi + \\theta) \\\\\nm_{y} = y + rsin( \\phi + \\theta )\n$$\n\nwhere $m_{x}$ and $m_{y}$ are the location of a landmark and \n$$\nr = \\sqrt{ (m_{x}-x)^{2} + (m_{y}-y)^{2}} \\\\\n\\phi = atan2\\left( \\frac{(m_{y}-y)}{(m_{x}-x)} \\right) - \\theta\n$$\n\nusing  in matlab, we can get $h_{ij_{i}}$. any suggetions?\n", "tags": "slam ekf mapping data-association", "id": "5238", "title": "joint compatibility branch and bound (jcbb) data association implementation"}, {"body": "i am cautiously moving forward with my new irobot create 2, planning on using a raspberry pi with rosberry installed to control the create 2. discovered a problem with the pin out specs between the irobot roomba open interface (oi) specification and the create 2 serial to 3.3v logic document. here is the discrepancy (marked by discrepancy):\n\npin ((oi)) serial to 3.3v\n\n\nvpwr roomba battery voltage\nvpwr roomba battery voltage\nrxd  roomba tx discrepancy\ntxd  roomba rx discrepancy\nbrc  ground discrepancy\ngnd  ground\ngnd  roomba brc discrepancy\n\n\nthe discrepancy is with pins 3,4,5 &amp; 7.\n\ndon't want to fry my raspberry pi, any clarification and/or help appreciated.\n", "tags": "raspberry-pi irobot-create", "id": "5240", "title": "irobot create 2 discrepancy betweenopen interface specifications and create 2 serial to 3.3v logic"}, {"body": "the hole is a slightly too big for the motor's shaft. i thought about hot gluing them together.\n\nlink to picture here\n", "tags": "motor quadcopter multi-rotor", "id": "5248", "title": "how to attach a motor to a blade?"}, {"body": "i have an application that needs a xbee and another module to be turned on and off digitally via a microcontroller.\n\nthe setup is 2 xbee's and an application board is connected to the microcontroller. on poweron i need 1 xbee and the microcontroller to come on and do its routines. after the uc gets the signal from the xbee (wirelessly from a basestation) the board has to turn on the other xbee and application board. and when the operation is over, the xbee and board is to be powered back down. i dont want to put them in sleep or low power state, just power both those devices off. \n\ni was thinking of using a relay. but i cannot find a 3.3v 1a smd equivalent system. i am looking for a smd type of footprint to go on a very compact board. \n\nwhat options do i have?\n\nthe xbee needs around 1a power and the application board 500ma.\n", "tags": "power", "id": "5250", "title": "need a relay alternative"}, {"body": "i've develop a quadrotor (only simulation on my pc using ros) and the feedback controller resumes more or less the following structure:\n\n\n\nwhere you can think that process is the dynamic movement of the quadrotor (motion equations), the inner loop is the attitude controller (it just sets the orientation about all 3 axis) and the outer loop is the position controller that takes care where the quadrotor actually is. why are they separated? because in many papers i found out that the attitude controller (pitch, roll, yaw) need to run at higher frequency then any other controller in the system. the position controller instead needs to run at lower frequency.\n\nthe following picture is a better explanation of my description. don't be scared...it is more simpler than one could think:\n\n\n\nnow i did it as in the paper. but i discovered that my quadrotor was really unstable and i spent days and days trying to correct the gains of the controller without getting a stable system. my intuition said to me that maybe they are running at wrong frequency, so i put a different frequency values for the position controller being sure it is not a multiply of the main frequency (something like 1000hz and 355 hz for example.)\n\nlately i removed the timer in my program (c++) and let the position controller run at the same frequency as the attitude controller just because i run out of ideas and suddenly worked everything nice.\n\nso here is my question. what should i consider when my system has an outer/inner controllers? how to be aware of that?\n\nregards and happy new year!!!\n", "tags": "control quadcopter", "id": "5251", "title": "feedback controller: is there any influence between outer and inner loop when running at different frequencies?"}, {"body": "i'm trying to understand the source code of arduplane. the mavlink message is decoded using a set of  functions, e.g. \n\nwhen i grep recursively for , i could not find where it is defined. i wonder if i'm missing anything.  \n\n\n\nupdate\nhere is the source code of ardupilot, including arduplane.\nhttps://github.com/diydrones/ardupilot\n", "tags": "ardupilot", "id": "5255", "title": "where is the function _mav_return_ defined?"}, {"body": "i've been working on a quad copter for awhile now, recently i've finished the interface for pid tuning and its leading me to question several design decisions. \n\nthe quad uses a raspberrypi as its pilot, the entire loop takes less than 20ms. imu data is gathered, the throttle speeds are calculated, and then finally sent to an arduino(micro) over an spi interface. where they are , to each esc.\n\ncan a quadcopter fly with a loop that slow? 20ms = 50hz? \n", "tags": "arduino quadcopter raspberry-pi", "id": "5259", "title": "is my quadcopter loop fast enough?"}, {"body": "at first happy new 2015!!!\n\ni'm looking for my next simulator development: a tanker is flying at constant speed (350 knots) (no acceleration, no change of altitude or direction). the tanker is approached from behind by a uav which needs to refuel or transfer data through a wire. the uav knows the direction, the speed and relative position from the tanker in order to approach it smoothly. it knows that at about 5 m from the tanker is the contact successful.\nhere a picture i found on internet but it is clear more than thousand words:\n\n\n\nto achieve the task i thought to implement a \"simple\" pid which controls the position and the velocity, but for this i have in my mind two different designs approaches:\n\n\nsolution one: the motion equation of my system provide the position $x,y,z$ and velocity $vx, vy, vz$ of the uav (to simplify things i will consider just $x$ but of course $y,z$ must be eventually considered too). those are feedback with the desired position (5m) and velocity (350 knots) of the tanker. the feedback line is separated for each state and pids are working quite indipendently as in the following picture:\n\n\n\n\nplease note that to simplify things i never considered the acceleration.\n\n\nsolution two: this is the most tricky one and i was yesterday thinking about it all the time. in this case only one state vector is going to be feedback to the desired setpoints. in this case i would feedback only the velocity then integrate it and feed the result into the second pid. maybe the following picture is clearer:\n\n\n\n\nbut here i'm not really sure if the second idea is conceptually wrong or could be affordable. i'm pretty sure that the first one is working and leads to good result, but i was wondering if the second one is affordable or is not recommended for a control design.\n\nregards\n", "tags": "control pid uav", "id": "5260", "title": "the aerial refueling problem: sketch of a feedback controller"}, {"body": "so i'm in the process of building my robot and it has encoders on every wheel measuring speed and position and a compass sensor measuring heading.\n\ni have 3 seperate pid loops at the moment, i can either control the robots speed or i can control the robots position or i can make it follow a heading using a line following type algorithm.\n\nnow i am completely stuck on how to merge these control loops, how do i control the robots heading and its speed. so i can say to it, \"go at 20 degrees at 3m/s\" or \"go at 45degrees for 5 metres then stop\"\n\nreally i would like to be able to control all 3, heading speed and position so i could say \"go at 20 degrees for 10 metres at a speed of 5m/s\" however i have no idea how to merge the loops.\n\nat the end of the day there are 3 inputs (heading, speed and position) and 1 output (pwm to motors) so do i need to use some kind of miso control scheme, if so which one? \n\ni've looked at cascaded control but that only accepts 1 set point where i want to set 3 different set points. maybe some kind of state machine? i'm so stuck!!\n", "tags": "motor sensors control pid", "id": "5261", "title": "how to program parallel pid control loops? so i can give my robot multiple set points to follow"}, {"body": "i have some questions regarding building a gearbox for a motor to create a robot actuator. given my desired motor and loads, i've determined that a gear ratio in the 400-700 range is adequate for my application. \n\nhere is my motor choice for reference: http://www.robotshop.com/ca/en/banebots-rs-550-motor-12v-19300rpm.html\n\nhere are my questions:\n\n1) mounting gears to a motor: if i have a motor with a shaft diameter of 0.12in (3.2mm), what size gear bore should i use, and how do i attach a gear to the shaft in practice? i'm not that mechanically inclined (yet). \n\n2) say i build a gearbox of ratio 625:1, as such: https://www.youtube.com/watch?v=lf-4qvbwy88 i have no idea of how \"durable\" such a set up would be. for my application, i am looking at moving an 8kg mass from 0.6 meters away, coming out to a total torque of 47 newton meters. how can i tell if the gears will break or not? \n\nfor reference, these are the gears i'm looking at (and i'm pretty sure they're the same ones in the video): http://www.vexrobotics.com/276-2169.html \n\n3) assuming those gears above were to fail, what type of gear material would be recommended for my load application of max 47nm? \n\n4) what efficiencies can one expect from gears of different types? i've been assuming 50% conservatively as another answer mentioned. \n\nthank you for any help, and please let me know if anything was unclear.\n", "tags": "motor robotic-arm torque gearing", "id": "5267", "title": "questions on gears for a robot actuator"}, {"body": "a friend and colleague of mine who studies robotics says that bipedal robots present much greater challenges for stability and locomotion than those with more legs.\n\nso why is there so much effort to develop bipedal robots?  are there any practical advantages?\n\nof course, i see the advantage of having arm-like free appendages, but it seems like 4 legs and 2 arms would generally be a better design.\n", "tags": "mobile-robot design legged", "id": "5276", "title": "why make bipedal robots?"}, {"body": "i would like to control the position and velocity of a dc motor like this one (zy my 1016 - 24v - 10a - 200w). the motor comes with a 13 tooth sprocket and the diameter of the shaft is 6mm. it's not possible to apply an encoder to the back of the motor (see picture)\n\nthe angular velocity in the description is 2750 rpm, which encoder do you recommend?  \n", "tags": "motor quadrature-encoder", "id": "5279", "title": "which encoder should i use with a 24v dc motor and 6mm shaft?"}, {"body": "i'm attempting to customise some code for my diy pentacopter frame.\n\nto that end, i've modified the some existing code, and saved it under ap_motorpenta.cpp and ap_motorspenta.h . i'm currently trying to upload the code onto my flight controller, but am currently unable to do so due to the following problems.\n\nproblems\n\nunable to upload to my apm 2.6    ( #1)\nunable to select my pentacopter frame.    (#2)\n\nproblem (#1)\n\ni've saved my customised files in the ap_motors library, and have compiled the arducopter 3.2 code in ardupilot-arduino-1.0.3-gcc-4.8.2-windows , after which i upload it using mission planner. however, when i am uploading the hex file, i get the following error \n\n\n  \"uploaded succeeded, but verify failed : exp e2 got 60 at 245760\"\n\n\nhowever, when i try uploading it directly from the modified arduino ide, i get a series of warnings , followed by the messages\n\n\n  avrdude:verification error, first mismatch at byte 0x3c000 0x60 !=\n  0xe2 avrdude: verification error; content mismatch\n\n\nfollowed by the message \n\n\n  \" avrdude done.thank you. \"\n\n\ndoes this mean that the uploading of the firmware to my flight controller is successfull? also, is there any difference between uploading via mission planner and the modified arduino ide?\n\nproblem #2\n\nin the mission planner, originally there is the option to choose one of several frames, (i.e quad/hexaocto) etc. after uploading my firmware, how would i go about selecting my penta frame for use?also is there any further thing that i would have to do?\n\napologies in advance if the questions are rather inane, as i have little programming experience to speak of.\n\ni would really appreciate any help i can get.\n\nthanks in advance !\n", "tags": "arduino quadcopter microcontroller ardupilot", "id": "5282", "title": "uploading edited code for arducopter"}, {"body": "i am planning to develop a monocular visual odometry system. is there any indoor dataset available to the public, along with the ground truth, on which i can test my approach?\n\nnote: i am already aware of the kitti vision benchmark suite which provides stereo data for outdoor vehicles.\n\nif anyone has access to the datasets used in the following paper [svo14], it would be even more great: http://rpg.ifi.uzh.ch/docs/icra14_forster.pdf\n", "tags": "computer-vision odometry", "id": "5283", "title": "public dataset for monocular visual odometry"}, {"body": "i am trying to design a robot to lift tote-crates and transport them around in a localized area. i want to be able to carry 3 tote-creates at a time.  this robot needs to be able to pickup the creates. i only want the robot to carry three at a time so keep is small and mobile. i was thinking of a design with a central lift that could carry the crates. what would you suggest as a simple ingenious way to create this robot? \n", "tags": "design wheeled-robot mechanism", "id": "5292", "title": "lifting robot to lift small crates"}, {"body": "i'm absolutely fascinated by the notion of a driverless car.  i know there is a lot involved with it and there are many different approaches to the problem.\n\nto narrow the scope of this question to something reasonable for the se network, i'm curious to know if there is a common sequence of subproblems that every driverless car needs to solve at each timestep to make an autonomous car possible for real life, point to point transportation possible.  i imagine that once the starting point and target destination on a given map are set, a self driving follows an algorithm that loops through certain operations to solve certain problems along the way.  i'm more interested in knowing what those problems are specifically at a high level, rather than detailed algorithms to solve them.  do all self driving cars solve the same subproblems along the way?\n", "tags": "automatic", "id": "5293", "title": "how do self driving cars really work?"}, {"body": "power block designing noob here.\n\ni have a beaglebone 2x xbeepro(s) and another 500mah device connected to the board i am building a pcb around.\n\ni need some advice on weather to use linear voltage regulation vs switching mode regulation. \n\nsecondly if i am using linear voltage regulation setup do i need multiple regulators for the different devices? \n\nmy plan is to use a 2s 1000mah battery -> fuse -> 2x 1.5a lm1084's in parallel output feed to the beaglebone and a lm3940 for both the xbees. or its better to have each xbee on its on lm3940 drawing power from a seperate lm1084?\n\nlinear regulators tend to get hot on full load, hows the performance of switching mode regulators ?\n", "tags": "power battery", "id": "5300", "title": "linear or switching power supply for a embedded project"}, {"body": "i've seen lots of examples on how to communicate from arduino to the computer, but the few that talked about computer to arduino communications were very hard to understand.\n\nmy question is: what code can i use to control my arduino uno with my keyboard\n\nif it helps, i'm trying to set up a wasd steering behavior.\n", "tags": "arduino programming-languages", "id": "5302", "title": "code control your arduino with keyboard"}, {"body": "i want to buy a 3d camera with depth sensor. can anyone can give me advice on which one will be the best?\n\ni have a experience with kinect, but configuring kinect for linux is painful and also kinect generate sometimes a big latency.\n\ni am looking for low latency, a good depth sensor and a good api for linux. i am thinking about a currera r.\n\nupdate:\n\ni found a quite cheap and good camera from ximea.\n\nthay have a very nice support for libraries like opencv/matlab etc, so for newbie like it is perfect.\n", "tags": "sensors beginner", "id": "5307", "title": "3d camera for beginner"}, {"body": "i'm working on a project requiring hd (stereo) video processing. most of high resolution (5mp+) sensors use mipi-csi interface. \n\ni managed to get a board with an exynos5 soc. the soc itself has 2 mipi-csi2 interfaces, the problem is that the pins to those interfaces are not exposed and it's (almost) impossible to reach them. so i decided to use the usb3.0 buses.\n\nthe problem is when i get to significant bandwidth (~5.36 gibibits/s per sensor), i don't think usb3.0 will work out.  but this could be solved with a compressed stream (via a coprocessor)\n\ni was thinking that cypress' cyusb306x chip was a good candidate for the job, but one of the problems is that i can't do bga soldering by hand nor have been able to find a bga soldering service in switzerland.\n\nany ideas on other interfaces i could implement or other coprocessors with mipi-csi2 interface?\n\njust a final remark, space and weight are important as this is supposed to be mounted on a drone.\n", "tags": "cameras usb stereo-vision", "id": "5310", "title": "interfacing high-resolution image sensors with arm board"}, {"body": "i am planning to use 2.4ghz xbeepro 63mw devices for a project that requires a coverage area of around 1.5-2km. \n\nwhen i go to select an antenna there are various options like circular,virtical, horizontal polarized etc.\n\nwhich antenna would give a coverage for a field? i cant have it directional (one point to another point). by devices will be moving around on a field. \n\nwhat type of polarization is recommended for this kind of a setup? my base xbee will be on a elevation of around 40m from the ground so i have a clear line of sight for all the moving modules. \n\nthere are going to be around 20-30moving modules streaming data at around 2-5readings per second. \n\na +12dbi antenna should suffice the application? and what about polarization? \n", "tags": "battery wireless", "id": "6310", "title": "what type of antennas to use for xbeepro 2.4ghz"}, {"body": "i'm building a quadcopter for my final year project. i have a set of equations describing attitude and altitude but they involve $i_{xx}$, $i_{yy}$ and $i_{zz}$. none of the papers i have read describe how these are calculated. they simply choose it before their simulation. can anyone help?\n", "tags": "quadcopter kinematics", "id": "6313", "title": "how do you calculate the moment of inertia of a quadcopter?"}, {"body": "i'm trying to get an arduino to talk with a beaglebone black. \ni have followed this tutorial for getting ttyo4 open on the bbb and used the following command on to set the serial line correctly: \n\nwiring is set up according to this tutorial. \n    stty -f /dev/ttyo4 cs8 9600 ignbrk -brkint -imaxbel -opost -onlcr -isig -icanon -iexten -echo -echoe -echok -echoctl -echoke noflsh -ixon -crtscts\n\nnext data is sent using the following method: \n\n\n\nthe arduino uses the followingvoid loop(){\n code to check for serial communication:\n\n\n\nhowever it seems no message is received. it does not give any error either. \n\nas an alternative i have also tried a variant of the code suggested in the wiring tutorial resulting in the following code:\n\n\n\ncalled with  this printed s123 but the arduino remained silent. \nedit i have now also tried to exactly follow the wiring tutorial so that gave me the following sketch: \n\n\n\nand on the bbb we turn on the echo script with\n\n\n\nthe effect remains that there is no error but also no data delivery. \n", "tags": "arduino control serial communication beagle-bone", "id": "6314", "title": "communicating between a beaglebone black and an arduino using ttyo4"}, {"body": "i am building an estimator that solves for the camera pose relative to a reference frame which contains a known set of features and edges. currently, the system works with an unscented kalman filter with four known points (red leds) in the reference frame. i am now hoping to improve robustness by adding edges to the model as well as robust features. i would like to add additional points that are uncovered by some opencv feature finding function (fast,cornerharris,...).\n\nso far i found the paper \"fusing points and lines for high performance tracking\" and \"robust extended kalman filtering for camera pose tracking using 2d to 3d lines correspondences\" which seem to detail how to fuse edge and feature matching for pose estimation.\n\nis there a strategy to populate the known set of edges and features when it is impractical to measure them with a ruler/tape measure? my first thought is to start with a small known set of features, my red leds, then run some slam algorithm and keep all features/edges that have some minimum certainty.\n\nthanks a bunch!\n\ni have misunderstood the ransac algorithm. this is not appropriate for my application. \n\nfor those interested, i am hoping to use a similar approach to the one presented in the following paper.\n\nyoungrock yoon, akio kosaka, jae byung park and avinash c. kak. \"a new approach to the use of edge extremities for model-based object tracking.\" international conference on robotics and automation, 2005.\n", "tags": "kalman-filter computer-vision pose", "id": "6315", "title": "pose estimation, how to populate set of known edges and points?"}, {"body": "after working for a long time on my arduino due, i needed a better and more powerful prototyping platform for my future projects. for which, i have placed an order for nvidia jetson tegra k1 board which runs on linux and supports cuda based development. being a newbie to linux, i have no idea where to start from and what to do for getting started with code execution on the jetson board. please suggest the initial steps required and from where can i get familiar to linux environment...\n\nthank you\n", "tags": "linux", "id": "6322", "title": "getting started with jetson tegra k1"}, {"body": "i'm currently thinking of extending the battery life of my quad by powering each motor and esc individually. i  will be using 1 dedicated battery for each motor, and 1 dedicated battery for the flight controller itself, bringing the total to 5 batteries for the entire quad.\n\nmy thinking is that by powering each motor with a dedicated battery, given a power draw/consumption, the flight-time of my quad will be increased by 4x as each motor will have 4x the capacity to draw from. putting the problem of weight aside, would this be a feasible idea?\n\nalso, i am currently using just 1 battery to power all motors, and as such, i only have to plug in the single battery and i can calibrate my escs. how would i calibrate my escs if i am using dedicated batteries for my apm 2.6 and each motors?would i be able to get away with powering my apm using the bec on my escs?\n", "tags": "quadcopter power battery", "id": "6323", "title": "powering a multirotor with dedicated batteries for each motor"}, {"body": "i am confused by what precisely the term \"indirect kalman filter\" or \"error-state kalman filter\" means.\n\nthe most plausible definition i found is in maybeck's book [1]:\n\n\n  as the name indicates, in the total state space (direct) formulation, total states\n  such as vehicle position and velocity are among the state variables in the filter,\n  and the measurements are ins accelerometer outputs and external source\n  signals. in the error state space (indirect) formulation, the errors in the ins-\n  indicated position and velocity are among the estimated variables, and each\n  measurement presented to the filter is the difference between ins and external\n  source data.\n\n\n20 years later roumeliotis et al. in [2] write:\n\n\n  the cumbersome modeling of the specific vehicle and its interaction with a dynamic environment is avoided by selecting gyro modeling instead. the gyro signal appears in the system (instead of the measurement) equations and thus the formulation of the problem requires an indirect (error-state) kalman filter approach.\n\n\ni cannot understand the bold part, since lefferts et al. in [3] write much earlier:\n\n\n  for autonomous spacecraft the use of inertial reference units as a model\n  replacement permits the circumvention of these problems.\n\n\nand then proceed to show different variants of ekfs using gyro modeling that are clearly direct kalman filters according to maybeck's definition: the state only consists of the attitude quaternion and gyro bias, not error states. in fact, there is no seperate ins whose error to estimate with an error-state kalman filter. \n\nso my questions are:\n\n\nis there a different, maybe newer definition of indirect (error-state) kalman filters i am not aware of?\nhow are gyro modeling as opposed to using a proper dynamic model on the one hand and the decision whether to use a direct or indirect kalman filter on the other hand related? i was under the impression that both are independent decisions.\n\n\n[1] maybeck, peter s. stochastic models, estimation, and control. vol. 1. academic press, 1979.\n\n[2] roumeliotis, stergios i., gaurav s. sukhatme, and george a. bekey. \"circumventing dynamic modeling: evaluation of the error-state kalman filter applied to mobile robot localization.\" robotics and automation, 1999. proceedings. 1999 ieee international conference on. vol. 2. ieee, 1999.\n\n[3] lefferts, ern j., f. landis markley, and malcolm d. shuster. \"kalman filtering for spacecraft attitude estimation.\" journal of guidance, control, and dynamics 5.5 (1982): 417-429.\n", "tags": "localization kalman-filter navigation errors", "id": "6325", "title": "ambiguous definition of error-state (indirect) kalman filter"}, {"body": "i am trying to build a hexapod with camera interfacing using a beaglebone black for college project. i'm not sure what power supply to give so it can power up to bot, having in mind that it should be portable (mobile) and it should power about 18 servo motors along with the camera, wifi and the processor. your help is needed very badly as i'm nearing the deadline for the project.\n", "tags": "power servos beagle-bone hexapod", "id": "6327", "title": "beaglebone black power supply for hexapod"}, {"body": "i'm trying to build a hexapod with beaglebone in the linux environment (im thinking of using ubuntu). what is the best language to use for coding purpose to make robot controls, camera and wifi integration etc.\n", "tags": "control programming-languages beagle-bone linux", "id": "6328", "title": "language to code beaglebone"}, {"body": "how would one go about passing power through a motor?\n\nlet's say we have some basic robot which has a motor that slowly spins a limb, on each end of that limb, there is a motor which again spins a limb. because the first motor is always going to be spinning, any wires would twist and eventually break, so a wired approach wouldn't work. the same goes for the subsequent motors.\n\ni know that dc motors use brushes to get past this, but how is this generally solved in engineering/robotics? this must be a problem that has come up before, and there must be a solution to it.\n\nany ideas? :)\n", "tags": "motor stepper-motor power", "id": "6331", "title": "passing power through a motor"}, {"body": "reading some papers about visual odometry, many use inverse depth. is it only the mathematical inverse of the depth (meaning 1/d) or does it represent something else. and what are the advantages of using it?\n", "tags": "slam computer-vision odometry", "id": "6334", "title": "what is inverse depth (in odometry) and why would i use it?"}, {"body": "i'm not sure if this is the correct forum for this question about automatic control, but i'll try to post it anyway.\n\nso, i've just started to learn about control systems and i have some troubles understanding bode plots. my textbook is really unstructured and the information i find on the internet always seem a little bit too advanced for a beginner like me to grasp, so i hope you can help me to understand this. \n\ni would like to know what the information we find in the bode plot can tell us about how the corresponding step responses behave. i know that a low phase margin will give an oscillatory step response and that the crossover frequency decide the rise time. but how can we see if the step response has a statical error in the bode plot and what does the phase graph of the bode plot actually mean?\n", "tags": "control automatic", "id": "6339", "title": "understanding the bode plot"}, {"body": "i plan to use the lt1157 in my application pcb to act as a switch control from a micro controller side to control the on/off state of 2 module boards which will be connected in the pcb. \n\n\n1st load is 5v 1a.\n2nd load is 3.3v 500ma.\n\n\nthe lt1157 will get a 5v input at the vs terminal. \n\ndoes anyone know how much voltage is required to be used at the in1 and in2 pins? the datasheet doesn't say how much voltage can be used here. i am guessing it will be 5v, but can it do logic level with 3.3v? my microcontroller board gives an output of 3.3v and not 5v so i'll have to make a logic level converter before feeding the pins in1 and in2 if it's not 3.3v tolerant. \n\nplease confirm, if anyone has used this ic before. \n", "tags": "circuit", "id": "6341", "title": "lt1157 logic level question"}, {"body": "i'm building a robot which is actually a rotating ball. all my circuitry will be inside this ball. i'm using a raspberry pi as the brains. apart from raspberry pi, i've an h-bridge ic (l298n), a 6-axis accelerometer + gyroscope (mpu6050), and probably some more additional digital components. these will work with a 5v or 3.3v supply. another set of components are electromechanical devices like a 9kg torque servo and 2 1000rpm dc motors.\n\nhere are my questions:\n\n\neverything will work on battery. i can get a 3.3v and 5v supply from a 9v battery using l1117-3.3v and 7805 regulators respectively. i know that it's not at all reliable to share the power source of the control circuitry with high load devices like motors and servos. should i have a dedicated separate supplies for electromechanical components and the control circuitry?\nservo will run on 6v supply and motor will run on a 12v supply. how should i go about this one? again, separate batteries for servo and motors?\ncan of this work on a single high capacity battery, somewhat like 10000mah?\n\n\nhere are some of my calculations:\n\nservo current (6v): at no load: ~450ma, at around 6kg load: ~800ma\n\nmotor current (12v): at no load: ~500ma, at around 6kg load: ~950ma\n\nraspberrypi and other digital circuitry (5v + 3.3v): ~600ma (that includes an xbee)\n\nthus, the overall current at a 6kg load (with two motors) comes around ~3.3a\n\nit would be really awesome if this thing gets done with a maximum of 2 batteries. else, it may get messy while placing the batteries inside the ball. space is limited!\n", "tags": "motor power servos battery", "id": "6345", "title": "battery system for a robot with a raspberrypi or microcontroller"}, {"body": "i'm studying for a test in automatic control and i have some troubles understanding sensitivity functions and complementary sensitivity functions. \n\nthere's one assignment from an old exam that sais\n\"someone suggests that you should reduce perturbations and measurement noise simultaneously. explain why this is not possible.\"\n\nthe correct answer sais:\n\"since the sensitivity and complementary sensitivity transfer functions add up to 1, i.e. $s+t=1$, one cannot improve both the output disturbance and measurement error suppression at the same time.\"\n\ni don't really understand this answer and my textbook is not to much help either, so i would appreciate alot if someone could explain how they got to this answer? also, is the sensitivity function always representing the perturbations in the system and the complementary sensitivity function the measurement noise? my textbook seem to imply this, but i'm really not sure if this is always true.\n", "tags": "control noise automatic", "id": "6348", "title": "what does the sensitivity function mean?"}, {"body": "our goal is to drive an autonomous robot with a differential locomotion system using two identical electric motors and an arduino uno (1 for each wheel). from our understanding, over time the motors can lose accuracy and may result in one motor rotating more than the other.\nis there a way to account for possible error in the speeds of the motors so that the robot can end up in a very precise location?\n\nour thoughts were to have an indicator which would allow us to count the number of rotations of each motor and compensate if a noticeable difference began to appear.\n", "tags": "arduino mobile-robot two-wheeled", "id": "6351", "title": "accounting for error in multiple electric motors"}, {"body": "background:\ni am new to pid, for my first pid project i am using a simple p-loop and 300 degree linear potentiometers for position feedback. i am using the roboclaw 2x60a motor controller.  the motor controller has 64 speeds between.  sometimes the potentiometers can vary as much as +-4 degrees when not in motion.  i am using an arduino mega with a 10bit adc to control the motors.  \n\nmy question:\nhow can i filter or reduce the variance in the potentiometers?  in addition, it takes a certain amount of time for the motors to react to the command, and it seems to throw off the p loop.  how do i account for the latency, in my program?\n\nexample:\nfor this example the p loop was run every 33-36 milliseconds.\ni will tell the motor to go to 250 deg/sec, and it will go to 275 deg/sec, the p loop then reacts by lowering the value sent to the motor however the speed then increase to 400 deg/sec and then the p loop lowers the value again, then the speed will drop to 34 deg/sec.\n\nthanks so much for any help,\njoel\n", "tags": "arduino motor pid software avr", "id": "6355", "title": "dealing with position inaccuracy and latency in pid loop"}, {"body": "i am completely new to this site and robotics, but i have experience in programming, and programming microcontrollers.  \n\ni would like to create a grid of \"pixels\", where each \"pixel\" is a metal or wooden dowel that is programmed to push in and out, like a piston.  \n\ni'm imagining a lot of pixels, maybe 40x40, where each could be quite small in diameter (1/4\").  the arduino would have control over the linear movement - up and down - of each pixel.\n\ncould anyone point me in the right direction for accomplishing this?\n", "tags": "arduino mechanism servos servomotor", "id": "6356", "title": "how do i achieve this? grid of dowels powered by piston-like movement"}, {"body": "has anyone used the xbee wifi modules? done a range check on them?\n\nwith my laptop i get a range of around 400m on industrial level accesspoints on a football field, well how good are these devices ? if i get a sma connector version and use a higher gain antenna am i looking at ranges from 250-500m ? (talking 18-22dbi gains here).\n", "tags": "wifi", "id": "6366", "title": "how can i improve the range of an xbee s6b?"}, {"body": "i have data from an accelerometer that measures x,y,z acceleration and data from a gyroscope that measure pitch, roll and yaw. how would i combine this data to find robot location and orientation in 2d or 3d space? \n", "tags": "localization accelerometer algorithm gyroscope", "id": "6367", "title": "how to combine an accelerometer and a gyroscope to find robot location and orientation in 2d/3d space"}, {"body": "i am starting with a project using arducopter. i am a person familiar with arduino, but seeing the arducopter for the first time. commands codes and everything is completly different compared to normal arduino programming. i am not getting any help or commandlist for specific purposes in arducopter. any body can help me in leading to any links which can help me out..\n", "tags": "arduino ardupilot", "id": "6371", "title": "basic programming in arducopter"}, {"body": "i have the following code: \n\n\n\n(code that isn't interesting has been removed)\n\nrunning this causes the following output to appear: \n\n\n\nhowever if i have the following code on during launch:\n\n\n\nthen it does not show me an initial message being sent.\n\nchanging\n\n\n\ninto \n\n\n\ndoes appear to work but it then sends a message every cycle rather then just the one at the start. \n\ndoes anybody know what is wrong here? \n", "tags": "ros c++", "id": "6374", "title": "ros send message on startup doesn't seem to work"}, {"body": "i'm in the early stages of working with a simple robot arm, and learning about the jacobian and inverse kinematics.\n\nfrom my understanding, the jacobian can be used to determine the linear and angular velocity of the end effector, given the angular velocity of all joints in the arm. can it also be used to determine the cartesian position of the end effector, given the angles and/or positions of the joints?\n\nfurthermore, suppose that i want to determine the required angular velocities of the joints, in order to bring about a desired linear velocity of the end effector. can this be done by simply inverting the jacobian and plugging in the desired parameters?\n", "tags": "kinematics inverse-kinematics forward-kinematics manipulator jacobian", "id": "6382", "title": "can a jacobian be used to determine required joint angles for end effector velocity/position?"}, {"body": "i've made many pcbs at home but still there are some mistakes. i tried ironing, drawing methods but it doesn't work very well. i use eagle cad for design pcbs. please help me. \n", "tags": "design", "id": "6383", "title": "pcb making at home"}, {"body": "for a high school project i will be building a robot that will draw an image on a whiteboard for you based on what instructions you give. to accomplish this a motor will move the pen on each axis similar to how a 3rd printer moves but without the z axis. as far as code goes i'm fine but i was wondering if anyone could give me an insight on how to go about building the robot (i.e. what motors, best system for moving on axises etc) all help is appreciated thanks\n", "tags": "motor mechanism", "id": "6386", "title": "robotics advice needed"}, {"body": "i've been searching the internet for an answer to this question, but i haven't come across anything that will help me. basically, we have a meka humanoid robot in our lab, which has a shell head in which a pointgrey usb 3.0 camera is embedded. i use the pointgrey_camera_driver  for obtaining the images from the camera. the head has 2 degrees of freedom (up/down, left/right). i am intending to use this camera with the ar_pose package to detect and track ar tags on objects. i understand that camera's must be calibrated for effective use (forgive me, i don't know much about camera) which i do with the camera_calibration package.\n\nmy question is: since this camera is \"mobile\" meaning since the head can move so does the camera, how would i go about calibrating it? currently, i have the head fixed at a position and i've calibrated the camera in that position and got the parameters in the yaml file which i can load for rectification. in particular, if the head moves does the calibration file that i obtained in the previous position become invalid? if so, as asked before, how would i calibrate this camera for all of its possible field's of view (which can be obtained by moving)?\n\nthis camera has different video modes and in the mode i'm using i can get a frame rate of 21hz (i.e. after driver is launched i get 21hz for rostopic hz /camera/image_raw). however, when i rectify the image using image_proc, i get a frame rate of only about 3hz on rostopic hz /camera/image_rect_color. is this normal? is there a way for me to increase this frame rate?\n\nplease let me know if any more information is required.\nthanks for your help!\n", "tags": "ros cameras calibration", "id": "6395", "title": "mobile camera calibration and rectification frame rate"}, {"body": "background:\n\ni am using an arduino mega connected to a roboclaw 2x60a motor driver.  i asked this question about the system, but i have since narrowed the scope of the problem.  i tried adding a bunch of different size capacitor between the 5v and gnd, when the roboclaw is switched off then a 470 micro farad capacitor seems to eliminate all noise but when i turn on the roboclaw no capacitance valued i tried, (4.7,10,100,220,320,470,540,690,1000,1100)microfarads seems to eliminate any noise.  i even tried hooking up a 12v battery with a 5v regulator to the logic battery on the roboclaw and connecting it to the ground on the arduino.  then i tried using a separate battery for the pots and connecting the aref to the +5v on the battery.\nno matter what i try when the roboclaw is on the potentiometer value will vary as much as +-6 degrees.  i found the degrees using:\n\nmap(analogread(a0),0,1023,0,300) \n\nin addition i took a bunch of data and graphed it and found that if i took 25 instantaneous data points and averaged them together it would help significantly reduce the variance.  i chose 25 because it take 2.9 ms, 100 worked really well but it took 11 ms.  to help explain the averageing of analog read, here is my code:\n\nunsigned int num = 0;\n\nfor (int i = 0; i&lt;25; i++){\n\n\n\n}\n\npotreading = num/25;\n\nmy question:\n\nwhat is my next step in eliminating this noise?  is there a formula i can use to find a better capacitance value? should i try putting capacitors on each potentiometer between 5v and gnd?  any other ic i should try to help with this?  on my previous question someone mentioned optocouplers, what size would work best and where in the circuit do they go?  is there code i can write to help eliminate the size of the variance beyond what i have written?\n\nthanks so much for any help,\njoel\n", "tags": "arduino motor electronics noise driver", "id": "6398", "title": "eliminating electrical noise from my motor driver"}, {"body": "i would like to know the simple difference between kinematic, dynamic and differential constraints in robotic motion planning.\n", "tags": "motion-planning", "id": "6405", "title": "difference between kinematic, dynamic and differential constraints"}, {"body": "i would like to know the difference between state space and control space in relation to motion planning. i would like a simpler explanation.\n", "tags": "motion-planning", "id": "6406", "title": "state space and control space"}, {"body": "i'm fairly new to 3d printing. i'm considering motherboards i might use to control my printer. i'm hoping to find a board that i can easily control using either:\n\n\nreplicatorg\nmattercontrol\n\n\ni like these programs because they seem reasonably current, mature and straight-forward for beginners.\n\nmy question is can i control a rambo v1.2 board from either of these programs? these programs don't include explicit support for the rambo as far as i can see, but maybe i'm missing how printing software works at this point?\n\n\n\nwhat is a rambo?\n\nthe rambo v1.2 board is a creative-commons/open source design. it integrates an arduino, stepper-motor drivers, heater &amp; fan controls, power management and more.\n\nan example implementation looks like this:\n\n\nfor more background info on what a rambo board is, you may read about it on the reprap community wiki.\n", "tags": "3d-printing reprap", "id": "6408", "title": "can replicatorg or mattercontrol drive a reprap rambo motherboard?"}, {"body": "atlas gets an upgrade - the new video of the atlas robot is out so i'm curious about the ide with which they are coding this thing.\n", "tags": "dynamics", "id": "6409", "title": "the ide using for programming the atlas robots"}, {"body": "i  have a beagleboneblack and would like to use it to control a servo for my robot. i'm mostly programming in ros and as such am looking preferably for a c++ solution. is there an easy way of controlling a servo on a bbb running ubuntu 14.04 on the kernal 3.8? most tutorials i have tried referred to files i did not have so i'm unsure.\n", "tags": "ros servos servomotor beagle-bone c++", "id": "6412", "title": "how do i control a servo using a beaglebone black running ubuntu"}, {"body": "as far as i can tell, both lqr and pid controllers can both be applied to the cart-pole (inverted pendulum) problem.  what are the pros/cons to using one controller over the other for this particular problem?  are there any reasons/situations where i should prefer one over the other for this problem?\n", "tags": "control pid", "id": "6417", "title": "comparing lqr and pid controllers for inverted pendulum problem"}, {"body": "i am hoping someone might be able to nudge me in the right direction (apologies for the long post but wanted to get all the information i have gained so far down.\n\nbasically i am after a solution to record the path my vessel took under water for later analysis\u2026like a bread crumb trail.\n\nrequirements:\n\nideally have a range of at least 30meters however if there were no other options i would accept down to 10meters.\n\nworking fresh and salt water.\n\nthe vessel is (25cm x 8cm) so size and power consumption are a factors.\n\nit would be traveling roughly parallel to the sea bed at variable distances from the sea bed (range of 0-30 meters)\n\ndoes not need to be super accurate, anything less than 5 meters would be fine.\n\nmeasurement speed range of 0 \u2013 4 mph.\n\nmeasure direction the object was moving in (i.e. forwards, sideways, backwards)\u2026i am planning to use a compass to ascertain n, s, e, w heading.\n\noptions i have discounted:\n\n\naccelerometers:\n\n\nthis was my initial thinking but in doing some reading it seems they are not suited to my needs (unless you spend loads of money, and then the solution would end up being too heavy anyway).\n\n\noptical flow:\n\n\nlooks too new (from a consumer perspective) / complicated. i don\u2019t know what its range would be like. also requires additional sonar sensor.\n\ncurrent favorites:\n\n\nsonar:\nhttp://www.alibaba.com/product-detail/1mhz-waterproof-transducer-underwater-ultrasonic-sensor_1911722468.html\n\n\nsimplest use is distance from object, however can use doppler effect to analyse speed of a moving object.\n\n40m range, nice!\n\npresumptions:\n\nif i fired this at an angle to the seabed i could deduce the speed the floor was \u2018moving\u2019 below which would give me the speed of my vessel?\n\ni am also presuming that i could interpret direction of movement from the data?\n\ni presume that the sensor would need to be aimed at an angle of around 45 degrees down to the seabed?\n\n\nlaser rangefinder:\n\n\nalthough it works differently to the sonar the premise of use looks the same, and thus i have the same queries with this as i do with the sonar above.\n\npresume if i mounted the sensor behind high quality glass (to waterproof it) then performance would not be impacted that much.\n\nthis is a lot more costly so if it does not give me any advantage over sonar i guess there is no point.\n\n\nwater flow meter:\nhttp://www.robotshop.com/en/adafruit-water-flow-sensor.html\n\n\nsuper low cost and simple compared with the other options, i would potentially use a funnel to increase water pressure if it needs more sensitivity at low speed.\n\nwould then just need to calibrate the pulses from the sensor to a speed reading.\n\nsignificant limitations of this is it would be registering a speed of 0 if the vessel was simply drifting with the current\u2026.its speed over the seabed that i am interested in.\n\ncurrent favorite option is sonar (with the option of using water flow meter as second data source)\u2026however are my sonar presumptions correct, have i missed anything?\n\nany better ideas?\n", "tags": "sonar laser underwater rangefinder acoustic-rangefinder", "id": "6418", "title": "tracking landspeed underwater"}, {"body": "i am trying to implement a particle filter in matlab to filter a robot's movement in 2d but i'm stuck at the weight function. my robot is detected by a camera via two points, so a single measure is a quadruple (, , , ) and states are the usual (,,) to detect its pose in 2d. as far as my understanding goes i should assign a weight to each particle based on its likelihood to be in that particle position with regards to the current measurement.\n\ni also have the measure function to calculate an expected measurement for a particle, so basically i have, for each instant, the actual measurement and the measurement that a single particle would have generated if it were at the actual state. \n\nassuming all noises are gaussian, how can i implement the weight function? i kind of noticed the  function in matlab, but i can't actually find a way to apply it to my problem.\n", "tags": "mobile-robot wheeled-robot particle-filter", "id": "6423", "title": "particle filter weight function"}, {"body": "is there a sensor that will produce a sinusoidal signal phase locked to a high rpm (7000 rpm) shaft? i am attempting to build a coaxial helicopter based on the architecture described in this paper which requires increasing and decreasing drive torque once per revolution, and i would like to do this modulation in hardware.\n", "tags": "sensors", "id": "6428", "title": "sensor that will produce a sinusoid phase locked to a high rpm shaft"}, {"body": "i have stumbled upon an equation (http://i.stack.imgur.com/hv64e.png), where the probability of an occupancy grid map cell is calculated. my teacher insists that it's possible to approximate the algorithmic complexity of this long equation, but i'm not so sure.\n\nthe description of the factors used in this equation are described here on page 11 (item 26). with keeping in mind that this calculates occupancy probabilities of a 2 dimensional array from sensor measurements, is it really possible to approximate the algorithmic complexity of actually calculating occupancy with this equation in bigo by just taking a look at it and not delving much deeper into the details? \n", "tags": "algorithm probability occupancygrid", "id": "6429", "title": "grid mapping probability calculation algorithmic complexity"}, {"body": "this is not a robotics question, but this stack exchange is the closest i could find to mechanical engineering. please refer me to a better place to ask this, if one exists. hopefully someone might just know this.\n\n\n\ni got a pull-back car for my boy at mcdonalds, and it has two gears. it starts slow, then speeds up after about two seconds. it's impressive to me, especially given the inherent cheapness of toys sold by mcdonalds. it feels solidly built as well.\n\ni couldn't find anything related to this concept. the wiki on pullback motors does not include any information on multiple gears.\n\nany ideas on how this works? \n", "tags": "mechanism", "id": "6430", "title": "how does a two-gear pull-back car toy work?"}, {"body": "hello i am building a differential drive robot which is equipped with quadrature encoders on both of the motors. my aim is to be able to predict the heading (yaw angle) of the robot using a kalman filter. i am using an mpu 9150 imu. as of now m just interested in yaw angle and not the roll/pitch. as i understand, i will be needing the z-axis of gyro to be fused with the magnetometer data in order to properly estimate the heading angle. my problem is how do i implement the bias and covariance required for the kalman filter to work. gyroscope would be my process and magnetometer data would be my update step yeah?. from the datasheet i have found the angular random walk of my gyroscope to be 0.3 degrees/second for 10 hz motion bandwidth and a constant bias of 20 degrees/second at room temperature. if i am not mistaken i should include the bias in my state prediction equation?. also how do i get the covariance matrix q. \n", "tags": "kalman-filter imu", "id": "6434", "title": "differential robot yaw estimation using kalman filter"}, {"body": "a time has come for my robot to get some more permanent computer than my laptop balanced on top of it.\ni have selected a mini itx board that can be powered directly from battery and some components that go with it including a wifi card and now i'm thinking about the antenna i will need.\n\nconstraints i have identified so far:\n\n\nthe robot's body is a closed aluminium box, so i think this rules out keeping the antenna inside.\nthe robot is intended to work outdoors, so it needs to be waterproof.\nvibrations might be an issue.\n\n\nand the questions:\n\n\nwhat parameters should i watch when selecting an antenna?\nis it ok to use indoor stick antenna and seal the mounting point with hot glue?\ndoes it change anything if the antenna will be sticking out of largish sheet of alluminium?\nthe robot will also have gps, is it possible that the two will interact badly under some circumstances?\n\n", "tags": "mobile-robot wifi", "id": "6436", "title": "choosing a wifi antenna for outdoor robot"}, {"body": "i'm building a robot and powering it with a raspberry pi. i am looking at this battery pack, but i am flexible with which one to use.\n\nmy problem is that i need to be able to charge the robot while it is still on, and apparently that is not good for a single battery pack to be charging while being used (they seemed to say so in the video). am i wrong? otherwise, how could i go about charging the robot while keeping the raspberry pi running?\n\nedit: this is my first robot (other than my lego nxt kit), so i don't have any prior experience with robot batteries.\n", "tags": "mobile-robot battery", "id": "6437", "title": "how to provide power to a robot/raspberry pi?"}, {"body": "i'm developing my fligth controller board on tiva launchpad for quadcoper and while calibrating pid i discovered an unexpected behaviour: sometimes quadcopter seems to experience random angle errors. while trying to investigate it, i've figured out that my code if fairly trying to compensate tham, as soon as they appear - but do not cause them. even more - i've discovered that such behaviour appears only when two (or more) motors are adjusted, while one motor system shows pretty good stabilisation.\n\nhere is code for pmw output for different motors:\n\n\n\nand here is recorded angles for system with one motor and two motors:\n\n\nto be sure that it's not the algorithm problem, while recording this angles only integral part of pid was non-zero, so angles were not even stabilised.\n\nmy question is - could esc noise each other (in my quad they are quite close to each other - just few sentimeters away) to cause such behaviour?\nthanks a lot!\n", "tags": "quadcopter pid esc", "id": "6442", "title": "quadcopter multiple esc angles glitch"}, {"body": "my set up consists of a brushed motor (ex cordless drill type) connected to a motor controller which is in turn connected to a lipo battery and an r/c receiver. all my cables are fitted with xt 60 connectors except for the cable that goes to the receiver which is a 3 wire pin (usual white, red and black).\n\nthe above set up is one of a pair which i am using in my battle robot. the motors are connected to drive wheels, left and right respectively. the problem is the motors are turning in opposing directions.\n\nfor some reason i neglected to switch the polarity of the wires of one  motor at the time i attached the xt 60 connectors and i really am not looking forward to re-soldering.  \n\nso my question is whether there is any fast way of reversing the direction of rotation without soldering?  for instance can the r/c transmitter (a turnigy 9x without any modding)be programmed to switch up for down (hence forward for reverse)? \n\nor can i maybe switch the pin connector going into the receiver (i don't think so because the ground is probably common, but worth asking just in case i guess). \n\nany ideas or should i just get soldering?\n", "tags": "motor", "id": "6445", "title": "ways of reversing motor direction easily"}, {"body": "just to give a bit a context, here are the equations i'm using for the angular accelerations.\n\n\u03c6** =(1/jx)\u03c4\u03c6\n\nand\n\n\u03b8** =(1/jy)\u03c4\u03b8\n\nso my plant gains would be \n\n\u03c6**/\u03c4\u03c6 =(1/jx) along x axis\n\nand\n\n\u03b8**/\u03c4\u03b8 =(1/jy) along y axis\n\nthe basic pid structure is \n\ngain=kp(desired-measured)+ki(integral(desired-measured))+kd(differential(desired-measured)\n\nlets just say my plant gain for angular accl around x axis is \u03c6g and my pid gain is pg. to obtain a controller, do i do\n\n(\u03c6g)(pg)=open loop gain=l\n\nand for closed loop l/(1+l). \n\nmy question is, am i right about what i'm doing and do i upload the algorithm in time domain form or frequency domain form (silly question as frequency domain is for analysis but my only control experience is purely theory and entirely focused on analysis using root locus and nyquist)\n", "tags": "arduino control pid", "id": "6446", "title": "how do you design quadcopter pid algorithm?"}, {"body": "all-most all slam back-end implementation compute chi2 estimates. chi2 computation is usually used to compute the best-fitness score of model to the data.\nhow it is related to optimization framework for slam?\n\nrgds\nnitin\n", "tags": "slam theory", "id": "6447", "title": "role of chi2 in slam back-end optimization"}, {"body": "** if there's a better stack exchange site for this, let me know--the mechanical engineering one is closed so i'm not sure where else is appropriate **\n\ndo mechanical systems have fuses? \n\nin an electrical system, like a charging circuit for example, we prevent large loads from damaging the system with fuses and circuit breakers. these will disconnect the inputs from the outputs if the load get too high. \n\ndo mechanical systems have a similar mechanism? something that would prevent gears or linkages from breaking when the load gets too high by disconnecting the input from the output?\n", "tags": "mechanism", "id": "6452", "title": "fuses for mechanical systems"}, {"body": "what's the quickest way to calculate a relative coordinate of a frame, as shown in the code below. the kuka robot language this \":\" is referred to as the geometric operator. \ni would like to perform this calculation in matlab, scilab, smathstudio or java, could you please advise on which library to use and/or how to proceed?\n\n\n", "tags": "programming-languages", "id": "6459", "title": "relative frame calculation"}, {"body": "i have a bbb and it works quite well however when i power it over the barrel connector or over the vdd pins rather then use the usb connection it doesn't automatically boot. when i put the barrel connector and usb in at the same time and then remove the usb it continues running. this is running on ubuntu arm. i have tested that the power supply is between 4.95 and 5.04 v and is capable of sustaining this to just over 1 ampere\n\nedit it appear the bbb does boot when supplied with 4.7 v lab power supply at ~0.4a power consumption. so that suggest something is wrong with the power supply. but how do i test it seeing i was able to verify that it can supply 1a at 5v.\n\nthis power supply works by feeding a 12v battery into a step down to convert it down to 5v if that matters.\npower-supply power linux beaglebone-black beagleboard\n", "tags": "beagle-bone linux", "id": "6462", "title": "beaglebone black doesn't autoboot when powered over jack or vdd pins"}, {"body": "a co-worker said he remembered a 2011(ish) icra (ish)  paper which used just a touch/bump sensor and odometry to do slam. i can't find it - i've paged thru the icra and iros agendas and paper abstracts for 2010-2012, no joy. i found a couple of  interesting papers but my coworker says these aren't the ones he remembered.\n\nfoxe 2012 http://staffwww.dcs.shef.ac.uk/people/c.fox/fox_icra12.pdf\ntribelhorn a &amp; dodds 2007:  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.507.5398&amp;rep=rep1&amp;type=pdf\n\nbackground - i'm trying to make a lego mindstorms bot navigate and map our house. i also have a ir sensor, but my experience with these is that they are really noisy, and at best can be considered an extended touch sensor.\n\nregards, \n winton\n", "tags": "slam", "id": "6463", "title": "single touch based sensor and odometry slam in noisy rectilinear environment"}, {"body": "i am studying electronics engineering. i am fond of robots since my childhood. i have small doubt i.e. if i want to get placed in\nrobotics and automation based company ,what should i must study(reference books/softwares etc) perticularly for cracking an interview ? in simple words,as electronics engineer what other specific skills (like embedded c programming etc) should i go through?\n", "tags": "control microcontroller electronics", "id": "6467", "title": "help for cracking robotics interview"}, {"body": "i read up on the wheels of curiosity, and also about the suspension. but is there a name for the steering? it looks similar in nature to the front landing gear on an airplane, but searching those terms didn't turn up and answer. i've attached a picture with the area of interest highlighted. \n\n(image: gene blevins/reuters)\n\n\n", "tags": "wheeled-robot", "id": "6469", "title": "is there a name for the steering style/wheel actuation used on curiosity?"}, {"body": "i am working on my final project by autonomous quadcopter. my tasks are to make a quadcopter  which should do object avoidance and it should auto land using ultrasonic sensors.\nany possible ans to it, how should i connect hc-sr04 ultrasonic sensor to my apm 2.6 board?\neven apm 2.6 has a port i2c.\n", "tags": "quadcopter", "id": "6471", "title": "how to connect hc-sr04 ultrasonic sensor to apm 2.6?"}, {"body": "i wanna know how much does  pioneer p3dx cost? nothing mentioned in the website and i don't want to fill out the form regarding this matter. \n", "tags": "mobile-robot", "id": "6475", "title": "what is the price of pioneer p3dx"}, {"body": "i am looking to build some custom hardware (nothing too fancy, just some motors, cameras and the like), and i need it to be controlled by my laptop (its going to have to do a non-trivial amount of image processing).\n\nis there a way to attach $n$ motors to a laptop where $n&lt;10$ via usb/e-sata? it seems like something that should be very easy to solve, but i can't seem to find it anywhere.\n\ni am not looking to get an arduino/raspberry pi, really just connect the motors, and be able to control them individually. i am comfortable adding more power from a second source to supplement the usb power.\n\nideas?\n", "tags": "motor usb", "id": "6476", "title": "how to run custom hardware from a laptop"}, {"body": "what does this sentence mean :\n\n\"the chassis maintains the average pitch angle of both rockers.\"\n\nput in other words, \" the pitching angle of the chassis is the average of the pitch angles of the two rocker arms\" \n\nwhat is a pitching angle in this context? \nplease explain both pitching angles.\n", "tags": "mobile-robot wheeled-robot", "id": "6478", "title": "rocker bogie suspension system - pitch angle"}, {"body": "hello i'm a new rc enthusiast, \n\nis anyone interested in rc's controlled through xbox remotes? the project is to use an xbox one or xbox 360 remote to either hijack a dx3e or dx3c remote or create a transmitter compatible with the spectrum receiver out of the xbox remotes. i've seen applications that use wifi but i'm not sure thats the route i'm looking for. from what i've read there is limited range and signal loss through the wifi network plus it may create a lag larger than what would be desirable in racing. the rc is a losi scte short course race truck. i'm not to savvy with electronic jargon but will study and learn what i can. thanks for your thoughts.  \n", "tags": "brushless-motor", "id": "6488", "title": "creating an xbox remote that replaces spectrum dx3c or dx3e without wifi"}, {"body": "i decided to work on a 2 wheeled robot position mapping problem. for that i have 2 dc motors with encoders from pololu.\ni have the following two questions:\n\n\ndo i need to know the model of each motor in order to tune a controller?\nwhat are the steps/approach used to get a good mapping (let's say 1 mm maximum error)?\n\n", "tags": "pid wheeled-robot odometry", "id": "6493", "title": "2 wheeled, 2 motor robot control"}, {"body": "how can i send a jpeg image to a microcontroller via usart?\n", "tags": "electronics embedded-systems", "id": "6495", "title": "how can i send a jpeg image to a microcontroller via usart?"}, {"body": "given a dh matrix for a set of joints, how would you convert the data into homogeneous transformation matrices for each joint? i've looked online, but can't find a good tutorial.\n", "tags": "kinematics joint dh-parameters", "id": "6500", "title": "dh matrix to homogeneous transformation matrix for each joint"}, {"body": "i would like to build a motorized robot with stereo vision that sends visual input to a pc over wifi for processing. a raspberry pi would have been perfect for the task if it would be possible to connect 2 cameras. the pi 2 would be powerful enough for two usb webcams (with minimum 8fps) i guess, but the shared usb seems to be a bottleneck (2 cameras + wifi dongle).\n\nwhat other options are there to send input from 2 cameras and control a motor (or an arduino)?\n", "tags": "computer-vision cameras", "id": "6508", "title": "low power computer for stereo vision"}, {"body": "i am wondering what the use is of two pid loops to control a quadcopter. one pid for stability control and another pid for rate control.\n\nwhy can't you just use one pid per axis to control a quadcopter where you take the current and desired angle as input and motor power as the output?\n", "tags": "quadcopter pid", "id": "6510", "title": "multiple pids in quadcopter"}, {"body": "i've noticed that the industrial robot arms have very smooth, fast, and strong movement.  does anyone know what type of servos they use.  i'm trying to build my own and don't want to have the jerky movement that is seen in a lot of diy robot arms?  thanks. \n", "tags": "robotic-arm servos", "id": "6511", "title": "what type of servos are used in industrial robot arms like universal robot ur5?"}, {"body": "i am going to start a new project consisting in implementing an autonomous driving rc car.\nthe car as it is now, has a camera installed on each side of the car, i.e. 4 cameras in total. they are connected to a board which is able to read and process the video input.\n\ni have been researching about obstacle detection using a single camera (without stereo cameras, e.g. single camera vision and mapping system ) and although it seems possible it also seems quite complex. modifying the cameras set-up is not an option. i already have some video processing algorithms, like dense optical flow, which might help me, but i am not sure whether i might me able to implement the system in the time i have (4 months). i also don't know how reliable would be the final solution.\n\nif the first approach is not feasible, as an alternative option i also could install distance sensors in the car to detect obstacles. it seems that usually the most preferred choice is to use ultrasonic sensors. i would need to install them and i would not take advantage of the cameras, but it seems that the final complexity would be lower.\n\nis the first approach feasible? what are the pros and cons of each approach? if i implemented the second option, how many sensor would i need?\n", "tags": "sensors computer-vision ultrasonic-sensors", "id": "6515", "title": "computer vision with single camera vs. distance sensors for obstacle detection"}, {"body": "i have a project where i need a motor that can turn some number of rotations which will spool up a cable attached to a spring-closed device to open it up. when power is disconnected, the spring closure will cause the spool to unwind and the device to close.\n\nin the closed position, no power is available. (i.e. the closure mechanism needs to be 100% passive.)\n\nin order to keep this open for some time, i will need a motor that is capable of being stalled for long periods without having the windings burn up. i know some motors can do this, such as the motors they use on spring closed hvac dampers, but i don't know how to find them or if there's a particular name i should be using to find them. i know i could probably do this with a stepper motor, but that seems overkill for the application.\n\nthe only requirements are higher torque to open this mechanism, no gearing that prevents the motor from spinning when power is disconnected, and the ability to be stalled.\n", "tags": "motor", "id": "6516", "title": "stallable motor - 100% duty cycle higher torque motor that can be stalled without burning up"}, {"body": "we implemented a pid controller for our quadcopter which allows us to fly from point a to b. the precise position of the quadcopter is measured using an external tracking system.\n\nnow that we can fly from a to b, we would like to implement a controller to fly more complex trajectories with multiple set points. e.g. from a to b to c or flying in a circle using sample points.\n\nwe tried to use our regular pid controller for this but this of course doesn't work well since the pid controller forces the quadcopter to stabalize at any set point. we would like to have a controller that allows the quadcopter to fly a trajectory fairly smoothly. i think this has to be a controller that takes into account multiple set points in the trajectory at the same time so that it can already slow down/speed up based on the trajectory that is ahead.\n\ncan someone point me to some controllers / algorithms that i can look at to realize this? do i need a completely different controller to do this or will it be an adapted version of the pid controller that i have now?\n", "tags": "quadcopter pid", "id": "6522", "title": "pid controller for trajectory with mutliple setpoints"}, {"body": "in order to identify the dynamics of my dc motor, i am trying to command it with xcos using the arduino tool box. the problem that i am facing is how to give the motor an input command such that i get some given angle position as output. i can only control the input voltage to the motor via pwm.\n\ni have been thinking about converting the angle to voltage but i can't figure it out.\ncan somebody help me?\n", "tags": "arduino motor control", "id": "6523", "title": "building an open loop controller for a simple dc motor position problem"}, {"body": "apologies if this is a stupid question, but if i have a 3-axis magnetometer, and i calculate the vector magnitude as\n\n\n\n...then should i not always get the same value, regardless of the sensor's orientation? mine is all over the place, and i feel as though i'm missing something obvious.\n", "tags": "imu magnetometer", "id": "6526", "title": "3-axis magnetometer question"}, {"body": "i am designing a pan-tilt camera mount using standard hobby servos.    many existing designs use the servo shaft as a revolute joint, as opposed to simply a torque producing element.  as a revolute joint the servo mechanism is subject to different torques and forces.  is using a servo shaft as a revolute joint recommended practice or should a bearing be used?\n", "tags": "rcservo", "id": "6531", "title": "using hobby servo as axle"}, {"body": "i am beginner in robotics .i have taken admission for electronics engineering one year back as we don't have specifically robotics engineering branch in my country.now i am suffering from questions like what is the scope of electronics( not electrical) engineering in robotics/ automation?i am unable to distinguish between the role of electronics engineer and computer engineer in robotics as in both cases programming is required\n\nalso,if i don't like to do programming(coding),are there any other options to stick to robotics / automation field as per my branch(electronics engineering ) is concerned?. \n", "tags": "control electronics embedded-systems", "id": "6532", "title": "what is the scope of electronics engineering in robotics/automation?"}, {"body": "i'm not entirely sure if this is the right area to post this question, but looking at the other subjects on stackexchange, this seems to be the best fit.\n\ni am a complete beginner to hydraulic systems, and i've wanted to learn more about this area. i'm designing a hydraulic system that involves using hydraulics to push/pull objects using pistons. i have looked at what the basic requirements are for a hydraulic system, but there is one thing that escapes me.\n\ni come from an electronic background, and i noticed that the hydraulic pumps (for example, this one) seem to lack a motor to drive the fluid. am i wrong? if not, i've been looking everywhere for a motor that can/should be attached to said pump, but i cannot seem to find anywhere that sells them. is it just a simple dc motor (with correct specs), or should there be a specific motor designed for hydraulic pumps?\n\nlooking around, i came across this, but looking through the specs, i don't see a power requirement, and being used to seeing power consumption in datasheets, i'm not even sure it is a motor!\n", "tags": "motor", "id": "6536", "title": "motor for a hydraulic pump in a hydraulic system"}, {"body": "i will be using at least one programmable real-time unit (pru) to send pulses to a stepper motor driver but before i begin, i am trying to lay out the structure of my programs.\n\ni am using this library pru linux api \nfor loading assembly code into the pru instruction memory but there doesn't seem to be much documentation other then whats at that wiki and the source:\ngithub-pru-packageh\n\nmy c program will be calculating the position of the sun using an algorithm and executing the assembly/writing a pulse count to the pru(s) data memory so they can just switch on/off a gpio at my desired frequency and for the number of pulses required to turn a stepper the appropriate number of steps. i am not even sure if this is an acceptable method but i am pretty new at this and it seems like a simple way to accomplish my task\n\nmy questions regarding the library functions are:\n\n\nis there a significant performance difference between using  or  to give the pru(s) access to the pulse count?\nwould it be better halt the pru assembly program after has completed the tasks for each pulse count then re-execute it with new values, or keep the pru program running and poll for a new pulse count to be written in?\n\n\ni plan to send a pulse count every 10 seconds or so.\n\nany suggestions on revisiting the whole structure and logic are welcome as well.\n", "tags": "c beagle-bone", "id": "6541", "title": "beaglebone - pru questions"}, {"body": "for example, i have a brush-less outrunner with 14 poles and 12 stator windings. it has three connectors. can this motor be controlled in a way that it performs a single step of 30 degrees (360/12)?\n", "tags": "motor brushless-motor", "id": "6544", "title": "what's the smallest rotation a brush-less motor can perform?"}, {"body": "i know some languages like php, c/c++ and java but i'm not expert in these languages. i want to create an artificial intelligent robot that can do these task;\n\n\nable to communicate with computer (usb, bluetooth or other)\nable to perform some specific task\npresent a  visual interface (finding path, speed and others)\naccess its micro controller device and attached devices\nand so on..  (editor note: solve world hunger?) \n\n\ncan any one please suggest which programming language will be good for programing this type of robot. i have heard about c/c++ and assembler and robotc and labview but i am unable to decided which language to use for my project.\n\nsorry for my bad english!\n", "tags": "microcontroller artificial-intelligence robotc", "id": "6553", "title": "create artificial integelent robot ability to communicate with computer : which language i should use"}, {"body": "i'm trying to understand how an electronic musical instrument (called an e-chanter) works (imagine a recorder or other wind instrument, but with the holes replaced with metal contacts, and the sound played electronically, so no air is needed).\n\nbasically, there are several metal contacts, as shown in this link: http://www.echanter.com/home/howto-build#toc-wires-screw-sensors\n\nthey each appear to be wired only directly to one pin of the arduino:\n\n\n\ni can't figure out for the life of me how this works.  can anyone explain it, are the fingers being used as some kind of ground or what on earth is going on.  i have a physics background so can understand some technical info, but just can't fathom how on earth this magic works.\n\nthank you very much\n", "tags": "arduino sensors", "id": "6560", "title": "don't understand how sensor works, metal wired directly to i/o pin of arduino"}, {"body": "i am trying to derive the analytical jacobian for a system that is essentially the equations of motion of a body (6 degrees of freedom) with gyro and accelerometer measurements.\nthis is part of an extended kalman filter.\n\nthe system state is given by:\n$\n\\mathbf{x} = \\left(\n\\begin{array}{c}\n\\mathbf{q}\\\\\n\\mathbf{b_\\omega}\\\\\n\\mathbf{v}\\\\\n\\mathbf{b_a}\\\\\n\\mathbf{p}\\\\\n\\end{array}\n\\right)\n$\n\nwhere $q$ is the quaternion orientation of the body expressed in the global frame, $b_\\omega$ and $b_a$ are the biases in the gyro and accelerometer respectively (expressed in the body frame) and $v$ and $p$ are the velocity and position of the body expressed in the global frame. all vectors are [3x1] except $q$ which is [4x1] in $[w,x,y,z]^\\top$ format, and $r$ (below) which is [3x3].\n\nthe equations of motion $\\frac{dx}{dt}=\\dot{x}$ (t is time) are:\n$$\n\\dot{\\mathbf{q}} = \\frac{1}{2}\\mathbf{q} \\otimes \n\\left(\n\\begin{array}{c}\n0\\\\\n\\hat{\\omega}\\\\\n\\end{array}\n\\right) \\\\\n\\dot{\\mathbf{b_\\omega}} = 0 \\\\\n\\dot{\\mathbf{v}} = r^\\top (\\hat{\\mathbf{a}} + [\\hat{\\mathbf{\\omega}}\\times]r \\mathbf{v})+ g \\\\\n\\dot{\\mathbf{b_a}} = 0 \\\\\n\\dot{\\mathbf{p}} = \\mathbf{v}\n$$\nsecond-order terms are ignored. $\\hat{a} = a - b_a$ and $\\hat{\\omega} = \\omega - b_\\omega$ are the corrected accelerometer and gyro biases ($a$ and $\\omega$ are known) and are expressed in the body frame. $r$ is the rotation matrix (dcm) formed from $q$ and $g$ is the gravity vector $[0,0,9.81]^\\top$. these equations have been validated against an aerospace engineering software library.\n\ni need the jacobian $f = \\frac{d\\dot{x}}{dx}$ but i cannot find this jacobian in any texts (i do find the error-state jacobian eg this paper).\ni am struggling with doing this myself because i don't know how to handle the quaternion norm constraints. i also am concerned about the validity of a solution given through numerical differentiation.\n\nany help or explanation would be greatly appreciated. this is going towards an open-source robot localisation project.\n", "tags": "control kalman-filter jacobian", "id": "6564", "title": "jacobian matrix of 6dof body (with imu)"}, {"body": "i am a complete newbie and recently joined a robot team at my school in order to gain some experience. i have been assigned a task of driving a servo using a pololu mini maestro usb servo controller. i am using the beaglebone black (bbb) with the python adafruit library. how do i make the bbb communicate with the servo controller? if you guys could point me in the right direction, i'd really appreciate that. right now, i don't even know where to start. not sure if it matters, but this is the servo i am using: https://www.pololu.com/product/1053\n", "tags": "motor microcontroller", "id": "6565", "title": "communicating between a beaglebone black and a servo controller"}, {"body": "in my project, i've successfully analyzed the arena and have detected the obstacles using an overhead webcam. i also have computed the shortest path. the path data is transmitted to the robot via zigbee, based on which it moves to its destination.\n\nthe problem is: my robot is not taking accurate turns which would cause error in the rest of path it follows.\n\ncould anyone please suggest any methods/techniques for feedback from the robot so as the path is corrected and the robot follows the original path computed without any deviation? (basically a tracking mechanism to avoid deviation from the original computed path)\n", "tags": "computer-vision navigation motion-planning", "id": "6566", "title": "robot navigation feedback using image processing"}, {"body": "bosch, freescale, invensense, st and maybe others are releasing 9-dof ahrs platforms containing their own fusion software and outputting filtered/sane/fused data (attitude as quaternion and linear acceleration).\n\ni would like to use these for the quality of their respective company fusion algorithm. and would like to merge gnss position and velocity data to it.\n\ni have found multiple examples of heavy (> 20) states kalman filters merging raw 9-dof imu data and gnss position/velocity.\n\nbut i have a hard time finding a computationally lighter version of gps+ahrs fusion as these new 9-dof ahrs already fuse imu raw data themselves and this process should'nt be done twice.\n\nwould you maybe have pointers on the algorithm(s) or type of filter to use ? thank you.\n", "tags": "quadcopter kalman-filter imu gps sensor-fusion", "id": "6570", "title": "fusion of gnss position data and prefused 9-dof ahrs data"}, {"body": "currently i am building a robots with 2 incremental encoders with a optical mice sensor. the reason to install a optical mice sensor is to provide better feedback when slippage happen on the encoders.\n\ni wonder if i could apply a kalman filter to get a better distance feedback with these 2 kinds of sensors? especially when the control input is unknown?(for example i push the car with my hand, but not applying a voltage to the motors)\n\ni have read some examples to use kalman filter (gyro+accel / encoder+gps), either one of the variable used is in absolute measurement, while in my case, two feedbacks are dead-reckoning. \n\nany help is appreciated =] !!!!!\n", "tags": "kalman-filter quadrature-encoder", "id": "6571", "title": "kalman filter with incremental encoder + optical mice"}, {"body": "what kind of linear actuator can be used for very small forces (pushing ~0,1 kg over a flat, smooth surface) in steps of ~50mm over a total length of about 250mm? i only seem to find actuators that are very large and are overpowered.\n\nextra details:\nspeed requirements: >5mm/s\nduty cycle: 12 times in half an hour, once per day\nit must be compact and not much longer than 350mm when concealed\nprice must be under $100\n", "tags": "actuator", "id": "6572", "title": "what kind of linear actuator can be used for very small forces over relatively long distances"}, {"body": "good day!  \n\ni am helping my little ones 6 and 7 to develop a robot that can pick up and stack cubes three high as well as gather blocks.  they came up with the design that enables them to pick up three cubes at a time when lined up and then pull up the claw, turn, drive to another cube, place the cubes on the stand still cube and release.\n   well they got the claw made with two rods connected to gears to motor and the rods reinforced, then they made insect like legs 3 - pairs of two on the rods, with gripper feet pads on the ends of the legs. all of this works as it opens and closes! the problem is that when they try to close the claw on the cubes and pick up all three cubes, the first - closest to motor feet have a nice tight grip, the second - middle feet - have a lighter grip and just barely can lift the block, and third - farthest from motor doesn't even have a grip on the blocks.\n   i think it's because the second and third set of feet are farther from the motor. but how can they evenly disperse the tension load so the claw can pick up all three blocks?\ni tried putting elastics on the feet for better grip and unless we put ten on each foot for the third set and maybe five on the second set it wont work.  even though it's a quick fix i would like to help them figure it out the proper way of spreading the load so to speak.  we also tried putting a small band on the third set of legs.  the robot could still open and close and that worked for the third set but not the second.  we tried putting a band on the second and third but the legs wouldn't open anymore.  i could use a lighter band but is there another way? we only have one little motor to run it so we can't give all the leg sets it's own motor and even if we did there would be weight issues.\n  thank you in advance!\n", "tags": "design mechanism vex", "id": "6575", "title": "how to distribute tension load on a three footed claw?"}, {"body": "as a non-native speaker i have a (maybe trivial, but to me not clear) question concerning the verb 'to teach'. of course, from school (and online dictionaries) i know the past tense of 'teach' is 'taught' not 'teached'. but in robotics 'to teach' has a special meaning (like: 'to make special ponts/orientations known to the (arm-)robot', e.g. by guiding the robot to those points/orientations.) \n\ndoes it make sense to have a different past tense for 'teach' (i.e. 'teached') in this case ? maybe a reference were it is used/explained ?\n\n(i would say 'no. the past of teach is taught, and that's it.', but some of my colleagues - also not native speakers - have a different opinion.)\n", "tags": "reference-request", "id": "6576", "title": "robotics jargon question: how to conjugate 'teach'?"}, {"body": "any one can advice me on the ideal perception sensors for pick and place application using a robotic manipulator with ros support. i have generally looked at things like kinect and stereo cameras (bumblebee2) which provide depth that can be used with pcl for object recognition and gripper positioning. are there any other sensors would be preferred for such application and if not what are the drawbacks of stereo cameras in comparison to kinect or other sensing capability. \n\nthanks,\n\nalan\n", "tags": "sensors robotic-arm kinect stereo-vision", "id": "6581", "title": "stereo camera vs kinect"}, {"body": "we are building a hobby drone(quad-coptor) with a camera for footage. so to control the quad, i have been suggested(web,here) to use minimum of four channels.for power and for turning etc., so which means i need one channel for every separate task to be done. for eg., if i want to rotate the camera, then i suppose i need the 5th channel and so on..\n\nnow my question would be - i have seen a lot of drones(ardrone, walkera) which are controlled by an android or an iphone app. so the wifi used to connect the drones is, single channel or multi-channel? if single channel then how can i control different tasks like to control turning of the quad or camera in different axis?\n\nalso if i want the gps location from the quad, do i require to have another transmitter?\n\ni am using(planned to) a raspberry pi 2, openpilot-cc3d for flight control.\n\np.s this is my first drone, kindly show some mercy if i ask/don't understand your comments.\n", "tags": "quadcopter navigation radio-control wifi", "id": "6583", "title": "is our laptop wifi single channel or multiple channel? this is for controlling a bot"}, {"body": "though, denavit-hartenberg notation is commonly used to describe the kinematics of robot manipulator, some researcher prefer the product of exponential instead; and even the claim that it's better.\n\nwhich one should i use, and which one is generally better; is final solution same for both kinematics and dynamics?\n\nany suggestions?\na mathematical introduction to robotic manipulation\n", "tags": "robotic-arm industrial-robot dh-parameters product-of-exponentials", "id": "6586", "title": "denavit-hartenberg convention or the product of exponentials formulation, when dealing with the manipulator kinematics?"}, {"body": "the first time after importing a project into the eclipse workspace we find that eclipse cannot find the wpilibj.\n\non any import line: eclipse says \"unresolved import edu.\n", "tags": "microcontroller wheeled-robot", "id": "6587", "title": "frc roborio eclipse can't find edu.wpi.first.wpilibj"}, {"body": "in this picture, a sketch of a quadcopter is displayed with rotor's direction of motion.  the magnitude of the rotational velocity is depicted by the thickness of the lines (thicker lines are higher velocity, thinner lines are lower velocity).\n\n\n\ni'm told this is the correct way to produce turning motion, but my intuition (which is usually wrong) tells me that the two pictures should be reversed.  my argument is  as follows:  for the picture on the left, the two rotors of higher velocity are spinning clockwise.  if the motion of the rotors of greater velocity are clockwise, shouldn't the quadcopter also rotate clockwise?  what am i missing here?\n", "tags": "quadcopter dynamics", "id": "6590", "title": "how do quadcopters turn left and right?"}, {"body": "i'm an electronics newbie.\n\nthe part i'm talking about is this:\n\n\n\nmy question is, can this type of part be used to control brushless dc motors?\n", "tags": "arduino brushless-motor", "id": "6592", "title": "can i use a pwm hho controller to control a brushless dc motor?"}, {"body": "i'm new in forum and since i made some research the past days i'd like to get some guidance about constructing &amp; programming a quadcopter from scratch since i'm completely new on a project like that.\n\nquadcopter frame: thinking about to construct an aluminum 70cm diameter frame which will weight around 500g. what kind of motors should i get in order the frame with the board,motors etc. will be able to lift?\n\nboard: i'm thinking to use arduino uno or raspberry pi 2.0 ( with a little bit of research i made i conclude that raspberry could make my life a little bit easier since you can add wifi on it. the quadcopter will be controlled via a pc/laptop through wifi). what can you suggest and why?\n\nesc: as far as i've seen in most of similar projects people using escs in order to control the motors throttle. can you avoid that, with programming pids that make the same job in order not to use more hardware?\n\nabout pids and code in general: thinking about to simulate the whole project in simulik, matlab and somehow (if it's possible) to convert the matlab code into c++ and download it on the chip. what do you think about that?\n\nabout the whole project: i'm trying to minmize the hardware as much as it's possible (use only 4x motors, the board with the chip on it, cables and probably some sensors) in order to minimize the total weight of construction and ofc the price.\n\nthat's all for a start. i'm gladly waiting for your answers and ideas.\n", "tags": "quadcopter", "id": "6593", "title": "guidance & info about quadcopter project"}, {"body": "i am beginner in robotics.i want to make serious start from scratch with interest but confused from where to start.so can anyone give some suggestions for 1.as a beginner in robotics ,are there some simple and basic robots or circuit\ndesigns which i can make by myself in the home(so that i can gain practical knowledge of robots)? 2.or should i first read books (can anyone suggest some good reference  book names,articles ,links,free on-line  video lecture series)?\n", "tags": "beginner", "id": "6596", "title": "suggestions for beginer in robotics"}, {"body": "i have recently been studying kalman filters. i was wondering that if sensor model of a robot gives a unimodal gaussian ( as is assumed for lkf) and the environment is pre-mapped, then the sensor reading can be completely trusted( ie. max value of kalman gain), removing the need for odometry for localization or target tracking purposes and hence the need for the kalman filter. please clarify.\n", "tags": "kalman-filter", "id": "6597", "title": "need of kalman filters in unimodal measurement model"}, {"body": "in robot kinematics, we have $e^{(\\theta*twist)}$, where $twist$ is 6*1 vector.  how do i get the 4 by 4 transformation matrix by using product of exponentials?\n", "tags": "robotic-arm kinematics forward-kinematics product-of-exponentials", "id": "6599", "title": "how to get the 4*4 matrix from the twist using product of exponentials, in robot kinematics?"}, {"body": "i posted a similar question before as i was just getting started with the project but i wasn't specific enough so got a weak response from the se community. but now i am at a point where i have python code which is supposed to rotate a servo through pololu's maestro serial servo controller (https://www.pololu.com/product/207). based on the \"serial interface\" section of the user's guide (https://www.pololu.com/product/207/resources), i sent a sequence of numbers starting with decimal 170 and 12, which are the \"first command byte\" and the \"device number data byte\", respectively. the user guide says that 12 is the default device number, so i tried changing it to 18 because that's how many servos my servo controller can drive. but that doesn't make much difference because the servo doesn't rotate at all. the numbers after that are the same as the example from the user's guide. i am not sure what the 4, 112 and 46 are doing, but the 0 targets the servo port \"0\" on the servo controller (the port to which my servo is connected). the servo doesn't move, regardless of what sequence of numbers i put in. i have very little experience, so if you guys could point me in the right direction or at least point to some useful resources on the web, i'd be very grateful.\n\n\n", "tags": "python", "id": "6601", "title": "help with code that is supposed to drive a servo controller"}, {"body": "what is the most common zigbee ip modules to create full wireless mesh mode? \ni know that it should be 1) coordinator and 2) router to create full-mesh\n\nbut i am interesting about what kind of modules it would be better to buy by comparing price, quality and tutorial material. it would be graet if you know some zigbee modules based on arm(cortex) or atmel mcus and if you have some additional tutorial materials to control those modules and  understand it.\n\ni am looking for zigbee modules only, but not xbee!!!\nbecause they have difference in organization of network. \nex.: \nfirst of all, zigbee mesh can create own zones to control devices. \nbut in xbee mesh which from digi comany can create only full mesh and only one big or huge zone to control devices. \n\nsecondly, zigbee modules - aes encryption. can lock down network and prevent other nodes from joining.\nxbee - aes encryption.** and it is coming soon...\n", "tags": "mobile-robot microcontroller radio-control communication wireless", "id": "6604", "title": "choose zigbee modules for full wireless mesh"}, {"body": "i am looking for cheap wheeled robot that can be controlled remotely. i do not really care about how (rf, bluetooth, wifi, ir, other?), as long as i can control around 10 robots without interference in a small arena (they are always in the line of sight).\n\ni would like to emphasize that i do not need them to be programmable and it is important that they are cheap.\n", "tags": "control wheeled-robot", "id": "6606", "title": "cheap wheeled robot without tether (does not need to be programable)"}, {"body": "i have been asked to write code to implement serial communications with a camera in order to control its pedestal (movable base) as well as set a few dozen other camera options. the catch is that i have to make it usable by ros. \n\nwhat would be the best practice to implement this functionality in ros? i understand  the concept of services, but i think that there should be a better way than creating a different service/file for each option.\n\nthanks,\ndaniel.\n", "tags": "ros cameras serial", "id": "6607", "title": "best practice to write a ros service for a serial-communication class with many options"}, {"body": "note: i'm a firmware developer experienced with sensors and networks, but not much with motors.\n\ni am trying to build a small hobby robot, like a cat-sized spider. i am thinking of using servo motors with position control, so i don't have to use encoders to know where the motor is. assuming six legs (i know, spiders have eight), with each leg being able to move up-down and left-right, that already translates to 12 motors. if you want to bend a knee, that gets the number to 18.\n\n18 motors on such a small robot is overkill, isn't it?\n\ni have thought of a couple of ideas, but not having a strong mechanical background, i cannot tell whether they are doable/sane.\n\none of my ideas is to use a magnet on the end of the limb (the end inside the chassis) and a small permanent magnet above it. the magnets attract each other and this keeps the limb firm under the weight of the robot. a stronger controllable magnet (a coil) would attract the limb even more and let it lift in the air. the following drawing may help:\n\n\n\nthis would allow the up-down movement of the leg, and a servo would control its left-right movement. however, i fear that such a system would not be strong enough to hold under the weight of the robot, or whether a reasonable coil would be compact enough.\n\nin short, my question is, how can i control six legs each with two or three degrees of freedom with a reasonable number of motors? is having one motor per degree of freedom the only possibility?\n", "tags": "mobile-robot servomotor", "id": "6613", "title": "multiple limbs on small robots"}, {"body": "i would like to control my 7 dof robot arm to move along a cartesian trajectory in the world frame.  i can do this just fine for translation, but i am struggling on how to implement something similar for rotation.  so far, all my attempts seem to go unstable.  \n\nthe trajectory is described as a translational and rotational velocity, plus a distance and/or timeout stopping criteria.  basically, i want the end-effector to move a short distance relative to its current location.  because of numerical errors, controller errors, compliance, etc, the arm won't be exactly where you wanted it from the previous iteration.  so i don't simply do $j^{-1}v_e$.  instead, i store the pose of the end-effector at the start, then at every iteration i compute where the end-effector should be at the current time, take the difference between that and the current location, then feed that into the jacobian.\n\ni'll first describe my translation implementation.  here is some pseudo openrave python:\n\n\n\nthe rotation is a little trickier.  to determine the desired rotation at the current time, i use spherical linear interpolation (slerp).  openrave provides a quatslerp() function which i use.  (it requires conversion into quaternions, but it seems to work).  then i calculate the relative rotation between the current pose and the target rotation.  finally, i convert to euler angles which is what i must pass into my angularvelocityjacobian.  here is the pseudo code for it.  these lines are inside the while loop:\n\n\n\nthen v_euler is fed into the jacobian along with v_trans.  i am pretty sure my jacobian code is fine.  because i have given it (constant) rotational velocities ok.  \n\nnote, i am not asking you to debug my code.  i only posted code because i figured it would be more clear than converting this all to math.  i am more interested in why this might go unstable.  specifically, is the math wrong?  and if this is completely off base, please let me know.  i'm sure people must go about this somehow. \n\nso far, i have been giving it a slow linear velocity (0.01 m/s), and zero target rotational velocity.  the arm is in a good spot in the workspace and can easily achieve the desired motion.  the code runs at 200hz, which should be sufficiently fast enough.  \n\ni can hard-code the angular velocity fed into the jacobian instead of using the computed  and there is no instability.  so there is something wrong in my math.  this works for both zero and non-zero target angular velocities.  interestingly, when i feed it an angular velocity of 0.01 rad/sec, the end-effector rotates at a rate of 90 deg/sec.\n\nupdate: if i put the end-effector at a different place in the workspace so that its axes are aligned with the world axes, then everything seems works fine.  if the end-effector is 45 degrees off from the world axes, then some motions seem to work, while others don't move exactly as they should, although i don't think i've seen it go unstable.  at 90 degrees or more off from world, then it goes unstable. \n", "tags": "kinematics jacobian", "id": "6617", "title": "jacobian-based trajectory following"}, {"body": "i'm wondering about good software/package to draw the robot manipulator and indicate dh parameters and different axes?\n\nany suggestions!\n", "tags": "robotic-arm industrial-robot robotc simulation", "id": "6620", "title": "what is the best software/package to draw the robot manipulator and indicate dh parameters and different axes?"}, {"body": "a professor in my university is asking me to study robotics with him. by robotics i understand programming a robot to move around, avoid obstacles, figure out a maze, etc.. he sent me some manuals for khepera ii. \n\nwhen i first read the specs, i was surprised by the low specs:\n\n\nmotorola 68331 cpu @ 25 mhz\n512 kb ram\n512 kb flash\n\n\nbut then i looked at some of the new arduino boards and they had similar specs.\nso maybe that's ok, i guess the cpu speed and ram aren't that important if i'm going to control the robot from a normal computer that can handle real time computation.\n\nwhat about the software? i glanced at the manuals and saw only c and assembly code.\n\nkhepera i is from 1995 and khepera ii is from 2001. i think robots have advanced much since 2001.\n\nis using khepera ii adequate for university level learning, considering i can probably give 200-300$ for a newer one?\n\ni ask in terms of hardware of the board, motors and sensors, as well as in the programmability. this question might seem vague. i'm ready to improve it by giving more detail upon request.\n", "tags": "mobile-robot", "id": "6622", "title": "is khepera ii still adequate for learning"}, {"body": "i am looking  for sensors to give me the position of a ball  on a plate in order to make a ball and plate problem  .\n\nwhat came to my mind is to use image processing but since i never did some serious image processing i don't know if it is a good idea.\n\neventually can you please help me to find some 'cheap' sensors in order to get the position of the ball on the plate.\n\nthank you for your attention.\n", "tags": "control sensors", "id": "6624", "title": "ball and plate possible sensors use"}, {"body": "here is what i did on ubuntu 14.04 lts running on a toshiba satellite, intel i7, nvidia with usb 3.0 and 2.0 ports.\n\n1, 2, 3 refer to scripts found here\n\n\nsetup ros by running this install-ros.sh script\nsetup opencv by running this install-opencv.sh script\nsetup pcl by running this install-pcl.sh script\ninstalled  libfreenect2 via the instructions at the master branch\nmade the changes and installed\ncloned the repository into an empty catkin workspace\nsourced the respective setup.zsh files from my /opt/ros/... and from the devel/ folders.\n\n\nat this point, i have encountered no issues\n\ni tried running: \n\n\n\nand i get the following message:\n\n\n\nso i assume i need to run roscore or something like that. so if i run: \n\n\n\nin one terminal and:\n\n\n\nin another terminal, i get the following segmentation fault:\n\n\n\ni must be overlooking something really trivial. thanks for taking the time to help me. \n", "tags": "ros kinect", "id": "6632", "title": "ros, iai_kinect2 issues"}, {"body": "i have implemented 2d-slam using ekf. the map is based-feature in which there is only one landmark for the sake of simplicity. i've read some papers regarding this matter. they plot the $\\pm3\\sigma$ plus the error. i would like to make sure that i'm doing the right thing. in my project, i have the estimate of the landmark's position and its true values. the true values here are the ones that the sensor measure not the ideal case. for example, the ideal case of the landmark position  is (30,60) but this value is not accessible by any means, therefore i will consider the true values the ones that are coming from the sensor. \n\nnow the error in the landmark's position in x-axis is formulated as follows \n\n$$\n\\text{error}_{x} = \\hat{x} - x\n$$\n\nthe below picture shows the error in blue color. the red color represents the error bounds which is $\\pm 3 \\sigma_{x}$\n\nmy question is now is this the way people plot the errors in the academics papers because i've seen some papers the bounds doesn't not look like mine. even though mine decreases monotonically however in some papers it is more curved and it seems more reasonable to me. any suggestions?\n\n\n", "tags": "slam ekf simulation mapping", "id": "6639", "title": "how to plot $\\pm 3 \\sigma$ of a landmark in ekf-slam"}, {"body": "it is possible to distinguish the properties \"time-varying\" and \"nonautonomous\" in dynamical systems regarding lyapunov stability analysis?\n\ndoes it make a difference if the system depends explicitly on $t$ or indirectly on $t$ due to a time-varying parameter?\n\ni want to explain the problem in detail:\n\nlet a dynamical system denoted by $\\dot x = f$, with state $x$.\nwe say that a dynamical system is nonautonomous if the dynamics $f$ depend on time $t$, i.e. $$\\dot x = f(t,x).$$\n\nfor instance the systems $$ \\dot x = - t x^2 $$ and $$ \\dot x = -a(t)x,$$ are nonautonomous. let $a(t)$ be a bounded time-varying parameter, i.e. $||a(t)||&lt;a^+$ and strictly positive, i.e. $a(t) &gt; 0$. \n\nparticularly, the second example is more likely denoted as a time-varying linear system, but of course it is nonautonomous. \n\nin lyapunov stability analysis autonomous and nonautonomous systems must be strongly distinguished to make assertions about stability of the system, and the lyapunov analysis for nonautonomuos systems is much more difficult.\n\nand here for me some questions arise. when i want to analize stability of the second example must i really use the lyapunov theory for nonautonomous systems? \n\nit follows for the candidate $v = 1/2 x^2$\n\n$$\\dot v = -a(t)x^2,$$\n\nwhich is negative definite. is the origin really asymptotically stable, as i suppose, or must i take the nonautomous characteristic into account in this case?\n\ni would suppose it makes a difference if a system depends explicitly on $t$ as in the first example or just indirect due to a time-varying parameter, since $t$ approaches infinity, but a parameter does not.\n", "tags": "control dynamics", "id": "6642", "title": "\"time-varying\" and \"nonautonomous\" dynamical systems and their lyapunov analysis"}, {"body": "is there a way to make a quadcopter maintain steady hovering (no lateral movement, constant altitude) while tilted left or right?  if so, how can i accomplish this? \n", "tags": "quadcopter", "id": "6643", "title": "can a quadcopter hover while tilted?"}, {"body": "i want to build a automatic agricultural robot for my final year diploma project. the basic idea is to program 8051 to drive the robot in a fixed path in farm for ploughing the farm which i am planning to do by setting a particular distance till which it will go straight and then take a u turn and plough in next lane. width of the farm will also be set so when it completes full farm it'll stop and go back to starting point. the only catch is to reprogram it as per size of the farm of the person who uses it. so i want to add a number pad with which he can set the length and width of the farm as well as width of each lane as per his needs without professional help. can this be done using 8051 or should i go for avr or pic microcontrollers. i have just started studying programming and interfacing of 8051 so i am not that good in programming. if its possible how do i do it. can someone please help me with circuit diagram for this project. after everything i said i need in my project if i still get an empty port in microcontroller i would love to add a fertilizer sprayer or water irrigation system and a gsm module so that a farmer can simply ask the robot to start working using his mobile phone. as i am making just a prototype i want it to be as small as possible. suggestions are welcomed.\n", "tags": "microcontroller wheeled-robot electronics artificial-intelligence", "id": "6648", "title": "automatic agricultural robot using 8051"}, {"body": "consider a tank like robot with a motor driver channel for each side of the robot (two motors on the left and two motors on the right) and an imu.\ni'm interested in driving the robot in a straight line using the yaw data from the gyro and magnetometer of the imu, removing the noise caused by slightly different behaving motors, and the possibility to change the desired direction angle. for example some event comes and i want the car to switch the desired direction to +120 degrees and turn while driving.\n\ni'm using arduino uno, minimu-9 v3 and two drv8838 single brushed dc motor driver from pololu.\n\ncan you please give me some hints and a short pseudo-code example?\nthanks!\n", "tags": "arduino pid imu", "id": "6649", "title": "pid control of tank-like robot and imu"}, {"body": "i'm a beginner in robotics. \n\ni'd like to ask what are the minimum/recommended specs for a microcontroller to run a real-time system such as linux rtai?\n\nwhat is the popular microcontroller for running linux rtai? \n\nthank you.\n", "tags": "microcontroller embedded-systems real-time", "id": "6650", "title": "microcontroller for running linux rtai"}, {"body": "with the bee hive collapses, growers are desperate for pollenation options. is anyone working on swarms of tiny flying robots to augment the bees? they could look for a certain color, poke around inside the flower for a moment, and move on to the next. when they need recharging, they fly back to their hive (the same reason bees fly back). \n\nof course, replacing germinators that run the seeds through their digestive systems would be a different problem.\n", "tags": "mobile-robot", "id": "6651", "title": "are robotic pollenators being designed?"}, {"body": "how a ros node written in python could subscribe to multiple topics and publish to multiple topics?\n\nall examples i found were for a single topic. is this an event-driven model so subscription to multiple \"events\" is allowed or it is more like a loop, so it can listen only to one \"source\" at a time?\n", "tags": "ros", "id": "6652", "title": "how to get a python node in ros subscribe to multiple topics?"}, {"body": "i have equations of a dynamic system. i need to figure out what this physical system is. \n\nthe equations are:\n\n\\begin{align}\n\\dot{x}_1&amp;=bx_1+kx_2+x_3\\\\\n\\dot{x}_2&amp;=x_1\\\\\n\\dot{x}_3&amp;=\\alpha (u-x_2)-\\beta x_3\n\\end{align}\n\nall i can figure out is that it is maybe a mass-spring-damper system, plus a feedback control, but i am not quite sure about the terms $x_3$ and $\\dot{x}_3$. what do these two terms mean?\n", "tags": "control dynamics", "id": "6657", "title": "what dynamic system could these equations represent?"}, {"body": "i was watching sebastian thrun's video course on ai for robotics (freely available on udacity.com).  in his final chapter on graphslam, he illustrates how to setup the system of equations for the mean path locations $x_i$ and landmark locations $l_j$.\n\nto setup the matrix system, he imposes each robot motion and landmark measurement constraint twice.  for example, if a robot motion command is to move from x1 by 5 units to the right (reaching x2), i understand this constraint as\n\n$$-x_2+x_1= -5$$\n\nhowever, he also imposes the negative of this equation \n$$x_2-x_1=5$$\nas a constraint and superimposes it onto a different equation and i'm not sure why.  in his video course, he briefly mentions that the matrix we're assembling is known as the \"information matrix\", but i have no why the information matrix is assembled in this specific way.\n\nso, i tried to read his book probabilistic robotics, and all i can gather is that these equations come from obtaining the minimizer of the negative log posterior probability incorporating the motion commands, measurements, and map correspondences, which results in a quadratic function of the unknown variables $l_j$ and $x_i$.  since it is quadratic (and the motion / measurement models are also linear), the minimum is obviously obtained by solving a linear system of equations.\n\nbut why are each of the constraints imposed twice, with once as a positive quantity and again as the negative of the same equation?  its not immediately obvious to me from the form of the negative log posterior probability (i.e. the quadratic function) that the constraints must be imposed twice.  why is the \"information matrix assembled this way?  does it also hold true when the motion and measurement models are nonlinear?\n\nany help would be greatly appreciated.\n", "tags": "slam", "id": "6659", "title": "graphslam: why are constraints imposed twice in the information matrix?"}, {"body": "i have been using the freeimu library successfully but now i want to add an external magnetometer that i can mount away from my motors.\n\ni've figured out how to modify the freeimu library to use an external magnetometer and i am getting data.\n\nwhat i can't figure out is what i need to change now that my magnetometer orientation has changed.\n\non the free imu it is mounted like this \n\n\nthe external compass is mounted like this - upside down - rotated 180\u00b0 around x\n\n\ni am changing the value inside the \n\n\nfunction as all the other code calls this to get the magnetometer data.\n\nso far i have tried:\n\n\nchanging the sign for the y and z values after i have got them from the magnetometer.\nchanging the sign for the y value only\nchanging the sign for the z value only\nadded 180 to both the y and z values\nsubtracting 180 from both the y and z values\nsubtracting 180 from y and adding 180 to z\nadding 180 to y and subtracting 180 from z\nchanging nothing\n\n\nthe calibration gui gives always gives me strange results, and doing the changes above just rotated/mirrored the magnetometer red and green graphs. i am unable to get rid of the key hole shape.\n\n\nthe red is xy. the green yz. the blue is zx. does the fact that zx works mean that my issue is with the y value?\n\nthis is how it looks using the on board magnetometer.\n\n\nwhat should i try next?\n\nthanks\njoe\n\nedit\n\ni tried rotating the external magnetometer so it is in the same orientation as the freeimu magnetometer and i still get the same result so i don't think its the difference in orientation that is causing the problem.\n\nso then i thought maybe its because the freeimu is mounted central to the rotation axis and the external magnetometer is mounted about 20cm above. i tested this by rotating only the external magnetometer around itself and i still got the same result. \n\nthis is all seems strange, do you think its possible that the external magnetometer i have brought is faulty? any way to confirm it is working properly on its own?\n\nthanks\n\nedit\n\nmanaged to get circles plotting by changing the gain from 0 to 1. it seem my new magnetometer was being saturated.\n\nnow i just need to work out how to change my values around so the orientation is correct.\n", "tags": "quadcopter imu calibration magnetometer", "id": "6661", "title": "freeimu external magnetometer"}, {"body": "i'm a software researcher, who in my spare time mentors a robotics team, helping on the software side of things.  for years, i keep coming back to the same question.  how to determine the robots position, and heading during our competitions.  we have tried a number of things with varying degrees of success/failure. encoders on the drive wheels, accelerometers, gyroscopes, etc.  i recently bought an imu with a 3 axis accelerometer, 3 axis gyro, and 3 axis magnetometer, all preprocessed by an arduino, and outputting the value to a serial port.  i thought surely there must be a way to take all these measurements, and get a composite view of position and heading.  we are using mechanum wheels on this particular robot, so wheel encoders are not particularly useful.  i've looked around and there's a lot of talk about orientation using quaternion with sensor fusion using similar boards, but it very unclear to me how to take the quaternion and the estimation and come up with x,y distance from the starting position.  now my time window for these measurements is small, ~15 seconds, but i need it to be pretty accurate within that window.  i'm about ready to abandon using the imu, and try something else.  one idea is to use a usb ball mouse to try and track robot motion but i'm certain that the mouse is going to get banged around way too much leading to noise and invalid results.  as a side note: robot's about 2ft x 3ft base weighting in at 120 lbs.  any thoughts or suggestions appreciated.\n", "tags": "imu sensor-fusion", "id": "6662", "title": "how to track robot position"}, {"body": "i'm planning to build an omnidirectional platform that will support about 180kg robotic arm. the platform will be equipped in meccanum wheels. i would like to have some kind of suspension to avoid wheels loosing contact with the floor on small bumps (let's say 2cm). \n\nthe first suspension type i thought about was rocker bogie type, but i'm afraid, that changes of arm center of mass during its movement will introduce too much stress on rocker bogie mechanism. \n\nwhat other choices would you recommend? or maybe rocker bogie will be fine after all?\n", "tags": "mobile-robot wheeled-robot design mechanism", "id": "6667", "title": "heavy omnidirectional platform suspension"}, {"body": "i've a 2d sensor which provides a range $r$ and a bearing $\\phi$ to a landmark. in my 2d ekf-slam simulation, the sensor has the following specifications \n$$\n\\sigma_{r} = 0.01 \\text{m} \\ \\ ,\\sigma_{\\phi} = 0.5 \\ \\text{deg}\n$$ \n\nthe location of the landmark in x-axis is 30. ekf imposes the gaussian noise, therefore the location of the landmark is represented via two quantities namely the mean $\\mu_{x}$ and the variance $\\sigma_{x}$. in the following graph \n\n\n\nthe green is the mean $\\mu_{x}$ which is very close to the true location (i.e. 30). the black is the measurements and red is $\\mu_{x} \\pm 3 \\sigma_{x}$. i don't understand why the uncertainty is big while i'm using rather accurate sensor. the process noise for the robot's pose is $\\sigma_{v} = 0.001$ which is small noise. i'm using c++. \n\n\n\nedit: for people who ask about the measurements, this is my code \n\n$$\nr = \\sqrt{ (m_{j,y} - y)^{2} + (m_{j,x} - x)^{2}} + \\mathcal{n}(0, \\sigma_{r}^{2}) \\\\\n\\phi = \\text{atan2} \\left( \\frac{m_{j,y} - y}{m_{j,x} - x} \\right) + \\mathcal{n}(0, \\sigma_{\\phi}^{2})\n$$\n\n\n\nwhere  is ( i.e. $\\mathcal{n}(0, 1) )$\n\n\n\nfor the measurements (i.e. the black color), i'm using the inverse measurement function  given the estimate of the robot's pose and the true measurement in polar coordinates to get the location of a landmark. the actual approach is as follows \n\n$$\n\\bar{\\mu}_{j,x} = \\bar{\\mu}_{x} + r \\cos(\\phi + \\bar{\\mu}_{\\theta}) \\\\\n\\bar{\\mu}_{j,y} = \\bar{\\mu}_{y} + r \\sin(\\phi + \\bar{\\mu}_{\\theta})\n$$\n\nthis is how it is stated in the probabilistic robotics book. this means that the measurements in the above graph are indeed the predicted measurements not the true ones. \n\nnow under same conditions, the true measurements can be obtained as follows\n\n$$\n\\text{m}_{j,x} = x + r \\cos(\\phi + \\theta) \\\\\n\\text{m}_{j,y} = y + r \\sin(\\phi + \\theta)\n$$\n\nthe result is in the graph below, which means there is no correlations between the true measurements and the robot's estimate. this leads me to the same question - why the uncertainty behaves like that? \n\n\n", "tags": "mobile-robot slam ekf noise", "id": "6672", "title": "the uncertainty is big while the sensor is rather accurate at measuring a landmark in ekf-slam"}, {"body": "most academic papers characterise the rate of rotation along the x axis as \u03c6\"=(1/jx)\u03c4\u03c6. as far as i cant tell, this characterises the rate and not the actual angle \u03c6 itself and yet the pid controllers academics use to control this takes \u03c6setpoint-\u03c6measured as its error signal. should the error signal not be \u03c6\"setpoint-\u03c6\"measured (using gyro values) instead. why are they using the euler angle instead of its second derivative to control the rate?\n\nis it possible to stabilise a quadcopter using euler angles only?\n", "tags": "control quadcopter pid", "id": "6673", "title": "do you have to have a rate controller for a quadcopter?"}, {"body": "i salvaged some parts off my dead roomba 650, and i'm trying to use the drive motor assembly . i got the pinout of the connector but i don't know what voltage / pwm / other specifications are there for this motor. \n\ni've attached the picture of the drive motor assembly.\n\nany help would be appreciated!\n\nthank you,\npratik\n\n\n\nedit: the image is here: drive motor module\n", "tags": "motor pwm irobot-create h-bridge roomba", "id": "6674", "title": "drive motor voltage / other specifications of roomba 650"}, {"body": "can anyone help me, because i am doing a project on robotical surgeries and i would like someone to help me and advise me. i wonder if anyone could give me some data on tests he or she has run in a surgical robot... thank you for your attention!\nanything else will be much appreciated!\n", "tags": "sensors robotic-arm automatic", "id": "6680", "title": "i am doing a project on robotic surgeries! can anyone help me and give me some details related to this topic?"}, {"body": "what are the major differences between motion planning and trajectory generation in robotics? can the terms be used interchangeably?\n", "tags": "motion-planning", "id": "6683", "title": "what is the difference between motion planning and trajectory generation?"}, {"body": "i have the create 2 and have it hooked up to an arduino. almost all the commands work fine except when retrieving sensor information. if i send a request for packet 18 i get back values that while consistent don't match up, unless i am missing something. so if i press the clean button i get 127 or 11111110 and if i then press spot i get something like 11111010. i might be messing up my endianness but regardless the data isnt formatted how i expected it to be according to the spec sheet. i have 3 create 2s and they all do the same thing. any ideas? i am using a 2n7000 along with the tutorial from the site but i dont think that has anything to do with the formatting of the byte.\n\nthis is the library i am using: https://github.com/domamato/create2\n\nsorry to take so long to get back on this, anyways the data we get is always formatted this way. it is not a baud rate issue since it understands the commands properly.\n\n\n\n\n\nnote that the schedule and clock buttons return nothing\n", "tags": "arduino irobot-create", "id": "6686", "title": "irobot create 2 and open interface 2 spec not syncing up with incoming data"}, {"body": "the links twist could be obtained, and thus the spatial manipulator jacobian could be done, but when it comes to the body jacobian, it is becomes difficult. moreover, the adjoint transformation relates both jacobain, but however that is 4*4 while the jacobian is 6*n; how does it works? as in the picture, he is getting a body jacobian for each link, not one jacobian matrix for the whole robot, i don't know.\nany help is highly regarded.\nlike this example or here for\nfull details\n", "tags": "robotic-arm kinematics inverse-kinematics manipulator jacobian", "id": "6694", "title": "how to find the body jacobain, for each link in a robot manipulator?"}, {"body": "currently i am building a omnidirectional robot with 4 dc motors with embedded incremental encoder.\n\nhowever, with a constant pwm input, i am not able to control the motor to rotate in a \"relatively stable\" state, refer to the figure, it can be observed that the linear speed of the motors can varied in 10cm/s range. i believe one possible reason is the pwm signal generated from my arduino mega controller is not good enough.\n\nand my problem is how can i implement a stable pid controller in this case? as the speed of the motor varies even with the same input, i believe extra work like adding a filter is needed?\n\nany advice is appreciated >.&lt; thank you\n\n\n", "tags": "motor pid", "id": "6696", "title": "dc motor pid control with unstable velocity feedback"}, {"body": "\n\n\n\n\n", "tags": "pid", "id": "6700", "title": "help me in quadcopter controlling"}, {"body": "how do you stream video feed from a camera on a drone? i would think that at high altitudes wi-fi won't work. \n\nso what would you usually do, and how?\n", "tags": "quadcopter cameras", "id": "6701", "title": "drones and camera streaming"}, {"body": "how do you select the following two angles in the design of a rocker bogie system:\n\n\nangle between two arms of the main rocker, and;\nangle between two arms in the bogie.\n\n", "tags": "mobile-robot wheeled-robot", "id": "6703", "title": "angles in a rocker bogie system"}, {"body": "i'm looking for ways to detect human presense behind walls in close proximity (around 10 feet) in whatever way possible! problem is i can't code! (i hope it's ok i'm posting here.) \ni know there are different sensors but they all seem to be for detecting by motion of target humans. \nhow do you detect still persons?\nis there a sound amplification device that magnifies human breathing x 20?\nor detect body heat?\nor pick up radiation waves or something off humans?\n", "tags": "sensors sensor-fusion ultrasonic-sensors", "id": "6705", "title": "detect human in proximity?"}, {"body": "i'm constructing a 2 wheels balancing robot which uses a pid controller.  i've tuned my parameters on numerical simulations based on a continuous inverted pendulum system so that the simulated inverted pendulum balances by controlling the horizontal (linear) cart acceleration $\\ddot{x}$.\n\nnow that i've done this, i want to take the next step and turn my pid control commands into electrical commands onto a dc motor to give the desired linear acceleration $\\ddot{x}$. however i'm not sure how exactly to do this for my specific robot's motors.  are there experimental tests should i run to determine how to convert pid commands into dc motor acceleration commands? or is there a formula to do this based on the motor's specifications?\n\nupdate\n\nthe non-linear dynamic equation i'm using is\n\n$$l\\ddot{\\theta}=gsin(\\theta)+\\ddot{x}(t)cos(\\theta)+ld(t)$$\n\nwhere $\\ddot{x}(t)$ is the linear acceleration, $g$ is the acceleration due to gravity, and $\\ddot{\\theta}$ is the angular acceleration, and $d(t)$ is an external disturbance to the system. to simplify things, i've linearized the equations around $\\theta\\approx0$, yielding\n\n$$l\\ddot{\\theta}=g\\theta+\\ddot{x}(t)+ld(t)$$\n\ni've assumed that the only control input is the cart's linear acceleration $\\ddot{x}(t)$, and chose this control command as $\\ddot{x}(t)=k_1\\theta(t) + k_2\\int_0^t\\theta(t) dt + k_3\\dot{\\theta}$, where $k_i$ are the pid gains.\n", "tags": "motor control pid actuator dynamics", "id": "6706", "title": "converting a linear acceleration command into a dc motor command?"}, {"body": "i need a computer on the board like raspberry pi for vending machine (i want to replace the original controller). this is list of some requirements:\n\n1) it should have pins to connect to the mdb protocol &amp; other stuff through gpio.\n\n2) good performance. there will be a display with browser showing rails application running. i've tried a raspberry pi b+, but it's too slow (it can't even run a browser with speed like a laptop pc). so, i want to choose a more powerful system like odroid, wandboard etc.\n\n3) custom video output. sometimes i need to display fullhd(1920x1080), sometimes i need to show at 768x1024 (yes, the computer should simply rotate video output)\n\n4) i don't want to connect microcomputer to display directly, not through hdmi, dvi or something like this. (this is not required, but very desirable).\n\nplease help me choose. nowadays i try to choose from odroid, wandboard or pandaboard. are there any other computers? what version of the computer is advisable?\n", "tags": "arduino raspberry-pi", "id": "6708", "title": "i need help with choosing a computer on the board"}, {"body": "there is always a way to do this using arduino, rasberry pi etc.  however, in many cases in discussion in forums i've come across things where whole 'logic' can be uploaded to $0.50 chip. instead of $50 dollar part. drastic change. this defines a line between a one time thing that you made as a hobby, and something you can sell around. \n\nso basically if i want led to get brightest at loud sound and almost off on silence.\nor with button that switch to 100% on all the time. \n", "tags": "design circuit", "id": "6714", "title": "what is the cheapest way to make led sensitive to sound around?"}, {"body": "i am a total newbie in robotics so please bare with me. \n\ni have a school project where my team has to design a robot that is capable of picking up 3 golf balls in different sizes at predefined locations. then it has to drop these balls into their respective holes. \n\nwe are using an arduino chip in our robot. \n\ni thought i could perhaps define a path for the robot, an invisible virtual path you may call. so imagining the platform as cartesian plane, can i tell the robot go to where i want it to go? for example, go to (5,12)\n\nor do i need some sort of sensors so the robot figures it out by itself. thanks for your time!  \n", "tags": "arduino sensors navigation", "id": "6715", "title": "is it possible for a robot to navigate through predefined coordinates?"}, {"body": "i am attempting to build a raspberry pi based quadcopter. so far i have succeeded in interfacing with all the hardware, and i have written a pid controller that is fairly stable at low throttle. the problem is that at higher throttle the quadcopter starts thrashing and jerking. i have not even been able to get it off the ground yet, all my testing has been done on a test bench. i have ruled out bad sensors by testing each sensor individually, and they seem to be giving correct values. is this a problem with my code, or perhaps a bad motor? any suggestions are greatly appreciated.\n\nhere is my code so far:\n\nquadserver.java:\n\n\n\nsensor.java:\n\n\n\nedit:\n\ni have re-written my code in c++ to see if it will run faster but it's still running at about the same speed(about 15 ms per cycle or about 66 hz).\n\nthis is my new code in c++:\n\n\n\nin addition two of the motors seem like they are lagging when i turn the quadcopter fast in one direction. also for some strange reason the quadcopter seems less responsive to p gain; i have it at 20 in the c++ version and it is working about the same as when i had it at 1.5 in the java version.\n\nedit:\n\nafter doing some more testing i have determined that reading from the mpu6050 and writing to the pca9685 board that i am using to control the escs is the source of the delay. does anybody know how to speed this up?\n\nedit:\n\ni managed to speed up my code to about 200 hz by changing the i2c baud rate, but the quadcopter is still thrashing. i have spent hours trying to tune the pid controller, but it doesn't seem to help at all.\n", "tags": "quadcopter pid raspberry-pi", "id": "6720", "title": "raspberry pi quadcopter thrashes at high speeds"}, {"body": "i and my team have to design a robot using an arduino chip. the objective of the robot is to grab golf balls at a set of golf pins at different heights and pre-defined locations. we couldn't figure out a possible mechanism that could collect the balls and drop them into the trailer except for a robot arm. however, we don't have experience and time in designing a sophisticated system for the arm like recognizing where the ball is and then grabbing it accordingly. what would a feasible option be compared to a non-sophisticated  robot arm?\n\nnote:the robot must be autonomous.\n", "tags": "arduino control robotic-arm", "id": "6721", "title": "how feasible is the idea of operating a robotic arm in a non-sophisticated way?"}, {"body": "i'm developing a stabilisation system for an 'off-the-shelf' quadcopter by using an arduino mega and an imu. the imu is reading the angle of the quad, calculating motor commands by using a pid controller and applying them to the motors. it works well when constrained in a test bed, however in reality, although the quad is straight and level, it's drifting to one side because of its recent motor commands correcting the pitch/yaw. is there any way i can (without using a vision system) keep the quad in one place without drifting?\n\ni've looked into obtaining velocity by integrating the acceleration value, however it's extremely noisy and doesn't give a meaningful reading.\n", "tags": "arduino quadcopter pid imu rcservo", "id": "6733", "title": "minimising lateral drift in a pid (arduino) controlled quadcopter using a 6dof imu"}, {"body": "we have an epilog laser cutter around here and i was wondering if it would possibly work as a base for a 3d printer? here is a dropbox photo album of the laser cutter. i am thinking i will have to get a new control system but i am unsure if i will be able to use the motor controllers or if they are embedded in the current control's board. i am also unsure if it has fine enough control on the z axis but if not that can be modified.\n\nwhat would be a good head to look at?\nany other thoughts?\n", "tags": "laser 3d-printing", "id": "6739", "title": "turning an epilog laser into a 3d printer?"}, {"body": "i'm trying to understand how to obtain the kp, ki, kd values after finding a combination of k and a that works for me. do i just expand the equation and take the coefficients? \n", "tags": "control pid tuning", "id": "6740", "title": "in the pid equation k[((s+a)^2)/s] what values correspond to the pid coefficients kp, ki, kd?"}, {"body": "i am a newbie in robotics. as far as i know, there're generally two ways to acquire depth information of a scene. one is the stereo vision method which applies two cameras. the other is the rgbd sensors such as kinect and primesense. it seems that both methods are in use currently. however, i do not know what are their advantages against each other.\n\ni think the kinect is a perfect solution over stereo vision ignoring its expense. so i have two questions here:\n\n\nis there advantages of binocular methods over kinect besides expense?\nas i know, both the two methods are confined with a limited distance detection range. in real world applications, we sometimes also need depth data at distance. is there a method that we can acquire or estimate data information at far distance( not considering  laser-based detectors ).\n\n\nfurther more, my application may be a small flight vehicle. which method and equipment should i choose? will the traditional binocular camera be too slow for my application?\n", "tags": "computer-vision stereo-vision", "id": "6741", "title": "how many methods can i use to acquire depth data?"}, {"body": "i'm building a submersible rov, so i need a way to navigate. so using a compass would help but this brings up the question, does an electronic compass work underwater?\n\nmy thoughts are the water might act as a faraday cage, and therefore interfere with the magnetic field. therefore it might not even work. maybe a gyroscope might be a better solution.\n", "tags": "sensors compass", "id": "6750", "title": "does an electronic compass work underwater"}, {"body": "i'm working on the dynamics model of a rrrr articulated robot, i'm following euler-lagrange approach and developing my code in m-file in matlab; i'm looking for dynamic model of this form: $$\nd(q) \\ddot{q} + c(q,\\dot{q})\\dot{q}+ g(q) = \\tau\n$$\nwhere $d$ and $c$ are $4 \\times4$ matrices and $g$ and $\\tau$ (torque) are $4\\times1$ vectors; by formulating the kinetic and potential energies;;\n\nthe problem is that, i'm getting very long equations, and the term in $d$ matrix are very huge and nonlinear, involving sin and cos; i'm talking about a several pages per equation;\nafter i published the code - 7 pages - and the output i got around 45 pages in total;\ni searched around there was some guy he faced the same problem before, but there was no helpful proposal.\n\nany suggestions ?? \n", "tags": "robotic-arm kinematics dynamics matlab", "id": "6754", "title": "why i'm getting very long terms in the inertia matrix (or dynamics model) of the robot using matlab script?"}, {"body": "i'm currently calibrating the mpu6050 chip using an arduino mega 2560. i am using the j rowberg 12c dev libraries. i can get it to print raw accelerometer and gyroscpe values (very unstable, wildly changing values). in the digital motion processing chip library, i can get it to print euler angles, quaternions, real world acceleration and actual acceleration but there is no option to get gyroscope data. \n\ncan i use the dmp library to get gyro data or is it only possible to get raw unprocessed gyro values?\n", "tags": "arduino imu accelerometer gyroscope", "id": "6764", "title": "how do i get mpu 6050 gyroscope data using \"mpu6050_6axis_motionapps20.h\" library"}, {"body": "my question is general so please bear with me. i'm now interested in buying a quadcopter and develop some functions that it does for example an android app to control it, or objects detection. \nso my question is what are the available quadcopters which has a software that allows me to do such things not just a flying toy?\n\np.s: i'm asked to buy a kit within 600$ and not build it by myself\n", "tags": "quadcopter", "id": "6765", "title": "open source software for quadcopters"}, {"body": "i have to measure the frequency of a little circle in rotation. you can image this circle flying in air, because this circle can't touch anything that is not in rotation. so i can't use some simple trick to count the number of complete rotation in an amount of time.\n\nso i supposed my only chance was to use an accelerometer, a gyroscope, or a magnetometer. the accelerometer can detect the centripetal acceleration, and the gyroscope and the magnetometer with some calculus directly the frequency.\n\nthe problem is the high frequency of this circle (can reach up to 50 hz). doing some simple calculus we know we need a gyroscope that can measure big angular velocity: 50*360\u00b0/s = 18.000\u00b0/s. also the accelerometer need big range of values (the radius of the circle is only 5 cm): w (angular velocity) = (2 * 3,14) / (1/50) = 314 rad/sec acc_centr = w^2 * r = 98596 * 0,05 ~ 5000m/s^2 ~ 500g\n\nnow i have seen there are some accelerometer or gyroscope for industrial purposes with enough range, but my question is:\n\nhow i can understand if a magnetometer can used is this kind of application? considering there is no magnetic field near the circle, can a magnetometer be used to measure quickly change in inclination? in the datasheet i can read how ofter the sensor can communicate with my arduino, but nothing about how quick the rotation can be.\n\nis the reason that a magnetometer don't have the limits of a gyroscope or an accelerometer?\n", "tags": "electronics", "id": "6767", "title": "magnetometer to measure high angular velocity in small object"}, {"body": "i am currently doing a project for school and we are told that we must use a micro controller that ends up controlling some external hardware, now i know the crazyflie is controlling the motors which counts as external hardware but is it a micro controller? my second question is i want to purchase the kit so i can assembly it myself however i saw that you can use an expansion board so you need not solder and also i plan on not buying a remote its possible to control the crazyflie via my iphone correct? i would appreciate it if someone could answer my questions. thank you in advance\n", "tags": "quadcopter", "id": "6772", "title": "is the crazyflie control board considered a microcontroller"}, {"body": "i'm very passionate about robots from my childhood.i'm a java developer.\ni love sci-fi movies.i have a little bit knowledge in embedded systems and electronics.\nmy ambition is to build a robot like jarvis (in iron man movie).which is a voice controlled robot.i would like to implement that in my house as a home automation system.it would take voice as input and take appropriate action.. please help me to do this. \nany kind of help is appreciated..\n", "tags": "mobile-robot quadcopter microcontroller mechanism embedded-systems", "id": "6776", "title": "iron man jarvis like robot"}, {"body": "i'm trying to send some commands to the roomba. however is behaving strange.\n\nthis is the manual that i'm using.\nhttp://www.irobot.com/~/media/mainsite/pdfs/about/stem/create/create_2_open_interface_spec.pdf  \n\nfirst of all. i have consulted several manuals, some of them say that the default baudrate is 115200, however it works for me at 57200. \n\ni'm trying to get a response from the roomba sending the following comand \n\nexamples:\n\u2022 to turn on irobot create\u2019s play led only:\n128 132 139 2 0 0\n\nhowever, the roomba goes crazy and start going around. any idea what's happening or what i'm not doing? or what should i do first? \n\nthank you. \n", "tags": "irobot-create roomba", "id": "6782", "title": "sending commands to roomba from pc"}, {"body": "look at this robot here http://www.meccanotec.com/step781b.jpg\ni can see rods which have a lot of holes and planes which also have holes. this seems to be a way to create flexibility in how the things are connected together to create the final robot. is there a name for this type of equipment, metals with holes. where can i get it? i am aware of people using lego blocks to create robots, but am not sure about what these metal rods and plates with holes are.\n\nis there a free application in which i can design a mechanical structure like the one in the image and add gears and then simulate it to see how it will rotate and bend should a real robot like that be created?\n\nwhat would be the quickest way to create a robot like this?\n\nedit: thankyou; frank and lanyusea. if one wants to do a simulation of the mechanical model, in others words play with the robot on the computer before actually building it (with all those gears in action), which software is most suitable for that purpose?\n", "tags": "robotic-arm", "id": "6783", "title": "what equipment has been used to design this robot"}, {"body": "i am talking about robots like this one: http://www.meccanotec.com/step781b.jpg\n\nhow would a person know what type of motor to use in design of such a robot? what i want to understand is that stepper motors have different step sizes, different torques among other things. how do we determine what type of stepper motor is most suitable to be used in a given robot?\n", "tags": "robotic-arm stepper-motor", "id": "6784", "title": "how to know what type of stepper motors to use when designing a robot"}, {"body": "i want to make a robotic workshop.. i recruited 10 members to work...  please give some tips about robotic workshop\n", "tags": "irobot-create", "id": "6787", "title": "is it tough to make a robotic workshop of your own"}, {"body": "i recently got a lynx biped scout and found that it is really hard to actually come up with a working \"gait\" or walking pattern. \n\nmaking a servo move is easy, that's not the problem, i previously built a robotic arm from scratch (i have pictures if anyone is interested) and that one can be controlled via arduino and a few potentiometers as it only has 4 degrees of freedom so it's not too hard to keep track of the different limbs.\n\nhowever the scout is a different beast entirely. it's a purpose built kit with 12 servos and to control them i'm using the lynx ssc-32 sequencer which is distributed freely on their website. the only problem is that making them all move in sequence to produce a convincing walking motion is actually really hard.\n\nhas anyone got any patterns for this robot they would be happy to share? \n", "tags": "walk", "id": "6791", "title": "does anyone have any walking patterns for a biped scout? (lynxmotion)"}, {"body": "i have a inverted inertia wheel pendulum.\ni suppose that if i have a wheel with larger inertia at its top, the system would be more stable.\n\nhow can i prove or disprove my conjecture?\n", "tags": "control pid stability", "id": "6792", "title": "how can i know which system is easier to control using pid controller?"}, {"body": "i was studying the basics of legged locomotion and came across the unilateral force and torque constraints at the foot-ground interface.\n\ni understood the implication of the unilateral constraint on the force ( the ground can only push the foot but not pull it) but i am unable to understand what does the unilateral torque constraint translate into physically in this case. can anyone clarify it?\n", "tags": "mechanism motion force legged walk", "id": "6801", "title": "unilateral torque constraint on the foot-ground interface"}, {"body": "i am using an arduino mega with an mpu6050. i can get gyroscopic data as well as euler angles. the problem is my gyro data keeps going back and forth between 0 and -1 despite me not moving it at all (it stays on -1 the most). what can i do to filter what i assume is noise? i am going to use the gyro data for a quadcopter pid rate controller so i cant really have it telling me i am rotating at -1 deg/sec. that would be catastrophic for the quadcopter\n", "tags": "arduino imu accelerometer gyroscope", "id": "6804", "title": "how can i filter gyroscopic data?"}, {"body": "i am building a humanoid robot with dc motor actuated fingers. there are 16 brushed dc motors to be position controlled with help of hall effect sensors implanted at the joints of each fingers. i need a developed driver board to control these 16, 3 watt, 12 v,dc motors.\nalso each motor is equipped with an incremental encoder for speed control. \n\nthank you\n", "tags": "motor servomotor driver", "id": "6807", "title": "driver board to control 16 brushed dc motors"}, {"body": "basic question concerning sensor fusion:\n\na standard 10 dof imu, i mean this cheap things for the tinkerer at home, provides 10 values:\n3 accelerometers\n3 gyroscope\n3 magnetic field measurements\n1 pressure sensor (+ 1 temperature) \n\ni know that the accel-data provide long term stability, but are useless for short term and the gyroscope is more or less vice versa. \n\nso there are tons of strategies to \"marry\" this values, but how does the magnetic field measurement fit into this framework?\n\nbasically the magnetic field measurement should provide an attitude, too. like the other two sensors combined. i guess this measurement alone is neither reliable.\n\nso how do all these sensors fit together?\n\nbr \n", "tags": "imu sensor-fusion", "id": "6808", "title": "provides a 10 degree-of-freedom imu reduntant data?"}, {"body": "i emphasize that my question is about sampling, not resampling.  \n\ni'm reading the probabilistic robotics book by thrun et al, chapter 4 on non-parametric filters.  the section on particle filters has an algorithm which states that for each particle index $m$ (see line 4):  \n\nsample $x_t^{[m]} \\sim p(x_t|u_t,x_{t-1}^{[m]})$\n\nthe text's explanation of this step is quoted as:\n\n\n  line 4. generates a hypothetical state $x_t^{[m]}$ for time t based on\n  the particle $x_{t-1}$ and the control $u_t$.  the resulting sample is\n  index by $m$, indicating that it is generated from the $m$-th particle\n  in $\\chi_{t-1}$.  this step involves sampling from the state\n  transition distribution $p(x_t|u_t,x_{t-1})$.  to implement this step,\n  one needs to be able to sample from this distribution.  the set of\n  particles obtained after $m$ iterations is the filter's representation\n  of $\\bar{bel}(x_t)$.\n\n\nif i understand correctly, this step says that the m-th sampled particle $x_t^{[m]}$ is obtained by advancing the previous m-th particle with control command $u_t$.  i assume that the motion is not deterministic, so the result of this motion is a conditional probability, based on the particle's previous position $u_t$.  \n\nhowever, i'm confused over how exactly to construct this conditional probability $p(x_t|u_t,x_{t-1}^{[m]})$.  is this information usually given?  or is it constructed from the distribution of the other particles?  \n", "tags": "particle-filter", "id": "6810", "title": "particle filter sampling step"}, {"body": "using the sci messages, i would like to determine the current operating mode or state of a irobot roomba 780. finally, i would like to detect and separate four states: \n\n\ncleaning\nin docking station\nreturning to docking station\nerror (e.g. trapped on obstacle)\n\n\nwhat is a fast and reliable way to detect those states using sci data?\n\nthe roomba sci sensor packets \"remote control command\" and \"buttons\" seem to return the currently called commands and not the currently executed ones.\n", "tags": "roomba", "id": "6816", "title": "determine current roomba state / operating mode"}, {"body": "suppose i have a mechanical system which is free to move on a given rail [-5m, 5m] like a motorized cart. the whole system can be mathematically expressed through a linear timeinvariant system equations.\n\nif i need to control only the position (for example saying the controller: \"move to +2.3\") i can simply design a pid controller that, given a set point moves the cart to that position.\n\nnow i need much more and i want to control the position and the velocity of the cart. so i need for example to say: \"move to +2.3 with a specific velocity's profile\". of course the vel = 0 at the end position.\n\nquestion: how should i design such a controller? do i need specifically a special type of controller? or i have a huge choice?\n\nany help, graph, link and example is really appreciated.\n\nregards\n", "tags": "control pid", "id": "6817", "title": "basic general question about controllers"}, {"body": "is it possible to remote control a 'robot' relative to the driver with an angle sensor (or any other sensor)?  for example, if the robot starts in this position\n\n\n\nand the joystick is in this configuration\n\n\n\nthen if the robot turns around,\n\n\n\npushing the controller forwards will still make the robot go forward\n\n\n\neven though from the robot's pov, he's going backwards.\n\nany ideas/solutions?\n", "tags": "control sensors", "id": "6818", "title": "remote control relative to driver"}, {"body": "so i was making sure my circuit for an airboat i was working on was safe. and checking a motor, it has 35 amps max current running 11.1v at 1000. (however my esc has a 30 current, 40 burst amps). \n\nthe recommended tested prop for this is a 11x9 3-blade and runs the motor at 20 amps. doing some quick calculations via on online calculator, it appears to give a value of 5.4lbs of force (way off to the 2.65 lbs measured, but regardless...). when i type in a prop i want (13x6) it gives a thrust value of 7.53 .\n\nnow, if the motor current is proportional to the thrust, 5.4 lbs / 20 amps = 7.53 / running amps. and therefore the amount of amps the prop would be, in theory a little less than 30 if this is indeed the case. thus safe for my application. \n\nthis would also make sense in physics terms as power = current*voltage which is proportional to thrust, but just need to make sure.\n\nso does this thought process work for choosing a prop? my device will  be doing very short runs (less than 25 seconds), so near-boundaries should be safe...\n", "tags": "quadcopter brushless-motor", "id": "6821", "title": "is motor current proportional to thrust?"}, {"body": "i want to design a data logger for my quadcopter using the arduino mega board. i want to record the roll, pitch and yaw angles each second or 5 seconds, so they can be viewed later after a flight has ended. there's just one thing i don't understand, and that's how to translate the pitch/roll/yaw angles into a pulse of a specific length that the flight controller receives.\n\nfor example, when i press the control for the pitch, the transmitter sends out a pulse to the receiver of the drone and the speed of the drones' motors change accordingly for it to pitch either forward or backward. i can tap into these commands between the flight controller and the transmitter, and be able to record the length of the pulse that was sent out. however, what is the link between the pitch angle and the size of the pulse? basically, how can i convert the pulse that was recorded by the arduino board and convert it into the pitch angle in degrees? \n\ngenerally, for the transmitter i use, a 1500us-pulse means zero pitch; from 1501-2000 means pitch forward, and from 1000-1499 means pitch backwards (of course, the actual values vary slightly, but this is just a general reference for this question). so for instance, if i sent a pulse of 1400us, how would that translate into an angle in degrees? what's the formula to convert it?\n\ni hope i'm clear, and if this question sounds stupid, please excuse me, but i haven't been able to find good information on it!\n\nthanks!\n", "tags": "arduino quadcopter", "id": "6822", "title": "what is the link between a quadcopter transmitter pulse and the roll/pitch/yaw angles?"}, {"body": "\n\nhow could i tune the above piv controller? i am trying to get the system to have a settling time of &lt; 1 second, p.o &lt; 15% and zero steady state error.  \n", "tags": "control", "id": "6824", "title": "how to tune a piv controller?"}, {"body": "i'm working on a building a rover and would like some advice on selecting motors. in particular, i want to understand the difference between precision and planetary gear motors.  my robot will way about 10-15lbs i think and would like it to be responsive and quick. i have two sabertooth 2x12 motor controllers (which can supply up to 12amps). i have been looking at these motors and i am not sure which is better choice for my application.  \n\nthese are the two sets of motors i am thinking about.\nhttps://www.servocity.com/html/precision_robotzone_gear_motor.html\nhttps://www.servocity.com/html/3-12v_precision_planetary_gear.html\n\ngoogling does provide some info on planetary gears, but the application of these two is still is unclear to me.\n\nthanks\n", "tags": "mobile-robot motor gearing", "id": "6826", "title": "difference between planetary and precision gear motors"}, {"body": "there are several robotics datasets for slam, like this one.\n\nin this webpage you can see that the depth image is scaled by a factor of 5000, so that float depth images can be stored in 16 bit png files:\n\n\n\ni do not understand why this value is chosen. why not simply 1000, so that there is a conversion of meters to millimeters? \n", "tags": "slam", "id": "6829", "title": "robotics slam datasets - scaling factor"}, {"body": "i am trying to program the create2 irobot using python. there is a script called openinterface.py. where can i download this script?\n", "tags": "software irobot-create python", "id": "6831", "title": "where can i get openinterface.py?"}, {"body": "i'm following this getting started tutorial, connected the board to usb and it's detected as mass storage, got the driver installed (win 64), and at the third step i wasn't able to connect to beaglebone webserver at 192.168.7.2, anything i did wrong? please help.\n\nhere is some troubleshooting info from getting started page, i've followed them all. i'm using chrome, tried the node-webkit based application, not in a virtual machine, not using ssh just trying to access the webserver.\n\n\n  troubleshooting\n  \n  do not use internet explorer.\n  \n  one option to browse your board is to use this node-webkit based\n  application (currently limited to windows machines):\n  beaglebone-getting-started.zip.\n  \n  virtual machines are not recommended when using the direct usb\n  connection. it is recommended you use only network connections to your\n  board if you are using a virtual machine.\n  \n  when using 'ssh' with the provided image, the username is 'root' and\n  the password is blank.\n  \n  visit beagleboard.org/support for additional debugging tips.\n\n\nupdate:\n- i tried to install ubuntu on my machine and connect the beaglebone, it need not any driver and i can immediately access the webserver after ejecting the mass storage, enabling the 'usb-to-ethernet interface'. however in windows ejecting the mass storage still does nothing. still trying to make it connect in windows.\n", "tags": "beagle-bone", "id": "6833", "title": "can't connect to beaglebone webserver?"}, {"body": "i plan to use p8.13 and p8.15 of the beaglebone in a i2c bitbang mode.\ndo i need to use external pull up resistors in my circuit? or can i use the internal pull up which is available on the beaglebone black itself?\n", "tags": "beagle-bone circuit", "id": "6835", "title": "internal pullup sufficient for i2c in beaglebone black?"}, {"body": "i'm looking to build a new (first) quadcopter without the conventional flight controller and radio, with an onboard rpi and applying some newfound knowledge on autonomous control to improve my coding skills.\n\nalthough, since i've never actually built a quadcopter, i don't actually have any experience in using brushless motors.\n\ni'll be using a rpi b+, so controlling them over i2c was something i looked into. the b+ though only has two i2c interfaces. it also only has two hardware pwm pins and i'm unsure whether software pwm would be enough. i found the afro simonk-based escs from hobbyking which have i2c (intended for the mikrokopter).\n\ni've looked around and people have used the adafruit 16-channel pwm/servo drivers to control them. is this an option to look into? or is there perhaps a better way?\n\nalso, would be it particularly safe if the rpi is run off the esc's bec? it's confusing because, when the esc is powered on, well, it'll be powered on before the rpi comes up. what do escs do when they have bad input?\n", "tags": "quadcopter raspberry-pi brushless-motor esc multi-rotor", "id": "6837", "title": "controlling an esc for brushless motors with an rpi"}, {"body": "i have a task of developing a simulation of an adaptive robot control system but i don't seem too have anyone to discuss my uncertainties with. i want to keep the simulation as simple as possible as i have a very tight deadline and it's only a one off project that most probably will never be used in my life again. \n\nthe minimal behaviour that the agent is supposed to exhibit is wall and obstacle avoidance. it can be extended to avoiding small objects and exploring large ones. \n\ni've decided to go with a simple feedback control system. \nto begin with i'm struggling to decide how to represent the map of agent's environment. what i mean is, what if i want a wall to be from coordinate [0,0] to [0.5]. i could hard code it, e.g. have a matrix with coordinates of all obstacles but how small units do i make... i.e. what if i have two neighbouring coordinates [0,0.01] and [0,0.02] but the agents gets a 'clear to go' to coordinate [0,0.05]. in this case it doesn't know that it actually is about to walk into a wall. i've heard of something called occupancy grid map but i don't exactly get how it works and how to implement it. \n\nanother thing that i am struggling with is how do i distinct between a wall and an obstacle? and then, how do i let the agent know how big that obstacle is so that it can either avoid it or explore it. \n\neh, i'm really puzzled with this project.\ni would really appreciate any thoughts or directions. thank you. :-)\n", "tags": "control localization simulation", "id": "6838", "title": "implementation of wall and obstacle avoidance"}, {"body": "i have a 4-dof robot arm system with 4 revolute joints arranged in an open-chain fashion like below:\n\n \n\nassume that each link\u2019s mass is a point mass located at p_i and each link\u2019s center of mass is at p_i.\n\nwhat i am trying to do is calculate the center of mass jacobian matrix of the arm.\ni found some related materials online center of mass jacobian.but i am still not very sure about how to calculate it. could anybody give me some hint? thanks!\n", "tags": "robotic-arm jacobian", "id": "6842", "title": "how to calculate the center of mass jacobian matrix of a robot arm"}, {"body": "been working on a robot recently which uses ultrasonic sensors for an integral part of the navigation.\nwhile testing the sensors i noticed a strange behaviour, the sensors seem to frequently stop functioning and bring the entire arduino mega i'm working with to a stop. the strange part is that these stops seem to be entirely random, on some occasions the sensor will read values consistently (at maybe 20 vals per second) for 10+ seconds, then all of a sudden the sensor will slow to reading only 2-3 values per second with stalls between.\n\ni have tested several sensors and different codes for pinging distances yet the problem has persisted.\n\nthis leads me to believe the issue is with the arduino mega itself, but i am unsure how to verify this. any advice?\nthanks in advance!\n\nps: other pins on the mega seem to be working fine, i.e. analog pins for ir reflectance sensors and pwm pins for driving 2 dc motors.\n", "tags": "arduino ultrasonic-sensors", "id": "6843", "title": "sporadic sensing rates for hc-sr04 ultrasonic distance sensor"}, {"body": "i want to build a quad copter. i want to know how do we calculate thrust or lift generated by using a motor, i am not aware about the capacities of motor. so can you explain how to calculate thrust or lift generated by assuming a motor. and what is the maximum payload that a quad copter can lift for a given thrust. \n", "tags": "quadcopter", "id": "6844", "title": "aerodynamics of quadcopter"}, {"body": "im looking for a good source for robotic components like sheel/tracked robot chasis, motors, sensors, communication and mechanics. i thought about using raspberry and arduino as platforms for automation, is that an good idea? im asking as i dont know yet much about the motors/drives uses for powering robots.\n\nthanks!\n\nuli\n", "tags": "mobile-robot motor", "id": "6845", "title": "recommendation for good source of robotic components"}, {"body": "i have an arduino mega board and the adafruit ultimate gps logger + gps module shield. i have these two connected together using headers and have the entire thing mounted on my drone. currently, i have a code that i found online and modified slightly to get gps coordinates in nmea format and parse them for the information i actually want. i can store these in an sd card.\n\nthe thing is, i want to use the arduino gsm shield to somehow send this data, either from the sd card or directly, to a folder in dropbox. i have no idea how to do that, if it's possible at all. i just started working with arduino about a month ago, so i apologize if my question sounds particularly noob-ish.\n\ncould anyone on this forum at least guide me on how to approach this problem? thanks!\n", "tags": "arduino gps", "id": "6846", "title": "datalogging from arduino mega to dropbox"}, {"body": "i'm trying to do graph optimization with g2o, mainly in order to perform loop closure. however finding minimal working examples online is an issue (i've found this project, as well as this one. the second one though has the form of a library, so one cannot really see how the author uses things.)\n\nin contrast to online loop closure, where people update and optimize a graph every time they detect a loop, i'm doing graph optimization only once, after pairwise incremental registration. so in my case, pairwise registration and global, graph-based optimization are two separate stages, where the result of the first is the input for the second.\n\ni already have a working solution, but the way that works for me is quite different from the usual use of g2o:\n\n\nas nodes i have identity matrices (i.e. i consider that my pointclouds are already transformed with the poses of the pairwise reg. step) and \nas edges, i use the relative transformation based on the keypoints of\nthe pointclouds (also the keypoints are transformed). so in this case\ni penalize deviations of the relative pose from the identity matrix.\nas information matrix (inverse of covariance) i simply use a 6x6\nidentity matrix multiplied by the number of found correspondences\n(like this case). \nthe result of the graph is an update matrix,\ni.e. i have to multiply with this the camera poses. \n\n\nalthough this works in many/most cases, it is a quite unusual approach, while one cannot draw the graph for debugging (all nodes are identities in the beginning, and the result after optimization is a 3d path), which means that if something goes wrong getting an intuition about this is not always easy.\n\n  \n\nso i'm trying to follow the classic approach:\n\n\nthe vertices/nodes are the poses of the pairwise registration\nthe edges are the relative transformations based on the keypoints/features of the raw pointclouds (i.e. in the camera frame, not transformed by the poses of the pairwise registration)\nthe output are the new poses, i.e. one simply replaces the old poses with the new ones\ndrawing the graph in this case makes sense. for example in case of scanning an object with a turntable, the camera poses form a circle in 3d space.\ni'm trying to form all the edges and then optimize only at one stage (this doesn't mean only 1 lm iteration though).\n\n\nhowever i cannot make things running nicely with the 2nd approach.\ni've experimented a lot with the direction of the edges and the relative transformation that is used as measurement in the edges, everything looks as expected, but still no luck. for simplicity i still use the information matrix as mentioned above, it is a 6x6 identity matrix multiplied with the number of correspondences. in theory the information matrix is the inverse of covariance, but i don't really do this for simplicity (plus, following this way to compute the covariance is not very easy).\n\n  \n\nare there any minimal working examples that i'm not aware of?\nis there something fundamentally wrong in what i describe above?\nare any rules of thumb (e.g. the first node in both approaches above is fixed) that i should follow and i might not be aware of them?\n\nupdate: more specific questions\n\n\nthe nodes hold the poses of the robot/camera. it is unclear though at which reference frame they are defined. if it is the world coordinate frame, is it defined according to the camera or according to the object, i.e. first acquired pointcloud? this would affect the accumulation of the pose matrices during incremental registration (before the g2o stage - i try to form and optimize the graph only once at the end, for all the frames/pointclouds).\nthe edge (src->tgt) constraints hold the relative transformation from pointcloudsrc to pointcloudtgt. is it just the transformation based on the features of the two in the local coordinate frame of pointcloudsrc? is there and tricky point regarding the direction, or just consistency with the relative transformation is enough?\nthe first node is always fixed. does the fixed node affect the direction of the edge that departs/ends_up from/at the fixed node?\nis there any other tricky point that could hinter implementation?\ni'm working in millimeter instead of meter units, i'm not sure if this will affect the solvers of g2o in any way. (i wouldn't expect so, but a naive use of g2o that was giving some usable results was influenced)\n\n", "tags": "slam", "id": "6849", "title": "graph optimization with g2o"}, {"body": "i need the specifications for the create 2. i need it for research purposes. so i think i'm going to need a high computational computer on board.\n\nplease suggest some nice configuration. \n", "tags": "irobot-create", "id": "6852", "title": "i need the specifications for irobot create 2"}, {"body": "we bought a new create2 robot and started using it. but when we issue the dock command the robot moves for a bit and does not go back to the base.\n\nthe base is not hidden or obstructed and the create2 is just a couple of feet away. we need help to figure out why it does not see the base.\n\njust to clarify that even using the dock button on the create2 does not make the create2 go back to the base\n", "tags": "irobot-create", "id": "6853", "title": "dock command does not seem to work"}, {"body": "my name is dylan we are doing a project on irobot create and we would like to know the specifications graph showing the battery discharges in volt per time and my robot is the irobot ceate 1.\n\nthe battery is the roomba advanced power , it's a 14.4v nickel metal hybride battery pack and she deliver 3000mah.\n", "tags": "irobot-create", "id": "6854", "title": "the specifications graph showing the battery discharges in volt per time"}, {"body": "i'm trying to implement the tracking problem for this example using pid controller. the dynamic equation is \n\n$$\ni  \\ddot{\\theta} + d \\dot{\\theta} + mgl \\sin(\\theta) = u\n$$\n\nwhere \n\n$\\theta$ : joint variable. \n\n$u$ : joint torque\n\n$m$ : mass. \n\n$l$ : distance between centre mass and joint. \n\n$d$ : viscous friction coefficient\n\n$i$ : inertia seen at the rotation axis.\n\n$\\textbf{regulation problem:}$\n\nin this problem, the desired angle $\\theta_{d}$ is constant and $\\theta(t)$ $\\rightarrow \\theta_{d}$ and $\\dot{\\theta}(t)$ $\\rightarrow 0$ as $t$ $\\rightarrow \\infty$. for pid controller, the input $u$ is determined as follows\n\n$$\nu = k_{p} (\\theta_{d} - \\theta(t)) + k_{d}( \\underbrace{0}_{\\dot{\\theta}_{d}} -  \\dot{\\theta}(t) ) + \\int^{t}_{0} (\\theta_{d} - \\theta(\\tau)) d\\tau\n$$\n\nthe result is \n\n\n\nand this is my code  \n\n\n\nand  is \n\n\n\n$\\textbf{tracking problem:}$\n\nnow i would like to implement the tracking problem in which the desired angle $\\theta_{d}$ is not constant (i.e. $\\theta_{d}(t)$); therefore, $\\theta(t)$ $\\rightarrow \\theta_{d}(t)$ and $\\dot{\\theta}(t)$ $\\rightarrow \\dot{\\theta}_{d}(t)$ as $t$ $\\rightarrow \\infty$. the input is \n\n$$\nu = k_{p} (\\theta_{d} - \\theta(t)) + k_{d}( \\dot{\\theta}_{d}(t) -  \\dot{\\theta}(t) ) + \\int^{t}_{0} (\\theta_{d}(t) - \\theta(\\tau)) d\\tau\n$$\n\nnow i have two problems namely to compute $\\dot{\\theta}_{d}(t)$ sufficiently and how to read from  file since the step size of  is not fixed. for the first problem, if i use the naive approach which is \n\n$$\n\\dot{f}(x) = \\frac{f(x+h)-f(x)}{h}\n$$\n\nthe error is getting bigger if the step size is not small enough. the second problem is that the desired trajectory is stored in  file which means i have to read the data with fixed step size but i'v read about  which its step size is not fixed. any suggestions!\n\n\n\nedit:\n\nfor tracking problem, this is my code \n\n\n\n\n\n\n\n\n\ntrajectory's code is \n\n\n\nthe result of the velocity is \n\n\n\nis this correct approach to solve the tracking problem?\n", "tags": "control pid dynamics", "id": "6859", "title": "how to implement tracking problem with pid controller"}, {"body": "i am using a mx 106-r dynamixel servo for a project that i am making. \ni am making a robotic arm controlled by this servo. \ni accidentally moved the horn of the servo while it was on and hence due to excess current input, it wont work anymore. \ni suspect that the h-bridge inside the motor got burnt. \n\ncan somebody tell me what exactly went wrong ?\nhow can i test the motor ?\nhow can i repair it (if possible) or else where can i find a service centre (i live in india).\n\ni am in deep trouble right now. please help !\n", "tags": "robotic-arm servos h-bridge", "id": "6860", "title": "dynamixel mx-106-r burnt"}, {"body": "i want to measure the real time rpm of the wheels. i think incremental rotary encoder would be good. but i am confused on how to interface it with dc brushless geared motors. from the images i am not quite sure if only one rotary encoder would suffice or do i need any other sensor also with it?\ni am doing my project on arduino uno.  \n", "tags": "motion forward-kinematics quadrature-encoder", "id": "6862", "title": "how to calculate the real time rpm of motor with rotary encoder?"}, {"body": "i want to build a cheap robot programmable in scratch graphical language, that could be employed during lessons in school. scratch code is interpreted on a pc, so on the robot there should be only the code that receives specific commands (i.e. drive forward) and transmits sensors' measurements. \n\ni'm looking for a wireless technology that will allow me to exchange information between robot and pc with at least 30hz rate. it should also allow to work at least 16 robots simultaneously in the same room and have a range of at least 20m. \n\ni did tests with bluetooth, but sometimes there are connectivity issues, and pairing devices can be a hassle in a classroom. i have also tried wifi modules, but pinging it showed average time of 19ms, but maximum of more than 500ms, so i'm afraid that it won't be able to control linefollower robot for example. \n\ncan you point me to some other, preferably cheap (under 10$ per module) wireless technologies? or maybe my worries about wifi are exaggerated?\n", "tags": "mobile-robot radio-control wireless wifi", "id": "6863", "title": "what wireless technology to use to control robots in classroom?"}, {"body": "i am trying to work with the create2. in using the \"get distance traveled\" command (id 142) i am getting back incorrect data. my simple test case logic is\n\ni am working with the create2_tethereddrive.py example\nand adding this\n\n\n\ni consistently numbers near -25 for moving forward, and +25 for moving backward.\nif i wait for 2 seconds, i get -50 for moving forward, and +50 for moving backward. the documentation says it should return the distance traveled in mm, so these numbers seem to be off by a factor of -8.\n\nanyone have any suggestions? thanks.\n\np.s. i had to add this function to the example as well\n\n\n", "tags": "irobot-create", "id": "6869", "title": "roomba create 2 problem reading distance traveled"}, {"body": "i teach a university sophomore level matlab programming class for engineers, and i am planning on using the create2 for their final project. there is a nice simulator and matlab toolbox for the create, but the toolbox utilizes some of the commands that no longer exist on the create 2, thus it doesn't work correctly. and of course is doesn't support any of the newer commands. in addition, i want to be able to \"cut the cord\" so i am using a raspberry pi on the create to pipe data to the serial port, and tcpip sockets to send the data from a remote computer running matlab to the pi/create. if anyone is working on a similar configuration, i'd love to trade notes and share the pain.\n", "tags": "irobot-create", "id": "6870", "title": "controlling the irobot create 2 with matlab"}, {"body": "are there some genaral rules for the robustness between monocular and stereo vision when considering object detection? i am especially interested in the automotive field - considering distance/obstacle/car detection (see video links below).\n\nsomeone told me monocular vision is more robust than stereo. i guess this may be true if the monocular algorithm is well written (and especially verified over lots of input data)... but once you input (image) data that has not been verified it may probably provide unexpected results, right? with stereo vision one does not really care about the contents of the image as long as texture/lighting conditions allow stereo matching and the object detection is then done within the point cloud.\n\ni consider following usage:\n\n\nmonocular\nstereo\n\n\nthe monocular sample video seems to have sometimes problems detecting the cars in front (the bounding boxes disappear once in a while). the stereo sample seems to be more robust - the car in front clearly is detected in all of sequnce image frames.\n", "tags": "computer-vision stereo-vision", "id": "6880", "title": "monocular vs. stereo computer vision robustness for object detection"}, {"body": "i'm writing some quad copter software and beginning to implement an altitude hold mode. \n\nto enable me to do this i need to get an accurate reading for vertical velocity. i plan to use a kalman filter for this but first i need to ensure that i'm getting the correct velocity from each individual sensor.\n\ni have done this but i'm not 100% sure its correct so i was hoping to get some confirmation on here.\n\nmy first sensor is a lidar distance sensor, i calculated acceleration and velocity using the following code:\n\n\n\nthe second sensor is an accelerometer. i calculated acceleration and velocity using the following code:\n\n\n\nit would be great if someone could confirm if this is correct (or not)\n\nthanks\njoe\n", "tags": "sensors accelerometer lidar", "id": "6882", "title": "calculating acceleration and velocity"}, {"body": "i'm trying to select a brushed dc motor for a project. i tried following the advice on sizing electric motors, mentioned in this question, but a few details were missing, and i'm unsure if i properly followed the procedure.\n\nfor my application, i need:\n\n\nnm = number of motors = 2\nwd = wheel diameter = 12 cm\nwp = estimated weight of platform = 5 kg\nminc = maximum incline under load = 5 degrees\nvmax = maximum velocity under load = 5 km/hr\nfpush = maximum pushing force = 1.25 kg\nur = coefficient of rolling friction = 0.015\n\n\nthese are my calculations:\n\nstep 1: determine total applied force at worst case.\n\n\n\nstep 2: calculate power requirement.\n\n\n\nstep 3: calculate torque and speed requirement.\n\n\n\nare my calculations correct? intuitively, the final  and  values seem right, but my calculation for  doesn't exactly match the one used in the link, which doesn't explicitly do the conversion to radians / second and therefore doesn't result in the proper units.\n\nhere's my python script for reproducing the above calculations:\n\n\n", "tags": "mobile-robot motor design torque force", "id": "6890", "title": "verifying motor selection calculations"}, {"body": "currently im working on a rgb-d slam with a kinect v1 camera. in the front-end the slam estimates the pose with ransac as an initial guess for the icp. with the pose estimation i transform the pointcloud to a pointcloud-scene which represents my map.\n\nto smooth the map im trying to implement a graph optimizing algorithm (g2o). \nuntil now, there is no graph representation in my frontend, so i started to integrate that.\n\nim trying to build a .g2o file with the following fromat:\n\nvertex_se3 i x y z qx qy qz qw\n\nwhere x, y, z is the translation and qx, qy, qz, qw ist the rotation in respect to the initial coordinate system. and,\n\nedge_se3 observed_vertex_id observing_vertex_id x y z qx, qy, qz, qw inf_11 inf_12 .. inf_16 inf_22 .. inf_66\n\ntranslation and rotation for the edge is the pose estimate that i compute with ransac and icp (visual odometry). \n\nnow im getting stuck with the information matrix.\ni read the chapter 3.4 the information filter in thrun's probabolistic robotics and several threads in this forum, such as:\n\nthe relationship between point cloud maps and graph maps\n\nand\n\ninformation filter instead of kalman filter approach\n\nfrom the second link, i got this here. \n\n\n  the covariance update\n  $$p_{+} = (i-kh)p$$\n  can be expanded by the definition of k to be\n  \n  $$ p_{+} = p - khp$$\n  $$ p_{+} = p - ph^t (hph^t+r)^{-1} hp$$\n  \n  now apply the matrix inversion lemma, and we have:\n  \n  $$p_{+} = p - ph^t (hph^t+r)^{-1} hp$$\n  $$ p_{+} = (p^{-1} + h^tr^{-1}h)^{-1}$$\n  \n  which implies:\n  $$ p_{+}^{-1} = p^{-1} + h^tr^{-1}h$$\n  \n  the term $p^{-1}$ is called the prior information,$$h^tr^{-1}h$$ \n  is the sensor information (inverse of sensor variance), and this gives us  $p^{-1}_+$, which is the posterior information. \n\n\ncould you please point this out for me. \nwhat data do i need to compute the information matrix? \n", "tags": "slam kinect matlab", "id": "6895", "title": "rgb-d slam - compute information matrix"}, {"body": "i'm confused about how to compute the error in orientation. all the documents i've read don't explain how to do it.\n\nthe error in position is simply the difference between the points. \n\nlet's assume we have the orientation along the effector axis, and we represent the rotation with quaternions. i have two questions:\n\n\nis describing the orientation with quaternions a good approach?\nhow can we compute the error in orientation with the quaternions to use this in jacobian transpose?\n\n", "tags": "inverse-kinematics jacobian", "id": "6896", "title": "jacobian transpose: how to calculate orientation error"}, {"body": "i'm currently working on my first robotics project using the initio kit from 4tronix powered by raspberry pi. the setup was fairly simple, and i've been testing it out over the last couple of days. all of the sensors work as expected; however, my motor tests are failing. when i input commands to actually move the robot, i can hear the dc motors running but they're not getting enough power to do anything. in the instructions, it says if this issue is encountered, that the power selection jumper might not be set correctly and provides this diagram:\n\n\n\nfor comparison, here's how i have the wiring for the motors setup:\n\n\n\ni'm not entirely sure what it means to have the power selection jumper being set incorrectly and would greatly appreciate it if someone could explain this to me or point out if they see anything wrong with my setup.\n", "tags": "mobile-robot motor raspberry-pi first-robotics", "id": "6900", "title": "low power to motors -- motor power jumper issue"}, {"body": "i've recently been learning about slam and have been attempting to implement ekf-slam in python. i've been using this great article as a guide. some progress has been made, but i'm still confused by certain stages.\n\nfirstly, does the inverse sensor model have to compute range and bearing, as opposed to cartesian coordinates? why is this approach used?\n\nsecondly, what format should my robot provide its heading in? currently i just use a running offset from the origin angle (0), without wrapping it between 0 and 360. turning right yields positive degrees, and left negative. i ask this as i assume the sensor model expects a certain format.\n\nthirdly, when computing the jacobians for adding new landmarks, (page 35) is jz simply the absolute rotation of the robot (-540 degrees for example) plus the bearing the landmark was detected at?\n\nand finally, what's the best approach for managing the huge covariance matrix? i'm currently thinking of a good way to 'expand' p when adding new landmarks.\n\nhere's my current implementation: http://pastebin.com/r7wumgy7\n\nany help would be much appreciated! thanks.\n", "tags": "slam ekf python", "id": "6905", "title": "slam noob here, a few questions regarding ekf-slam"}, {"body": "i would like to build quad that uses bigger propellers like 15\" . my question is what kind of motor shall i use ? low or high kv? do all motors support this kind of propellers ? will they burn because of it ?i found they say cw and ccw motors does that mean you can't set way they spin ? i'm totally new in this so thank you for answer .\n\nokey so given this one it should be able to hold 15\" prob since it's in description\nshall i get 12a esc since on 15 size prob they used max 8.8 or shall i get 25a esc cause max continous is 20 ?\n", "tags": "quadcopter brushless-motor", "id": "6906", "title": "quadcopter propeller size + motor"}, {"body": "i've recently been learning about slam and ekf-slam. \n\ni've began my implementation in python, but have had trouble managing the updating of p, especially when it comes to adding new landmarks. currently there is no 'p' but just a few separate matrices that i have to stitch together when needed.\n\nmy implementation can be seen here: http://pastebin.com/r7wumgy7 \n\nhow best should i manage the large covariance matrix, should i be using one matrix, like the algorithm suggests? thanks in advance.\n", "tags": "slam ekf python", "id": "6908", "title": "ekf-slam, how best to manage the 'p' covariance matrix, programatically"}, {"body": "i've been working through this informative guide on ekf-slam but i'm having difficulty understanding the jacobians required for the 'landmark update', on page 35.\n\nwhat exactly is jxr and jz taking as input? is it taking the the current rotation of the robot, plus the addition of the odometry update? ie, the rotation that is now stored in the 'x' state vector. or are they taking the angle from the inverse sensor model, and if so, what's the 'delta' angle from?\n\nthanks.\n", "tags": "slam ekf python", "id": "6909", "title": "ekf-slam computing the jacobians for landmark updates"}, {"body": "\n\ni am confused about the right way to look for my theta1-theta5.\nprobably from the offset limit of the angles or calculation from x0 to x5 angle rotation or from atan2(x,y).\n", "tags": "robotic-arm dh-parameters", "id": "6910", "title": "how to find theta1 to theta5 after d-h parameter"}, {"body": "what is the reduced form of this block diagram? i can't see any solution way :(\n\n\n", "tags": "control", "id": "6913", "title": "what is the reduced form of this block diagram?"}, {"body": "i draw a robotic arm in solidworks, but i'm not so sure about how to find out the dof, forward and backward kinematic.\n\n\n\ncould anyone help me understand how to work out the kinematic solution of this robot arm?\n", "tags": "robotic-arm kinematics automatic matlab", "id": "6918", "title": "how do i work out the kinematic solution of a robot arm?"}, {"body": "i'm working on the control of a quadcopter and i'd like to understand how come controlling the yaw does not increase the overall thrust. my understanding is that the control is carried out through 2 pids per axis (roll, pitch and yaw). the output of the last pid is sent as a pwm signal to correct the rotor speeds of the propellers. the mixing looks something like that:\n\n$t_{frontleft} = thrust + roll_{pid} + pitch_{pid} + yaw_{pid}$\n$t_{frontright} = thrust - roll_{pid} + pitch_{pid} - yaw_{pid}$\n$t_{rearleft} = thrust + roll_{pid} - pitch_{pid} - yaw_{pid}$\n$t_{rearright} = thrust - roll_{pid} - pitch_{pid} + yaw_{pid}$\n\nall the quadcopter controls seem to work that way from what i could gather. so the basic idea to control yaw is to add $yaw_{pid}$ to the clockwise motors and substract the same amount $yaw_{pid}$ to the counterclockwise motors to make the quadcopter turns clockwise. which translates into a increase of speed of clockwise motors and a decrease of speed for counterclockwise motors from the same amount.\n\nbut we know that each motor produces thrust and torque according to those equations:\n\n$t = c_t\\rho n^2 d^4$\n\n$q = c_q\\rho n^2 d^5$\n\nwhere $t$ is thrust, $q$ is torque, $c_t$ and $c_q$ are system dependent constants, $\u03c1$ is the air density, $n$ is rotor speed, and $d$ is rotor diameter. which means that the thrust produced by each motor is proportional to the propeller speed squared. \n\nso if $n$ is the speed of all propellers before correction, the thrust of the clockwise propellers after correction will be proportional to $(n+\\delta)^2$ and the thrust produced by the counterclockwise propellers to $(n-\\delta)^2$. the total thrust for these 2 propellers will be proportional to:\n\n$(n+\\delta)^2 + (n-\\delta)^2 = 2n^2 + 2\\delta^2$\n\nas you can see, there is an increase of $2\\delta^2$ in the overall thrust produced by those 2 propellers (and $4\\delta^2$ when we take the 4 propellers into account). of course, in real life, when we control the yaw the quadcopter does not go up.\n\nso what am i missing? \n\n(the same stands for roll and pitch control but since the quadcopter turns around the roll or pitch axis, the total thrust is no longer entirely on the vertical axis and i could imagine that the projection on the vertical axis is not increasing, but that does not work with yaw)\n", "tags": "control quadcopter torque", "id": "6922", "title": "how is it possible to maintain the total thrust when controlling yaw of a quadcopter?"}, {"body": "are events like robocup advantageous to the development of robotic  advancement? \nor is it merely entertainment which advances robotics by allowing entry level participation which helps maintain interest? \ndo the darpa grand's provide a better vehicle for advancement? (pun intended)   \n", "tags": "design", "id": "6927", "title": "how important are events like \"robocup\" to the advancement of robotics in general?"}, {"body": "i'm very new to create 2. i want to send commands using bluetooth. i have already bought the bluetooth usb radio. what other devices do i need to get or how can i set up sending commands over bluetooth. any help is appreciated. \nthanks.\n", "tags": "irobot-create", "id": "6929", "title": "how to send commands to create 2 over bluetooth"}, {"body": "if i have a robot path in 2d space, \n\ni.e. a vector of (x,y) locations, and i need to generate artificial imu data (simulate them), how would i go about it? \n\nhow do i model equations to generate the values given a time frame and positions?\n\ni've come across imusim i'd like to know how to model them and generate using matlab or something similar.\n", "tags": "imu accelerometer gyroscope simulation", "id": "6931", "title": "simulate imu (2d gyro and accelerometer) data"}, {"body": "someone could tell me if there are wearable devices such as glasses, with sensors that can detect eye movement?\n\nin particular, i would need a device like google glass, having a sensor or a camera that is facing the eye, and it can capture the movement, possibly interfaced with a mobile device.\n\nalternatively, are there micro-cameras on the market, which can be connected via bluetooth or usb to a mobile device?\n", "tags": "sensors motion", "id": "6934", "title": "glasses with eye sensors"}, {"body": "i would likte to find the joints positions using joint angles, link lengths etc.\n\nhow can i define the position of the each joint using dh parameters?\n", "tags": "robotic-arm joint dh-parameters", "id": "6936", "title": "joint positions of a robot"}, {"body": "i am trying to set up my stereo vision system on a car. however, i meet several problems and do not know how to solve them.\n\n\nhow to select the baseline? i want the distance measurement to be far at 30 or 50 meters and near at around 5-10m. is it possible to choose a baseline that meets my requirement?\ni have tried stereo calibration of two cameras and also learned how to compute depth value from disparity map. however i don't know how to compute depth value if the focal lengths of the two cameras are different. it seems all the theorems i can find on the web only concern cameras of the same focal length. \n\n", "tags": "stereo-vision", "id": "6942", "title": "how to set up binocular cameras on a car?"}, {"body": "so the idea is that there would be one robot acting as overwatch, which would detect all of the obstacles in an area (which are not necessarily static), and then send the data about the obstacles' positions to another robot that would navigate around the obstacles to a goal.\n\nmy initial thought was to have the overwatch robot be in an elevated position in the centre of the area, then sweep around using an ultrasonic sensor. this way, it could keep track of the obstacles in a set of polar coordinates (distance, angle). but then i realised that this method doesn't account for collinear obstacles.\n\nso the question is, what is the best way to detect a bunch of non-static obstacles within an area?\n\nas a side note, i have seen a system similar to this, where there was a robot detecting obstacles (in that case, a crowd of people) and another robot pathfinding around the obstacles (the people), but i'm unsure exactly how that system was detecting the obstacles.\n", "tags": "sensors computer-vision sonar ultrasonic-sensors", "id": "6944", "title": "dynamically detect changing obstacles"}, {"body": "i have a generic problem to create a controller for the following system:\n$$\\ddot{x}(t) = a y(t)$$ \nwhere $a$ is a constant real value.\nthe system could be seen as an equivalent of a mass-spring-damper system, where damper and spring are removed. also $x(t)$ is the $x$ dimension and $y$ is simply the force moving the mass. but in this case i need to drive the force using $x(t)$ and not the contrary.\n\ntransforming according laplace i get:\n$$ y(t) = \\frac{1}{a}\\ddot{x}(t)$$\n$$ y(s) = \\frac{1}{a}s^{2}x(s)$$\n$$ g(s) = \\frac{y(s)}{x(s)} = \\frac{s^{2}}{a}$$\n\nconsidering that $a = 1$ i implemented a possible example in simulink. \n\n\n\nplease not that i put the output given by the scope for showing up the resulting answer of the system.\n\nso i have 2 questions:\n\n\nis it possible to develop such a system? as far as i know the degree of the numerator should be $=&lt;$ the degree of the denominator. so is the above system possible?\nis it possible to create a pid or pd controller to stabilize the output of the system?\n\n\nregards\n", "tags": "control pid matlab", "id": "6945", "title": "help to dimension the right controller for the following tranfer function"}, {"body": "i want to make a component that will be a square plate that will behave like it has a motorized hinge on all four sides. that is, it can \"open\" by pivoting around any one of its four sides. i want it to pivot by up to 45 degrees. \n\ni thought about designing it so that 3 hinges could be detached while one pivots, but i wonder if there's a simpler way to do this. \n", "tags": "design motion", "id": "6946", "title": "square with hinge on all four sides"}, {"body": "i'm working on an robot that would be able to navigate through a maze, avoid obstacles and  identify some of the objects in it. i have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation.\n\ni am just a first year electrical engineering student, and so need help on how i can use the bmp image. i will be making my robot using the arduino mega microcontroller.\n\nso how should i get started on it.\n\nif you need me to elaborate on anything kindly say so. \n\nlink: http://ceme.nust.edu.pk/nerc/files/theme_ind_2015.pdf\n", "tags": "arduino control localization", "id": "6952", "title": "using a bitmap maze image to navigate the maze"}, {"body": "i am using a tri-axis accelerometer and tri-axis gyroscope to measure the linear acceleration of a body. i need to get the orientation of the body in euler form in order to rotate the accelerometer readings from the body frame into the earth frame. please help i'm so stuck\n", "tags": "accelerometer gyroscope frame", "id": "6953", "title": "how to calculate euler angles from gyroscope output?"}, {"body": "how to make a robot move using arduino other than timing to predefined locations? and without the use of sensors?? i want to make my car move to different loactions on a board..want to know the possible options without using sensors and encoders??\n and how does cartesian robot work for predefined locations..does it require sensor too?\n", "tags": "arduino navigation", "id": "6955", "title": "how to make a robot move using arduino other than timing to predefined locations?"}, {"body": "slam noob here but trying to implement an algorithm that fuses odometry data and mapping based on wifi signal strengths for a 2d robot.\n\n1)\nafter various readings of different resources,\ni came across this - http://www.qucosa.de/fileadmin/data/qucosa/documents/8644/dissertation_niko_suenderhauf.pdf\nthat explained what sensors are used in mapping and how they are categorized.\n\nthere are range-bearing sensors (stereo cameras,rgb-d cameras) that provide both distance and angle (range and bearing), from which is easy to locate (x,y) coordinates of landmarks ---> i can develop a map.\n\nbut in case i'm using wifi signal strengths (received signal strengths) etc, in which case it is range-only (meaning, i can only establish from a robot pose(x,y,theta) as to how far this signal is coming from), how am i developing a map at all?\n\nmy question is similar to this - what algorithm can i use for constructing a map of an explored area using a number of ultrasound sensors? but not quite same.\n\neven if i were using imu/gps, how am i using gps to develop a map? what is my state space there? if i am getting gps signals / wifi signals/ radio signals, am i estimating the transmitter/ap's location as the map? or the walls of a room i'm navigating in, as a map?\n\na lot of slam literature talks about motion model and measurement model, the former gives me the pose of the robot quite easily because of the odometry and imu. \n\nthe latter though is more for development of a map. am i right in understanding this? if yes, say \na] i have walls in a room and i'm using lidar scanner - \nthis still gives me the location of the wall using the number of beams that give me bearing, and the average distance from all the beams.\n\nb] or if i have just a single laser scanner, i can still use a camera (distance) and the heading of the robot to calculate the location of wall (the map). https://shaneormonde.wordpress.com/2014/01/25/webcam-laser-rangefinder/#more-403\n\nbut if i have wireless signal strengths, i have a distance (distance of the transmitter from which i'm getting the rss, not the distance of the wall) as to where they are coming from. but how am i estimating the location of walls here?\n\n2) what does the term \"correspondences\" mean in slam literature? \n", "tags": "localization slam artificial-intelligence mapping wireless", "id": "6956", "title": "using range-only sensors for mapping in slam"}, {"body": "i designed a mini quadcopter which is about 4.5x4.5cm(main body). the main body is the pcb. \n\n![enter image description here][1]\n\nit weighs about ~20 grams with the battery. i'm using the mpu6050 with the dmp using the i2cdevlib. i am using raw radians for pitch, roll, and yaw these measures are read from the mpu6050's dmp. the motors are attached to the body using electrical tape(black thing  around motors). the motors are 8.5mm in diameter and are driven by a n-channel mosfet. the mode of control right now is bluetooth(hc-05 module). the code being used is my own.\ni have a control loop on all axes, the pitch and roll have the same values since the quadcopter is symmetrical. the problem i have is that pid tuning is next to impossible, the best i got was a ~2 second flight ([video in slow-motion][2]).\n\nat first i was using my own code for the control loop, but it wasn't as effective as the arduino pid library. \n\nthe output of the pid loops are mapped to -90 to 90 on all axes. this can be seen in the code\n\n\n\nmy full code is below, but what do you think the problem is?\n\ncode\n\nhttp://pastebin.com/cng6vxr8\n", "tags": "arduino quadcopter pid", "id": "6957", "title": "micro quadcopter pid problem"}, {"body": "i need to make shallow (max 2m) underwater wireless sensor network. data payload is about 10kb/s. i know that vlf band (~3-30khz)could be the best solutions for that, but cause of time-to-market i cannot make hardware and software from the ground.\n\nmaybe someone could share own-self experience in this filed. if band 100-900mhz could be enough to send 10kb/s from one device to another - from 2m underwater to over a dozen cm from water surface? maybe some ic for ultrasonic communication exist? another ideas?\n", "tags": "sensors wireless communication", "id": "6969", "title": "shallow underwater wireless sensor network"}, {"body": "i have a my mobile robot and plan to use the dynamic window approach to collision avoidance. i have read the paper ,but have one inequality i can't derive it.\n\ncould you tell me? thanks!\n", "tags": "navigation motion-planning", "id": "6979", "title": "question about dynamic window approach?"}, {"body": "since the day i bought it i always use ethernet over usb connection, now i need to use rj45 lan cable to connect beaglebone from my laptop, but my laptop can't even detect lan connection from it, what could go wrong? do i need straight or crossover cable? do i need to configure something first on my beaglebone?\n\n\n  update: managed to connect it through crossover cable and assign it ip\n  address by running dhcp server on my laptop.\n\n\n\n\n\n  as seen above my laptop assign ip 169.254.223.76, but when i tried to\n  connect to that ip using putty it gives me connection refused.\n\n\n\nplease help.\n", "tags": "beagle-bone", "id": "6980", "title": "beaglebone not accessible through lan?"}, {"body": "i've been implementing an extended kalman filter, following thrun's probabilistic robotics implementation. i believe my correct step may be wrong, as the state appears to be corrected far too much.\n\nhere's a screen capture showing the issue https://youtu.be/gkspfk27yvg \n\nnote, the bottom status reading is the 'corrected' pose coordinates.\n\nthis is my correct step:\n\n\n\nh = the range and bearing of state landmark.\n\nq = (landmark.x - self.x[0])^2 + (landmark.y - self.x[1])^2\n\nmy sensor covariance errors are 1cm per meter, and pi/180 for the bearing. my assumption was that the correction should be relative to the size of the robot's pose error. which is very small in this example, as it only moved forward less than 30cm.\n\nis the kalman gain applied correctly here, and if yes, what other factors would result in this 'over-correcting'?\n\nthanks.\n", "tags": "slam ekf python", "id": "6982", "title": "ekf over-correcting?"}, {"body": "i have a simulated robot moving in a discretized 2d grid world that (for various simplification and time-restriction reasons) has no noise. the problem is how the robot creates its initial map of the world. algorithms like slam and occupancy grid mapping are based on uncertainty, but in this case there is no uncertainty.\n\nso i'm wondering if there is a relatively simple algorithm for mapping the environment with noiseless position.\n", "tags": "mapping", "id": "6984", "title": "mapping algorithm without noise"}, {"body": "i recently bought a usb 2.0 bluetooth adapter. it claims to have support from linux kernels of versions 3.4 and higher. i have a beaglebone black with debian gnu/linux 7 image and kernel 3.8. i am developing on beaglebone black by hosting it through usb with .\n\ni have tried both hot plugging and  plugging in before boot and failed. \n\nthen, i tried this tutorial. however, i cannot find the  directory on my beaglebone black device. i looked up and assumed i needed to install the  package, but my beaglebone black has no internet access.\n\ni have also tried , as suggested by an answer of a similar question to this, with no luck. the weird thing is, while  itself prints\n\n\n  bus 001 device 001: id 1d6b:0002 linux foundation 2.0 root hub \n  bus 002 device 001: id 1d6b:0002 linux foundation 2.0 root hub\n\n\n only prints\n\n\n  bus 001 device 001: id 1d6b:0002 linux foundation 2.0 root hub\n\n\nthen hangs. information regarding bus 002, which i believe the device is connected to, is not printed. i have to restart the  connection to get back to work. \n\nhow should i approach to get the dongle to work on my beaglebone black? if the  package is sufficient, how do i install it on my beaglebone black without internet access. why does  hang?\n\nany help is appreciated!\n", "tags": "beagle-bone", "id": "6985", "title": "enable bluetooth adapter for beaglebone black"}, {"body": "i wanted to calculate the amount of thrust generated from the engines. i am using the blade 180 cfx model.\n\nhttp://www.bladehelis.com/products/default.aspx?prodid=blh3450#quickspecs\n\nafter some research i have found a way to calculate the thrust using:\n\nt = ( pi d^2 rho p^2)/2 where p is the power multiplier and can be calculated using: p= prop constant * (rpm/100)^power factor\n\ni am unable to find the values for the prop constant and the power factor. is there a way i can get this information? or an alternative way to calculate the thrust generated?\n", "tags": "power", "id": "6987", "title": "calculating thrust generated from electric engines"}, {"body": "first of all, i am in high school[to tell you that i am a newbie and lack knowledge]\n\nwhat i want to achieve for now is a thing that can differentiate between poly bags[polyethylene] and other stuffs. or a thing that could detect polyethylene.\n\ni have to built a robot and therefore we have only a few method accessible.\n\nanyway any knowledge or suggestion or external links provided by you, about this topic would be welcomed by me.\n", "tags": "mobile-robot sensors", "id": "6990", "title": "detect polyethylene"}, {"body": "i was looking for an implementation of a pid controller in java and i found this one:\n\nhttps://code.google.com/p/frcteam443/source/browse/trunk/2010_post_season/geisebot/src/freelancelibj/pidcontroller.java?r=17\n\nso, for what i could understand about it i am using it this way:\n\n\n\nbut he never stabilizes. he doesn't behave like a pid at all... this is the output i get:\n\n\n\ncan someone help me tell me what i am missing?\n\nthank you!\n", "tags": "control pid", "id": "6992", "title": "my pid controller in java is not operating correctly"}, {"body": "quadcopter frames seem to consistently follow the same x design. for example:\n\n\n\ni'm curious to know why that is. it certainly seems like the most efficient way to use space but is it the only frame design that would work for quadcopters?\n\nfor instance, would a design like this work?\n\n\n\nwhy or why not?\n", "tags": "quadcopter design frame", "id": "6999", "title": "quadcopter frame design"}, {"body": "i want to submit my gains for the pid regulator via mavlink. \nunfortunately, i am not very used to mavlink and there are several functions which may be used for that purpose (i think). my string is currently json formatted and i was directly sending it to the serial port before. \n\nis there a straight forward way to submit the data like it is (see below) with mavlink, or is it better not to transfer a json string with mavlink and submit each single value? if yes, what is the function of choice. \n\nso far i noticed that for most of the sensors, there are already mavlink function defined. for the pid gains i found not so much.\n\n\n", "tags": "c++ mavlink", "id": "7002", "title": "easiest way to submit a longer non standard character string via mavlink"}, {"body": "the dominant approach for solving ode in control systems books is  since the majority of these books use matlab. i'm not acquainted with how the  works but lately i started reading about euler's method in this book numerical methods for engineers. if the step size is very small, then the results are satisfactory. for simulation, one can actually set the step size to be very small value. i've used  in here for regulation and tracking problems. i faced some difficulties for using  for tracking problem since the step size is not fixed. now for the same experiment, i've used the euler's method with step size 0.001 sec. the results are amazing and so friendly in comparison with . this is a snapshot from the result \n\n   \n\nand this is the code \n\n\n\nmy question is why  is preferred in many control books assuming the step size is very small. \n", "tags": "control", "id": "7004", "title": "euler\u2019s method or ode45 for solving ode for control systems"}, {"body": "when choosing a battery for a robot, should you use a lipo or lifepo?\n\nfor lifepo, the pros:\n\n\ncan deliver higher sustained amps\nmany are built to be drop-in replacements for lead-acid batteries and can use the same charger\n\n\nthe cons:\n\n\nenormously expensive (about $1/watt*hour/kg)\nlower energy density than lipo (around 110 watt*hour/kg)\n\n\nfor lipo batteries, the pros:\n\n\ncheaper (about $0.2/watt*hour/kg)\nover twice the energy density than lifepo (around 250 watt*hour/kg)\n\n\nthe cons:\n\n\nmore complicated and unsafe to charge (see videos of lipos catching on fire)\nmost can't safely deliver high amps\n\n\nis there anything i'm missing? i see lifepo batteries used on a lot of larger platforms, probably due to the higher continuous amp rating. i see ebay flooded with tons of cheap high-capacity chinese lipos, but almost none of them have documentation, which probably means they're junk.\n\nwhen should i use lifepo vs lipo?\n", "tags": "battery", "id": "7008", "title": "how to decide between lipo or lifepo for robot battery"}, {"body": "i'm working in a project implementing a vision system. i'm a student and this is the first time i'm doing something like this, it has been a challenge.\n\ni'm using a controller (netduino+2, .net microframework) and a camera (cmucam5 - pixy) and for now it's working well. i'm communicating with the robot(fanuc m430ia) using modbus, and aquiring the data from the camera using i2c. \n\nbut, the next challenge is using 2 cameras to implement stereo vision and i'm not shure how to achieve that. i'm reading a lot about that and i understand the process and generally how it works, but i think my case is very specific.\n\nmy cameras detect the center of an object and give me the coordinates, so, i have that, and that's good. \n\nwhat do you think it's the more reasonable approach?\n\n(sorry for my english, let me know if i'm not being explicit, i'll edit the question if i see there's not enough information)\n", "tags": "microcontroller robotic-arm cameras stereo-vision", "id": "7012", "title": "algorithm simple stereo vision"}, {"body": "i am a student who is currently working on a computer science project that will require soon computer vision and more specifically stereoscopy (for close depth detection). i am now looking for a great camera to do the job and i found several interesting options:\n\n1- a custom built set of two cheap cameras (i.e. webcam);\n\n2- the old, classic, economic and proven kinect;\n\n3- specialized stereo sensors.\n\ni found a couple months ago this sensor: https://duo3d.com/product/duo-mini-lv1\n\ni tought it was interesting because it is small, stereoscopic and brand new (encouraging a fresh usa company). however, if we take apart the additional apis that come with it, i just don't understand why you would buy this when a kinect (or \"cheap\" cameras) are at least 4-5 times less expensive and still have great if not better specifications.\n\nsame applies for this one: http://www.ptgrey.com/bumblebee2-firewire-stereo-vision-camera-systems\n\ncan someone please explain to me why i would need such a device and if not, why they are needed for some?\n", "tags": "sensors computer-vision kinect", "id": "7014", "title": "what main factors/features explain the high price of most industrial computer vision hardware?"}, {"body": "i wonder is there any simple (can be computed in microcontroller level) option which is suitable for 3d object perception (depth, position, pose or coordinate estimation) of flying robots except lidar, stereovision, omnidirectional camera, laser scanner or any other machine vision based techniques \n", "tags": "mobile-robot sensors cameras stereo-vision lidar", "id": "7016", "title": "simple way of 3d perception"}, {"body": "my department recently purchased irobot create 2. we want to recreate the code from the csharp create 2 driving tether program to use as a base for our intro to computer science course. currently the code we are using to talk to the irobot is http://www.robotappstore.com/knowledge-base/how-to-program-roomba-for-net-developers/23.html. not sure if the irobot is getting the commands as well as if the serial port is making a connection. we are using visual studio 2012 as the programming environment. any recommendation and or input would be appreciated.\n\nthank you \n", "tags": "irobot-create roomba", "id": "7018", "title": "irobot create 2 c# connection"}, {"body": "am 2 weeks old to arduino projects..i had been using timing all this while to control my rover...now, i wanted to shift to using encoders..am facing quite some problems..am using arduino uno and a two amp motor shield..this is code i am trying to use..am using a 8v li-po battery\n\nhttp://www.myduino.com/index.php?route=product/product&amp;product_id=170&amp;search=rover+5 (link to rover)\n\nhttp://www.myduino.com/index.php?route=product/product&amp;product_id=131&amp;search=motor+shield (link to motoshield)\n\nmy question is there are four pins coming out of encoders from each side...what i did was connected the red and black to 5v and gnd respectively and the white and yellow of the first encoder to pin 2 and the white and yellow of second encoder to pin 3...is what am doing correct??\n\nand sometimes when i use this code, in the motorshield both the green and red light starts thereby stalling the motor..why does that happen?\n\ncan anyone of you suggest a link to a simple encoder code to make the motors move forward in a straight using feedback..\n\nthanks\n\n// interupt 0 -> pin 2\n\n// interupt 1 -> pin 3\n\nvolatile unsigned long positionl = 0;  //vehicle position count from left encoder\n\nvolatile unsigned long positionr = 0;  //vehicle position count from right encoder\n\nint motorla = 5;\n\nint dirla = 4;\n\nint motorra = 7;\n\nint dirra = 6;\n\nvoid setup() \n\n{\n\npinmode (motorla, output);\n\npinmode (dirla, output);\n\npinmode (motorra, output);\n\npinmode (dirra, output);\n\nserial.begin(9600);\n\n}\n\nvoid loop() \n\n{\n\nmovefwd(5300);\n\ndelay(2000);\n\nmoverev(3000);\n\ndelay(2000);\n\nwhile(1);\n\n}\n\nvoid encoder1()\n\n{\n\npositionr++;\n\n}\n\nvoid encoder2()\n\n{\n\npositionl++;\n\n}\n\nvoid movefwd(unsigned int x)\n\n{\n\npositionl=0;\n\npositionr=0;\n\nattachinterrupt(0, encoder1, change);   \n\nattachinterrupt(1, encoder2, change);\n\ndigitalwrite(dirla, low); // left a forward \n\ndigitalwrite(dirra, high); //right a forward\n\nwhile((positionl &lt;= x) || (positionr &lt;= x))\n\n{\n\n\n\n}\n\n// stop all motors\n\nanalogwrite(motorla, 0);\n\nanalogwrite(motorra, 0);\n\n// disables the encoders interrupt\n\ndetachinterrupt(0);\n\ndetachinterrupt(1);\n\n}\n\nvoid moverev(unsigned int x)\n\n{\n\npositionl=0;\n\npositionr=0;\n\nattachinterrupt(0, encoder1, change);   \n\nattachinterrupt(1, encoder2, change);\n\ndigitalwrite(dirla, high); // left a forward \n\ndigitalwrite(dirra, low); //right a forward\n\nwhile((positionl &lt;= x) || (positionr &lt;= x))\n\n{\n\n\n\n}\n\n// stop all motors\n\nanalogwrite(motorla, 0);\n\nanalogwrite(motorra, 0);\n\n// disables the encoders interrupt\n\ndetachinterrupt(0);\n\ndetachinterrupt(1);\n\n}\n", "tags": "arduino sensors", "id": "7024", "title": "rover 5 with 2 encoders help"}, {"body": "i would like to compare my results of visual odometry with the groundtruth provided by the kitti dataset.\nfor each frame in the groundthruth, i have a projection matrix.\nfor example:\n\n\n\nhere the instructions provided by the readme:\n\n\n  row i represents the i'th pose of the\n  left camera coordinate system (i.e., z\n  pointing forwards) via a 3x4\n  transformation matrix. the matrices\n  are stored in row aligned order (the\n  first entries correspond to the first\n  row), and take a point in the i'th\n  coordinate system and project it into\n  the first (=0th) coordinate system.\n  hence, the translational part (3x1\n  vector of column 4) corresponds to the\n  pose of the left camera coordinate\n  system in the i'th frame with respect\n  to the first (=0th) frame\n\n\nbut i don't know how to produce the same kind of data for me.\nwhat i have for each frame in my case:\n\n\nthe tf transformation from the init_camera (the fix one from the (0,0,0)) to the left camera which is moving. so i have the translation vector and the quaternion rotation.\nthe odometry data: the pose and the twist\ncamera calibration parameters\n\n\nwith those data, how i compare with the groundtruth ? so i need to find the projection matrix from the data above but don't know how to do it.\n\ncan someone help me ?\n\nthank\n", "tags": "mobile-robot odometry stereo-vision", "id": "7040", "title": "how to get the projection matrix from odometry/tf data?"}, {"body": "i'm trying to implement two pids for stabilizing quadrotor for position tracking. the inputs are $x_{d}(t), y_{d}(t), z_{d}(t)$ and $\\psi_{d}(t)$. for position tracking, usually the small angle assumption is assumed. this assumption allows for acquiring $\\theta_{d}$ and $\\phi_{d}$. these are the results \n\n\n\nthe x-axis position is driving me crazy. after alot of attempts for tuning the pids, i felt something wrong is going on. is this a normal behavior for pid controller? also, what i've noticed is that once $\\psi$ reaches to zero, the platform starts oscillating (after 1.5 second in the figure).   \n\nfor solving odes and computing the derivatives for the velocities, i use euler methods. \n\n\n\nit is simulation in matlab. \n", "tags": "pid quadcopter", "id": "7043", "title": "how to tune the two pids for quadrotor"}, {"body": "i am trying to determine the depth of view for a hypercatadioptric camera (camera lens system and a hyperbolic mirror) based on 1. \n\nthe following illustration seems pretty clear. for an image point $p$, we are looking for a virtual point $p_u$, given the parameters of the optical system. \n\n\n\ni have troubles finding the right equations in the paper, though. there is  which seems to be connected to $p_u$, but is not defined anywhere.\n\nmy goal is to replicate the diagrams they have later in the paper, like e.g. this one:\n\n\nwhich for a mirror (blue) gives the virtual image points of the scene (red).\ni would like to calculate the depth of view, so the area at which the image blur is below a threshold.\n\n1 zhang, s., zenou, e.: optical approach of a hypercatadioptric system depth of\nfield. in: 10th international conference on information sciences, signal processing\nand their applications\n", "tags": "computer-vision cameras", "id": "7048", "title": "depth of view for a hypercatadioptric camera"}, {"body": "a few days ago, i just shared my concerns about the price of computer vision hardware on this same exact forum (see what main factors/features explain the high price of most industrial computer vision hardware?) and i think a new but related post is needed. so here we go.\n\nhere are some details to consider regarding the overall scanner i want to build:\n\n\nrestricted space: my overall scanner can't be larger than 3 feet cube.\nsmall objects: the objects i will be scanning shouldn't be larger than 1 foot cube.\nclose range: the camera would be positioned approximately 1 foot from the object.\nindoor: i could have a dedicated light source attached to the camera (which might be fixed in a dark box)\n\n\nhere are the stereo cameras/sensors i was looking at (ordered by price):\n\n\ntwo logitech webcams (no model in particular)\n\n\ncheap\nharder to setup and calibrate\nneed to create your own api\nbuilt for: what you want to achieve\n\nintel realsense: http://click.intel.com/intel-realsense-developer-kit.html\n\n\n$100\nhigh resolution: 1080p (maybe not for depth sensing)\nworkable minimum range: 0.2 m\nunspecified fov\nbuilt for: hands and fingers tracking\n\nkinect 2.0: https://www.microsoft.com/en-us/kinectforwindows/\n\n\n$150\nlow resolution (for depth sensing): 512 x 424\nunworkable minimum range: 0.5 m\nexcellent fov: 70\u00b0 horizontal, 60\u00b0 vertical\nbuilt for: body tracking\n\nstructure sensor http://structure.io/developers\n\n\n$380\nnormal resolution with high fps capability: 640 x 480 @ 60 fps\nunspecified minimum range\ngood fov: 58\u00b0 horizontal, 45\u00b0 vertical\nbuilt for: 3d scanning (tablets and mobile devices)\n\nzed camera: https://www.stereolabs.com/zed/specs/\n\n\n$450\nextreme resolution with high fps capability: 2.2k @ 15 fps (even for depth sensing) and 720p @ 60 fps\nunviable minimum range: 1.5 m\noutstanding fov: 110\u00b0\nbuilt for: human vision simulation\n\nduo mini lx: https://duo3d.com/product/duo-minilx-lv1\n\n\n$595\nnormal resolution with high fps capability: 640 x 480 @ 60 fps\nworkable minimum range: 0.25 m (see http://stackoverflow.com/questions/27581142/duo-3d-mini-sensor-by-code-laboratories)\nphenomenal fov: 170\u00b0 (with low distortion)\nbuilt for: general engineering\n\nbumblebee2: http://www.ptgrey.com/bumblebee2-firewire-stereo-vision-camera-systems\n\n\ntoo much expensive (not even worth mentioning)\n\n\n\nnote: all prices are in date of april 18th 2015 and might change overtime.\n\nas you can see, some have really goods pros, but none seems to be perfect for the task. in fact, the zed seems to have the best specifications overall, but lacks of minimum range (since it is a large baselined camera designed for long range applications). also, the duo mini lx seems to be the best for my situation, but unlike the zed which generates really accurate depth maps, this one seems to lack of precision (lower resolution). it might be good for proximity detection, but not for 3d scanning (in my opinion). i could also try to build my own experimental stereo camera with two simple webcams, but i don't know where to start and i don't think i will have enough time to deal with all the problems i would face doing so. i am now stuck in a great dilemma.\n\nhere are my questions:\n\n\nwhat good resources on the internet give you a good introduction on 3d scanning concepts (theoretically and programmatically)? i will be using c++ and opencv (i already worked with both a lot) and/or the api provided with the chosen camera (if applies).\nshould you have a static camera capturing a moving object or a moving camera capturing a static object?\nshould i use something in conjunction with stereo camera (like lasers)?\nis it profitable to use more than two cameras/sensors?\nare resolution, fps and global shuttering really important in 3d scanning?\nwhat camera should i get (it can also be something i didn't mention, in the range of $500 maximum if possible)? my main criteria is a camera that would be able to generate an accurate depth map from close range points.\n\n\nthanks for your help!\n", "tags": "sensors computer-vision kinect", "id": "7050", "title": "questions regarding 3d scanning and camera choice"}, {"body": "i working on sliding mode control (smc) of a 4 dof manipulator, i don't know\nhow to select the discontinuity gain matrix, $ k$ , the surface constant (the diagonal gain matrix $\\lambda$  components).\n", "tags": "control robotic-arm industrial-robot", "id": "7052", "title": "how to select the parameters of the sliding mode control of a robotic arm?"}, {"body": "i have an omni-directional robot, such as a x-drive or mecanum drive that i need to track the position of. i can put encoders on the wheels, but that is all i can do in terms of the sensors. i have no external beacons that i can link to. the issue is that i needed to keep track of x-y position, including strafing, and my heading. does anyone have any resources that could help me with this.\n", "tags": "localization", "id": "7056", "title": "locating omni-directional robot"}, {"body": "i can't really find a real straightforward tutorial for that. there are a lot for arduino but i only have an original beaglebone, an esc, and brushless motor with me. please help.\n", "tags": "brushless-motor beagle-bone", "id": "7057", "title": "how to control brushless motor+esc with beaglebone?"}, {"body": "my question is more on a basic/conceptual level.\n\ni'm looking into a way to approach an object in map, that i have detected earlier. my robot is localized in a map using slam. and object position is 2d point that i recieve from my algorithm. (object is actually a face picture on a wall). is there a smart way to approach the point and \"look\" at it?\n", "tags": "mobile-robot mapping", "id": "7059", "title": "how to approach an object"}, {"body": "how to convert the value you get for the angle (packet id 20) into degrees?\ni am using the create2 robot and when i did not understand the data i am getting back. the documentation it says it's in degrees but what i get back is a huge number like 4864 when i turned the robot just 45 degrees.\n", "tags": "irobot-create", "id": "7062", "title": "create2 angle (packet id 20)"}, {"body": "i saw a high end robot arm once that i could move and bend it to any form i wanted with no resistance, as if the robot arm didn't have any weight. i'd like to know more about them. what is this class of robot arm design called? where can i get some more information about its design and applications?\n", "tags": "control robotic-arm joint", "id": "7066", "title": "what is the technology used for no-resistance robot arms?"}, {"body": "i want to use a raspberry pi to pan a camera mounted on a servo wirelessly from ~100 feet away. what are some good servos and transceivers for this?\nto clarify, there should be no physical connection between the raspi and servo.\ndo i need an additional raspi on the servo end?\n", "tags": "motor raspberry-pi wireless", "id": "7067", "title": "how can i communicate wirelessly between a raspi and servo?"}, {"body": "how is a new team strategy during a robocup competition sent to each player of a robot team? robots in the standard platform league (i.e. spl), for example, are fully autonomous and there is no connection with non-team members (except pulling from the gamecontrol).\n", "tags": "soccer", "id": "7068", "title": "how does the strategy changes in a robcup soccer competition without connection to outside?"}, {"body": "the gmis (general machine intelligence system) from a new article posted at codeproject.com looks interesting.  do you think that it could be a breakthrough in the field of robotics\n", "tags": "mobile-robot robotic-arm", "id": "7069", "title": "is gmis really a breakthrough?"}, {"body": "currently i am programming for a robotic simulation. i have a endeffector which aproaches a target, on the way to the target is an obstacle. now i redirect my endeffector, so that it does not hit the target.\n\nwhen i want to do the same for the whole arm i want to push the arm away from the obstacle as well. now i have it working so far that i can redirect the arm. but my calculation for the jacobian seems to be faulty.\n\nfor my setup, and what i need for that.\n\ni have a robotic arm, 7dof. let $x_0$ be the closest point on the arm to the obstacle. and $j_0$ the corresponding jacobian.\nalso i have given the following term:\n$\\dot{x_0} = j_0 * \\dot{\\theta}$ \n\n$\\theta$ are my joint angles. i can calculate the jacobian for the endeffector, but do not know on how to calculate it for a point ob the arm.\n\ndoes anybody have an idea on how to calculate the corresponding jacobian.\n\ncheers\n", "tags": "robotic-arm jacobian", "id": "7070", "title": "jacobian for point on robotic arm"}, {"body": "i have 3 ultrasonic sensor (hc-sr04), i want to use one of them as transmitter, and the other as receiver, i want to let the first one send ultra sonic waves and the other receive these waves from the same transmitter.\n\nhow can i do that ? \n\ni tried to send trigger for each ultrasonic and connect them on different pins on pic, but its now work.\n\nits something like this project but using hc-sr04 \n", "tags": "microcontroller ultrasonic-sensors", "id": "7073", "title": "make hc-sr04 receive from another one"}, {"body": "how do you calculate the pid values and stabilise the quadcopter using the on board sensors the gyro accelerometer and magnetometer\n", "tags": "sensors", "id": "7080", "title": "building a quadcopter using stm32f3 discovery board"}, {"body": "i was just wondering if it was possible to buy or build a programmable drone with a robotic arm,hand, knife.\n\ni want to program a drone to harvest crops.\n\n-object recognition from live video stream to server\n\n-identify and grab objects with arm, make cut if necessary\n\n-transport produce to collection site\n\ni know this would take much knowlege from many fields but do any you have any forsight into the limitations of doing this other than energy for power.\n\nestimates on cost of hardware?\n", "tags": "robotic-arm design quadcopter dynamic-programming", "id": "7089", "title": "programable drone with robotic arm and hand?"}, {"body": "i am working on my master's thesis about design and construction of universal robotic arm. \ngoal of my work is to design 5 dof robotic arm. something like on the picture: \n\ni need it to be able to lift a weight about 5kg. it has to move in \"action radius\" 1m. rotation speed should be about 1m/s. the conclusion of my work should be like: \"you can buy abb robotic arm or you can but this..it can lift that much, can turn that speed and weighs that much\". basic construction should be done too. maybe with some simulation.\n\nfirst of all - i picked really bad master's thesis for me, i know that know.\nsecond - i have like month to finish it.\ni would like to ask someone how to proceed. \ni know that first step is to pick servos/actuators/gearboxes, but which one?\nwhat is realistic weight of the whole arm which should lift another 5 kg of weight? how strong motors should i pick or with what gearboxes? \n\nis anyone able to help me via maybe emails? \n", "tags": "robotic-arm servos actuator arm", "id": "7091", "title": "design and construction of universal robotic arm (5kg, 1m)"}, {"body": "i know there are lots of consumer depth image sensors: kinect, primesense, structure.io, leap motion, ... but i'm looking for something that is more suitable for integration into robot, something without case or with proper mount, compact and available for at least next five years if robot is going to production. something similar to the sensors used in this drone https://www.youtube.com/watch?v=gj-5rnduz3i\n", "tags": "sensors kinect", "id": "7095", "title": "depth image sensor for integration into robot"}, {"body": "i have a first robotics spec national instruments crio.  i would like to connect a usb wireless xbox controller to it in order to control it from a distance with minimal extra hardware (which is why i am not using the more traditional wifi radio method).  to this point i have been able to find either\n\na. a sidecar for the crio which allows it to act as a usb host or\n\nb. a method that does not use ni specific hardware to connect the two together\n\nif someone who is knowledgeable on the subjects of industrial system and robot control could provide some assistance that would be greatly appreciated, thanks!\n", "tags": "control industrial-robot wireless usb first-robotics", "id": "7097", "title": "connecting usb xbox controller to national instruments crio"}, {"body": "guys i am making a home automation system very simple one with infrared remote ccontrol of tv remote now the problem is i wanna buy a some relays to switch 230v ac using my arduino board but can't understand which one to buy i don't want to buy relay module but i wanna buy relay.\n", "tags": "arduino", "id": "7099", "title": "got difficulty in reading specification?"}, {"body": "i have a quadcopter using a multiwii(arduino mega) controller. i am curious if there is any way to connect it with ros capable rpi. (that i could add to the quad itself).\n", "tags": "arduino ros quadcopter", "id": "7105", "title": "is it possible to use ros on rpi with controller like multiwii or similar?"}, {"body": "i would like to build a 70cm articulated robotic arm (not a scara one) that can lift between 10kg and 15kg (10kg would be awesome already, this payload includes the weight of the arm+gripper) and moves at 1 m/s (dreaming again :)). the goal is to make it similar to an human arm since i want to control it \"remotely\", so the joint should not be able to rotate more than my arm ^^\n\nso i know that i cannot uses servos available (like the overpriced dynamixel ones..)  with that payload. i also already excluded common linear actuators and less common ones like the pneumatic actuators (because of latency).\n\nfrom what i read arms like the baxter ones uses elastic series actuators, so i guess that i should go that way, but there aren't a lot of details on how that works (getting a lot of 100x100 photos where you see nothing) and i have a lot of questions. \nthe only thing i could understand is that it uses 2 motors and a spring.\n\ndo they use brushed or brushless motors? dc motors or steppers ? i read that steppers aren't that good to handle collisions and have difficulties when used at their limits.\n\nalso, how is the spring mounted on the motor ?\n\nto sum up, i'm collecting any experience, diagrams, intel, or documents that you have on that topic :)\n\nps : my budget can't exceed +/- 1500$ for that arm.\n", "tags": "motor robotic-arm stepper-motor servomotor actuator", "id": "7109", "title": "building industrial-like robotic arm"}, {"body": "i want to run two hc-sr04 on one pic16f877a and send the value mesured by the two ultrasonic to serial port.\n\nthis is my code using pic c compiler :\n\n\n\nbut the computer received random values ! \nwhat is the problem ?\n", "tags": "microcontroller programming-languages ultrasonic-sensors", "id": "7110", "title": "read from ultrasonic hc-sr04"}, {"body": "i am implementing a denavit-hartenberg forward transform for a 3-axes cnc mill. i know that the kinematic for such a machine is trivial and doesn't need dh, but i need to make appliable for other robots too. my implementation does the math correctly(i've verified that with another tool), but the transformation doesn't give me the results i would expect. \n\ni assume that for 3-axes cartesian robot with orthogonal prismatic joints(=cnc mill) the resulting transformation matrix should give me the input parameters(d1-d3) back in its translation vector, but it somehow doesn't. also, the resulting orientation matrix should have some \"nice\" values(90, 180, 270, etc.) and no odd ones(0.0528, 0.5987, etc,.). \nis my assumption wrong?\n", "tags": "forward-kinematics cnc dh-parameters", "id": "7111", "title": "dh forward kinematics for a cartesian robot (cnc mill)"}, {"body": "i'm trying to create a function that allows me to more easily start a motor, but i'm running into a problem, i don't know the type to use for the  argument. i'm using a vex 269 motor. here's the function.\n\n\n\n\n\ni just don't know what type to put for the  argument. what type would it be?\n", "tags": "robotc vex", "id": "7112", "title": "what is the robotc type for motors?"}, {"body": "i've read the accelstepper documentation on airspayce.com and it seems to be not possible to accelerate a stepper starting with a speed greater 0. acceleration always starts from speed=0, i tried it with several variations of the code below...\n\n\n\ni also tried to set the speed in the library's method  directly, but i'm not that good in c++ and don't get it to work.\n\nanybody any ideas?\n\n\n\nupdate\n\ni tried to write some custom code in accelstepper.cpp's method  \nmy idea was to set the speed manually during acceleration/deceleration if the speed is below my intended value. at first i thought it couldn't be a big deal, but now i see that my cpp skills seems to be not good enough or i don't understand the library quite well. \n\ni tried\n\n\n\nthis results in a very slow stepper movement..\n", "tags": "arduino control stepper-motor c++", "id": "7116", "title": "accelerate a stepper using arduino accelstepper starting with a pre-defined speed"}, {"body": "i've looked around but can't find the answer, to what i hope is a simple question. i'm working with a ti-sensortag, and i want to be able measure the rotation around the unit's z-axis. basically i want to attach the tag to a clock pendulum, lie the clock on a table so the tag and clock face point up, and to measure the angular displacement of the pendulum as it swings back and forth. i'm hoping the mental image translated well! \n\nmy understanding is that i can solve for displacement by multiplying my gyroscope readings by my sampling period, but i'm not sure how to compensate for drift. so my questions are: is my approach sound, and is the answer to drift to use the changing x and y accelerations? or would i need to somehow incorporate the magnetometer readings?\n\nthanks!\n", "tags": "sensors kinematics gyroscope", "id": "7120", "title": "measuring angular displacement using the ti-sensortag"}, {"body": "i have been working on trying to get the angle of the create 2. i am trying to use this angle as a heading, which i will eventually use to control the robot. i will explain my procedure to highlight my problem.\n\ni have the create tethered to my computer. \n\n\ni reset the create by sending op code [7] using realterm.\nthe output is:\n\n\n  bl-start\n  str730\n  bootloader id: #x47175347 4c636fff\n  bootloader info rev: #xf000\n  bootloader rev: #x0001\n  2007-05-14-1715-l\n  roomba by irobot!\n  str730\n  2012-03-22-1549-l\n  battery-current-zero 252\n  \n  (the firmware version is somewhere in here, but i have no clue what to look for--let me know if you see it!) \n\n\n\ni mark the robot so that i will know what the true angle change has been.\ni then send the following codes [128 131 145 0x00 0x0b 0xff 0xf5 142 6]. this code starts the robot spinning slowly in a circle and request the sensor data from the sensors in the group with packet id 2. the output from the create seen in realterm is 0x000000000000, which makes sense.\ni wait until the robot has rotated a known 360 degrees, then i send [142 2] to request the angle difference. the output is now 0x00000000005b.\n\n\nthe oi specs say that the angle measurement is in degrees turned since the last time the angle was sent; converting 0x5b to decimal is 91, which is certainly not 360 as expected. \n\nwhat am i doing wrong here? is the irobot create 2 angle measurement that atrocious, or is there some scaling factor that i am unaware of? are there any better ways to get an angle measurement?\n", "tags": "irobot-create roomba", "id": "7121", "title": "irobot create 2: angle measurement"}, {"body": "i'm working on a project to make a smartball that can detect the velocity(km/h) , spin(degrees per second) and flightpath(trajectory) of the ball using intel edison with the 9dof block (lsm9ds0 : 3-axis accelerometer, 3-axis gyroscope, and 3-axis magnetometer) &amp; the battery block, i'm reading values from the 9dof block by rtimulib(library for imu chips). i've been working on integrating the acceleration data from the accelorometer to get the velocity then get the position, i know that this method is not really accurate as the integration error cumulate very fast but i rely on that my calculations will be done in a very short time (about 3 seconds) then i re-calculate again from the beginning after every kick so that error doesn't cumulate hardly, also i only need an acceptable accuracy not a very high one. i discovered then that i'm dealing with projectile motion(ball kicking), so after considering this &amp; searching in projectile motion equations i found that i must know the initial velocity and the angle of projection(theta) to be able to get my requirements. my problem that i don't know how to get any of these , i tried different approaches like getting the horizontal distance &amp; getting the height to get their resultant(using pythagoras) then get the angle(assuming it's a right angle) in a very small time at the beggining of the projection , but i still couldn't get the height. the gyroscope outputs roll, pitch &amp; yaw angles related to the sensor orientation but i'm still not using this as i'm assuming that the sensor will be fixed inside the ball so it's orientation will not be the same as the projection angle.now what i really want is any approach/idea on how to get velocity &amp; flightpath of a projectile using accelorometer and gyroscope data. hope i made it clear , any help on how to get my requirements is really appreciated, thanks so much.\n", "tags": "gyroscope", "id": "7124", "title": "using accelorometer and gyroscope to get velocity, spin & flightpath of a ball in projectile motion"}, {"body": "i am beginner in robotics and i want to create a line follower+ obstacle avoider+ remote controlled robot and i am using the drive differential algorithm but i want to keep 2 common motors for the three circuits, i have got the three circuits ready and i want it to switch among these 3 circuits wirelessly. please tell me a solution.   \n", "tags": "motor wireless line-following circuit", "id": "7125", "title": "keeping 2 motors common for three circuits"}, {"body": "i was wondering how i could determine a robot's distance from a fixed point when the robot itself is constantly changing positions. i can keep encoders on the wheels and can also get data from a gyroscope and an accelerometer.\n", "tags": "localization", "id": "7128", "title": "determining a robot's distance from a certain point when the robot's position is constantly changing"}, {"body": "i'm wondering if there's a feature to \"flip\" the rotation direction with dynamixel (i'm using mx-106). for example, if i give +1.57 to the motor, then it interprets it as . and the other way around.\n\ni'm using its ros driver package that doesn't seem to explicitly claim that there's a feature to do this, although there is a question where a user reported he was able to do this from source code. but i failed to replicate. so i first wanted to ask about the capability of the device itself since i don't know if the limitation comes from the dynamixel device or from ros driver.\n\nthank you.\n\n\n\n(update) usecase of mine is that i have multiple robots where the direction of how dynamixel is attached is different per robot, and ideally like to flip the motor's direction at driver's level so that i can keep using the same controller software.\n", "tags": "servomotor dynamixel", "id": "7129", "title": "(dynamixel) inverse rotation direction"}, {"body": "i am building a quadcopter using the arduino uno with a 6dof accelerometer and gyro. i will be adding a separate 3 axis magnetometer soon for heading. i have successfully implemented the code that reads the data coming in from these sensors and prints them out. \n\ni have also checked for bias by averaging 100 measurements. my code calculates the pitch from the accel and pitch from the gyro respectively:\n\npitchaccel = atan2((accelresult[1] - biasaccely) / 256, (accelresult[2] - biasaccelz) / 256)*180.0 / (pi); \n\npitchgyro +=((gyroresult[0] - biasgyrox) / 14.375)*dt;\n\ni am then using a complementary filter to fuse the two readings together like this:\n\npitchcomp = (0.98*pitchgyro) + (pitchaccel*0.02);\n\ni am stuck on how to proceed from here. i am using the same procedure for roll, so i now have readings for pitch and roll from their respective complementary filter outputs. \n\ni have read a lot of articles on the dcm algorithm which relates the angles from the body reference frame to the earth reference frame. should that be my next step here? taking the pitch and roll readings in the body reference frame and transforming them to the earth reference frame? repeat the entire procedure for yaw using the magnetometer? if yes, how should i go about doing the transformations? \n\ni understand the math behind it, but i am having a hard time understanding the actual implementation of the dcm algorithm code-wise. \n\nany help is appreciated!\n", "tags": "arduino quadcopter accelerometer gyroscope", "id": "7137", "title": "understanding the various attitude estimation methods"}, {"body": "i'm working on my own rov project, but i find openrov have a ready to use image for my bb so want to use that instead of making my own program, and i already deployed the image, but i can't find which three pins find that send pwm signal for esc's? please help.\n", "tags": "mobile-robot beagle-bone", "id": "7138", "title": "pins in openrov that control the motors?"}, {"body": "i have a ptu system whose transfer function i need to determine. the unit receives a velocity and position, and move towards that position with the given velocity. what kind of test would one perform for determining the transfer function...\n\ni know matlab provides a method. the problem, though, is that i am bit confused on what kind of test i should perform, and how i should use matlab to determine the transfer function.\n\nthe unit which is being used is a flir ptu d48e\n\n---> more about the system \n\nthe input to the system is pixel displacement of an object to the center of the frame. the controller i am using now converts pixel distances to angular distances multiplied by a gain $k_p$. this works fine. however, i can't seem to prove why that it works so well, i mean, i know servo motors cannot be modeled like that.\n\nthe controller is fed with angular displacement and its position now => added together give me angular position i have to go to. \nthe angular displacement is used as the speed it has to move with, since a huge displacement gives a huge velocity.\n\nby updating both elements at different frequency i'm able to step down the velocity such that the overshoot gets minimized. \n\nthe problem here is: if i have to prove that the transfer function i found fits the system, i have to do tests somehow using the  function in matlab, and i'm quite unsure how to do that. i'm also a bit unsure whether the ptu already has a controller within it, since it moves so well, i mean, it's just simple math, so it makes no sense that i'll convert it like that.\n", "tags": "control", "id": "7139", "title": "determining transfer function of a ptu for visual tracking"}, {"body": "what is the difference between smoothing and mapping (sam) and simultaneous localization and mapping (slam)? these general approaches seem closely related. can someone describe the differences?\n", "tags": "slam", "id": "7140", "title": "what is the difference between sam and slam?"}, {"body": "i am new to robotics, however i am designing a pid to control the angular velocity (azimuth, elevation) of a couple of faulhaber motors.\nthe input to the pid control is the actual angular velocity, which is not observed though, since it is derived from the position of each motor at time $t$.\n\nthe pid sample period is aprox. , whereas the input data rate from the joystick is aprox. , corresponding to a sample period of . the joystick input gets transformed into the desired angle speed, that the pid will control.\n\ni was initially filtering position data using a normal 2d linear kalman filter, but the angular velocity is not linear (by formula), hence i switched to extended kalman filtering.\n\nmy questions are the following:\n\n\nis this latter approach that makes use of ekf correct?\nwhich are the parameters that i have to check in order to properly set the update rate of the pid loop?\n\n\nthx in advance!\n", "tags": "pid ekf", "id": "7146", "title": "estimating angular speed from position for control purpose"}, {"body": "i am trying to exert a desired load of 0.07 n.m on a bldc motor shaft whose length is 0.750in and diameter is 0.3125in (0.008m). i can go to a machine shop and get a small adjustable cylindrical coupling made for my shaft. but i need it to exert close to desired torque at a speed of 2100 rpm (220 rad/s). i tried doing some calculations, according to the formula \n\ntorque = speed * mass * (radius)^2\n\nif i solve this equation with t = 0.07 n.m, speed = 220 rad/sec, radius = 0.004 m, i get around 20 kg for mass, which is huge!!!. it is more than the mass of the motor. can you please suggest a convenient way to load the motor. thank you.\n", "tags": "motor brushless-motor", "id": "7148", "title": "an easy way to exert desired load on a motor shaft?"}, {"body": "is it possible to transmit live audio/video feed and at the same time, receive commands through uart using only 1 rf transceiver connected to the arduino board?\n\ni want to control the arduino through serial communication (uart) which can be accomplished by using rf connection to control it from a remote. i also want to transmit live audio and video feed from the arduino using the same rf transceiver. is this possible?\n\ni found avctp, but i'm not sure if it enables serial communication. also, i don't like to use bluetooth for some reasons.\n\nthanks in advance!\n", "tags": "arduino microcontroller radio-control serial communication", "id": "7153", "title": "is it possible to have a/v feed and serial communication on an rf transceiver at the same time on an arduino?"}, {"body": "i have an stm32f3 discovery board. i want to go to the next step and i want to try to use timers in a few configurations. \n\nhow can i calculate variables (such as prescaler, period)? i looked in all datasheets, manuals and didn't find anything that can describe these values as  - input capture mode, op, pwm, etc. \n\ni think that prescaler is for downgrading a frequency from 1-65575. \n\nso if i have fcpu=72mhz and want to generate a signal of frequency=40khz, am i supposed to do: 72mhz/40khz=1800? \n\nnow should i subtract this prescaler with -1?\n", "tags": "microcontroller", "id": "7156", "title": "stm32f3 timers & computing"}, {"body": "i am trying to use c++ to talk to the create 2 robot. does anyone have basic code to write/read from the create 2 using c++ or c?\n\ni am having trouble with converting create 2 commands (like ) into one .\n", "tags": "irobot-create c c++", "id": "7158", "title": "c++ and create 2"}, {"body": "do structured light camera sensors like the structure.io, intel realsense or microsoft kinect work outdoors?\n\ni read these sensors wont work outdoors because of ambient ir light. can someone provide this with proper references/tests? i mean what degree of ir illumination is needed for the sensor to stop working etc.\n\nthere are videos on youtube that show microsoft kinect working outdoors:\n\n\nprairie dog ii: ugv kinect sensor outside - limited outdoors range\noutdoor kinect data collection - heavy interference with direct sunlight\n\n\nhowever, the (not yet released) new intel realsence r200 specification says \"range up to 3-4 meters indoors, longer range outdoors\" while the older f200 says \"0.2 meters - 1.2 meters, indoors only\". i am really interested in seeing if the r200 will really work outdoors.\n", "tags": "kinect cameras", "id": "7159", "title": "do structured light camera sensors work outdoors?"}, {"body": "currently working on a uav using an odroid xu3 lite as the center core running the out-of-box sd card image of ubuntu.\n\nthis is (supposedly) an upgrade from the previous xu that we were running. we're using opencv functions houghcircles, lines, and rgb color detection, searching for shapes and color blobs of 3 different colors (white, red, green) for a competition. this is all running a lag time a little over 2 seconds on the opencv playback frame with an output every second or so on the terminal when that's off.\n\nthe xu3 is running even slower, though, despite being a better machine. what could i do to improve the speed or is the xu3 not fitting the power requirements? one thing i think is an issue could be that the stock ubuntu version is bloated and i've been looking around for other images, and i'd like to avoid using android. \n", "tags": "computer-vision software uav c++ linux", "id": "7161", "title": "odroid xu3 speed issue with image processing"}, {"body": "i'm trying to use a dual quaternion hand eye calibration algorithm header and implementation, and i'm getting values that are way off. i'm using a robot arm and an optical tracker, aka camera, plus a fiducial attached to the end effector. in my case the camera is not on the hand, but instead sitting off to the side looking at the arm.\n\nthe transforms i have are:\n\n\nrobot base -> end effector\noptical tracker base -> fiducial\n\n\nthe transform i need is:\n\n\nfiducial -> end effector\n\n\n\n\ni'm moving the arm to a series of 36 points on a path (blue line), and near each point i'm taking a position (xyz) and orientation (angle axis with theta magnitude) of camera->fiducial and base->endeffector, and putting them in the vectors required by the handeyecalibration algorithm. i also make sure to vary the orientation by about +-30 degrees or so in roll pitch yaw.\n\ni then run estimatehandeyescrew, and i get the following results, and as you can see the position is off by an order of magnitude. \n\n[-0.0583, 0.0387, -0.0373] real\n[-0.185, -0.404, -0.59] estimated with handeyecalib\n\nhere is the full transforms and debug output:\n\n\n\nam i perhaps using it in the wrong way? is there any advice you can give?\n", "tags": "robotic-arm stereo-vision calibration", "id": "7163", "title": "hand eye calibration"}, {"body": "i'm wondering that, pid control is a linear control technique and the robot manipulator is a nonlinear system, so how it is possible to apply pid control, in this case. i found a paper named: pid control dynamics of a robotic arm manipulator with two\ndegrees of freedom. on slide share page, is this how we use pid control for robotic arm, is there any name for this approach? and how to remove the ambiguity that pid is linear control technique and the robot is nonlinear system. any suggestions?\n", "tags": "control pid robotic-arm industrial-robot dynamics", "id": "7165", "title": "how to implement pid control for robotic arm?"}, {"body": "i am working on a project that involves speed regulation of a bldc motor under no-load and load conditions. i wish to use another machine operated as generator, acting as load on the motor, as shown in this video. \n\nthe coupling used in this motor/generator arrangement looks handmade out of a rubber tube or somethhing. i am considering using it as an alternative to a flexible coupling. purchasing an actual flexible coupling is not an option for me. moreover, i need the coupling on an urgent basis. \n\nmy question is, can this arrangement (or something similar) be used to couple a 15w motor to a similar rating machine, if the rated torque is not exceeding 0.1 n.m?\n", "tags": "control brushless-motor", "id": "7167", "title": "is this rubber/pvc coupling a good enough for small torque (0.1 n.m)"}, {"body": "i am very new to robotics. but i will be writing algorithm for my robot to move around and gather information from its surroundings and process it. it will also process audio-visual signals. but i am in confusion about which micro-controller to use so it would be performance efficient and consumes less power.\n\nthe controller should also be capable of communication with wireless network (internet through wi-fi) and should also support memory integration.\n\nalso i know to program in java and c. please suggest which would be the best language to use for programming.\n\nthanks.\n\np.s. i would really like to use a microprocessor as it is highly customizable. please suggest the best to use\n", "tags": "artificial-intelligence", "id": "7175", "title": "need suggestion about which microcontroller/processor and language to be used in my project"}, {"body": "i have recently been asked to review a raspberry pi hat (from a programming view) that will allow pwm control of upto 16 servos, however i am hoping to use this time to work on a hexapod idea i have been thinking about for a while, which requires a minimum of 18 servo's, and preferably 20 (camera/sensor pan and tilt).\n\nmy question is:\n\nwhat is a relatively cheap and uncomplicated way of extending my control over those extra 4 servo's?\n\nit would appear most servo controller hat/shiels for arduino and raspi are upto 16 servos, and can be extended by buying another shield, are there any other options?\n\nany advice in this subject would be greatly appreciated, preferably dumbed down a bit, and i don't know a great deal about micro controller hardware (more of a software guy)\n", "tags": "raspberry-pi servomotor rcservo", "id": "7176", "title": "using 20 servos at once, with raspberry pi"}, {"body": "how can i test whether my gazebo installation works properly or not? i'm trying to \"save myworld\" and \"save as\" options but no window is shown.\n", "tags": "gazebo", "id": "7177", "title": "how to test gazebo works properly. save windows don't show any component"}, {"body": "do we have to build ros for robotic researchs/applications? what is the main advantage? when or in which situations ros is mandatory?\n", "tags": "ros", "id": "7178", "title": "is ros (robot operating system) mandatory?"}, {"body": "let's say my redundant robot is at an operationnal position $x$. is the set of all possible joint configuration \"continuous\", which would mean that it is possible to explore all the possible configurations without moving the end effector. is there a way to show that it is true or false? i am using a kuka lbr robot with 7 dof so maybe there is a specific answer for this one.\n\ni have been searching it and did not find any result but i will gladly accept any link or answer that you may have.\n", "tags": "control inverse-kinematics", "id": "7181", "title": "is the geometric inverse problem's solution \"continuous\" for a redundant robot?"}, {"body": "i'm looking to find a way to operate a small servo using a 4-20ma linear analog signal generated by a plc in an industrial setting. \n\nthe purpose of this is to allow for automation for a task that currently is done by manually turning and adjusting a potentiometer with removable dial. basically, i'm trying to ghetto together an oldschool motor operated potentiometer (mop) so it can be removed quickly and easily without affecting the operation of the original process.\n\ni've spent hours looking for servo controllers/encoders that are capable of this, but i haven't been able to find any. any way i could get pointed in the right direction would be fantastic. surely such a thing must exist!\nthanks so much!\n", "tags": "rcservo", "id": "7186", "title": "what would i need to control a dc servo using a 4-20ma linear analog signal?"}, {"body": "i have a 2-link, 2 degree of freedom robotic arm, that only measures linear acceleration at each link(through an accelerometer), and rotational velocity on each joint (through a gyroscope).\n\ni know that through using the jacobian matrix, i can compute link velocity and acceleration from joint angles, and through the inverse of the matrix i can compute joint velocities from joint angles and link acceleration.\n\nhowever, i am not sure if i can compute joint angles using only the link linear and rotational acceleration? i am aware that the joint angle could be estimated by integrating the joint velocities (and applying some sort of filter), but is there an algebraic way this can be computed? it doesn't seem likely to me. \n", "tags": "robotic-arm accelerometer gyroscope jacobian", "id": "7188", "title": "can a jacobian matrix be used to derive joint angles from end-effector linear and rotational velocity (without a filter)?"}, {"body": "i noticed that the create2 does not always provide sensor data while it's moving. am i supposed to stop the robot, request sensor data then start it again? or am i missing something? it seems to work most of the time but once in a while i get no data back. i am trying to make it move from one point to another by starting it and then reading distance to see how far it travels every .1 seconds but sometimes it i just keeping getting no data.\n\ni noticed this using python and c code as well.\n\ni am using the usb port with the bit rate they recommend (115200).\n", "tags": "irobot-create", "id": "7189", "title": "create2 reading sensor date does not always work"}, {"body": "i'm searching for a cheap (under 100$) and efficient 3d sensor, which detects obstacles and moving objects, for robot applications like quadrotor navigation, swarm robotics, etc. can you suggest a sensor that can be either a commercial product or a \"do it yourself\" project?\n", "tags": "mobile-robot sensors swarm", "id": "7190", "title": "cheap and efficient 3d sensor?"}, {"body": "i'm a complete beginner in robotics with background in programming...\n\ni started thinking about a robot project yesterday and want to order some motors to test with. i saw the specs for the motors, torque etc, and i think i remember enough physics from high school to do simple calculations. for example, if the motor rotates an arm, then given the torque and the length of the arm, how much could it lift? also, if it doesn't lift it straight up, but at an angle, i could with a bit of thinking tweak the calculations a bit... if there would be several joints attached to each other, the calculations would be more complex, but i could then probably create a program in node.js, for example, to be able to experiment with different values.\n\nhowever, i would assume that these kinds of calculations would be very common when designing robots. so i would assume there are already programs for these types of calculations created. i don't know exactly what i mean with \"these types of calculations\", because i don't know yet what i don't know, so would like to ask which programs are there that you guys use for making calculations when you design your robots?\n\npreferable they should be open source...\n", "tags": "robotic-arm mechanism software", "id": "7194", "title": "beginner question about software for calculations"}, {"body": "i am planning a tank like robot for hobby purpose. i have control engineering background, however i never applied on robotics. \ni would like to test different control theory, namely mpc. i saw a lot of publications regarding the kinematics and inverse kinematics of such a robot, however i am wondering if somebody can point out regarding the dynamics modelling of such a system,taking into account the forces, mass etc?\n", "tags": "wheeled-robot", "id": "7198", "title": "dynamic model of a tank like robot"}, {"body": "given my control system \n\n\ni have found the region of the complex space that satisfies my specifications, determining poles position in 0.5 +- 0.2i. \n\nnow i want to find the gains that fix the desider pole (with matlab), but i have not understand well how to do it: anyone can suggest me an example on how to do that, with or without matlab?\nthanks\n\nedit: in the first image the sum blocks are +-, not ++\n", "tags": "control pid tuning matlab", "id": "7199", "title": "pole placement gains tuning"}, {"body": "i teach ftc robotics to high school students, and while i'm a proficient programmer and can teach them coding fairly well, my mechanical skills are a bit soft.  i'm looking for good sources for myself and the students to go through that gets a little more in depth than \"this is a gear, this is a chain, this is gear ratio, etc.,\" but maybe not quite the level of building professional / industrial robots.  \n\ni've used the vex robotics curriculum as a starting reference - http://curriculum.vexrobotics.com/curriculum - but it doesn't go through some more advanced topics (for example, how to drive a single gear / drive shaft with multiple motors to achieve more power without having to gear down and lose speed.)\n\nare there any good intermediate sources like this?  do i need to just bit the bullet and get a college level mechanics text?\n", "tags": "mechanism", "id": "7203", "title": "education sources for robot building?"}, {"body": "i am trying to build a quadcopter from scratch. i have selected few parts but i have no idea whether the quadcopter will come together and fly.\n\ni would appreciate your feedback on whether the parts i have selected are compatible (ubec, motor). if not, i would appreciate suggestions.\n\nthe frame for my quadcopter is in the x configuration and i am making my own. i am expecting the average weight of the quad to be around 800g. i hope the motors and prop combination can hover it well.\n", "tags": "multi-rotor", "id": "7206", "title": "building my first quadcopter"}, {"body": "i saw a video of a robot used in special education with children on the autism spectrum (https://www.youtube.com/watch?v=fqcjfebqxgq).  my son isn't autistic, he has tourette syndrome, adhd, executive function problems, and ocd.  a robot could be quite helpful for him.  where can i buy one?  i don't need it to look like a human being.  it just needs to be interactive and reasonable cute.  as my son is getting ready for bed, he needs someone to talk him through his steps, give him positive feedback, and ask questions like \"okay, you're in your pajamas.  great!  what else do you need to do to get ready for bed?\"  and the robot would have a mental list (preprogrammed) of everything that's needed (brush teeth, wash face, put on eczema ointment, put dirty clothes in hamper).  my son is 12 and would like to get ready by himself -- without mama or papa -- but he gets sidetracked when he's in his room on his own.  the robot doesn't need to be able to \"see\" him brushing his teeth.  he just needs to be able to hear my son saying, \"i brushed my teeth.\"  because when the two of them together decide he has made it through his routine, then they can call me in, and i'll check, and then we'll do our bedtime reading.\n\nthat's an example of what i have in mind.  there are other situations where i could imagine a robot being helpful for him.\n", "tags": "artificial-intelligence", "id": "7210", "title": "recommendation of robot for special education"}, {"body": "i have a stepper motor which has an internal controller. \ni would like to determine them both, but don't know how i should approach the problem. \nthe system receives a input velocity and position, and moves toward that position using that velocity. the input could also just be a velocity. \n\nthe plant is a pan and tilt unit, which has 2 stepper motors. i tried with ident but only got a fit of 5 %...  my input was a noisy signal, and output  was the position it writes out. \n", "tags": "control", "id": "7212", "title": "estimate transfer function of stepper motor?"}, {"body": "over the past few weeks, i have been attempting to interface the irobot create 2 with an arduino uno. as of yet, i have been unable to read sensor values back to the arduino. i will describe by hardware setup and my arduino code, then ask several questions; hopefully, answers to these questions will be helpful for future work with the create 2.\n\nhardware:\nthe irobot create 2 is connected to the arduino uno according to the suggestions given by irobot. instead of the diodes, a dc buck converter is used, and the transistor is not used because a software serial port is used instead of the uart port.\n\nsoftware:\nthe following is the code that i am implementing on the arduino. the overall function is to stop spinning the robot once the angle of the robot exceeds some threshold. a software serial port is used, which runs at the default create 2 baud rate.\n\n\n\nquestions:\n\n\nam i loading the sensor values into the array correctly? this same code works when a bump and run program is implemented, but that requires knowing only one bit rather than two bytes.\nhow many bytes can be read over the serial connection at a time? a previous post (help sending serial command to roomba) highlights that one byte can be sent at a time. does this imply that the reverse is true? if so, would a solution be to use a char array to read the values instead and then to append two chars to form an signed int?\nis serial communication synchronization a problem? i am assuming that synchronization is not a problem, but is it possible for the bytes to be split on the nibble boundaries? this would present a problem because there is not a nibble datatype. \n\n", "tags": "arduino irobot-create roomba", "id": "7215", "title": "arduino-create 2: reading sensor values"}, {"body": "i want to move a robot to a certain distance say 1 meter.\nwhat are the different ways that i can implement to do so?\nfor example i can measure the circumference of the wheel and assign time of rotation to move it. what are other techniques to achieve this?\n", "tags": "wheeled-robot movement", "id": "7216", "title": "what are the different ways to control distance to be covered by a robot?"}, {"body": "i was having problems reading sensor information from my irobot create 2 and sent an email asking for help from the irobot staff. they were super helpful and gave me an answer(the next day!!!) that helped push along my project. i was requesting data from the create2 to print to the screen so i could figure out how to write a code that would read the data. i started with this section of code that was not working for me (i trimmed some of the code off that controlled other functions):\n\n\n\nthey told me that the code was actually working fine but i was trying to print out the value of the sensor packet without parsing it in any way. they then recommended i change the code in program2 to this:\n\n\n\nthis works beautifully and prints to the screen if the bumper is pressed or a wheel drops. my question is how to deal with responses that are longer than one byte (ie packet id: 22 for voltage)? \n\nwhen i try packet id: 22 it prints to screen and it sends the high byte of 3f and a low byte of d7. i can manually combine them to get 3fd7 and convert it to a decimal of 16.343 volts but i would like to figure a way to print to screen the the voltage by having my program do this for me. many of the sensors send data in 2 bytes and i need a way to make sure that it is combined automatically. \n\nrobb \n", "tags": "irobot-create python roomba", "id": "7219", "title": "issue with multiple bytes from irobot create 2"}, {"body": "i would like to know what are the differences between positioning and localization systems. in most review papers they are used interchangeably. are they the same?\n for example:\ngps(global positioning system): gives coordinates of receiver and\nslam(simultaneous localization and mapping): constructing or updating a map of an unknown environment\n\nis difference :\n\npositioning: only gives information about receiver coordinates.no information about enviorement\n\nlocalization: gives information about receiver coordinates and also enviorement. positioning is a subtopic of localization\n", "tags": "localization slam gps", "id": "7221", "title": "what is the difference between positioning and localization systems"}, {"body": "i have a requirement for a motor that pulls a piece of rope until the rope is taught. however i'm at a loss as to how to achieve this, i'm sure it must've been done before but i'm not sure how to best describe this in a way that would get me more results. i wondered if there are any sensors or pre-established methods for sensing resistance to motion in electrical motors?\n", "tags": "motor", "id": "7225", "title": "robotic winch force sensor"}, {"body": "i am at the moment trying to identify a system using frequency sweep. i have been using mathematica and have created a frequency sweep as such.\n\n\n\nthe max frequency is 10 hz, i sample the data using 1000 hz. but at what rate should i input it to the system, and what rate should i read from it?\n", "tags": "control", "id": "7226", "title": "at what frequency should i input and read values?"}, {"body": "i am building a micro cnc machine, probably from 2 cd roms as the x and y axis and a floppy drive as the z, or 1 hard drive as the x, 1 cd drive as the y and 1 floppy as the z, but i am not sure how i wire this to work with emc2, a linux cnc program that works with your parrallel port.\ndo i connect my steppers directly to the control drivers(the board i buy off ebay right?) and then connect to my pc?or do i need to interface the board instead then go to the driver and them my parralel port?\ni also am wondering why people opt for x/y at the bottom and z at the top, why not put x/y/z stacked, so you could place it upside down for a top down view?(i would do this but it might be done for accuracy,3 layers of potential failure.\n", "tags": "driver stepper-motor cnc", "id": "7227", "title": "building a micro cnc machine"}, {"body": "this post is a follows from an earlier post (irobot create 2: angle measurement). i have been trying to use the wheel encoders to calculate the angle of the create 2. i am using an arduino uno to interface with the robot.\n\ni use the following code to obtain the encoder values. a serial monitor is used to view the encoder counts.\n\n\n\nthe code above prints out the encoder counts; however, when the wheels are spun backwards, the count increases and will never decrement. tethered connection to the create 2 using realterm exhibits the same behavior; this suggests that the encoders do not keep track of the direction of the spin. is this true? \n", "tags": "irobot-create roomba", "id": "7229", "title": "irobot create 2: encoder counts"}, {"body": "what kind of input output test can be peformed on at physical system which has constraints to identify the transfer function.  the system which is being discussed is a pan/tilt unit.. \n\nthe input it receives is either both a position and velocity or only a velocity.. \n", "tags": "input", "id": "7230", "title": "system identification on a physical system with constrains"}, {"body": "lately i've been interested in comparing the energy density of model rocket engines to lithium polymer batteries (attached to motors and propellers) for propelling things upwards.\n\nto get a feel for this, i decided to compare an estes c6-5 motor to a 3dr iris + quadcopter.\n\nestes c6-5 has initial mass of 25.8g, and produces 10 n s total impulse. so, the \"impulse density\" is about 10 n s / 25.8g = 0.38 n s g^-1.\n\n3dr iris+ weighs 1282g without battery. 3.5ah battery weighs 250g and will power hover for about 20 minutes (so about 10.5a draw). thrust produced to hover on earth is 9.8n kg^-1 * 1.532 kg = 14.7n. \"impulse density\" is 14.7n * 1200s / 250g = 70.6 n s g^-1 .\n\nso, according to my math here, the lipo is about 0.38/70.6 = 186 times more energy dense than the model rocket engine.\n\nof course, the model rocket engine will lose 12.48g of propellant by the end of the flight so it will be effectively a little lighter, but that's not going to affect things by a factor of 100.\n\ndoes this seem right to you? am i missing anything?\n", "tags": "quadcopter rocket lithium-polymer", "id": "7234", "title": "are lipo really 100 times more energy dense than model rockets?"}, {"body": "the quadrotor system is  multi-odes equations. the linearized model is usually used especially for position tracking, therefore one can determine the desired x-y positions based on the roll and pitch angles. as a result,  one nested loop which has inner and outer controllers is needed for controlling the quadrotor. for implementation, do i have to put  inside  for the inner attitude controller? i'm asking this because i've read in a paper that the inner attitude controller must run faster (i.e. 1khz) than the position controller (i.e. 100-200 hz). in my code, both loops run at 1khz, therefore inside  there is no . is this correct for position tracking? if not, do i have to insert  inside  for running the inner loop? could you please suggest me a pseudocode for position tracking?\n\nto be more thorough, the dynamics equations of the nonlinear model of the quadrotor is provided  here, if we assume the small angles, the model is reduced to the following equations \n\n$$\n\\begin{align}\n\\ddot{x}      &amp;= \\frac{u_{1}}{m} ( \\theta \\cos\\psi + \\phi \\sin\\psi) \\\\\n\\ddot{y}      &amp;= \\frac{u_{1}}{m} ( \\theta \\sin\\psi - \\phi \\cos\\psi) \\\\\n\\ddot{z}      &amp;= \\frac{u_{1}}{m} - g   \\\\\n\\ddot{\\phi}   &amp;= \\frac{l}{i_{x}} u_{2} \\\\\n\\ddot{\\theta} &amp;= \\frac{l}{i_{y}} u_{3} \\\\\n\\ddot{\\psi}   &amp;= \\frac{1}{i_{z}} u_{4} \\\\\n\\end{align}\n$$\n\nthe aforementioned equations are linear. for position tracking, we need to control $x,y,$ and $z$, therefore we choose the desired roll and pitch (i.e. $\\phi^{d} \\ \\text{and} \\ \\theta^{d}$)\n\n$$\n\\begin{align}\n\\ddot{x}^{d} &amp;= \\frac{u_{1}}{m} ( \\theta^{d} \\cos\\psi + \\phi^{d} \\sin\\psi) \\\\\n\\ddot{y}^{d} &amp;= \\frac{u_{1}}{m} ( \\theta^{d} \\sin\\psi - \\phi^{d} \\cos\\psi) \\\\\n\\end{align}\n$$\n\ntherefore, the closed form for the desired angles can be obtained as follows\n\n$$\n\\begin{bmatrix}\n\\phi_{d} \\\\\n\\theta_{d} \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sin\\psi &amp; \\cos\\psi \\\\\n-\\cos\\psi &amp; \\sin\\psi \n\\end{bmatrix}^{-1}\n\\left( \\frac{m}{u_{1}}\\right) \n\\begin{bmatrix}\n\\ddot{x}^{d} \\\\\n\\ddot{y}^{d}\n\\end{bmatrix}\n$$\n\nmy desired trajectory is shown below\n\n\n\nthe results are \n\n\n\nand the actual trajectory vs the desired one is \n\n\n\nmy code for this experiment is \n\n\n\nthe code of the desired trajectory is \n\n\n", "tags": "quadcopter matlab microcontroller", "id": "7235", "title": "how to implement and code inner and outer pd controllers for quadrotor for position tracking"}, {"body": "how would you guys recommend making a variable ballast system for an underwater robot? i was thinking about this problem earlier and i was trying to figure out if there was a way to make one that didn't require a tank of compressed air. \n", "tags": "underwater", "id": "7242", "title": "underwater rov variable ballast"}, {"body": "i am looking for a library for disparity map / stereo vision computation. these are my requirements:\n\n\nc++\nmulti-platform (linux, windows, osx)\n(preferrable but not mandatory) not cuda based\nsuited for robotics (e.g. it should work even if the images are not perfectly rectified and the cameras are not perfectly calibrated)\nsuitable for tracking purposes (20fps or more)\nperforming even with low-res images (e.g. 320x240px)\nopen source\n\n", "tags": "computer-vision stereo-vision", "id": "7243", "title": "fast c++ library for stereo vision/disparity computation"}, {"body": "i have been looking at ccd and cmos sensors and cameras to decide which one to use in the process of automatic control of a printing process. by now i am getting the grips on almost all the essential numbers and abbreviations but there remains a problem with shutters.\n\ni understand that there are different types of shutters, both mechanical and electronic, and i can understand how they work. my problem concerns shutter speed. if i use a mechanical shutter, well then the maximum shutter speed depends on that particular element in the assembly, but how does it work for electronic shutters? i have never read \"max shutter speed\" in any specs. the only thing i usually see floating around are frames per second. but those do usally not pass a limit of about 120 fps. depending on how the sensor it is built one could think that the maximum shutter speed therefore is 1/120 or 1/240 if it uses half frames.\n\ncan this be right? it seems really slow. i will be faced with the task of recording crisp and clear images of paper which moves at about 17 m/s. that is never possible with shutter speeds that slow. will i be forced to use a mechanical shutter or am i misunderstanding something?\n", "tags": "computer-vision cameras", "id": "7244", "title": "conceptual problem regarding electronic shutters"}, {"body": "i am investigating a possible business opportunity in which quadcopters perform high-precision nutritional delivery via a burrito medium. i have never used a burrito, but i have read on the internet that they typically weigh 600-700 grams (1). this is much too heavy for commercially available platforms.\n\nhow many quadcopters would it take to lift a single burrito?\n\n(1): https://www.facebook.com/chipotle/posts/390817319252\n", "tags": "quadcopter distributed-systems", "id": "7245", "title": "how many quadcopters would it take to lift a burrito?"}, {"body": "iphone contains\n\n\ngyroscope\ngps\ntwo photo and video cameras\nself-sufficient battery that outlives the motor battery\nwifi\nbackup connectivity (cellular, bluetooth)\nprogrammable computer\nreal-time image processing capabilities and face detection\ngeneral purpose io (with something like this)\n\n\nand old models are available very cheap.\n\nwhat is the main benefit of having a separate dedicated flight controller and camera on hobbyist rotorcraft rather than a general purpose device like the iphone?\n", "tags": "quadcopter", "id": "7249", "title": "quadcopter - is iphone the ultimate flight controller?"}, {"body": "i think i have just found another bug - there was one that was mentioned in another post about the angle and distance. this one is about reading the encoder's counts. i was using them as a workaround for the other bugs but what i found in one instance is that the counts i was reading from the right encoder were incorrect. i was reading in a loop sleeping for 100msec while turning the create2. here is part of the counts where it definitely shows a problem:\n\n\n\nthis kept on going until i stopped. it seems that it has a problem when it reaches the max.\n\nhas anyone else ran into this or can explain or provide another workaround? \n", "tags": "irobot-create", "id": "7251", "title": "is there a bug in the encoder counts packets 43&44?"}, {"body": "i'm a robotics student but very new to this field.\n\ncan you suggest any websites which provide projects/helpful info that i can learn from?\n\nthanks\n", "tags": "mobile-robot", "id": "7254", "title": "cool robotics projects"}, {"body": "i'm looking to build a sensor which will detect the level of liquid in a tube.\n\nit does not have to be precisely accurate, just detect whether the level is approximately above a certain height.\n\n\n\nthe liquid level can be seen in the red oval\n\n\n\ni thought about monitoring this with a webcam and using opencv to detect the liquid level. but this seemed a bit overkill. especially if i have to have a dedicated pc to process the images.\n\nsurely there's a simpler solution.\n\nperhaps a component i can attach to a raspberry pi or arduino board ... \n\ni'm not very familiar with laser sensors so i don't know what is suitable. \n\nas long as it's reliable ...\n\nedit\n\ni should add that the tube contains toluene which is flammable, and it is vacuum sealed. so we can't just drill into it. some kind of optical/laser sensor might be  ok, as long as it can recognise a clear liquid. \n", "tags": "arduino sensors raspberry-pi laser", "id": "7256", "title": "sensing the level of a liquid in a tube"}, {"body": "i'm in charge of a module to control the smoothness with which a platform should move; the platform already implements a closed-loop control on its own but this firmware is closed and i do not have access to source code.\nit is therefore requested that a closed loop control should implemented on top of that pid, in a superior layer, above a module that already implements a closed loop control, so i have several question:\n\n\nit's conceptually correct implement a pid control in an upper layer to closed loop control that implements it?\nwhat features may be loose in the lower close loop?\nmaybe loop control closed negatively be influenced by the pid that implements the top layer?\nestimate the angular speed, yaw and pitch, based on the position of the motors using kalman filters can generate values too far from the actual values reported\n\n", "tags": "pid", "id": "7265", "title": "pid over another module that implements a pid control?"}, {"body": "i have a quadcopter built, and i need to be able to make it to autonomously follow a route and avoid obstacles where possible.\n\nmy general plan is to have an array of sensors on a pre-defined \"front\". the quadcopter will only go forward. generally i'd like to make  it so that if the sensors pointing at a higher angle detect something getting closer as the bot moves forward, the quadcopter will stop, descend until the distance to that detected object decreases, and then continues forward. similarly, i'd like the opposite event to happen if the sensors pointing at a lower angle detect something getting closer to the quadcopter.\n\ni'm thinking of having something like 9 small infrared distance detectors (pointing up, forward, down || left, forward, right), basically a 3x3 matrix.\n\nwould anyone have any ideas of the feasibility of this? i'd like to use a raspberry pi, but it will probably also need an additional board to read in the values from its sensors. in addition, i have no idea which sensors to use, or if infrared can even work. any suggestions are more than welcome.\n\ni was also thinking about ultrasonic sensors, but having 9 of them could get cluttered, and i'd worry about their short range when a crash means death for the quadcopter. i also fear they would cause interference with each other.\n", "tags": "mobile-robot quadcopter sensors", "id": "7266", "title": "how can i make a quadcopter avoid obstacles using infrared?"}, {"body": "these are the specifications of the motor:\n\n\n  25000rpm no load speed at 12v\n  \n  no load current - 1a, stall current - 10a\n  \n  0.36kgcm torque\n\n\nwhat is the definition of load current and load speed? which battery would be most suitable to power this motor?\n", "tags": "motor", "id": "7270", "title": "what is load current and load speed? which battery is best suitable for this motor?"}, {"body": "as the title states, is there any way to make a following drone that tracks a gps unit, and follows/orients camera to that? similar to this\n", "tags": "cameras gps line-following", "id": "7271", "title": "how would i implement a following drone with a camera using gps?"}, {"body": "let's assume i have the following situation, and need to find (x,y). \n\n\n\nis it possible? there does not appear to be more than one solution to the system, but my trigonometry is a bit rusty.\n\ni feel like i need one more distance.\n", "tags": "kinematics inverse-kinematics", "id": "7277", "title": "assuming i have the angle with respect to two beacons, and know the distance between them, can i localize myself?"}, {"body": "i'm working on a project where i need to model a system that is essentially comprised of a series of ball-and-socket joints attached to a base, which is attached in turn to a prismatic joint (rail). \n\ni've read roy featherstone's rigid body dynamics algorithms cover-to-cover, and i've also read the dynamics section from the springer handbook of robotics (also written by featherstone). \n\nit took me a long time to get acclimated to using his \"spatial vector\" and \"spatial matrix\" notation, but after re-creating all of his notation by hand as an exercise it works out to just be a nice way of concatenating 3x3 and 3x1 matrices and vectors into 6x6 and 6x1 matrices and vectors. the maths he invents to perform operations can be a bit tedious to read as he hijacks some standard notation, but overall everything is very compact, very easy to implement in matlab. \n\nmy problem is this: how do i add actuators to the model? he walks through explicitly configuring the joint definitions, link definitions, etc., but when it comes to actuators or applied forces he says something like, \"just add a $\\tau_a$ here and bob's your uncle!\" - it's not discussed at all. in the handbook of robotics he suggests introducing a false acceleration to the fixed base to add the gravitational force term, but doesn't show how to add it in local coordinates nor does he mention how to add the actuator input. \n\nany help would be greatly appreciated. i've considered starting over with a different book, but it's going to be a great expense of my time to re-acclimate myself to a different set of notation. i'd like to move forward with this, but i feel like i'm just a few inches shy of the finish line. \n", "tags": "actuator dynamics joint", "id": "7278", "title": "adding an actuator or force to a (featherstone) articulated rigid body model"}, {"body": "in terms of robotics, what are the differences between odometry and dead-reckoning? i read that odometry uses wheel sensors to estimate position, and dead-reckoning also uses wheel sensors, but \"heading sensors\" as well. can someone please elaborate on this point for me?\n\nthanks\n", "tags": "odometry deduced-reckoning", "id": "7287", "title": "odometry vs dead-reckoning"}, {"body": "i've posted a question regarding this matter that i couldn't solve. i'm reading this paper, the authors state \n\n\n  linear $x$ and $y$ motion control: from the mathematical model one can see\n  that the motion through the axes $x$ and $y$ depends on $u_{1}$. in fact $u_{1}$ is\n  the total thrust vector oriented to obtain the desired linear motion.\n  if we consider $u_{x}$ and $u_{y}$ the orientations of $u_{1}$ responsible for the\n  motion through x and y axis respectively, we can then extract from\n  formula (18) the roll and pitch angles necessary to compute the\n  controls $u_{x}$ and $u_{y}$ ensuring the lyapunov function to be negative\n  semi-definite ( see fig. 2).\n\n\nthe paper is very clear except in the linear motion control. they didn't explicitly state the equations for extracting the angles. the confusing part is when they say \n\n\n  we can then extract from\n  formula (18) the roll and pitch angles necessary to compute the\n  controls $u_{x}$ and $u_{y}$\n\n\nwhere formula (18) is\n\n$$\nu_{x} = \\frac{m}{u_{1}} (\\cos\\phi \\sin\\theta \\cos\\psi + \\sin\\phi \\sin\\psi) \\\\\nu_{y} = \\frac{m}{u_{1}} (\\cos\\phi \\sin\\theta \\sin\\psi - \\cos\\phi \\cos\\psi) \\\\\n$$\n\nit seems to me that the roll and pitch angles depend on $u_{x}$ and $u_{y}$, therefore we compute the roll and pitch angles based on the $u_{x}$ and $u_{y}$ to control the linear motion. \n", "tags": "control quadcopter", "id": "7288", "title": "linear motion control for quadrotor (clarification)"}, {"body": "i am working on a project about robot soccer vision. \n\nhow i utilize two webcams as a stereo vision in matlab for robot soccer matters?\n", "tags": "stereo-vision matlab soccer", "id": "7291", "title": "stereo vision in matlab"}, {"body": "i've been toying around with the idea of automating the process of testing aquarium water for certain chemicals. very briefly, salt water aquariums (reefs, specifically) require almost-daily testing for 3-4 chemicals (calcium, alkalinity, ammonia, phosphate). this is typically done by hand, using various kits. there are two main types \n\n\nyou combine several powders with a fixed amount of aquarium water, and then compare the color the mixture turns with a chart\nyou combine several liquids together with the aquarium water, and then add another liquid until the mixture turns a color. you then record how much of the final liquid you had to add for the color change to occur (titration).\n\n\nboth methods are straightforward, but tedious. to maintain an aquarium well, you really do need daily readings of all of those metrics, which easily adds up to 30 minutes+ daily.\n\nso - i'd like to be able to automate the process. the biggest question is, how do i reliably dispense the materials needed? we're talking in gram and milliliter uom here. the kits come with plastic syringes and spoons of correct volume for the powders. i need a way to measure out and dispense both of these, and a way to queue up several days worth (refilling daily defeats the purpose).\n\nany ideas?\n\nedit this is different from how to measure and dispense a finite amount of powder or liquid because of the units of measure involved. i need to be able to reliably dispense ~ 1g +/- 5% of a powder, or 1ml +/- 5% of liquid.\n", "tags": "electronics", "id": "7294", "title": "dispensing precise quantities of liquid and powder"}, {"body": "well, i will start directly in my problem. i'm working on a project and i only have 10 days left.\nthe idea is simple, a wheeled robot with 3 ultrasonic sensors to avoid obstacles.\ni've developed a code and it's working fine.\ni'm using: arduino uno, l293d driver for the 2 dc motors, 3 hc-sr04 ultrasonic sensors and the newping library.\n\ni've made some kind of a shield where i soldered common points for gnd and 5v in order to connect the l293 ic and the sensors pins easily. the problem is that the ultrasonic sensors only functioned once in the expected behavior! after that they were always sending the zero result and sometimes a number is showed when i disconnect the sensor! \nis it a power problem? i'm using the usb cable to power the arduino and the sensors (motors are powered using 2 li-po batteries)\nkindly provide me with guidance\n", "tags": "arduino power ultrasonic-sensors", "id": "7295", "title": "help with ultrasonic sensors on obstacles avoiding robot"}, {"body": "suppose i have perfect ai to control robotic arm. \n\nwhat characteristics should it fulfill to be able to take such common tools as screwdriver and linesman's and disassemble and then assemble conventional notebook computer?\n\nare there such models available?\n\nis seems to me, that such arms as owi-535 are only toys, i.e. they can just relocate lightweight objects and that's all. am i right?\n\nupdate\n\nalso suppose that my ai can look at assembly area with multiple hd cameras and can perfectly \"understand\" what is sees.\n", "tags": "robotic-arm", "id": "7297", "title": "arm to disassemble and assemble notebook at home?"}, {"body": "i have 2 of these 12v motors and a 12v battery\nhttp://www.enigmaindustries.com/motors/bosch_ev_warrior.htm\n\ni would like to know what the best solution for controlling this motor with an arduino uno would be.\ndoes the motor controller need to have a maximum current of 100a?\n\ni though of a 100a transistor connected to the pwm pin of arduino, and then, control the motor with pwm.\n\nis voltage regulator better than pwm?\n", "tags": "motor esc pwm microcontroller", "id": "7308", "title": "how to control a dc motor"}, {"body": "i want to make a compact (actuators motors and sensors are all in one) soft robot. actuators can be pneumatic or dielectric. i need suggestions about manufacturating. i'm open to new ideas.\n", "tags": "actuator manufacturing", "id": "7311", "title": "how can i make a compact soft robot"}, {"body": "currently designing a spherical wrist, i want to manipulate a 300gr payload.\nthe design has a 200mm span, so i'm guessing at a 1.1nm (considering the weight of structure &amp; motors).\n\n\ni've looked at maxon, faulhaber, but can't find any motor+gearbox+encoder under a 100gr.\nany suggestion ?\n", "tags": "motor actuator torque", "id": "7315", "title": "2nm small motor"}, {"body": "i'm looking into cctv, and am interested in minimising costs by having a motion tracking camera cover an area that would otherwise utilise multiple cameras.\n\nthere is already something like this on the market manufactured by a company called nightwatcher (i think).  however, it does not track, it merely senses using 3 pir's and points the camera in 1 of 3 positions. ping ponging between them if the subject is between sensors.\n\ni like the idea of this, but not the drawbacks, and was wondering if there was anything i could do with an arduino or similar to achieve a better result.\n\ni stumbled across this, but am not entirely sure about it. also this is for outside application, and that thread is for indoor (if that makes a difference).\n\nhttp://robotics.stackexchange.com/a/1397/9751\n\nedit...\n\njust in case i have mislead you, i want to have a unit where sensors detect movement and then a camera to face that position.\n", "tags": "cameras", "id": "7316", "title": "how can i make a motion tracking camera?"}, {"body": "i'd like to buy a small vacuum lifter so that i can move playing cards around with robotics. but my \"google-fu\" is failing me. i don't really know what search terms to look for... or what webpages to look to find this kind of component.\n\nin essence, i want an electronic version of a vacuum pen.\n\ni don't really know where to search for this kind of component. i've found pneumatic valves and other complicated machinery... but ideally i'd want a self-contained electronic vacuum pen. since i'm only planning to move playing cards around.\n\nanyone have an idea where to look for something like this? thanks.\n", "tags": "robotic-arm actuator", "id": "7317", "title": "vacuum lifter: moving playing cards"}, {"body": "i want to make 3d position and position change estimation from photos taken from flying robot. i need suggestions for fast photo matching. \n", "tags": "localization", "id": "7320", "title": "position estimation from photo fingerprinting"}, {"body": "folks at programmers stack exchange asked me ask here:\n\ni want to communicate with an arduino and sent integers it. i code this program in c++. i initialy used bulk_transfer(), but it sends only char data.\n\nthis in the api reference for libusb:\nhttp://libusb.org/static/api-1.0/group__syncio.html\n\nhere is the prototype of bulk_transfer()\n\nint     libusb_bulk_transfer (struct libusb_device_handle *dev_handle, unsigned char endpoint, unsigned char *data, int length, int *transferred, unsigned int timeout)\n\nas you can see, data is an unsigned char pointer, that is, a pointer to a buffer containing length unsigned chars. i can successfully transcieve strings. how do i transfer integers with sign?\n\ncurrently i am thinking about a system in which the arduino asks for the digit by sending a character and my program sends the number as reply followed by the sign, which is requested next. is this solution viable? or should i transfer the integer as a string? is there a better way?\n", "tags": "arduino communication usb c++", "id": "7327", "title": "how to transfer signed integers with libusb?"}, {"body": "lately, if you notice i have posted some questions regarding position tracking for nonlinear model. i couldn't do it. i've switched to linear model, hope i can do it. for regulation problem, the position control seems working but once i switch to tracking, the system starts oscillating. i don't know why. i have stated what i've done below hope someone guides me to the correct path. \n\nthe linear model of the quadrotor is provided here which is \n\n$$\n\\begin{align}\n\\ddot{x}    &amp;= g \\theta \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)\\\\\n\\ddot{y}    &amp;= - g \\phi \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2)\\\\\n\\ddot{z}    &amp;= \\frac{u_{1}}{m} - g \\\\\n\\ddot{\\phi} &amp;= \\frac{l}{j_{x}} u_{2} \\\\\n\\ddot{\\theta} &amp;= \\frac{l}{j_{y}} u_{2} \\\\\n\\ddot{\\psi} &amp;= \\frac{1}{j_{z}} u_{2} \\\\\n\\end{align}\n$$\n\nin this paper, the position control based on pd is provided. in the aforementioned paper, from (1) and (2) the desired angles $\\phi^{d}$ and $\\theta^{d}$ are obtained, therefore, \n\n$$\n\\begin{align}\n\\theta^{d}  &amp;= \\frac{\\ddot{x}^{d}}{g}  \\\\\n\\phi^{d}    &amp;= - \\frac{\\ddot{y}^{d}}{g}\n\\end{align}\n$$\n\nwhere \n\n$$\n\\begin{align}\n\\ddot{x}^{d} &amp;= kp(x^{d} - x) + kd( \\dot{x}^{d} - \\dot{x} ) \\\\\n\\ddot{y}^{d} &amp;= kp(y^{d} - y) + kd( \\dot{y}^{d} - \\dot{y} ) \\\\\nu_{1} &amp;= kp(z^{d} - z) + kd( \\dot{z}^{d} - \\dot{z} ) \\\\\nu_{2} &amp;= kp(\\phi^{d} - \\phi) + kd( \\dot{\\phi}^{d} - \\dot{\\phi} ) \\\\\nu_{3} &amp;= kp(\\theta^{d} - \\theta) + kd( \\dot{\\theta}^{d} - \\dot{\\theta} ) \\\\\nu_{4} &amp;= kp(\\psi^{d} - \\psi) + kd( \\dot{\\psi}^{d} - \\dot{\\psi} ) \\\\\n\\end{align}\n$$\n\nwith regulation problem where $x^{d} = 2.5 m, \\ y^{d} = 3.5 m$ and $z^{d} = 4.5 m$, the results are \n\n\n\n\nnow if i change the problem to the tracking one, the results are messed up. \n\n\n\n\nin the last paper, they state \n\n\n  a saturation function is needed to ensure that the reference roll and\n  pitch angles are within specified limits\n\n\n\n\nunfortunately, the max value for $\\phi$ and $\\theta$ are not stated in the paper but since they use euler angles, i believe $\\phi$ in this range $(-\\frac{\\pi}{2},\\frac{\\pi}{2})$ and $\\theta$ in this range $[-\\pi, \\pi]$\ni'm using euler method as an ode solver because the step size is fixed. for the derivative, euler method is used. \n\nthis is my code \n\n\n\nfor the trajectory code\n\n\n", "tags": "control quadcopter matlab", "id": "7331", "title": "position control for linear model of quadrotor (problem with tracking task)"}, {"body": "i'm new to robotics and this is my first time building a quadcopter.  i'm unable to work out why i keep losing escs.\n\nmost recently in testing, i've managed to calibrate all 4 escs and accurately control the speed of all 4 motors.  but after neatly securing them to the frame, 1 motor didn't work.  i recalibrated the escs again and, when running them again, the motor still didn't work.  however, the other 3 motors continued to run at first, but also suddenly just stopped altogether.\n\nresearch suggested that escs have a cut-off voltage, indicating that my battery might be too flat, so i immediately looked to recharging it. to my surprise, the (still very new) battery appeared to have bulged out, indicating that it had been damaged.\n\nfurther research suggested that the size of the battery i was using is insufficient for the amount of current drawn by the motors.  so, without any pwm applied, i reconnected a new fully charged battery in the hope of listening for any beeps to diagnose, and one esc immediately coughed up a huge puff of smoke.\n\nbefore all of this happened, i only managed to get 2 of the escs to run their motors.  despite several attempts at tweaking pwm signals and calibrating them, i ended up replacing the other 2.\n\nunless there's some obvious reason for my escs to keep dying on me, i can only assume that these specific escs are badly made and i should ask for my money back.\n\nthese are the components i'm using:\n\n\nraspberry pi 2 model b\nadafruit 16-channel 12-bit pwm/servo driver - i2c interface - pca9685\nrctimer mini esc 40a opto blheli firmware (oneshot125, support 2-6s)\nrctimer 2208-8 2600kv outrunner brushless motor\ngens ace 2200mah 11.1v 25c 3s1p lipo battery pack\n\n\nthe raspberry pi is powered through its micro usb interface by a 5v step-up voltage regulator connected to a 5000mah 3.7v lipo battery.  the pwm controller is powered to its vcc pin by the gpio1 (3.3v) pin from the raspberry pi, that also happens to power other sensors.\n\nat the time (when all 4 motors worked), i was able to accurately control them at either 50hz or 400hz with 1-2 millisecond duty cycles.\n", "tags": "quadcopter power esc pwm", "id": "7333", "title": "why do my escs stop working?"}, {"body": "i am using dc motors to build a robotic arm.  i want to make the base shoulder (which rotates and lifts) more stable and stronger.  how should i design this using dc motors?\n\nalso i would like to put the motor for the elbow in the base for efficiency.  which design best suits this? \n\nupdate\ni am building a robotic arm for a payload of approx. 1-2 kg and using dc high torque motors. in this model, i am using only a shoulder with a gripper. the gripper is self made by me weighing approximately 400 grams. i want to have a proper design and material choice so that the shoulder part remains less heavy and more stable.\n\nin addition to this i want to operate the movement of the gripper, i.e. the up and down motion, by using the motor in the base part. what should be my design and better alternative?\n", "tags": "robotic-arm design", "id": "7334", "title": "mechanical design for base of robotic arm"}, {"body": "i'd like to create a camera slider similar to this one.\n\n\nthe only part i'm not sure on is how to setup the camera drive.  it looks like i can buy a similar timing belt here, but i'm not sure how to set up the servo to drive the slider.  particularly how to keep the belt in contact with the drive pulley. \n\nmy fabrication skills are very limited so i need a simple or out of the box solution.\n", "tags": "servos", "id": "7339", "title": "driving a non-circular timing belt"}, {"body": "there is too many 2d position estimation with one camera. is there any 3d position estimation application or technique with one camera? if there is no application or technique why?\n", "tags": "localization", "id": "7344", "title": "image based 3d position estimation with one camera"}, {"body": "i've already made an arduino device which detects the trigger event, but now i want it to trigger the recording and storage of video when this event occurs.  if the camera could be wirelessly triggered a few feet away from the arduino unit, that would be optimal, but i can settle for running wires if need be.\n\ni'm looking for suggestions because i'm on a limited budget for this project. i want to avoid reinventing the wheel and ordering parts which i can't get to work with an arduino.\n\ni'm considering the use of this camera.\nhttps://www.sparkfun.com/products/11418\n\nthis is my first arduino project.  any help is very welcome.\n", "tags": "arduino wireless", "id": "7349", "title": "arduino triggers a camera to start recording"}, {"body": "are there good low cost cameras that are frequently used in robotics?\n\ni am assuming there are cameras that are good fit for robotics ...\n\n\nworks well with opencv\npc windows support - usb2/usb3 (gige, usb3 vision cameras seem pricey)\ngood image sensing performance\nadjustable focus - manual or motorized (fine focus control would be great)\n\n\ndo ip cameras make good cameras for robotic vision projects?\n", "tags": "computer-vision cameras", "id": "7358", "title": "suggestion for a camera"}, {"body": "i would like to make a robotic system which takes as input a video feed, runs some gpu-based image recognition on the video, and outputs commands to a set of motors. the goal is to have the motors react to the video with as little latency as possible, hopefully of the order of 10s of ms. currently i have a gtx 770m on a laptop running ubuntu 14.04, which is connected to the camera and doing the heavy image processing. this takes frames at 30hz and will output motor commands at the same frequency.\n\nafter a few days of looking around on the web for how to design such a system, i'm still at a loose end whether (a) it is even feasible (b) if so, what the best approach is to interface the laptop with the motors? the image processing must run on linux, so there is no leeway to change that part of things.\n", "tags": "control real-time", "id": "7359", "title": "interfacing gpu image processing with motor control at 30+hz"}, {"body": "i am learning and i am interested robotics, but also i need to update my web development skills so the question is - is there any idea for good web application that could be connected with robotics - service robots, industrial robots etc. maybe there already is some open source ongoing web application projects for robotics in which i can make contribution.\n\nthanks!\n", "tags": "design software", "id": "7361", "title": "idea for web application in robotics"}, {"body": "what is the difference between multiple robots and swarm robots? what is the key point? also what is multi agent systems? do multi agent systems works only for computer simulations or games? these terms are used similar applications.\n", "tags": "simulation multi-agent swarm", "id": "7363", "title": "what is the difference between multiple robots and swarm robots?"}, {"body": "are there electric motors, which apply force not in rotational motion, but in longitudinal motion?\n\nthey should have electromagnetic design and no gears and worms.\n\nsuch motors would be great for linear actuators. they could transfer both force and feedback.\n\nwhat is the name of such devices?\n", "tags": "motor actuator", "id": "7378", "title": "electric piston (longitudinal electric motor)?"}, {"body": "i'm working with a 4dof parallel-mechanism arm. i'm interested in writing planners for this arm (prm or rrt) in the configuration space, but i'm not sure how to identify obstacles/collisions. \n\nwhen writing planners for mobile robots in a 2d workspace, it was easy to define and visualize the workspace and obstacles in which the planner/robot was operating. this website (link) shows a great example of visualizing the workspace and configuration space for a 2dof arm, but how can i do this for higher dimensions?\n", "tags": "robotic-arm motion-planning", "id": "7383", "title": "determine the configuration space for a robotic arm"}, {"body": "i'm curious about this alloy and how they say it can be used as an alternative to a traditional compressor. can anyone explain how this would work?\n\nmy goal is to understand that use case so i can adapt alloys in other robotic projects.  my gut tells me this is perfect for some kinematics, or other mechanisms, but i'm missing some pieces in this puzzle (how would it work?)\n", "tags": "kinematics mechanism", "id": "7384", "title": "how can a memory alloy be used as an alternative to a compressor found in a refrigerator?"}, {"body": "123d software can construct a 3d model from photos taken from your phone. it doesn't process the photos in your phone.  instead, it sends them to the cloud to create 3d model. how can i construct a 3d model like this (only with one camera)? i searched it but i can only find information on laser/procetor scanners (simple and desktop use only). i think 123d uses only imu sensors and camera why do they use the cloud?  can a beaglebone or rasperry pi create 3d models like this?\n", "tags": "computer-vision 3d-printing 3d-reconstruction 3d-model", "id": "7385", "title": "3d scanner from phone camera"}, {"body": "i am reading the book \"introduction to robotics mechanics &amp; control\", john j craig., 3rd ed., forward transformation problem examples 2.2 and 2.4.\n\nex. 2.2 (page 29): frame {b} is rotated relative to frame {a} about x axis by 60 degrees clockwise, translated 20 units along y axis and 15 units along z axis. find p in frame {a} where p in frame {b} = [0 8 7]\n\nthe book's answer is [0.18.062 3.572].\nbut my answer is [0 30.062 11.572].\n\nex. 2.4 (page 33): vector p1 has to be rotated by 60 degrees clockwise, about x axis and translated 20 units along y axis, and 15 units along z axis. if p1 is given by [0 8 7], find p2.\n\nessentially ex.2.2 and 2.4 are the same problem. however, the transformation matrix for ex 2.4, has [0 8 7] as translation vector (the 4th column of t) instead of [0 20 15]. and, the given answer is [0.18.062 3.572].\n\ni am not sure if it is just typo, or i am missing some genuine operation. please let me know your opinion.\n\nthanks.\n", "tags": "forward-kinematics books", "id": "7386", "title": "introduction to robotics mechanics & control, john j craig., 3rd ed., forward transformation problem examples 2.2 and 2.4"}, {"body": "i'm trying to understand the scan-matching part of hector slam (ppt summary). it seems a little difficult to understand, in some cases, how is it possible to actually perform the alignment of the scans. can anyone explain about it?\n\nin my case, i'm working with a simulation. i'm moving my robot in a corridor-like featureless environment (only two walls) and i don't get a map. nevertheless, if i move in a sinewave motion, i'm able to get a map. moreover, if i have an additional feature, the algorithm even shows the real path as long as this feature is seen (right part of the image), otherwise it shows a very weird-looking oscillatory path which does not resemble a sinewave at all. something important to notice is that the width of the map is pretty accurate (real=4m, map's=4.014m), and the length of the movement is also somehow accurate (real=15m, map's= 15.47). i'm using a hokuyo urg-04lx laser range finder, no odometry, no imu. i'm running in ubuntu 14.04 and using ros indigo.\n\n\n\ni more or less understand how hector works, but i have no idea about why i'm getting this map and specially trajectory.\n\nthank you.\n", "tags": "localization slam ros mapping rangefinder", "id": "7387", "title": "hector slam, matching algorithm"}, {"body": "this paper mentioned the fingerprinting/model matching case. but i could not find an image based algorithm. any suggestion about image based localization\n", "tags": "mobile-robot localization", "id": "7388", "title": "fingerprinting/ model matching algorithms for localization"}, {"body": "i wish to build a chess playing robot with robot arm as shown on youtube, can anyone please tell me which robot arm would suit my purpose and whether it can be bought second hand or alternatively anybody willing to sell used chess arm robot? please help out.\n", "tags": "robotic-arm", "id": "7389", "title": "robotic arm for playing chess"}, {"body": "i'm relatively new to robotics, and i'm building a project for which i need a simple wireless connection between two circuits such that when the first circuit is switched on, the other circuit gets switched on too. i'm looking to preferably build something like this on my own, but i have no idea about wireless connections. i only know basic wired robotics. i also know c++ programming if that helps. apologies if such a question has already been asked.\n\nregards,\nhanit banga\n", "tags": "wireless", "id": "7395", "title": "simple wireless connection between two circuits"}, {"body": "i'm looking for some cheap hardware that would offer me results decent enough to continue my experimentation.\n\ni've been looking into how to obtain hardware for learning about stereo vision and 3d reconstruction, i found two basic ways: - buy 2 cheap webcams and diy - buy a stereo camera\n\nfor what i understood little variations in distance and inclination can easily compromise the diff map and so the diy version might end up requiring constant calibrations, however on the other end, so buying \"professional\" stereo camera range from 500 euro to infinite.\n\nfor the moment i trying something in between, like the minoru 3d, however the overall performance of the camera looks a bit poor also because it's a 2009 product, however i can't find any more recent product offering a similar solution.\n\ncan you suggest me what would be the best way/product/guide to archive decent results without spending a fortune ?\n\nthank you very much :)\n", "tags": "stereo-vision", "id": "7397", "title": "selecting hardware: stereo camera for beginners"}, {"body": "does anybody know where i can get a matlab toolbox or functions to work with a sick laser-scanner (windows os)? i'm using a sick-ldrs2110 with ethernet cable, but sopas software does not allow me to program recording times and other specific tasks. any tips are more than welcome! \n\nthanks!\n", "tags": "laser matlab", "id": "7398", "title": "matlab toolbox (windows) for sick lasers?"}, {"body": "after having been determined my control loops for my quadcopter project, i'm going to determine the motor commands (pwm duty cycle) from the motor forces/torques. i was following the guidelines of this document but when i was trying to do the inverse of the matrix m (page 17) it has determinant equal to 0. the procedure is correct? anyone can suggest me some other link for doing this conversion? i have searched in the internet but i haven't found so much about that. thanks\nthe part of the document that i'm referring is the following:\n\n", "tags": "control quadcopter pwm", "id": "7400", "title": "quadcopter force/torques duty cycle conversion"}, {"body": "i have been recently working on code for a robot maze solver using laser sensors and odometry data. i went through the pdf available online $\\textbf{'slam for dummies'}$ and understand the process conceptually. i am a master's student in control so that part wasn't hard but writing the code in c++ is the difficult task. the ekf slam codes available on the site $\\textbf{openslam.org}$ seem a bit advanced since i am a beginner. i couldn't find on the site (maybe missed) the simplest ekf slam algorithm for 2d implementation. \n\ncould anyone guide me to an open source code in c++ for 2d implementation so that i can build up on it to suit the robot i am working on?\n", "tags": "slam", "id": "7403", "title": "ekf slam c++ code on openslam.org"}, {"body": "i just un-boxed and set the create 2 to charge over night.\n\nhow do i program it? where is the software?\n\ndaniel\n", "tags": "irobot-create programming-languages", "id": "7406", "title": "how do i program the create 2"}, {"body": "i am doing a line following robot based on opencv. i have my onboard computer(an old pandaboard) running opencv. it will calculate the offset from the required path and communicate it to the arduino via usb. then it will do pid optimisation on the data, and adjust the speed of the left and right motors.\n\nto my dismay the communication part is not working, and i've tried hard for a day to fix it with no result. here is the relavent code running on the pandaboard:\n\n\n\nwhere  is the data to be send. this is the code running on the arduino:\n\n\n\nwhat happens when i run is that it will pause at the libusb read operation as the timeout is zero(infinity). at this point i've tried resetting the arduino, but this doesn't help. so how do i make my program respond to this start byte send my the arduino? where did i go wrong?\n", "tags": "arduino communication usb", "id": "7409", "title": "libusb and arduino communication not working"}, {"body": "i want to detect and identify each of the vehicles passing through a gate. \n\ni have the live video feed of the gate which i initially thought to process and detect the number plates with the help of opencv or any other graphics library freely available. the problem is, the size of number plates may vary very widely, and the language the number plates are written with(bengali) does not have a good ocr performance at all.\n\nthe next idea was to put a qr code in the windshield of the vehicles. (yes the vehicles supposed to enter the area are private and enlisted vehicles). but i am not confident that i will be able to detect and identify all the qr codes in real time with 100% accuracy, as the qr codes might get pixelated due to low resolution of video.\n\nso can anyone suggest any other cheap way we can adopt to detect and identify the vehicles? can nfc or any other cheap sensors be used for this purpose?\n", "tags": "sensors design computer-vision", "id": "7411", "title": "what is the cheapest way to detect and identify vehicles entering a gate in real time?"}, {"body": "i'm trying to design two pd controllers to control the roll and pitch angle of my quadcopter and a p controller to control the yaw rate. i give to the system the reference roll, pitch and yaw rate from a smartphone controller (with wifi).in the case of roll and pitch the feedback for the outer 'p' loop is given by my attitude estimation algorithm, while in the inner 'd' loop there is no reference angle rate, and the feedback is provived by a filtered version of the gyroscope data.\nas far the yaw rate is concerned, is only a p controller, the reference yaw rate is given by the smartphone, and the feedback of the only loop is provived by the smartphone. this is to illustrate the situation. my sampling frequency is 100hz (imposed by the attitude estimation algorithm, that is a kalman filter, that i'm using). i have tuned my controller gains with matlab, imposing a rise time of 0.1 seconds and a maximum percent overshoot of 2% with root locus. matlab is able to found me a solution, but with very large gains (like 8000 for p and 100 for d). i was doing the tuning, using a quadcopter model (for each euler angle) based on the linearized model for quadcopter or instance : $$\\ddot \\tau_\\phi = i_x\\ddot \\phi    -&gt;  g_\\phi(s) = \\frac{i_x }{ s^2} $$ only in order to have a 'reasoned' starting point for my gains, and then re-tune it in the reality. (the transfer function above is continous, in my model i have obliviously used the discrete version at 100hz of sampling rate). \nthis is to do a premise of my following questions.\nnow, i have to map my controller outputs to duty cycle. since i'm using a pwm at 25khz frequency, my period (in the tim channel configuration) is of 2879.\ni have checked the activation threshold (after which the motor starts move) and the threshold after which it stops increasing its speeds, and the first is 202\nand the second is 2389.\ni was following the very good answer of quadcopter pid output but i still have some questions.\n\n1) as far the throttle mapping is concerned, i have to map it in such a way that the values coming from my smartphone controller (in the interval [0, 100]) are not\nmapped in the whole [202, 2389] interval, but i have to 'reserve' some speed in order to allow the quadcopter to have an angular movement exploiting differences in the 4 motor speeds even with 100% throttle?\n\n2) coming back to the fact that matlab propose me huge gains for my controllers, this leads to the fact that i cannot directly sum the controller output to the duty cycle as stated in the metioned answer (because i will certainly go out of the [202, 2389] bound of my tim pulse). doing a proportion will result in altering the gains of the systems, so placing somewhere else the poles of my systems and the procedure done with matlab will became useless, right? so, what i'm doing wrong? i have tried to enforce matlab to bound the gainsm for instance in the [0,100] interval, but in this case it cannot find gains such that my constraints are verified.\nthank you\n", "tags": "control quadcopter pid matlab", "id": "7415", "title": "quadcopter pid output and duty cycle conversion"}, {"body": "currently using windows 8, what software packages for artificial intelligence programming (robotics branch) are used in today's professional environment as standard. lots of internet suggestions, but companies seem to keep this a closely guarded secret. and are the internet rumors true? would switching to ubuntu offer me more in terms of depth.\n\ncontext: educational field: computer science and artificial intelligence, current focus (though obviously experience in others) in programming languages stands at c++, c and python. looking to build, program and develop a human-like bot (not aiming for singularity at this point ;))and am asking this question in order to build my toolbox a little. \n", "tags": "design software artificial-intelligence programming-languages", "id": "7416", "title": "artificial intelligence software packages: professionals, university education is oft' a step behind. what's actually being used?"}, {"body": "i have an application that requires data to be streamed from multiple bluetooth modules to one host controller. somewhat like multiple clients and one server. \n\nthe throughput i am looking at is around 1920-bits per second per module. \nthe spbt2632c2a.at2 module only supports spp profile in which i can have a single link (one client one server). my application needs multiple modules ( max 5) to send information to one server.  \n\nis there a way to have one receiving station and have multiple transmitting module using spp? (all modules being the spbt2632c2a), or i need a different higher end module on the server side which supports multiple spp links?\n\nit advisable to look into a module like the bcm2070 and have a driver run system?\n", "tags": "electronics", "id": "7423", "title": "many to one bluetooth communication link"}, {"body": "i know that is a question that has been asked too many times, but still its not clear to me. i read online that it isn't but some people say that they control their robots under ros in applications with hard real time constraints. so, because i need some technical arguments (rather than a plain \"ros is not real time\") i will be more specific (suppose we have ros under a rtos):\n\n\ni read that ros uses a tcp/ip-based communication for ros topics and i know that tcp/ip is not reliable. that means i cannot use topics in a real time loop? for instance send a control signal to my system publishing it to a topic, and the system sending me some feedback via a topic?\nif i have a rtos (eg linux+xenomai) can i build a real time control loop for a robot using ros, or ros will be a bottleneck?\n\n\nmaybe the above are naive or i lack some knowledge, so please enlighten me!\n\nnote: i define as a hard real time system (eg in 1khz), the system that can guarantee that we will not miss a thing (if the control loop fails to run every 1ms the system fails).\n", "tags": "ros real-time", "id": "7429", "title": "is ros hard real time safe?"}, {"body": "edit: i realised i missed the point of the paper completely (thanks to very-skim reading ;) ). so, this part of it i'm relating to is about how much damping - not how much stiffness - should we display to obtain stability, given a structural stiffness. i changed the question accordingly - what is achievable stiffness of a impedance/admittance controlled robot, given its structural and control stiffnesses? (stiffness/compliance is, of course, mathematically just one of the terms in total impedance/admittance)\n\nlet us consider a haptic device with mechanical and control parts, and mechanical part is not infinitely rigid (compliant). basically, it would be a robot with impedance or admittance control. i thought perceivable stiffness can be just as simple as serial connection of two stiffnesses - and so the stiffer mechanical structure is, the better it can display control stiffness:\n\n$k = \\frac{k_e k_c}{k_e + k_c}$\n\nwhere $k_c$ is stiffness control. still, i cannot find any confirmation to this, although something very similar is stated in samur's \"performance metrics for haptic interfaces\". i would be very grateful if you could refer me to some sources or just plain prove it wrong or right (:\n\nin a paper (here, p. 728) i only found stability condition for virtual damping value in relation to virtual stiffness, given structural stiffness.\n", "tags": "control mechanism reference-request", "id": "7434", "title": "what is the achievable stiffness of a impedance/admittance controlled robot (incl. haptic devices), given its structural and control stiffnesses?"}, {"body": "i'm working on an robot that would be able to navigate through a maze, avoid obstacles and identify some of the objects in it. i have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation. \n\nup till now, i have converted/read the bitmap image of the maze into a 2d array of bits. however, now i need guidance on how to use that array to plan the path for the robot. i would appreciate if you could share any links as well, because i am new to all this stuff (i am just a 1st year bs electrical engineering student) and would be happy to have a more detailed explanation.\n\nif you need me to elaborate on anything kindly say so.\n\ni would be grateful!\n\nhere's the image of the maze.\n\n\n\nthis is just a sample image; the robot should be able to work with any maze (image) with similar dimensions. and you are welcome!\n\nthank you chuck!\n\nupdate\nheres the code for sub2ind in c++. kindly see if the output is correct:-\n\n\n\nheres the link to the output file.\nhttps://drive.google.com/file/d/0bwuks98dxycusk5fbnk1ddjnq00/view?usp=sharing\n", "tags": "arduino mobile-robot localization mapping planning", "id": "7438", "title": "how to make a directed graph?"}, {"body": "i'm using the control system toolbox provided by matlab to estimate the gains of my controller: using root locus design i get a graph like this one .\n\nmy question is: what is the x on the x-axis? maybe a pole position at a previous iteration of the optimization procedure that i have run to find a gain value that satisfies my requirements? it shouldn't be the open loop pole position, because my system is formed by two integrators multiplied by a constant (1/inertia). thanks\n\nedit: i add the requested details: i start from the following simulink diagram:\n\nmy trasfer function is $$g_\\theta(s) = \\frac{y(s)}{u(s)} = \\frac{\\theta(s)}{\\tau_\\theta} =  \\frac{1}{i_y s^2}$$ with\niy = 0.0054 (another little question, the point in which i'm taking out the torque is correct?)\nand then i select analysis , control design, compensator design. i select kp and kd as the gains to be tuned, and i use the root locus for specifying the constraints. then i click in siso design task, automated tuning, optimize compensator, which automatically tries to find gain values to satisfy my constraints. the white are is the area that satisfies the constraints, and i think that the pink squares are my poles position after having been completed the optimization procedure. this is correct? but in this case, what is the x(pole) shown? thanks\n", "tags": "control matlab", "id": "7440", "title": "matlab control toolbox root locus"}, {"body": "my question is: in a lot of cases it is possible to find in internet pd (instead pid) to control the euler angles of quadcopter? why the integral part is often neglected in this kind of applications? thanks\n", "tags": "control quadcopter pid", "id": "7445", "title": "why pd controllers for quadcopter angles control?"}, {"body": "i've seen that it is possible to use some micro controller to send commands to the roomba through the sci but i was more interested in changing the behavior of the roomba operation (e.g: change the priority of the behaviours)\nis there some ide for roomba?\n\nregards\n", "tags": "roomba", "id": "7449", "title": "changing behaviour roomba 880"}, {"body": "let me know if this should be on academia instead, but i posted it here to get responses specifically from people active in robotics development.\n\ni'm currently an undergraduate student completing majors in both mechanical engineering and computer science. i'm still fairly new to the field, but my interest is firmly in electronic and mechanical systems. next year i can take one of the courses below:\n\n\n\ni want to take all three and likely will eventually, but for the time being my schedule only allows for one. therefore, i was wondering if you could explain a little bit about how each is applied to the robotics field and which you believe will be most helpful for me to learn now.thanks in advance!\n", "tags": "beginner theory", "id": "7450", "title": "which math course will be most beneficial?"}, {"body": "there's an accelerometer in the imu. the output can then be integrated to estimate the position, at least in theory.\n\nbut in practice, there's a huge acceleration from gravity, which varies rather randomly across locations. vibrations etc can be filtered out with low-pass filters, but how do you filter out gravity? is it simply the case that the vertical vector is ignored when doing any calculations?\n\nmy application is, i want to build a quadcopter that could hover in one place even in the presence of (reasonable) winds: the quadcopter ideally would tilt towards random gusts to maintain a certain position. every single tutorial i could find on the internet only uses the accelerometer to estimate where down is when stationary, and simply assumes that using the gyroscope to hold the quadcopter level is enough. \n\ni also want to use the imu to estimate altitude if possible, of course as an input to something like a kalman filter in conjunction with a sonar system.\n\nobviously, for my application gps is far too slow.\n", "tags": "quadcopter imu", "id": "7454", "title": "how to use an imu to hover at a fixed location in a quadcopter in the presence of gravity?"}, {"body": "hi i want to implement an human arm robot and a task such as moving a glass between two points  using robotic toolbox for matlab  by peter coorke. i'm a student and i'm a newbie in this kind of things so i would find a good reference for solving the inverse kinematics of the human arm and  an algorithm that implements some kind of obstacle avoidance exploiting the redundancy of the manipulator (7dof) using null space motion.  anyone can suggest me a good reference to follow in this implementation with the toolbox? thanks\n", "tags": "robotic-arm inverse-kinematics manipulator matlab", "id": "7456", "title": "human arm inverse kinematics"}, {"body": "i couldn't find a sub stackexchange for artificial intelligence, but i think robotics comes close, and so i'm posting here.\n\ni recently saw ted talks on ai and the google car, with these being the most interesting to me:\n\n\nhod lipson - building \"self-aware\" robots \njuan enriquez - the next species of human\nray kurzweil - get ready for hybrid thinking\n\n\nthe third one led me to the 'criticism' section (labeled  on that wiki article, though it certainly at least partially reads as a criticism section as well) of kurzweil 'theory' of the brain, namely \"pattern recognition theory of mind\" (prtm).  after some link surfing on the people who have performed analysis of prtm and their respective academic contributions, i came to learn about cognitive architecture:\n\n\n  \"a cognitive architecture can refer to a theory about the structure of\n  the human mind. one of the main goals of a cognitive architecture is\n  to summarize the various results of cognitive psychology in a\n  comprehensive computer model. however, the results need to be in a\n  formalized form so far that they can be the basis of a computer\n  program. by combining the individual results are so for a\n  comprehensive theory of cognition and the other a commercially usable\n  model arise. successful cognitive architectures include act-r\n  (adaptive control of thought, act), soar and opencog.\"\n\n\nit appears that there are several interesting architectures, including the 3 mentioned above.  i read a bit about act-r, soar, opencog, dual, chrest, and clarion.  the list is not comprehensive.  it also appears that there are two main types of such architectures: connectionism and symbolic.\n\nthough i have many questions, my main question is this:\nwhat are some quantitative metrics and qualitative properties to measure and compare between the two architecture types?\n\nother questions\n\n\ncan all architectures be categorized as one, the other, or some\ncombination of the two, or is there a third, fourth, etc? \nhow are two main types alike? how are they different?\nwhat are some recommended further readings on this topic. \nwhat centres and organizations are leading development in this?\nwhat are some of the computer programming languages, related skill-sets, and\ncross-domain knowledge set utilized in r&amp;d and product offerings of\nsuch systems?\n\n", "tags": "artificial-intelligence", "id": "7457", "title": "cognitive architectures: how do you perform qualitative and quantitative comparisons?"}, {"body": "i'm looking for an equation (or set of equations) that would allow me to predict (with fair accuracy) how heavy a payload a quadcopter is capable of lifting.\n\ni assume the main variables would be the weight of the copter as well as the size + power of the 4 rotors. what general approach can one use to make such a determination?\n", "tags": "quadcopter", "id": "7460", "title": "how to calculate quadcopter lift capabilities?"}, {"body": "i am interested in building a quadcopter from scratch.\n\nbecause i like to err on the side of caution, i'm considering adding \"safety cages\" around each propeller/rotor, to hopefully prevent (at least minimize) the chance of the spinning rotor blades coming into contact with someone. without knowing much about the physics behind how \"lift\" works, i would have to imagine that cages present two main problems for rotors:\n\n\nthey add weight to the copter making it harder to lift the same payload; and\nthey're sheer presence/surface area makes it harder for the spinning rotor to generate lift and push down away from the ground\n\n\nthe former problem should be obvious and self-evident. for the latter problem, what i mean by \"surface area\" is that i imagine that the more caging around a spinning rotor, the more difficult it will be to lift effectively. for instance, a spinning rotor might have the ability to generate enough power to lift, say, 2kg. but if we were to construct an entire box (not cage) around the entire rotors, with 6 sides and no openings, i would imagine its lift capability would drop to 0kg.\n\nso obviously, what i'm interested in is a cage design that provides adequate safety but doesn't \"box in\" the rotor so much that it causes the rotor to be ineffective or incapable of providing lift. so i'm looking for that optimal tradeoff of safety (boxing/caging around the spinning rotor) and lift performance.\n\ni would imagine calculating and designing this is a pretty huge undertaking with a lot of math behind it. i'm just wondering if anyone has already figured all this stuff out, or if anyone knows of a way to model this safety-vs-lift-performance trade off in some way.\n", "tags": "quadcopter", "id": "7462", "title": "how do safety cages around quadcopter rotors/blades affect lift capabilities?"}, {"body": "i'm planning the design of a wrist for a humanoid robot. i would like to choose a design that is sturdy while allowing for dexterity comparable to a human wrist.\n\none option that was presented to me was to use a stewart platform. this setup appears to correctly recreate all possible movements of the human hand. my immediate concern is that this platform will use a total of six actuators which will require additional power and computational requirements. i don't want to commit to this design until i am certain that there isn't a better alternative.\n\nis a stewart platform a good choice for replicating the dexterousness of the human wrist? if not, what is a better solution?\n", "tags": "robotic-arm design actuator joint humanoid", "id": "7463", "title": "stewart platform as robotic wrist joint"}, {"body": "is it possible to apply kinematic decoupling for a 7 dof 7r manipulator with spherical wrist?  if it is possible, can anyone suggest a reference on how to apply this approach with a redundant manipulator with spherical wrist, or explain why it is not possible? \n\ni'm working with robotic toolbox (matlab) and the numeric algorithm can find the inverse kinematics solution without a problem if i don't specify the orientation.  and i was thinking about solving the problem a second time considering the spherical wrist.  will this approach work?\n", "tags": "inverse-kinematics manipulator matlab", "id": "7470", "title": "7dof inverse kinematics spherical wrist"}, {"body": "i got my hands on a few tower pro sg90 9g servos but cannot find their schematics or datasheet anywhere (besides that link).\n\ni have the following concerns:\n\n\nlooks like they're rated for 4.8v, but will they tolerate a 5v supply?\nhow do i determine the current they require, in amps, ma, etc.?\nthere's 3 wires: brown, red &amp; yellow-orange, what do each of these guys do?\n\nif i had to guess i'd say that red is power, another one is direction, and another one is the position to rotate to\n\n\n", "tags": "rcservo wiring", "id": "7472", "title": "wiring & driving towerpro sg90 servos"}, {"body": "i'm aiming to control a motorized joint at a specific speed. to do this, i'm planning on attaching a rotary encoder to do this.\n\ni'll be controlling the motor with a pid controller. with this pid controller, i need to control the joints based on their velocity.\n\nsince:\n\n\n\nit would make sense to do something like this:\n\n\n\nhowever, there's an issue; the encoder doesn't provide a high enough resolution to accurately calculate the speed (the sample rate is too high). i want to have updated data every 5-15 ms (somewhere in that range as my current motors seem to be able to respond to a change in that range)\n\nsome more information:\n\n\n14 bit precision (roughly 0.0219726562 degrees per \"step\" of encoder\ni'd like to be able to calculate as small of speed differences as possible\nas the motors will be going fairly fast (120+ degrees/second at highly variable speeds and directions), so the feedback has to be accurate and not delayed at all\n\n\nso, a couple of ideas:\n\n\ni can find encoders that i can sample at a very high rate. i was thinking about sampling the time between the changes of the encoder's value. however, this seems finicky and likely to be noise-prone\ni could do some sort of rolling average, but that would cause the data values to \"lag\" because the previous values would \"hold back\" the output of the calculations somewhat and this would play with my pid loop some\nnoise filter of some sort, although i don't know if that would work given the rapidly changing values of this application\n\n\nhowever, none of these seem ideal. is my only option to get a 16 bit (or higher!) encoder? or is there another method/combination of methods that i could use to get the data i need?\n", "tags": "motor pid algorithm", "id": "7474", "title": "how do i accurately calculate the speed of a rotary encoder at a high sample rate?"}, {"body": "i'm trying to make an hexapod with 18 servo motors and i'm asking how to control them with a raspberry pi. (never used it). i saw lot's of stuff to control 1, but 18, 20...\n\ncurrently i'm working on an arduino mega, and a ssc-32 board, but i found the result to slow and jerky.\n\nat this end, i want to add a camera and processing the image, i know an arduino can't handle that process but a raspberry pi can ?\n\nthank for all information about that subject :) \n", "tags": "arduino raspberry-pi cameras servomotor", "id": "7475", "title": "can i control more than 18 servo motor with a raspberry pi"}, {"body": "i need to get coordinates of the specific points from 2d cad file and transform them so that i could use them to move the robotic arm to those points. the problem is that i only get x y z coordinates and the robotic arm needs x y z tx ty tz coordinates to move to the certain position. \n\nany suggestions?\n\nedited:\n\nmy task: i need robotic arm to go through certain points on pcb board and heat soldering paste. i could do it manually by setting points with pendant. but a much easier way would be to get coordinates of those points from cad file and write a code using pc.\n\n\n\nthis is how code for linear motion to a certain point looks like\n\ni only could find this manual.\n\nthis is pendant manual maybe it will be helpful.\n\ni am second year student in \"robotics and mechatronics\". i'm currently in a internship at the scientific research institution. i really appreciate your help!\n", "tags": "robotic-arm", "id": "7483", "title": "how to transform x y z coordinates to tx ty tz?"}, {"body": "i have the following system:\n$$\\dot{x} = a(t)x+b(t)u$$\n$$y = x$$\n\n$a(t)$ and $b(t)$ are actually scalar, but time-dependent. if they would be constant, i could simulate the system in matlab using:\n\n\n\n\nhowever, it would be nice to simulate the system with dynamic state and input matrix. the matrices are based on measurement data, this means i would have for each discrete time step $t_i$ another matrix $a(t_i)$. any suggestions how to do that?\n", "tags": "dynamics matlab simulation", "id": "7485", "title": "matlab: system simulation with dynamic state matrix / input matrix"}, {"body": "obviously robotic circuits draw different amounts of power/current. so given the same battery, say, a 9v, then connecting it to 2 different circuits will deplete it at two different rates. robot/circuit #1 might drain the battery in 5 minutes. robot/circuit #2 might drain the battery in 20 minutes.\n\nwhat ratings do batteries have that allows us to figure out how long it will power a circuit for? bonus points: does this same rating uphold for solar panels and, in deed, all power supplies (not just batteries)?\n", "tags": "power battery circuit", "id": "7486", "title": "how to determine how long a battery will power a robotic circuit for?"}, {"body": "say i have this solar panel that outputs 6v at 330ma, or ~1.98 watts. if i connect that to arduino, which expects a 5v supply at (roughly) 50ma, then the arduino as a whole requires 5v * .05a = 0.25 watts to power it. to me, if i understand this correctly, then in perfect weather/sunlight, the solar panel will power arduino all day long, no problem.\n\nnow let's say we wire up 4 motors to the arduino, each of which draw 250 watts. now the arduino + 4 motors are drawing ~1.25 watts. but since the panels are still outputting 1.98 watts, i would think that (again, under perfect sunlight) the panel would power the arduino and motors all day long, no problem.\n\nnow we add 4 more motors to the arduino circuit, for a total of 8 motors. the circuit is now drawing 1.25 watts + 1 w = 2.25 watts. i would expect the solar panel to no longer be capable of powering the circuit, at least properly.\n\nmy first concern here is: am i understanding these 3 scenarios correctly? if not, where is my understanding going awry?\n\nassuming i'm more or less on track, my next question is: can solar panels be \"daisy chained\" together to increase total power output? in the third case above, is there a way to add a second solar panel into the mix, effectively making the two panels output 1.98 watts * 2 = 3.96 watts, which would then make them capable of powering the arduino and its 8 motors (yet again, assuming perfect weather/sunlight conditions)?\n", "tags": "power circuit", "id": "7489", "title": "understanding how solar panels can supply power to robotic circuits"}, {"body": "i'm looking for my robotics project to draw its power from one of 3 rechargeable batteries; basically whichever has the most \"juice\" in it. from the initial research i've already done, i believe i could connect each rechargeable battery (probably lipo) to a diode, and then wire each of the 3 diodes in series.\n\nhowever, being so new to robotics/electronics, i guess i wanted to bounce this off the community as a sanity check, or to see if there is a better way of achieving this. again, what i am looking for is a way for the circuit to automagically detect that battery #1 has more power than battery #2, and so it \"decides\" to draw power from #1. the instant #1 is depleted or deemed \"less powerful\" than #2, the #2 battery takes over. thoughts/criticisms?\n", "tags": "power battery wiring", "id": "7491", "title": "wiring necessary to route power from any one of several rechargeable batteries"}, {"body": "i have a calibrated stereo camera system that is mounted in a passenger car which means i am able to retrieve a point cloud from my stereo image. however, i need to find how well is the camera aligned with the vehicle - read: if the camera is perfectly facing forwards or not. i guess it will never perfectly face forwards so i need to get the angle (or rather 3d vector) between \"perfect forwards\" and \"actual camera pose\".\n\nwhat came to my mind is to drive the vehicle possibly perfectly forwards and use stereo visual odometry to detect the angle of vehicle movement as seen by camera (which is the vector i am looking for). the libviso library for visual odometry can output a 3d vector of movement change from one stereo frame to another which could be used to detect the needed vector.\n\nthe only problem may be to actually be able to drive perfectly forward with a car. maybe an rtk gps could be used to check for this or for correction. will anyone have a suggestion on how to proceed?\n\nthe stereo camera i use consists of 2 separate point grey usb cameras. each camera is mounted on a windshield inside the car with a mount like this one. the cameras were calibrated after mounting. the stereo baseline (distance between the cameras) is about 50 cm.\n", "tags": "stereo-vision odometry", "id": "7493", "title": "discover vector/angle between stereo camera pose and vehicle body"}, {"body": "i would like to build a mechanical module that acts like a spring with electronically controllable stiffness (spring rate).\n\nfor instance, let's imagine a solid, metallic cube, 0.5 m each side. on the top side of the cube, there is a chair sitting on top of a solid mechanical spring. when you sit on the chair, it would go down proportionally to your weight, and inversely proportional to the spring's rate. \n\nwhat i want is that this spring's rate be electronically adjustable in real time, for instance a microcontroller system might increase the spring's rate when it detects a larger weight.\n\ni'm using this example to best describe what i want to achieve because i'm not a robotics specialist and i don't know the inside terms.\n\nis there already an electro-mechanic module as the one i'm describing? (obviously nevermind the cube and the chair, it's the spring i'm interested in).\n", "tags": "actuator", "id": "7494", "title": "spring with electronically adjustable stiffness"}, {"body": "i would like to build a small two-wheeled robot similar to the one shown here.\n\nin order to keep the robot small, i intend to use two coreless micro motors like the one shown bellow. the power source would be 2 aaa or aa batteries, in order to reach 3 v. these batteries would represent the bulk of the weight of the robot. the rest of the robot would be virtually weightless.\n\nthe specifications of one of such motor are:\n\n\n\nmy question is if small dc motors of this type have enough torque to even make the robot start moving. i have been unable to find torque info on these kind of motors and i suspect the weight of the robot could be too much for them to handle. do you know the typical torque of such motor? is there another type of (cheap) motor more appropriate for this project?\n\n\n", "tags": "mobile-robot motor wheeled-robot torque", "id": "7497", "title": "torque of coreless dc micro motor"}, {"body": "i am combining two position measurements of a ball from two sensors in real time to obtain one triangulated position in x,y,z coordinates. as the data exchange of the measurements carries some latency, the data has to be extrapolated be able to obtain the current position. due to extrapolation an error appears in the triangulated data.\n\ni know that when the ball is in the air, the velocity of the ball should be constant in x and y directions and the velocity in the z direction should decay with g. the velocities in x and y however oscillate as function of time around a mean value which is the actual x respectively y velocity. the same goes for when i compute the acceleration in the z direction. it oscillates as function of time around g.\n\ngiven that i know how the ball should behave, i.e. that vx and vy should be constant and that the acceleration in the z direction should be z, how can i impose these conditions to better estimate the triangulated position? \n", "tags": "sensor-fusion", "id": "7498", "title": "measurement and physics model fusion"}, {"body": "can i use bipolar stepper motor driver to drive unipolar motor in unipolar configuration ?\n", "tags": "stepper-motor stepper-driver", "id": "7499", "title": "can i use bipolar stepper motor driver to drive unipolar motor in unipolar configuration?"}, {"body": "i am trying to control the velocity+position of a linear actuator.\n\nat this moment i am able to control the position or the velocity. but i'm trying to control both. what the control has to do: let the linear actuator drive to a position i.e. 0 to 100 cm with a constant velocity of 1cm/s.\n\ni control the actuator using a pwm signal. and i measure the velocity and position using a position sensor on the shaft.\n\nwhat kind of control is preferred, pid in cascade?\nif so,what would the code look like.\nany other kind of control would function better?\n\nthanks in advance!\n\nedit:\na more describing picture.\n\n\ni want a velocity controlled position controller.\nhopefully this will make it clear\n\nedit\n\nmy first try is with a trapezoid wave. maybe there is an easy way without to much calculation power to change it a s-curbe. then the accelartion/jerk will be alot smoother.\n        i let the microcontroller calculate 3 different formulas afterwards it will calculate it using loop iteration. this way i can use one pid for the position. the parameters in the following code will fictional:\n\n\n\na big problem with the code above is that the amount of accelartion loops is a constant. it can not be changed except when you already know the amount of loops it will take.\n\ni will be using two separate arduinos, they will be connected using a can-bus connection. anyway, they won't communicate through it unless the load becomes too high. this will make master/slave impossible. also the system has to be modular: adding another actuator to circuit won't be a problem. the actuator is speed controlled by using a pwm signal. the linear sensor will deliver a 0-10v signal which i will reduce to 0-5v by a simple voltage divider. the loop will be around 5 to 10 ms, will depend on the maximum looptime.\n\narduino has a 10-bit(1023) adc but use of oversampling i will probably try to increase it to 12-bit. to not decrease the reading speed i will decrease the prescaler of the adc.\n\nthe pwm output is 8-bit(255), i am trying to find a way to further increase. because i think 255 steps are too low for my application.\n\nbecause the arduino has limit internal memory, pre calculating all the positions is impossible.\n\nthank you all for the help so far!\n", "tags": "arduino pid microcontroller", "id": "7502", "title": "control both velocity and position (linear actuator)"}, {"body": "i'm working on an robot that would be able to navigate through a maze, avoid obstacles and identify some of the objects (boxes in which it has to pot the balls)  in it. i have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation.\n\nup till now, i have converted/read the bitmap image of the maze into a 2d array of bits. right now i am writing a code that should convert the 2d array (that represents the maze) into a connectivity map so that i could apply a path planning algorithm on it. mr. @chuck has helped me by providing a code in matlab. i have converted that code into c++, however the code isn't providing the right output. kindly see the code and tell me what i am doing wrong.\n\ni am sharing the link to the 2d array that has been made, the matlab code, and my code in c++ to convert the array into a connectivity map.\n\nlink to the 2d array:-\n\nhttps://drive.google.com/file/d/0bwuks98dxycuzdzwtvyzy0luefu/view?usp=sharing\n\nmatlab code:-\n\n\n\ncode in c++:-\n\n\n", "tags": "mobile-robot localization mapping planning", "id": "7506", "title": "converting a 2d array of bits to a connectivity map (code debugging)"}, {"body": "i want to design some circuits of my own. my area of expertise is in computer science engineering. i have listed out the components which are essential in the circuit. i want a software which can be used to design and simulate circuits for real time projects. please suggest me the best among them. thank you.\n\n@akhilrajagopal\n", "tags": "software electronics", "id": "7507", "title": "circuit design and simulation"}, {"body": "i have a sensor reduction model which gives me a velocity estimate of a suspension system(velocity 1) .\n\nthis suspension system estimate velocity is used to calculate another velocity(velocity 2) via a transfer function/plant model.\n\ncan i use velocity 2 to improve my velocity estimate (velocity 1) through kalman filtering or through some feedback system.??\n\n\n\nv1 is \"estimated\" using these two sensors.that is fed into a geroter pump (fs in diagram) which pumps fluid to manupulate the damper viscous fluid thereby applying resistance to the forces applied to the car body. there is no problem did i have an velocity sensor on the spring.i could measure it accurately but now i only have an estimate. i am trying to make the estimate better.assume i have a model/plant or transfer function already that gives me the v2 given a v1.\n", "tags": "control sensors pid kalman-filter", "id": "7510", "title": "improving velocity estimation"}, {"body": "what are these frequencies used for within the drone technology, and why these values?\n\n\n35 mhz\n433 mhz\n868 mhz\n2.4 ghz\n5.8 ghz\n\n", "tags": "quadcopter wireless radio-control", "id": "7513", "title": "what are the frequencies used for within drones?"}, {"body": "i am trying to understand the implementation of extended kalman filter for slam using a single, agile rgb camera. \n\nthe vector describing the camera pose is \n$$\n\\begin{pmatrix}\nr^w \\\\\nq^w  \\\\\nv^w \\\\\n\\omega^r \\\\\na^w \\\\\n\\alpha^r\n\\end{pmatrix}\n$$\n\nwhere:\n\n\n$r^w$ :   3d coordinates of camera w.r.t world\n$q^w$ :   unit quaternion describing camera pose w.r.t world\n$v^w$ :   linear velocity along three coordinate frames, w.r.t world\n$\\omega$ :   angular velocity w.r.t body frame of camera\n\n\nthe feature vector set is described as \n$$\n\\begin{pmatrix}\ny_1 \\\\\ny_2  \\\\\n\\vdots \\\\\ny_n\n\\end{pmatrix}\n$$\nwhere, each feature point is described using xyz parameters.\n\nfor the ekf acting under an unknown linear and angular acceleration $[a^w,\\psi^r] $ , the process model used for predicting the next state is:\n\n$$\n\\begin{pmatrix}\nr^w + v^w\\delta t + \\frac{1}{2}\\bigl(a^w + a^w\\bigr)\\delta t^2 \\\\\nq^w \\bigotimes q^w\\bigl(\\omega^r\\delta t + \\frac{1}{2}\\bigl(\\alpha^r + \\psi^r\\bigr)\\delta t^2\\bigr)   \\\\\nv^w + \\bigl(a^w + a^w\\bigr)\\delta t\\\\\n\\omega^r + \\bigl(\\alpha^r + \\psi^r\\bigr)\\delta t \\\\\na^w + a^w \\\\\n\\alpha^r + \\psi^r\n\\end{pmatrix}\n$$\n\n\n\nso far, i'm clear with the ekf steps. post this prediction step, i'm not clear how to perform the measurement update of the system state.\n\nfrom this slide, i was under the impression that we need to initialize random depth particles between 0.5m to 5m from the camera. but, at this point, both the camera pose and the feature depth is unknown.\n\n\ni can understand running a particle filter for estimating feature\ndepth if camera pose is known. i tried to implement such a concept in this project: where i read the camera pose from a ground truth file and keep triangulating the depth of features w.r.t world reference frame\ni can also comprehend running a particle filter for estimating the\ncamera pose if feature depths are known.\n\n\nbut both these parameters are unknown. how do i perform the measurement update?\n\ni can understand narrowing down the active search region for feature matching based on the predicted next state of the camera. but after the features are matched using ransac (or any other algorithm), how do i find the updated camera pose? we are not estimating homography, are we?\n\nif you have any idea regarding monoslam (or rgb-d slam), please help me out with understanding the ekf steps.\n\n\n\nto be more specific: is there a homography estimation step in the algorithm? how do we project the epipolar line (inverse depth or xyz) in the next frame if we do not have any estimate of the camera motion?\n", "tags": "slam ekf", "id": "7517", "title": "need help regarding ekf in monoslam"}, {"body": "i have a differential drive robot for which i'm building an ekf localization system.  i would like to be able to estimate the state of the robot $\\left[ x, y, \\theta, v, \\omega \\right]$ where $x, y, \\theta$ represent the pose of the robot in global coordinates, and $v, \\omega$ are the translational and rotational velocities. every mobile robot kalman filter example i've seen uses these velocities as inputs to prediction phase, and does not provide a filtered estimate of them. \n\nq: what is the best way to structure a filter so that i can estimate my velocities and use my measured odometry, gyroscope, and possibly accelerometers (adding $\\dot{v}$ and $\\dot{\\omega}$ to my state) as inputs? \n\nmy intuition tells me to use a prediction step that is pure feedforward (i.e. just integrates the predicted velocities into the positions), and then have separate updates for odometry, gyro, and accelerometer, but i have never seen anyone do this before. does this seem like a reasonable approach?\n", "tags": "localization kalman-filter gyroscope odometry", "id": "7519", "title": "structuring ekf to estimate pose and velocity with odometry inputs"}, {"body": "i have a term project which is controlling a two-link manipulator with harmonic drive installed at each joint.\nto control, i used computed control method to determine the torque needed for each joints based on the formula: \n $$\\tau_i =m(\\theta)(\\ddot{\\theta_i}+k_d\\dot{e}+k_pe)+v+g  $$\nto calculate the torque that each motor needs to produce through harmonic drive, i use: \n$$\\tau_{motor} =(j_m+j_g)\\rho\\ddot{\\theta_i}+\\frac{\\tau_i}{\\rho\\eta_g}$$\nwhere:\n $\\rho$ and $\\eta_g$ are gear ratio and efficiency of the harmonic drive. $j_m$ and $j_g$ are the motor and gear inertia, respectively. \n\nafter these calculation, i can see the effect of harmonic drive in the system by comparing input torque from motor in the model with harmonic drive ($\\tau_{motor}$) to that torque in the model without harmonic drive ($\\tau_i$) \n\nbut my professor doesn't agree the formula $\\tau_{motor}$ i used. he want me to include the stiffness $k$ of the harmonic drive.\n\nthis is what i have done\n\np/s: this model which consists of two-link manipulator+harmonic drive at each joint is built in matlab.  \n\ncan anyone suggest me the formula about it? \n\nthank you so much.\n", "tags": "actuator manipulator", "id": "7521", "title": "calculate required motor torque through harmonic drive"}, {"body": "i'm attempting to control a small vehicle at relatively slow (.5 m/s - 1 m/s) speeds, but with extreme accuracy (1mm). for the drive system, i'm considering using brushless motors as they have a much greater power / volume ratio than i am able to find with brushed motors, especially at this small size.\n\ni will be using wheels between 1\" and 2\" diameter, so the rpm i will be looking for is between 150 - 500 rpm at max. this would suggest either driving the motors at a low speed directly, or driving them at a high speed and gearing them down. as i understand it, both setups will give high torques, as brushless motors decrease torque with speed. with brushed motors, it's quite obvious that a gearbox is necessary as otherwise there is no torque in the system, but here the choice isn't as clear, which is why i am asking.\n\ntl;dr use brushless motors at high speed with gearbox or low speed (ungeared) for high torque / low speed / high precision application?\n", "tags": "motor brushless-motor", "id": "7522", "title": "highspeed with gearbox or low speed for brushless motor?"}, {"body": "aim: to use multi-threading and inter-process communication(ipc) when coding an autonomous robot.\n\nplatform: embedded linux (yocto)\n\nconstraints : limited cpu power.\n\nwe are building an autonomous underwater vehicle, to compete in the robosub competition. this is the first time i am doing something like this. i intent to use a middleware like ros, mira, yart, moos etc. the purpose of using one is that i want to modularise tasks, and divide the core components into subsystems, which should be run parallel(by multi-threading). but i have limited computational power (a dual core omap soc), and the middleware, while robust should also be very efficient.\n\ni need to use a middleware, because i don't want the program to be run on a single thread. my cpu has two cores, and it would be great if i could do some multi-threading to improve performance of the program. the middleware will provide for me the communication layer, so i don't have to worry about data races, or other problems associated with parallel processing. also i have no prior experience writing multi-threaded programs, and so using parallel processing libraries directly would be difficult. hence imo, middlewares are excellent choices.\n\nin your experience, which is the best one suited for the task. i don't really want to use ros, because it will be having a lot of features, and i wont be using them. i am a computer science student(under graduate freshman, actually) and don't mind getting my hands dirty with one which has not that much features. that's true if only it will take less toll on the cpu.\n", "tags": "ros communication underwater operating-systems", "id": "7533", "title": "which middleware for ipc and multi-threading in a autonomous robot?"}, {"body": "has anyone done this with ekf/pid on a small microcontroller? or know of code snippets to help implementing this?\n", "tags": "quadcopter odometry stability", "id": "7534", "title": "on-board monocular odometry for quadcopter stabilization"}, {"body": "i am working on my first hobby project and i'm not very familiar with sensors yet. i am trying to build a system which detects the presence of a person in a small room with a single entrance/exit door.\n\nthe idea is when the first person enters the room, the lights turn on and any following person doesn't affect state of the lights. after the last person leaves, the lights should turn off. in a programmatic sense, the lights should turn on when present person count is greater than 0.\n\ni have explored my options and found out that infrared sensors are usually used for this type of problem. what i am not sure is how to detect whether person has entered or left, so i would like to ask for some help with this.\n", "tags": "sensors", "id": "7535", "title": "detecting the presence of a person in a room"}, {"body": "from introduction to robotics by j.j. craig, chapter 2, page no. 36:\n\ncould anyone explain how that equation was derived/formed? i am stuck on this page due to failing to understand where the equation came from. thank you.\n\n", "tags": "design theory books", "id": "7537", "title": "inverting a transform (reading j craig's book on robotics)"}, {"body": "i'm no professional. at 29 i just became seriously interested in robotics a few months ago and have been researching everything i can since. now that i've come to understand how far robotics have truly come i have a desire to try to make my own.\ngranted, i know nothing about coding or programming. i have no idea where to begin. and i know it'll probably, the first time at least, be something small rather than a huge life altering project.\nthus, if anyone could suggest to me good resources for a beginner i'd massively appreciate it.\n", "tags": "beginner", "id": "7542", "title": "starter looking for advice"}, {"body": "let's say a pid is implemented and the errors are calculated using the sensor data, but the sensor data lags by certain amount of time because of the overhead. and the lag time is smaller than the sampling period, how well does pid performs? what i am thinking is that pid will calculate errors based on past data, and use that to control. how will using a kalman filter to estimate the actual sensor data help? \n", "tags": "pid kalman-filter", "id": "7547", "title": "lagging sensor data for pid"}, {"body": "this is a question for those of you who have experience using stereo cameras/modules like the zed, duo m, bumblebee cameras, etc. (not tof cameras). i can't find any sample disparity outputs out there on the internet, and i can't find any information on how they perform. basically here are a few things i'd like to know to those of you who used any of the cameras mentioned above (and others)\n\n\nwhat resolution and no. of disparities did you work with? \nhow was the framerate? \non what hardware?\ndid the camera have an asic of some sort to produce the disparity maps, or did it require a host?\nhow was the quality?\n\n\nfor those who used the zed camera, there is a promotional video on youtube. are the disparity maps really that good?\n", "tags": "cameras stereo-vision", "id": "7549", "title": "question for those who have experience using stereo cameras/module (e.g. zed, duo m, bumblebee, etc.)"}, {"body": "i understand the concept of using a pull-up/pull-down resistor when implementing a button/switch with arduino to avoid a floating state, and in fact i have implemented this quite often.\n\nbut i am not too sure if a pull-down resistor is necessary in chip-chip or chip-sensor communication.\n\ni am connecting a coin acceptor to the arduino (common ground). the coin acceptor's output pin gives a short pulse each time there is a coin inserted. so far i am connecting the output pin of the coin acceptor directly to an arduino pin and it works without any problem. is a pull-down resistor (on this line) usually required as precaution in this case?\n\nalso i have the same question when connecting 2 pins of 2 separate arduino's (also common ground) so that one arduino can read pulses from the other.\n\nthanks in advance for any experience shared!\n\ndave\n", "tags": "arduino", "id": "7552", "title": "pull-down resistor for inter-chip and sensor-to-chip communication"}, {"body": "i had a rc helicopter (with video,picture, and audio taking capabilities) that recently \"died\" (unrelated to short circuit). the reciever board short circuted, but the board that sent data to micro-sd card and had camera+mic was fine. i can access the data on the micro-sd card through the circuit, with a usb cable. the reciever board sent data via a 4 wire bundle to the camera board to make it take pictures/record audio. is there any way to still do this from my computer (from the usb), and turn it into a mini spy camera? (not remotely, jst through a cable)\n\ni got this heli a while back so i don't have the heli number but the camera board number is , and the reciever board number is \n\nreciever board image\n\ncamera/data board image\n\n", "tags": "control cameras circuit", "id": "7556", "title": "mini recorder from rc heli parts"}, {"body": "i recently got libfreenect running on my mac and was able to test out  which uses some of the 3d capabilities of the depth sensor.\n\ni noticed that the kinect would only respond / pick up movement that happened within a range of about 3-6 inches in front of the sensor.\n\ni thought this may be because the lights where on so i turned them off. it seemed to get a little better but it still only \"works\" if something is block the sensor almost completely.\n\ndoes anyone know if this is something that can be solved? i know it's an old sensor but i got it for $20 so i could do some prototyping with it.\n\nnotes:\n\n\nlaser project is on\nlight starts out blinking then goes solid green\nwhen not level light goes red\nrgb camera works but is a little choppy and sometimes shows tears in the picture.\n\n\nfreenect-glcplview output (snippet):\n\n\n\nfreenect-regview output (snippet)\n\n\n\nfound this which gives me the idea that this may be a usb issue: regular receipt of undersized packet.\n", "tags": "kinect", "id": "7557", "title": "troubleshooting xbox kinect 360"}, {"body": "i'm developing a project consists of an imu controlled by arduino through which you can send via a radio module, the data to the pc of the three euler angles and raw data from the sensors.\nfor filtering i used the code made available by sparkfun: razor ahrs 9 dof\n\nhttps://github.com/ptrbrtz/razor-9dof-ahrs/tree/master/arduino/razor_ahrs\n\nthe code does not provide radio transmissions and is tuned for 50 hz sampling rate, in fact its parameters are:\n\n\n\nin this project data is read every 20ms (50hz) and records of the sensors are set to the accelerometer odr 50hz and 25 bandwidth. with the gyroscope 50 hz odr.\nin my project i used a gyroscope different, namely that i used l3g4200d frequency odr starting at 100hz, i set then registers with the 100hz. my global data rate is 33hz max, beacouse the use of a radio, i read the complete date with a frequency of 33hz.\nhow can i tune the  and  of my setup? the  is the period, i have to consider the frequency odr that i set to register in the individual sensors or i have to set the global system sample rate limited to 33hz by the radio transmission?\n", "tags": "arduino imu gyroscope sensor-fusion", "id": "7558", "title": "problems about complementary filter imu tuning"}, {"body": "basically i got system with a sensor and an output. i want to apply a digital implemented feedback controller. the problem in this setup is the sensor. the specifications of the module says that the sampletime of the sensor does change in wide range, depending on the usecase; from 1.3 second to 10 second. but it stays constant until the system is disabled.\n\nmy first approach was tuning a digital pid-controller for the longest sampletime. this works fine. even if i change the sampletime to the shortest the system stays stable, which was expected because i'm still in roc.\nthe problem now is that the system's response is pretty slow.\n\nif i design the controller for my fastest samplingrate the results are satisfying but become instable for the slowest samplerate, which can be explained again by the roc\n\ni could use some kind of adaptive predefined gains which i change depending on the samplerate but i was wondering if there are  control strategies which are able to handle the sampletime changes?\n\nedit: \nto give a better overview i will add some details: \ni'm talking about a heating system which heats with radiation. as a sensor i use a pyrometer module with a samplingrate of up 1khz. the problem is, that the pyrometer is not able to produce reasonable readings whenever the radiator is turned on. (yes there are other alternatives to the pyrometer, but they start at $50k and are too expensive). the radiator has to be pulsed to operate it. so to maintain a decent heat up time and steady-state temperature the \"duty-cycle\" has to be at a decent rate(target is 95%). the minimum \"off-time\" of the radiator is 0.2 seconds before the measured values are reasonable. so at the end my sensor got an effective sampletime of 1-10seconds (by varying the duty cycle).\n\nthe hardware is hard too change, radiator and sensor have been evaluted for months right now. therefore i try to improve the results by \"just\" changing the control algorithm.\n", "tags": "control", "id": "7560", "title": "digital controller design for system with variable sample time"}, {"body": "i am building an autonomous underwater robot. it will be used in swimming pools. it should be capable of running in any normal sized pool, not just the pool in which i test. so i cannot rely on a particular design or feature. it has to know it's position in the pool, either with respect to the initial position or with respect to the pool. i have a imu, which is a pololu miniimu but finding the displacement with an imu is a near impossible task. \n\nwhat sensor can i use for this task? it should not be very expensive. (below 200$)\n\ntank size: 25x20x2.5 meters\n", "tags": "sensors localization sensor-fusion underwater", "id": "7564", "title": "how to localise a underwater robot?"}, {"body": "i know that we can use some algorithms like lqr, mpc, or even pid to make the robot follows the trajectory references. in the simulation like matlab, i usually specify the trajectory reference by a function. let say, given a sequence of points generated by a path planning algorithm, then i want to do a real experiment of trajectory tracking over those sequence of points. my question is:\n- how to specify the errors towards the path in real situation. my impression is the generated path by path planning algorithm is uncertain due to the error of the robot sensing. and unlike the line following robot which has a real physical line for the reference, the generated path from path planning is virtual, e.g. it does not exist in the real world. i am really confused about these matter.\n", "tags": "mobile-robot control", "id": "7565", "title": "how to determine the trajectory reference on the real robot trajectory tracking"}, {"body": "i'm studying introduction to robotic and found there is different equations to determine the position and orientation for the end effector of a robot using dh parameters transformation matrix, they are :\n\n\n\n\n\n\n\nexample: puma 560, all joints are revolute\n\nforward kinematics:\n\ngiven :the manipulator geometrical parameters.\n\nspecify: the position and orientation of manipulator.\n\nsolution:\n\n\n\nfor step 4:\n\n\n\nfor step 3 :here i'm confused \n\nhere we should calculate the transformation matrix  for each link  and then multiply them to get the position and orientation for the end effector.\n\ni've seen different articles using one of these equations when they get to this step for the same robot(puma 560)\n\nwhat is the difference between them? will the result be different? which one should i use when calculating the position and orientation?\n", "tags": "dh-parameters", "id": "7570", "title": "homogenous transformation matrix for dh parameters"}, {"body": "i am fairly new to the dh-transformation and i have difficulties to understand how it works. why are not all coordinates (x+y+z) incorporated into the parameters? it seems to me that at least one information is useless/goes to the trash, since there is only a, d (translatory information) and alpha, theta(rotatory information). \n\nexample: \nthe transition between two coordinate systems with identical orientation(alpha=0, theta=0) but with different coordinates(x1!=x2, y1!=y2, z1!=z2). \ndh only makes use of a maximum of two of these information.\n\nplease enlighten me! \n\ngreetings\n\n:edit: \n\nto clarify which part of the dh-transform i don't understand, here is an example. \n\nimagine a cnc-mill(cos1) on a stand(cos0) without any variable length(=no motion) between cos0-cos1. for some reason i need to incorporate the transformation from cos0-cos1(=t0-1) into the forward transformation of my cnc-mill. \n\n\ndh-parameters for t0-1 would be a=5mm, alpha=90\u00b0, d=2mm and theta=90\u00b0. assuming this is correct, the dx=10mm information is lost during this process?\nif i recreate the relation between cos0 and cos1 according to the dh-parameters, i end up like this: \n\n\nas far as i understand, on non parallel axis the information is not lost because the measurement of a/d would be diagonal, therefore include either dx/dy, dx/dz or dy/dz(pythagorean theorem) in one parameter. \n\nwhere is the flaw in my logic?\n", "tags": "forward-kinematics dh-parameters", "id": "7575", "title": "dh-parameters for forward kinematics for translatory motion only"}, {"body": "i recently decided to build a quadricopter from scratch using arduino and now i'm faced with an orientation estimation problem.\n\ni bought a cheap 10dof sensor with 3 axis magnetometer, 3 axis accelerometer, 3 axis gyro and a barometer and the complementary filter that i use to get orientation returns usable but noisy values.\n\ni tried the madgwick fusion filter too, but it returns unstable values that diverges from the ones i get with complementary filter. given that the madgwick filter implementation is correct, i pass acceleration values measured in gs, gyro values measured in rps (radians per second) and magnetometer values measured in ut, while sampling time is the same of my loop cycle. is there anything i have missed?\n\nis there any advantage using kalman filter?\n\nedit1:\n\nmy problem was due to an wrong choice of sampling time and now seems to work, but convergence is very very slow (i.e. it takes about 3 seconds to reach the right value after a quick flip of the imu). rising value of kp adds to much noise. i also tried to repeat filter update step more than once per cycle but it requires too much time exceeding the sampling time.\n\nhere some graphs, from top to bottom complementary filter, madgwick filter and madgwick filter with high kp:\n\n\n\n\n\n\n\nedit2:\n\ndifferent values probably are caused by cable plug and unplug. anyway raw data example from my sensor can be downloaded here\n", "tags": "arduino quadcopter kalman-filter imu", "id": "7578", "title": "orientation parameter for quadcopter with madgwick fusion algorithm"}, {"body": "i want to develop an autonomous driving rc car. for detecting obstacles, i plan to mount 3-5 ultrasonic sensors in the front and in the back the car. what is the  minimum necessary combined field of view of the sensors so the car never hits an obstacle? i.e. what is the minimum angle of detection of the combined sensors the car should have to detect any obstacle in its path?\n\nsome data about the car: (i don't know whether all the data is relevant)\n\n\nseparation between right and left wheel : 19,5 cm\nwheelbase (distance between the front and the back wheels):  31,3cm\nsteering axle: front.\nmaximum angle of steering: around 30 degrees. the car uses ackermann steering\n\n", "tags": "mobile-robot sensors wheeled-robot", "id": "7580", "title": "sensors' field of view in car driving"}, {"body": "i would like to start experimenting with robots. is lego mindstorm a good start? should i consider other platforms?\n", "tags": "platform", "id": "7588", "title": "is lego mindstorm a good start?"}, {"body": "i am trying to use invensense's mpu9250. i am using provided library to read euler angle. when the imu rotates about one axis, angles about other two axes change too. what could be potential cause to it? \n", "tags": "imu", "id": "7592", "title": "imu rotate about one axis, other two angles change too"}, {"body": "i created a program to simple time base delay (in seconds). i have problem:\n\nhow to read a interrupt flag from channel 1 etc.?\n\nwhen i use  an error occurs.\n\nwhen the interrupt occurred , uc should clear flag and set blue led in discovery board.\n\nhere is my program:\n\nmain.c\n\n\n\nso how should my  look like? each channel generate delay in +1 sec. when i am debugging this program, when led is set the period is equal to 1s (time for set led). \n", "tags": "microcontroller", "id": "7595", "title": "stm32_oc_timing and irqhandler"}, {"body": "i got an owi robotic arm, but was slightly disappointed at it having only horizontal position for gripper. what would be the easiest way to extend with gripper/wrist rotation, i.e. 6th degree of freedom? \n\n\n", "tags": "robotic-arm", "id": "7598", "title": "extend robotic arm with wrist rotation"}, {"body": "i've got robotics api library, demo-program and a robot. i want to develop app for it. the best solution is offline development on some kind of simulator. i'm completely new in such tasks - is there any ide for this? or a way do deliver byte-code to machine? thanks in advance!\n", "tags": "robotic-arm dynamic-programming", "id": "7600", "title": "kuka robotics api ide"}, {"body": "can esc in quads be programmed in such a way that only one side has throttle and no throttle at all on the other? this would cause the quad to flip i suppose? \n\nwith that, is there a way we can program the controller to like trigger a switch when we want the quad to flip? because i was thinking of doing a waterproof quad. so initially, it flies in the air normally with the 4 channel, and then i set it to float on water. after that, i was thinking of maybe triggering a switch on the controller so that this time it's just going to flip and nothing else. after it flips, i would trigger the switch back to normal operation. is that possible?\n", "tags": "quadcopter", "id": "7607", "title": "can esc be programmed to run full throttle only on one side of a quadcopter?"}, {"body": "recently i've bought a hexapod kit and 18 towerpro mg995 servos.\n\nmy objective is to apply also the pi camera, sensors and perhaps a claw...\nso i've been researching and i haven't found a clear answer when comes to the servo control board.\n\nwhich servo controller board shall i choose to complete my project?\n", "tags": "mobile-robot raspberry-pi servomotor rcservo hexapod", "id": "7612", "title": "raspberry pi hexapod 18dof, best servo control board?"}, {"body": "i have been researching on a cost-effective way to scan an area on a mav (exploraton) and later use it for cad/civil purposes(use the point cloud data for cad) but the major sensors available have their own problems.\n\nkinect - can't use outside,high computation power\nstereo - high computation power,somewhat expensive\nlidar - very expensive + not real time + heavy\n\ni need a system(on the mav/quadrotor) that can work over wifi/wireless, can scan outdoors , not very expensive and that gives data real-time.please suggest a system that can be as close to the above requirements.\n\nalso can stereo be operated over wifi? \n", "tags": "kinect mapping stereo-vision 3d-reconstruction", "id": "7613", "title": "how to efficiently do 3d mapping of an area on a mav?"}, {"body": "i\u2019m using the bma020 (from elv) with my arduino mega2560 and trying to read acceleration values that doesn\u2019t confuse me.\nfirst i connected the sensor in spi-4 mode. means\n\ncsb &lt;-> pb0 (ss)\n\nsck &lt;-> pb1 (sck)\n\nsdi &lt;-> pb2 (mosi)\n\nsdo &lt;-> pb3 (miso)\n\nalso gnd and uin are connected with the gnd and 5v pins of the arduino board.\n\nhere is the self-written code i use\n\n\n\nand now here is what really confuses me. i got 5 of this sensors. one is working with this code perfectly fine. the data i get is what i expect. i measure earth gravity in z-component if iay the sensor on the table, if i start turning it i measure the earth gravity component wise in x-, y- and z- direction depending on the angle i turn the sensor.\n\nfrom the other 4 sensors i receive data that is different. the values jump from -314 (about -1.2 g) to +160 (about 0.5g). with the same code, the same wires and the same arduino.\n\ni checked the register settings of all sensors, they are all the same. i checked the wire connection to the first component at the sensors, they are all around 0.3 ohm. i used an oscilloscope and made sure csb, sck and mosi work properly.\n\nam i missing something? what causes this similar but wrong behavior of 4 out of 5 sensors?\n", "tags": "arduino accelerometer", "id": "7615", "title": "problem with acceleration sensor"}, {"body": "i am trying to use 2x uarts with chibios on the stm32f072rb nucleo board. \n\ni initialized uart2 but i am still getting output on uart1 pins, which is totally weird.\n\n\n\nthe line  gives output on uart1. \n\nis there anything else i need to do?\n\nmcuconfig.h reads\n\n\n", "tags": "microcontroller serial c", "id": "7617", "title": "using 2x uarts on stm32f072rb"}, {"body": "how does one implement virtual model (continuous) while control system itself is discrete (plc)?\n\ni've done this in practice but what about theory, how does one explain this topic to a stranger? (lets say myself)\n", "tags": "control", "id": "7622", "title": "virtual model in plc discrete / continuous"}, {"body": "if there are input and the sensor measured outputs. what are the objective methods to compare performance besides looking at inputs and outputs matching or not?\n", "tags": "pid", "id": "7626", "title": "what are methods to compare pid controller performance?"}, {"body": "during the vacation here i wa thinking it could be fun to make an electric skateboard/longboard... \ni haven't decided on which longboard i want to use, but is nearly done with selecting the parts. \n\nthe board will be driven by one out runner [turnigy aerodrive sk3 - 6374-149kv][1] \nand to mount the motor i use this [kit][2] \nas for esc i was recommended to [this][3], and [this][4] as battery. \n\nwhich seems like be good seem to be an ok setup for my application.  the problem is a bit that i don't have any reason for why i have chosen the esc and the battery pack. surely they deliver the needed amps and the esc can withstand the max current but other than that, i don't see why i couldn't buy an other set.. especially if it is cheaper... \n\nwhat cheaper alternative do i have, and what the drawbacks do they have \n", "tags": "brushless-motor battery esc", "id": "7632", "title": "project - building a electric skateboard"}, {"body": "i'm a student who is doing electrical and electronics engineering. i'm currently doing my final project which is a quadcopter. one of my objectives in that is to make a electronic speed controller (esc) for the brushless motors that are being used. \n\ni made a design for the esc using proteus and i made the pcb also. i have attached the schematic. i used pic16f628a for the esc and wrote a small code in mikroc for the esc to work when powered up. unfortunately it didn't work properly. i tried sensorless control of brushless motors without getting any feedback.\n\ncan i know how much of current that i should provide for the motor? according to some articles that i read the brushless dc (bldc) motor requires around 10a at the startup for around 20 ms. i have posted the code also. i used two codes to run the motor. one with pwm and other without pwm (100% duty cycle). \n\ni am a rookie to the subject of bldc motor controlling. i am very grateful if anybody can help me to clear out the doubts and figure out the mistakes in my design to make it work properly. \n\nbelow given is the code that i tried. please help me to figure out the right way to program the chip.\n\n\n\nwhen i uploaded the above given code and when i set the delay to around 3000 &mu;s, the motor spun but at each time one of the mosfets got heated up until i cannot touch it anymore. here is the video of this scenario.\n\nthis is the other code (pwm);\n\n\n", "tags": "quadcopter brushless-motor esc", "id": "7633", "title": "brushless dc motor - electronic speed control - quadcopter"}, {"body": "i'm new to the robotics and electronics world, but i'm willing to dive into it. i'm a software developer and i want to create a project that uses gps and accelerometer data to show as a layer on google maps after transferred to pc.\n\nmy doubt is about which controller to get. in my country, there are generic controllers based on the atmega328 that are being sold with a massive difference of price from the original arduino (talking about the uno model). \n\nshould i start with an original model? \n\nshould i expect to break the controller, fry it, or break any components by connecting them wrong? \n\nwould the experience with a generic controller be less exciting than with the original arduino one?\n", "tags": "arduino beginner", "id": "7634", "title": "arduino original or generic for a beginner?"}, {"body": "i have a servo motor with quad optical encoder and i'm trying to control its position and velocity. by controlling both i meant that if i input that the motor should reach 90\u00b0 at 200rpm then it should. how can i do that? i am using an arduino uno. kindly share some code if possible. \n\nthough i have implemented the pid, i don't think it is correct because i didn't implement the feedforward controller (because i have no idea what that is) and i have not been able to find suitable gains for pid. the gains i find for small steps (or say degree rotation) do not work out well for large steps and vice versa. i have also not used a limit for integral sum (because i don't how much it should be).\n\ni am using a pittman motor.\n", "tags": "robotic-arm", "id": "7640", "title": "pid gains for motor position and velocity control"}, {"body": "i have some robot software i'm working on (java on android) which needs to store a pre-designed map of a playing field to be able to navigate around. the field's not got any fancy 3d structure, the map can be 2d.\n\ni've been trying to find a good format to store the maps in.\ni've looked into svgs and dxfs, but neither one is really designed for the purpose.\n\nis there any file format specifically designed for small, geometric, robotics-oriented maps?\n\nthe field i'd be modelling is this one:\n\n", "tags": "mapping", "id": "7641", "title": "mapping formats for small autonomous robots"}, {"body": "pomdps extend mdps by conceiling state and adding an observation model. a pomdp controller processes either\n\n\naction/observation histories or\na bayesian belief state, computed from the observations (belief-mdp transformation)\n\n\nin a complex, real-world system like a robot, one usually preprocesses sensory readings using filters (kalmann, hmm, whatever). the result of which is a belief-state.\n\ni am looking for publications that discuss the problem of fitting a (probably more abstract) pomdp model on top of an existing filter-bank. \n\n\ndo you have to stick to the belief-mdp, and hand over the filtered belief-state to the controller?\nis there any way of using history-based pomdp controllers, like mcts?\nhow do you construct/find the abstract observations you need to formulate the pomdp model?\n\n", "tags": "kalman-filter particle-filter planning filter", "id": "7644", "title": "how to use a pomdp-based planner on top of a probabilistic filter"}, {"body": "i'm searching filter to reduce noise and smooth the signal while dead reckoning with an imu (6dof gyro+accelerometer). what are the differences/advantages/disadvantages of the following filters:\n\n\nkalman\ncomplementary\nmoving average\nmahony \n\n\ni applied kalman and complementary filters to an imu and both of them gives time lag to actions with respect to filter parameters. also kalman filter works slower than moving average and complementary. how can i choose right filter and filter parameters?\n", "tags": "mobile-robot localization kalman-filter imu", "id": "7645", "title": "how do i choose the best filter for dead reckoning with an imu?"}, {"body": "i am trying to calibrate a monocular camera using ros with the help of this website: how to calibrate a monocular camera. when i run , i get:\n\n\n\nwhen i run , i get:\n\n\n\nfinally, when i run:\n\n\n\nit says:\n\n\n\ni even added the parameter at the end, , but that just makes the terminal stall indefinitely. \n\ncould someone please help me figure out what is going wrong and how i can fix it? also if it is important,  is saved at .\n", "tags": "ros cameras calibration", "id": "7647", "title": "ros calibration camera problems"}, {"body": "can someone please provide me with a list of sensors on the create 2?  i am hoping to get one soon, but want to be sure it has ultrasonic sensors and not just bump sensors before i do.\n", "tags": "irobot-create roomba", "id": "7648", "title": "irobot create 2 sensors"}, {"body": "i have a robotic arm mounted on a car. there's a camera attached to it. suppose the camera takes the image of a room, and finds that there's something, say an object, that has to be picked up. say it's 50 feet away from the robot. my question is that how will the robot reach the object in the first place, and secondly, when it has reached the object, how will it know the real world co-ordinates of the object, to pick the object up, using inverse kinematic equations. any help would be appreciated. thanks\n", "tags": "mobile-robot robotic-arm", "id": "7652", "title": "how do i control the robotic arm motion?"}, {"body": "i want to use ir sensors to detect whether my dustbin is full but i want to protect it from outside dust. i am planning to use the ir sensors on roomba.\n\n\nhow are they working despite a plastic wall?\nalso, what is the range of the sensor?\ncan they detect obstacle at about 25 cm?\nwhy is there a wall between the ir sensors?\nis there a reason they are positioned at certain angle?\n\n", "tags": "sensors roomba", "id": "7656", "title": "how does the cliff sensors on roomba work through a glass wall?"}, {"body": "i am trying to run the  using ptam according to this camera calibration tutorial. however, when i do so, i get the following error:\n\n\n\ni source my  before i run the code as well and it still does not work. here is my launch file:\n\n\n\nhere is what i get for :\n\n\n\nthe path where the  file is . \n\ni am not sure why this error keeps coming up because when i run , it says:\n\n\n\nso i'm thinking that ptam does include . if someone could please point out my error, that would be really helpful. i've been using this post as a guide, but it's not been helping me much: ros dynamic config file.\n\nas it says in the above link, i tried  and i could not find . i could only find the below. how do i proceed?\n\n\n", "tags": "ros cameras calibration", "id": "7659", "title": "ptam cameracalibrator error"}, {"body": "i have a mobile robot which is navigating around a room, i already have the map of the room. i am using the navigation_stack of ros. i am using rotary encoders for odometry. i am fusing the data from rotary encoders and imu using robot_pose_ekf. i am using amcl for localization and move_base for planning. \n\nnow, i have to write a complete coverage path planning algorithm and i am following this paper and i would like to ask what is the best way to generate the boustrophedon path (simple forward and backward motions) in a cell (can be rectangular, trapezium, etc.) with no obstacles? i read a paper where they use different templates and combine them in a certain way to come up with the boustrophedon path. is there any other way by which we can generate the boustrophedon path? if someone can suggest how to implement it in ros, that will be great.\n\nplease let me know if you need more information from me. any help will be appreciated.\n", "tags": "ros navigation motion-planning coverage", "id": "7660", "title": "implementing a boustrophedon algorithm in a given room with obstacles"}, {"body": "what will be the specifications of motors and propellers that can approx produce a thrust of 100kg in a quadcopter?\n\nwe are planning to lift a total weight of 50 kg along with the 20 kg weight of the quadcopter itself. so at 50% throttle the total thrust produced by it should be 150 kg with a per motor total thrust of 37.5 kg.\n\ni have looked at this answer to how to calculate quadcopter lift capabilities? but don't understand how to use this information to work out the specifications of motor and propeller required for my application.\n\nthe answer given in previous question is limited for small quad &amp; i require the specifications of bldc motor such as kv,torque,imax,v,power,etc and of propeller suitable for such motor.\n", "tags": "quadcopter", "id": "7662", "title": "how do i work out the specifications of motors and propellers for a quadcopter?"}, {"body": "i would like to find an electronic actuator that mimics the characteristics of a hydraulic actuator, in that the position remains fixed without power drain when the actuator is not moving. which actuators exist that match these criteria?\n", "tags": "electronics actuator", "id": "7664", "title": "what actuator types exist that remain locked in their last position like hydraulic piston?"}, {"body": "i'm sorry for this question that might not fit in here however, i would like to give it a shot. i've chosen this stack since the question is somehow related to mobile robots. i've came across a paper in mobile robot localization that has cited the following reference, \n\n\n  c. brown, h. durrant-whyte, j. leonard, b. rao, and b. steer.  kalman\n  filter algorithms, applications, and utilities. technical report\n  ouel-1765/89, oxford u. robotics research group, 1989.\n\n\ni couldn't find this reference. nothing show up in google not even in google scholar. in my university which allows me to access to a massive database, also nothing show up. since this is a technical report, i'm interested to read it to have more appreciation about kalman filter. has anyone came across this reference?\n", "tags": "mobile-robot localization kalman-filter", "id": "7666", "title": "where to get this reference about kalman filter, technical report"}, {"body": "from a technical standpoint what are the differences between the kinect v1 and the kinect v2 ?\n\ni'm interested both in the hardware equipment and the format of the data.\n", "tags": "kinect", "id": "7670", "title": "kinect v1 vs kinect v2"}, {"body": "as i'm advancing in my project i realized i need better hardware, particularly for video input and processing.\n\nfrom an intuitive feeling sounds like stereo cameras offers a more powerful and flexible solution, on the other hand the kinect looks like a great out-of-the-box solution for depth sensing and it also takes away a lot of computational complexity as it output directly the depth.\n\nso i would like to know what are the upsides and downsides of the 2 solutions and if they have any well known limitation and/or field of application and why.\n\nthank you \n", "tags": "kinect cameras stereo-vision", "id": "7671", "title": "kinect vs stereo cameras"}, {"body": "i plan to use the icreate as a platform to carry a tablet, or notebook pc and want to have power for some time so i need more than the 3000 mah battery. i want all to be powered from same battery system and use same charging source. so i need info as to how to wire in additional 14.4v nimh batteries in parallel with the existing and how to deal with the additional temperature sensors (i could ignore of course but...). can the built in power control deal with this? do i need to upgrade it somehow? i would appreciate suggestions as i do not want a completely separate power system for aux devices. charging all from standard home base is the goal even though it will take longer. i can deal with adapting the 14.4v to whatever aux devices i add. thanks.\n", "tags": "power battery roomba", "id": "7672", "title": "extending icreate battery power for auxilliary equipment"}, {"body": "what's least complex way to reduce power from a 10v 1.5a battery to 6v 1.5a\n\nthank you!\n", "tags": "battery", "id": "7678", "title": "how to reduce battery power 10v 1.5a to 6v 1.5a"}, {"body": "i am working on a project that requires motion detection and positioning. i've worked substantially with a camera but the issue with this is that i need something sleek, small and not heavy at all. cameras also tend to rely on luminosity and they don't work well in poorly lit spaces.\n\ni need someone who's worked on something like this or who knows the best sensor for this purpose.\n", "tags": "arduino", "id": "7680", "title": "arduino compatible sensor for motion detection and positioning"}, {"body": "i have a 2 dof robot arm with a camera attached to it. it takes an image and there's an object in that image, say a glass. of course, in order to move the arm to the required position to grasp the object, i have to solve the inverse kinematic equations. in order to solve them, i need the x and y, the coordinates where the arm has to reach to grasp the object. my question is how can i find the x and y of say the midpoint of the object from the image. thanks \n", "tags": "mobile-robot robotic-arm", "id": "7682", "title": "object grasping robot arm control"}, {"body": "in the scope of my phd, i would like to build an automated microscopy set-up that should take images of a sample of 2cm by 2cm. this should be done by taking pictures of 500 micrometers by 500 micrometers.\n\ntherefore i need to design an xy-stage moving my sample over the optical setup. i would use a raspberry pi to steer all the hardware. \n\ncould you direct me to material about how to best make an xy-stage ? my questions are about what types of motors to use (stepper?), how many, how to create a good sliding mechanism to avoid jerky steps, etc. \n\nsimple links to basic engineering of such set-ups would be more than enough for me to start, as i am a complete layman in this field.\n\nedit: i have found this blogpost. it does what i require, if i get small enough angle step stepper motors.\n\nedit2: i need a maximal range of motion of 10 cm in both directions. the overall size should not exceed 30x30 cm^2. step sizes should not exceed 10 microns. i do not care about moving speed. based upon the design in the link, buying a stepper motor with a 100:1 gear box could allow my very small radial steps (&lt;0.05 deg) which would result in about 5 micron steps, assuming a rotor radius of about 1cm.\nas far as price goes, it should not exceed commercially available options which start at about 5k usd\n", "tags": "raspberry-pi stepper-motor", "id": "7685", "title": "what is required to build a simple xy-stage?"}, {"body": "do you use simulators for developing your robot algorithms or do you test directly in your robot?\ni would like to get introduced into the simulators world, but don't know from where to start... can you recommend me one?\n\nregards\n", "tags": "simulator", "id": "7691", "title": "is anybody using robot simulators?"}, {"body": "a robotic arm should pick a cuboid up of a table, rotate it around its vertical axis and put it down on all possible positions. how many degrees of freedom are at least necessary?\n(all coordinates, that should be reached by the robotic arm, are in its workspace. it is not allowed to put the cuboid down and pick it up, once the robot has it )\n\n\nthe answer is  4 (3 translatory and 1 rotatory).\n\n\nbut i don\u2019t understand why. i thouhgt that it should be 3.\n2 prismatic joints:  1 to pick the cuboid up,  and another one to move it anywhere on the table.\n1 revolute joint to rotate the cuboid around its vertical axis. => 2 translatory and 1 rotatory. \n", "tags": "robotic-arm", "id": "7694", "title": "degree of freedom"}, {"body": "i measure the voltage esc drawing while increasing the dc motor speed. multimeter shows that as long as the speed increases the voltage value decreases. can anybody explain why this is happening? \n", "tags": "brushless-motor electronics esc", "id": "7698", "title": "voltage rpm relation"}, {"body": "my robotic project is running at every 1ms and the processes are taking about 0.9ms. i am running pid so my max clock rate is 1khz. about half of the processing time are taken by spi peripherals, imu and encoders. is there any recommendation on how i can run faster pid sampling rate?\n", "tags": "pid embedded-systems", "id": "7703", "title": "increase pid sampling rate on embedded system"}, {"body": "for my particle filter, i decided to try using the low variance resampling algorithm as suggested in probabilistic robotics. the algorithm implements systematic resampling while still considering relative particle weights. i implemented the algorithm in matlab, almost word-for-word from the text:\n\n\n\nas would be expected given the while loop structure, i am getting an error for accessing weight(i), where i exceeds the array dimensions.\n\nto solve this, i was considering circularly shifting my weight array (putting the first index used as the first value in weight, so that i never exceed matrix dimensions). however, i wasn't sure if this would negatively impact the rest of the algorithm, seeing as i'm having trouble understanding the purpose of the u calculation and while loop.\n\ncould anyone help clarify the purpose of u and the while loop, and whether or not a circular shift is an acceptable fix?\n", "tags": "mobile-robot algorithm particle-filter probability", "id": "7705", "title": "low variance resampling algorithm for particle filter"}, {"body": "i need to make this construction (door is closed by default, door is opened to the top). this is the scheme:\n\n\n\nred rectangle on the picture is the aperture, blue rectangle is the door (weight is about 0.5 kg), which moves top when door need to be opened. green stripe on the picture is the rail for the door.\n\nwhich electrical engine should i use?\n\nestimated time of door opening is about 10 seconds, i want to send signal to up the door, it should be drop down when power is lost or i should send a signal to drop it down.\n", "tags": "design electronics actuator", "id": "7708", "title": "how to make door opening to the top"}, {"body": "i'm looking for an algorithm for formationing multiple robots in 2d simulation. can you suggest resources about this topic. also i need suggestions and comments about these topics:\n\n\ncan i recruit algorithm from optimization algorithms like particle or ant?\nis there any way except \"go to goal\" for each robot\nis patter formationing algorithms feasible?\nsuggestions about a fast way of formationing/ aligning\n\n\nnotes:\n\n\nim not using a robotics simulator or physics engine for this. \nrobots are represented as dots.\nmulti robot system is homogeneous\nevery robot can sense obstacles and other robots in a sense range circle around the robot.  \nnumber of obstacles and robots can vary from 2 to 100 \nmulti robot system is not a central \n\n", "tags": "mobile-robot multi-agent swarm", "id": "7710", "title": "formationing algorithm for multiple robots"}, {"body": "for a project i am building a tele-op robot using the irobot's roomba as my drivetrain. in order for my robot to work, i need an extra castor. irobot provides .stl and .stp files for me to use and i used them and printed the files. (the file i printed was from this link: create\u00ae 2 bin modification.\n\nthis file is a new part to the drivetrain to allow another caster.\n\nand i downloaded the first link called \"full bin bottom with caster mount\"\n\nthe piece was great but it made the castor a different height then the wheels. i was wondering if anyone had this file but saved as something different so i can edit it in preferably solidworks. i was on the phone with irobot for over 2 hours today and they told me to post here. so please help!!!! :)\n", "tags": "irobot-create roomba", "id": "7712", "title": "prototyping with irobot roomba"}, {"body": "i've been working lately on slam algorithms implementing extended kalman filtering to brush up on some localisation techniques and i have been thinking forward to the hardware side of things. are there embedded chips such a microcontroller that are optimised for large linear algebra operations? what sort of embedded options are the best for processing these sorts of operations?\n", "tags": "slam kalman-filter", "id": "7718", "title": "optimal hardware for linear algebra operations"}, {"body": "i hope someone can help me here.  i am reading very large numbers from my naze32.  what is the max and min values from pitch, roll and yaw? how would i then convert them to degrees or radians?\n", "tags": "quadcopter", "id": "7720", "title": "reading crazy large numbers from naze32"}, {"body": "reaction control systems (rcs) on these vehicles are implemented by using small rocket thrusters. for me it looks like these thrusters work in some kind of \"pulse\" mode. and i can't understand - do they use some optimal control to calculate in advance the required impulse to reach the new desired state of the system or they use \"pulse\" mode just for precise magnitude variation of provided thrust (like average voltage in pwm(pulse-width modulation)) in a classic pid control loop?\n", "tags": "control automatic rocket", "id": "7722", "title": "what type of control law is used in \"reaction control system\" of apollo lunar module or space shuttle?"}, {"body": "i'm learning  to make a 3d simulation in matlab based on a model designed from solidworks.\nthere is an example: simulink+solidworks\n\nthe way used here is: \n\n\ncreate a 3d model in solidworks\ncreate a xml file applicable to import to matlab via simmechanics link\nimport the model to matlab/simulink. a simulink system is created.\n\n\nafter these steps, controlling the system will be implemented in simulink.\nbut i feel simulink is kind of strict to control. i want to be more flexible, apply any algorithm to the model. and using matlab *.m file to control is more efficient way. \n\nso my question is this: is there any way to do 3d simulation (matlab+solidworks) by using only *.m file to control, no simulink anymore? \n\nall model information will be contained in the *m.file. maybe the step 1 and 2 are inherited, but step 3 is different.\n", "tags": "matlab simulation", "id": "7723", "title": "matlab 3d simulation with solidworks model"}, {"body": "how do i find out around which axis the coordinate system has to rotate, if the rotation matrix is given?\n\n$ {^{a}r_{b} } $ = $ \\left(\\begin{matrix} 0 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\\\end{matrix}\\right)$ \n\n$ {^{a}r_{c} } $ = $ \\left(\\begin{matrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\\\end{matrix}\\right)$ \n\nfor $ {^{a}r_{b} } $ i thought, that it has to be a rotation around the z-axis, because \n$r(z,\\theta) =  \\left(\\begin{matrix} cos(\\theta) &amp; -sin(\\theta) &amp; 0 \\\\ sin(\\theta) &amp; cos(\\theta) &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\\\end{matrix}\\right)$ \n\nthe values at the positions $a_{13}, a_{23},a_{33},a_{32},a_{31}$ of $ {^{a}r_{b} } $ and $r(z,\\theta)$ are identical.\n\nso i solved $cos(\\theta) = 0$ =>$\\theta = 90\u00b0 $ => 90\u00b0 rotation around z-axis.\n\nbut how do i solve it, if there is more than 1 rotation, like for $ {^{a}r_{c} } $?  \n", "tags": "joint", "id": "7724", "title": "determine what the rotation axis is given a rotation matrix"}, {"body": "i am starting to develop robotics project which involves simulation (and maybe real world programs) of soft-body dynamics (for food processing) and clothes/garment handling (for textile industry or home service robots). it is known that soft-body dynamics and garment handling are two less explored areas of robotics and simulation, therefore i hope to make some development of (contribution to) projects that are involved. the following projects are involed:\n\n\nbullet physics engine - for dynamics\ngazebo - simulation environment\nros - robot os, i hope to use universal robot ur5 or ur10 arms and some grippers (not decided yet)\norocos - for control algorithms\n\n\ninitially i hope use \"ros indigo igloo preinstalled virtual machine\" (from nootrix.com), but apparently i will have to make updates to the bullet, gazeboo, add new ros stacks and so on.\n\nthe question is - how to organize such project? e.g. if i am updating bullet physics engine with the new soft-body dynamics algorithm then what executable (so) files should i produce and where to put them into virtual machine? the similar question can be asked if i need to update gazebo. \n\nthere seems to be incredibly large number of files. is it right to change only some of them.\n\nsorry about such questions, but the sofware stack seems to be more complex than the robotics itself.\n", "tags": "ros software programming-languages dynamics gazebo", "id": "7728", "title": "where to make changes for simulation project bullet/gazebo/ros/orocos"}, {"body": "vision is important part of robotics and frequently it is unavoidable component of control loop. e.g. many clothes/garment handling algorithms rely on visual cues in deciding how to proceed. the question is - does simulation environments (gazebo or some others) allow one to design world with robot and garment and simulate not only garment dynamics but simulate also what robot sees, how robot perceives garment in each simulation step? if it is not possible to simulate vision then how to simulate algorithms with vision as component of control loop?\n\nmaybe simulation of vision can be good research theme? are here some trend or good articles about it? some initial projects that could be expanded?\n\nactually - it can be stated as more general question - is it possible to simulate sensors in gazebo? e.g. food handling (soft-body handling) can involve tactile sensors. in principle gazebo can calculate deformation and forces of soft-body and format these data as the simulated values of sensor readings. maybe similar mechanism can be used for simulation of vision as well?\n", "tags": "computer-vision simulator research simulation gazebo", "id": "7729", "title": "is it possible to simulate vision (perception) in gazebo (or other simulators)"}, {"body": "i'd like to study the capabilities of industrial robot arms. for example, to answer the question how does price vary with precision, speed, reach and strength?\n\nis there a database of industrial robot arms including information like the price, precision, speed, reach and strength of each model?\n", "tags": "robotic-arm industrial-robot", "id": "7731", "title": "comparing industrial robot arms"}, {"body": "is there any way of estimation the battery life from pwm outputs which goes to motors in microcontroller level. i'm planning to estimate path range with this. microcontroller, sensor and other electronic device should be neglected. \n", "tags": "quadcopter battery", "id": "7735", "title": "estimation of battery life time from pwm signals in a quadrotor"}, {"body": "i am building a laser gun for pentathlon targets (also doing one).\ni would like to know how build a part of the gun and if i can count on a steady laser if it is attached to a motor.\n\nthe question is about the laser. i want it maybe attached to a small-(servo)motor to try to implement some cheat just for fun. assuming the motor has a good torque, can i assume that the laser will not move (not the sightliest bit) when the motor is turned off? (i don't have any to test)\nthis is for precision shooting, so small vibrations and a moving pointer would be really prejudicial.\nin case it does, what can i do to minimize the problem? is it all about ordering the motor with the highest torque?\n\ni also have a second question which is slightly off-topic, yet related, and robotics people usually have solutions for such problems.\ni also need to build the sights. here's a gun:\n\n\nas you can tell, its sights are a fixed plastic point in the front, and an adjustable large back. there are two bolts, one on each side. one makes the sight higher or lower, and the other makes it point more to the right or left.\nhow can such part be built with simple tools?\n\nthanls\n", "tags": "motor stability laser", "id": "7737", "title": "building parts, and keeping laser alignment steady"}, {"body": "i'm working on an robot that would be able to navigate through a maze, avoid obstacles and identify some of the objects in it. i have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation.\n\nup till now i have processed the bitmap image, and converted it into an adjacency list. i will now use the dijkstra's algorithm to plan the path.\n\nhowever the problem is that i have to extract the entrance point/node and exit node from the bmp image itself for dijkstra's algorithm to plan the path.\n\nthe robots starting position will be slightly different (inch or two before the entrance point) from the entrance point of maze, and i am supposed to move to the entrance point using any \"arbitrary method\" and then apply dijkstra algorithm to plan path from maze's entrance to exit.\n\non the way i have to also stop at the \"x's\" marked in the bmp file i have attached below. these x's are basically boxes in which i have to pot balls. i will plan the path from entrance point to exit point , and not from the entrance to 1st box, then to second, and then to the exit point; because i think the boxes will always be placed at the shortest path.\n\nsince the starting position is different from the entrance point, how will i match my robot's physical location with the coordinates in the program and move it accordingly. even if the entrance position would have been same as starting position there may have been an error. how should i deal with it? should i navigate only on the bases of the coordinates provided by dijkstra or use ultrasonics as well to prevent collisions? and if we yes, can you give me an idea how should i use the both (ultrasonics, and coordinates)?\n\n\n", "tags": "arduino mobile-robot localization motion-planning mapping", "id": "7739", "title": "navigating through a maze using path-planning (dijkstra)"}, {"body": "we can easily compute the rigid robot kinematics and dynamics. there is many resources, simulators and modelling tools about it. but i couldnt find any of these for elastic robots. can you suggest resources and modelling tools?\n", "tags": "kinematics simulator dynamics", "id": "7741", "title": "mathematical moddeling of elastic robots"}, {"body": "\ni am still in high school and am a part of the robotics club that competes in the ftc (first tech challenge). i am just about finishing my first calculus class (calc 1), and would be ecstatic to be able to apply this someway in a real world example such as robotics. [besides pid. it seems like only approximations anyways] \nso far, i've only been working with \"fabricated\" math problems. would deriving an equation from real life situations be too complicated?\n\nthank you!\n", "tags": "control software", "id": "7757", "title": "calculus in robotics"}, {"body": "until now i have been programming the robot using java on kuka's ide \"kuka sunrise.workbench\", what i want to do is control the robot arm via my c++.net application (i would use a camera or kinect to get commands). \ni'm reading the documents provided by kuka, but as i'm a bit in hurry, i want to understand how a c++ client application (running on my laptop) can send/receive information to/from the robot's controller \"kuka sunrise cabinet\" (running the server application) via fri. i still have issues grasping the whole mechanism.\n\na simple application (server/client) source code with explanation (or a schematic) would be more than helpful .\n", "tags": "robotic-arm c++", "id": "7760", "title": "create a simple c++ client application to control kuka's robot-arm lbr iiwa via fri"}, {"body": "i am specifically interested in dh parameters versus other representations in terms of kinematic calibration.  the best (clearest) source of information i could find on kinematic calibration is in the book \"robotics: modelling, planning and control\" by bruno siciliano, lorenzo sciavicco, luigi villani, giuseppe oriolo, chapter 2.11.  which requires a description of the arm in dh parameters, multiplying out the kinematics equation, partial differentiation w.r.t. each dh parameter, then a least-squares fit (with the left pseudo-inverse), then iterate.  \n\nis there some fundamental reason why dh parameters are used instead of a different representation (like xyz + euler angles).  i understand that there are fewer parameters (4 versus 6 or more), but for a calibration procedure like this i will be taking much more data than unknowns anyway.  all the robotics textbooks i have read just present dh parameters and say \"this is what you should use\", but don't really go into why.  presumably this argument can be found in the original paper by denavit, but i can't track it down.\n", "tags": "robotic-arm kinematics calibration dh-parameters", "id": "7763", "title": "why do we generally prefer dh parameters over other kinematic representations of robot arms?"}, {"body": "i have a blade 180qx quadrotor / quadcopter and i had to move a red wire shoved under the circuit board when i fixed a broken power wire. now the red wire shown out straight from the circuit board is not in the right configuration - or at least as it was. if i understood what it was for i might know how to place it. is this a horizon sensor (temperature)?\never since i had to move this wire, the quadrotor goes unstable when flying. the only appreciable change is the wire position.\nthe red wire was not attached anywhere else on the board. it was shoved under the circuit board inside the battery holder.\n\n", "tags": "quadcopter", "id": "7764", "title": "blade 180qx :what does this red wire do?"}, {"body": "background:\n\n\n6 propeller drone w 20c 3s 6400 mah 11.1 lipo battery\n4 propeller drone w 25c 2s 5000 mah 7.40 lipo battery\n\n\nbehavior:\n\n\ndrone 1 flies with ease\ndrone 2 struggles hover 2-3 inches above ground\n\n\nquestion:\n\nthe microcontroller, all props, escs, and motors are the same. i'm thinking the reason the drones are flying so differently is because of the difference in batteries. if the batteries are the reason, what would be the property that is most responsible for the difference in flight?\n", "tags": "battery", "id": "7765", "title": "drone battery question"}, {"body": "i need to build a conversion/mapping algorithm from a controller (pid etc.) output to the duty cycle in order to command my bldc motor via esc. i couldn't do it yet because l think l dont know the meaning of controller output. anybody highlights my way?\n", "tags": "arduino motor esc microcontroller", "id": "7766", "title": "duty cycle mapping"}, {"body": "i have a 16 channel servo driver board from adafruit (see here), and i communicate to it via i2c using a raspberry pi. the servo board is controlling a qbrain by sending a pwm pulse between 1ms to 2ms and it works great.\n\nproblem is, i'm trying to create a kill switch such that the signal from the servo board would cease, and the esc would stop because it detects no pwm signal. i have placed a toggle switch that cuts the vcc to the servo board, so technically it should no longer produce any pwm signal, however when the power is cut, the esc jumps to 100% throttle, i can only assume this is because the esc believes the signal is 100% duty cycle, but how do i solve this?\n", "tags": "raspberry-pi esc", "id": "7772", "title": "how to cut throttle signal to esc properly?"}, {"body": "the task of the robot is as follows.\nmy robot should catch another robot in the arena, which is trying to escape. the exact position of that robot is sent to my robot at 5hz. other than that i can use sonsor to identify that robot.\nis that possible to estimate the next position of other robot using a mathematical model. if so, can anyone recommend tutorials or books to refer..?\n", "tags": "arduino kalman-filter automatic probability", "id": "7774", "title": "modeling a robot to find its position"}, {"body": "i'm working on a 2-wheeled robot and have connected up a raspberry pi to an l298n motor driver.\n\ni'm sending the enable pin of a particular motor a software-generated pwm signal at 100hz with a 50% duty cycle.  i observe with an osciloscope:\n\n\na fairly clean square wave going into the enable pin as expected.\na fairly dirty square wave across the output motor terminals.\n\n\nthe motor turns at about 50% speed/torque as expected.\n\ni find myself wondering if it would be better to control the speed of the motor by placing a flat lower constant voltage across its terminals, rather than oscillating a square wave.  ie to do 50% speed/torque - instead of oscilating between 0v and 5v - just put a constant 2.5v across the motor terminals.  i wonder if the oscillation is a waste of power/energy.\n\nis this true?  or doesn't it make any difference?  do high-end motor drivers use a variable flat analog voltage to control speed/torque, or do they use a pwm?  if a pwm, does the frequency make any difference?\n", "tags": "motor", "id": "7778", "title": "electric motor speed control - pwm vs analog voltage?"}, {"body": "my background:\n\nmy experience is in solid mechanics and fea.  so i have zero experience in robotics/controls.  \n\nproblem description\n\ni'm developing a control strategy to stabilize a complicated 6-legged dynamical system.  torques ti from each leg's joints will be used to create a net moment m on the body, stabilizing the system.  this moment m is known from the pre-determined control strategy.  (side note: the dynamical solver is of the nonlinear computational type)\n\ndue to my lack of background, i have a fundamental confusion with the dynamical system.  i want to use joint torques ti to create this known net moment m on the body.  this moment m is a function of the\n\n\ncurrent positions/angles of all the leg segments\nreaction forces and moments (that cannot be controlled) of each leg\ncontrollable joint torques ti of each leg\ntime\n\n\n$(*)$ at a given time $(n-1)\\delta$t: \n\n\n  --from the control strategy, the desired net moment m is computed/known\n  \n  --one can read/sense the legs' positions, angles, reaction forces, and reaction moments (say, from well placed sensors), at this time $t = (n-1)\\delta$t.  \n  \n  --from this information, vector algebra easily yields the desired joint torques ti required to create the net moment m\n\n\n$(**)$ at the time $(n)\\delta$t:\n\n\n  --one applies the previously determined joint torques ti (determined at $t=(n-1)\\delta$t) to create the desired moment m \n  \n  --of course these torques ti are applied at the immediate proceeding time step because they cannot be applied instantaneously\n\n\nso this is exactly where my fundamental confusion exists.  the torques ti were calculated in $(*)$, based on data of angles/positions/reactions in $(*)$, with the objective to create moment m.  however, these torques ti are applied in $(**)$, where the data (angles/positions/reactions) are now different - thus the desired net moment m can never be created (unless you an magically apply actuation at the instantaneous time of sensing).  am i understanding the controls problem correctly?  \n\nquestions\n\n\nam i understanding the robotics problem correctly?  what are the terms and strategies around this dilemma?\nof course i could create the time steps between the sensing and the actuation to be infinitely small, but this would be unrealistic/dishonest.  what is the balance between a realistic time step, but also performs the task well?\n\n", "tags": "control actuator stability legged", "id": "7781", "title": "how do i decide the size of the time steps between sensing and control actuation?"}, {"body": "i saw buddy's page and want to purchase for my slam research. however, i wonder is it possible to program buddy for slam? \n\naccording to buddy's spec, they're only few ir's, sonars and a camera. as i know, most slam algorithms are implemented with powerful sensors such as rgbd/stereo camera, or even laser range finder.\n\nare there any pepers mention about ir-based slam?\n", "tags": "mobile-robot localization slam mapping rangefinder", "id": "7782", "title": "is it possible to do slam with few ir sensors like buddy?"}, {"body": "i am using ikfast in openrave for my inverse kinematics.  this is an analytical solver, so if your robot's dof matches the ik type's dof, then you get all possible solutions.  but if your robot has more dofs, then you need to pick some joints to have a constant value.  (however, if you use openrave's python interface it will discretize that joint for you.  i.e. give you a set of solutions for every 0.1 radians of that joint.  but my question holds for either interface.)  i have a 7 dof anthropomorphic arm with joints: roll-pitch-roll-pitch-roll-pitch-yaw as seen in this image:\n\n\n\nthe discretized joints are call \"free joints\" in openrave's terminology.  if i let ikfast decide, it picks joint 3 (upper arm roll) to be the free joint.  however, i have been using joint 4 (elbow) to be the free joint because it is easier for me to think about.  but then i realized that perhaps joint 5, 6, or 7 would be better to discretize because they are closer to the end of the chain.  won't the ik solutions suffer if joints closer to the start of the chain have a large discretization?  or is openrave picking the optimal joint to discretize?\n\ni was just wondering if there is some standard practices or known conventions for this sort of thing.\n\nput simply: i want a set of ik solutions for the end-effector at some pose.  i will fix a joint either near the start or end of the kinematic chain.  and what i set it to isn't going to be perfect.  lets say it is off from some \"ideal\" position by some epsilon.  now you can imagine that if i want the hand in-front of the robot, and i pick a bad angle for the shoulder (like straight up for example), the rest of the joints will have a hard time getting the end-effector to the target pose, if at all.  but if i fix the wrist to be at some awkward angle, there is still a good chance of getting the end-effector there, or at lease close.  what kind of trade-offs are there?  which will have a \"better\" set of solutions?\n", "tags": "inverse-kinematics", "id": "7786", "title": "which joints to discretize for ik"}, {"body": "i would like to seek some advise on how i should go about implementing a neural network on a dspic to control the motor speed of a robot (if it is at all feasible). currently, i am deploying a typical pid control loop to control the speed of the motors that have encoders attached to them. \n\nhere are some thoughts on what i think the set-up should be like:\n\ninputs -> desired speed, current speed\noutput -> pwm\nhidden -> not sure how many nodes are needed and how many layers\n\ni also face a contradiction on how to do supervised learning for this. assuming i pass in a desired speed and the current speed, the output,if using sigmoid function, would be from 0-1. i would take this an multiply by the maximum pwm value then compare to the pwm value required to generate that speed. is this the correct way to determine the error? i am a little unclear on this part. technically, i could manually determine the pwm values required for certain speeds, but that would result in a very small data set for training. \n\nalternatively, i considered passing the pwm values into the motor function, wait short period, then capture the current speed of the motor then compare that with the desired speed to get an error. \n\ni only just started coding some basic neural nets and i hope to get some ideas. \n\nthanks!\n", "tags": "motor pid pwm", "id": "7790", "title": "how to control motor speed via neural network on a dspic?"}, {"body": "i try to find out the relation between rpm vs. thrust for a battery+motor+propeller combination. the image shows my setup and also the measurement result. can anyone explain how l should use this datas (i know kv.v gives the rpm but my voltage values decreasing because of p=v.i relation etc.) \n", "tags": "quadcopter brushless-motor esc", "id": "7791", "title": "thrust measurement"}, {"body": "i have a spektrum dx5e transmitter and a ar610 receiver. i believe that they are paired because when i turn on the transmitter the receiver's light turns on. however i lost the bind plug but i somehow managed to bind them. i plugged in my servos and batteries into the correct slots but nothing happens when i move the sticks. what am i doing wrong? i have a y splitter for two power hd servos (contentious rotation) and another power hd micro servo for the steering but none of them are responding.\n", "tags": "wheeled-robot", "id": "7794", "title": "why is my robot not working"}, {"body": "can i connect a udoo board to a pc using a straight-through ethernet cable? or do i need a cross-over cable?\n\nas far as i know, most modern devices can use the two interchangeably. however, i am not sure if a udoo can do that. anyone with any experience?\n\nthank you for your help.\n\n(ps: i don't have a udoo on me at the moment, so i can't test it myself. couldn't find any information in the documentation either).\n", "tags": "embedded-systems", "id": "7795", "title": "can i connect a udoo to a pc using a straight-through ethernet cable or do i need a cross over?"}, {"body": "say i had an object with 4 motors/wheels attached (in a fairly standard arrangement).\n\ni need to calculate the amount of torque required from the motors to be able to move the object of x kilograms consistently (without skipping any steps) at a velocity of y, travelling up a slope of angle z.\n\ni'm guessing this would also depend on factors like the grip of the tyre and such?\n\nfairly straightforward question (i hope the answer is that way too).\nthanks in advance.\n", "tags": "motor motion torque wheel", "id": "7796", "title": "calculating required torque"}, {"body": "i am trying to implement an ekf-slam using the algorithm for unknown correspondences proposed in the book \"probalistic robotics\" by sebastian thrun in table 10.2 . \n\nby now i understand actually all of the algorithm except of the initialization of new landmarks in the covariance matrix $ p_{new} $. \n\nin that algorithm when a new landmark is detected the procedure is just the same as if a normal measurment update for an already observed landmark is done: the kalman gain $ k $ is calculated for the new landmark and then the covariance is updated with that kalman gain and the jacobian $ h $ of that new landmark like this $ p_{new}=  (i  -  k * h) * p$ . \n\nin my understanding a just new observed landmark would not have any effect on the rows and columns that correspond to already mapped landmarks or the robot pose in the covariance matrix. instead i think that just two rows and columns for x and y should be created with some uncertainity like proposed here: the uncertainty of initializing new landmark in ekf-slam .\n\ni tried to split down the calculation of $ p_{new}$ via claculating it blockwise to see if i could somehow come to the same initialization as shown in the link above. but i end up having a different covariance matrix where apparently the new landmark is effecting the rows and columns of the parts of the old covariance, which in my view can't be right.\n\ni hope i don't understand the pseudo code of the book wrong or i did a mistake in my try to come to the same initialization. any advice how the initialization of new lnndmarks work in that code or if it actually is the same as in the link will be appreciated.\n\nedit\n\nso basically what i am asking is: why would they do a normal kalman update of the covariance matrix in line 24 of table 10.2 for a new observed landmark? why is there no explicit case for the initialization of new rows/columns of new observed landmarks in the covariance matrix? it seems to me like they just do a normal measurement update even for a just newly observed landmark.\n", "tags": "mobile-robot slam ekf", "id": "7802", "title": "ekf-slam initialize new landmark in covariance matrix"}, {"body": "i'm using matlab to suppress low frequency components with a high pass filter. \n\nobjective\n\n\nfilter angular velocity measurements affected by high frequency noise and bias in order to get the best estimate of the angular position.\n\n\nthe output when the gyroscope is still looks like this.\n\n\n\nfirst approach\n\nthe easiest way to remove baseline is to remove the average and can be achieved with matlab using one line of code.\n\n\n\nsecond approach\n\nwe can design a high pass filter to attenuate low frequency components. if we analyze the frequency components of the signal we will see one peak at low frequency and \"infinite\" small components in all frequencies due to noise. with a second order butterworth filter with normalized cutoff freq  we will get what we are looking for.\n\n\n\nfiltered data\n\n\n\ntilting the gyro\n\nwhen we tilt the gyroscope the situation changes. with a sampling frequency of 300hz we get the following plot.\n\n\n\nthe first half of the dft is shown below in a normalized scale.\n\n\n\nyou can find the sample.mat file here\n\nthe first approach works great. i would like to apply the second one to this particular case but here there are other low frequency components that make to job harder.\n\nhow can i apply the second approach based on the high pass filter to remove the bias?\n\nedit 1 \nyou can find more information here\n\nedit 2\nhow can we filter this signal to remove bias while keeping the angular velocity information (from 110-th to 300-th sample) intact?\n\nif gyroscopes have the bias problem only when they are not experiencing any rotation, then the offset is present only in the first ~110 samples.\n\nif the above hypothesis is correct, maybe if we apply high pass filtering only in the first 110 samples and desactivate the filter during rotations of the gyro, the estimated angular position will be more accurate.\n", "tags": "gyroscope matlab filter", "id": "7803", "title": "gyroscope - how can i remove low frequency component with a high pass filter only?"}, {"body": "i have an arduino, wires, resistors, all of that good stuff. however, i don't have materials to build the structure of the robot. what do you guys recommend? i don't have a place to solder yet so i can't solder but is there a kit or material that you guys recommend? will it work well with motors and other stuff? thanks! \n\np.s. i plan on building a standard driving robot, but i want to be able to make other robots with the same materials/kit. i don't want a kit that only makes one robot, i want a lego-esque approach to building the structure where i can build whatever i want with it. (bump2)\n", "tags": "arduino beginner", "id": "7809", "title": "robot structure kit or materials"}, {"body": "i am working on a project that needs tracking location and speed of pedestrians/runners/athletes (so not really robotics, but i see a lot of related usage and posts in the robotics domain, and an answer to this question could help with follower robots). i'm interested in just the 2d location (latitude-longitude).\n\nusing just the gps position has noisy/jump samples and also the degradation due to multi-path near trees etc. from reading about filtering solutions, i understand that sensor fusion that fuses gps with the data from inertial sensors (ins) helps improve a lot of these issues. also, this kind of sensor fusion seems to be used in a lot of places -- robotics, wearables, drones etc. hence i think there might be off the shelf chips/modules/solutions for this, but i couldn't find any.\n\ni found a sensor hub from invensense that integrates the 9 dof inertial sensors and comes with the fusion firmware, but it doesn't seem to have hookups and firmware for fusing gps and providing filtered latitude-logitude. \n\nso, what should i be looking for? are there any off the shelf chips/modules/solutions that come with the built in sensor fusion software/firmware for doing gps+ins fusion? \n\ni understand that it will still need tuning some params as well as some calibration.\n", "tags": "localization kalman-filter sensor-fusion gps", "id": "7815", "title": "are there off the shelf solutions for gps+ins (accelerometer,gyro,magneto) sensor fusion for getting filtered/fused location and speed output?"}, {"body": "i want to use a sensor to find displacement with accelerometer.\nhow can i use accelerometer to find displacement? i want to use this for a quadcopter.\n", "tags": "quadcopter imu accelerometer uav", "id": "7816", "title": "displacement with accelerometer"}, {"body": "how quadcopter's arm length affect stability?\n\nas per my view i'll have better control on copter with longer arms but with stresses in arms and also it doesn't affect lift capabilities.\n", "tags": "control quadcopter stability", "id": "7819", "title": "deciding length of quadcopter arms"}, {"body": "we're building an 6dof joystick, and we need to accurately measure the displacement of our central device. we can easily use a mechanical connection to the edges, but there has been some discussion about what the best way to achieve this is. the range of motion will be fairly small, but accuracy is incredibly important for us.\n\nwhich sensors are most easily and accurately measured?\n\nmy impulse response is that rotational and linear potentiometers are the most reliable, but others have been arguing for using gyros/accelerometers. i've also heard that hall effect sensors can be used to great effect.\n", "tags": "control sensors accelerometer", "id": "7823", "title": "which sensor type most accurately measures position?"}, {"body": "i'm searching for a (commercial) projector that just projects a single laser point into the world (e.g. using two moving mirrors). however, i'm struggling because i'm not sure what such a thing is called. i either find area projectors that use lasers, party equipment or laser pointers. \n\nwhat is the name for such a device? \n", "tags": "laser", "id": "7825", "title": "what device do i need to project a laser to point at a specific location?"}, {"body": "how are the brushless motors in a gimbal assembly designed?\n\nobviously it doesn't need continual rotation, but it does need accurate control of precise position. i've noticed that the motors in my gimbal don't have the usual magnetic 'snap' positions that my other motors do. \n\nwhat are the primary design differences in these kinds of motor, if any?\n", "tags": "motor", "id": "7828", "title": "how is a brushless gimbal motor different from a regular brushless motor?"}, {"body": "i have created a three wheeled omni robot like the diagram below. now i am unsure of how to program it. i want to use a single joystick so one x and one y value. the values for x and y are between -1 and 1, also the motors can be set anywhere from -1 to 1. how do i use this data to make the robot move based on the joystick without changing orientations? after doing some initial research this seems like a complex problem, but i am hoping there is a formula that i can.\n\n\n", "tags": "wheeled-robot", "id": "7829", "title": "how to program three wheel omni"}, {"body": "i'm working on a python script which reads the data from the mpu6050 imu and returns the angles using sensor fusion algorithms: kalman and complementary filter. here is the implementation:\nclass mpu6050 reads the data from the sensor, processes it. class kalman is the implementation of the kalman filter. the problem is the next: none of the kalman, neither the complementary filter returns appropriate angle values from the y angle. the filters work fine on the x angle, but the y angle values make no sense. see the graphs below. i've checked the code million times, but still can't figure out where the problem is.  \n\n\n\nedit:\ni've added some information based on @chuck's answer:\n\n\n contains the temperature\nin my opinion the compl. filter is implemented correctly:  and  are angular velocities, but they are multiplied by , so they give angle.  are accelerations, but  and  are angles. check my comment, where the complementary filter tutorial is.\nyes, there was something missing in the kalman filer, i've corrected it.\ni totally agree with you,  is not the best solution. i've measured how much time the calculation takes, and it is about 0.003 seconds. the y angle filters return incorrect values, even if  or  is used.\n\n\nthe y angle filters still return incorrect values.\n", "tags": "kalman-filter", "id": "7831", "title": "complementary and kalman filter don't work for y angle"}, {"body": "i am currently working on a pose estimation problem for which i would like to use filtering. to explain the system briefly, it consists of two cameras and each has its own gps/imu module. the main assumption is that camera1 is fixed and stable, whereas camera2 has a noisy pose in 3d. i am using computer vision to obtain the pose (metric translation and rotation) of camera2 w.r.t. camera1, so that i can improve upon the inherent noise of gps/imu modules.\n\nthe problem here is that the translation obtained through the vision method is only up to an arbitrary scale, i.e. at any given instant, i can only obtain a unit vector that specifies the \"direction\" of the translation and not absolute metric translation. the camera based estimation, although accurate, has no idea about how much actual distance is between the cameras, which is why i have the gps, which gives me position data with some noise.\n\nexample: camera 2 is 5 m to the east of camera 1, the pose from my vision algorithm would say [1, 0, 0] ; 1 m north-east to camera 1, it would be something like [0.7, 0.7, 0]\n\nhence, would it be possible to consider the gps estimate of the metric translation as well as its covariance ellipse, and somehow link it with the normalized camera measurements to obtain a final, more accurate estimate of metric translation? i am not sure what kind of filters would be happy to use a measurement that has no absolute value in it.\n\nthanks!\n", "tags": "kalman-filter cameras pose", "id": "7832", "title": "kalman filter for estimating position with \u201cdirection\u201d measurements"}, {"body": "i am trying to build a 2-axis robot arm with pan and tilt mechanism. the gripper/holder will hold an object weighing 300 grams. the total weight of the arm including the motors will be around 2 kg. i have decided to use 180 degree servo motors. the maximum arm reach will be 340 mm. \nwhat i want to ask is:\n\n\nwhat kind of servos (analog/digital) will be suitable to support the\ntotal weight (2 kg) and the object weight (300 g)?\nhow do i calculate the required torque?\nhow many servos should i use to make sure that my arm doesn't flip\nover?\n\n\nplease suggest me if there is a better approach to designing the robot. i am fairly new to electronics and this is the first time i am building a robot.\nthanks in advance. \n", "tags": "robotic-arm design servos", "id": "7837", "title": "type of servo and torque calculation required for a 2axis robot arm"}, {"body": "are there any standards regarding single vs multiple mcu in a robotic system? more specifically, if a single mcu can handle all of the sensor data and actuator controls, is it better to use a single mcu or multiple mcus in a hierarchical manner? are there any references/papers regarding this topic? what are the arguments towards one or the other? i am looking for facts, not personal opinions, so pros, cons, standards and such.\n", "tags": "design", "id": "7838", "title": "mcu architecture design"}, {"body": "i am young researcher/developer coming from different (non-robotic) background and i did some research on camera localisation and i came to the point, where i can say that i am lost and i would need some of your help.\n\ni have discovered that there is a lot of slam algorithms which are used for robots etc. as far as i know they are all in unknown environments. but my situation is different.\n\nmy problems and idea at the same time is:\n\n\ni will be placed in an known room/indoor environment (dimensions would be known)\ni would like to use handheld camera\ni can use predefined landmarks if they would help. in my case, i can put some \" unique stickers\" on the walls at predefined positions if that would help in any way for faster localisation.\ni would like to get my camera position (with its orientation etc) in realtime(30 hz or faster).\n\n\nfor beginning i would like to ask which slam algorithm is the right one for my situation or where to start. or do you have any other suggestions how to get real time camera positions inside of the known room/environment. it must be really fast and must allow fast camera movements. camera would be on person and not on robot.\n\nthank you in advance.\n", "tags": "localization slam real-time", "id": "7841", "title": "real-time camera localisation in known environment"}, {"body": "i want to control a brushless motor with the \"emax simon series 30amp esc\" and arduino (leonardo) board. i am really confused how to do that. i can't understand which beep sounds mean what. i have tested many code examples but they weren't useful.  \n", "tags": "arduino brushless-motor esc", "id": "7850", "title": "emax esc simon series with arduino"}, {"body": "hi there i just found this old rx &amp; tx in my loft and need to know weather it is compatible with my apm micro 2.7.2. i already have telemetry but that does not give me manual control. my guess is i need a new rx because the current one will make a hash of the electronics on the apm. thanks in advance[![enter image description here][1]][1]\n\n[![enter image descriptere][2]][2]\n\n\n\n\n", "tags": "quadcopter", "id": "7852", "title": "olf futaba rx & tx with apm micro 2.7.2?"}, {"body": "could i have your opinions on pid type selection?\n\n\nsystem description\n\n\nhere comes a very simple system: $\\mbox{output}(t) = k * (\\mbox{input}(t) + \\mbox{systemvariable}(t))$. $k$ is constant and $\\mbox{systemvariable}(t)$ is a system variable which may change according to time.\nthe goal of the whole system is to maintain system output at $0$. it has to be as close to zero as possible. the controller has to compensate the $\\mbox{systemvariable}$.\nthe change ($\\mbox{systemvariable}$ ) is modeled by a very slow ramp.\n\n\ncontroller description\n\n\nthe controller's input is the output of the system. however, the measurements are always noisy, and i modeled band-limited white noise into the measurements.\nafter pid controller, the output goes into an integrator, since the pid controller always calculates the \"change\" of the plant input. \n\n\nquestions\n\n\nmy original thoughts: add a pid controller with p=1/k is enough. since every time the controller gets an error $e$, it can be calculated back that the compensation on controller output shall be $e/k$. however, matlab auto-tuning always give me a pid. why is that?\nwhat is the relation between p of pid and measurement noises? if p is large, the system will tend to be rambling largely, due to the noises. if p is small, the system will tend not to converge to the correct value or very slow. how to make the trade-offs? or how to prevent system from rambling largely and get quick system responses? \n\n\nthanks a lot!\n", "tags": "control pid", "id": "7853", "title": "how to tune pid for a y(t) = k*x(t) system?"}, {"body": "i am working with an stm32f103c8 which has a flash size of 64kbytes.\n\nnow i am using chibios 2.6 and the build file is a binary file of 82kbytes.\n\nusing st-link utility, the program is getting dumped into the microcontroller's flash. \n\nmy question is how come a 82kb code fits in the 64kb flash?\n\nhow is the size of that .bin calculated? i am attaching a picture of the display. \n\ni did a compare ch.bin with device memory and it doesn't report any errors found.\n\nall parts of the code work just fine, i don't see any problems anywhere tried all the features of the code, nothing breaks or behaves abnormally.\n\ncould someone please explain this?\n\nthanks!\n\n\n", "tags": "microcontroller", "id": "7855", "title": "how is the absolute flash size calculated in a microcontroller?"}, {"body": "i need an irobot create serial cable (one end 7-pin mini-din connector and the other end is usb) for turtlebot i.  how can i connect my bot to my pc?\n", "tags": "mobile-robot irobot-create serial roomba", "id": "7860", "title": "irobot create serial cable for turtlebot i"}, {"body": "i know that rc servo motors are designed for precise movement, rather than a d.c. motors' continual rotation. are most rc servo motors limited to movement within one rotation or can they actually be made to continually rotate? that is to say, is their movement limited to a specific arc? or does it depend on the type of rc servo motor?\n\ni have seen videos of industrial size steppers rotating constantly, but, more specifically, i was wondering whether a mg995 can.\n\n\n\ni don't own any rc servo motors yet, so i can't actually test it myself. i just want to make sure before i make a purchase. i keep seeing conflicting information, for example the instructable, how to modify a rc servo motor for continuous rotation (one motor walker robot), implies that a rc servo motor will not continually rotate, else otherwise, why would there be a need to modify it? \n\naddendum\n\ni have just realised, after further digging about on google, and as highvoltage points out in their answer, that i have confused steppers and servos.\n\nin addition, i found out how to hack the towerpro mg995 servo for continuous rotation.\n", "tags": "stepper-motor rcservo", "id": "7864", "title": "can rc servo motors continually rotate?"}, {"body": "i am currently working on a balancing robot project, which features fairly low-cost sensors such as an 9-dof imu with the measurement states\n\n$\\textbf{x}_\\text{imu} = \\left[a_x, a_y, a_z, g_x, g_y, g_z, m_x,m_y,m_z \\right]^\\text{t}$.\n\ncurrently i use the accelerometer and gyroscope readings, fused by a complimentary filter to get the angular deviation of the robot's upright (stable) position. the magnetometer values are tilt-compensated and yield the robots orientation with respect to the earth-magnetic field (awful when close to magnetic distortion). furthermore i have pretty decent rotational encoders mounted on the wheels which deliver information on a wheel's velocity.\n\n$\\textbf{x}_\\text{enc} = \\left[v_l,v_r\\right]^\\text{t}$.\n\ngiven these measurements i want to try to get the robots pose (position + heading).\n\n$\\textbf{x}_\\text{rob} = \\left[x,y,\\theta\\right]^\\text{t}$\n\ni do have minor theoretical knowledge on ekf or kf, but it is not sufficient for me to actually derive a practical implementation. note that my computational resources are fairly limited (raspberry pi b+ with rtos) and that i want to avoid using ros or any other non-std libs. can anybody help me on how to actually approach this kind of problem?\n\n", "tags": "sensors kalman-filter imu sensor-fusion odometry", "id": "7870", "title": "simple sensor fusion for pose estimation"}, {"body": "i want to conduct the following experiment:\ni want to set up a scene with a kuka lwr4+ arm, a 3d model of an object and a camera overlooking them. i want to find the pose of the object using some pose estimation algorithm and move the arm towards the object. \nin general i want a piece of software or a combination of cooperating software that can do all that without having to reinvent the wheel. is there anything available?\n", "tags": "software simulation visual-servoing", "id": "7872", "title": "simulation environment for conducting visual servoing experiment"}, {"body": "i am new to the create 2 and i downloaded real term to program, opened an interface to the robot and send numbers with it to the robot.\n\ni can only get the drive command to work. i only know how to make the robot go faster, turning around or slower.\n\ni would like to know how to make the other commands work along with making it go left and right.\n", "tags": "irobot-create", "id": "7874", "title": "what commands make the irobot create 2 go left and right not just forwards and backwards?"}, {"body": "recently we've encountered kalman filter algorithm for state estimation in a course of probabilistic robotics.\n\nafter taking several days to try to read kalman's original paper published in 1960, \"a new approach to linear filtering and prediction problems\", it firstly feel a bit difficulty to read, and it seems the majority is to show the orthogonal projection is the optimal estimation under certain conditions and solutions to wiener's problem.\n\nbut i did not find the exact algorithm in this original paper as the one in the textbook. \n\n\nfor example, is there an explanation of \"kalman gain\" in this paper ?\ndoes kalman's paper provide a mathematical derivation of kalman\nfilter algorithm?\n\n", "tags": "kalman-filter", "id": "7877", "title": "original paper of kalman filter"}, {"body": "can anyone please explain me these lines found on page 5 of \nkinematics equations for differential drive and articulated steering\nby thomas hellstr\u00f6m?\n\n\n  note that plugging in $r$ and $v$ for both left and right wheel result in the same $\\omega $ (otherwise the wheels would move relative to each other). hence, the following equations hold:\n  \n  $$\n\\begin{align}\n\\omega~ \\left(r+\\frac{l}{2}\\right) &amp;= v_r\\\\\n\\omega~ \\left(r-\\frac{l}{2}\\right) &amp;= v_l\\\\\n\\end{align}$$\n  \n  where $r$ is the distance between icc and the midpoint of the wheel axis, and $l$ is the\n  length of the wheel axis (see figure 6). \n  \n  \n  \n  figure \n  6\n  .\n  when left and right wheel rotate with different speeds, the robot rotates around\n  a common point denoted icc\n  my questions are:\n\n\n\nhow do these equations come to be?\nwhy does $\\omega$ have to be same if we want to analyse the behavior after changing the wheel velocity relative to other?\nhow do we know about the circle, in which the robot rotates by doing variations in one wheel velocity, surely passes through the center point between two wheels.\n\n", "tags": "mobile-robot", "id": "7879", "title": "forward kinematics: why \u03c9 should remain same?"}, {"body": "i'm a agricultural engineering student and complete newbie trying to build a simple mechanism attached to a drone that dispenses a grease-type fluid. however, since i'm not familiar with the field, i'm having a hard time googling because i don't know the correct terms to search for. \n\ni'm looking for a mechanism that will remotely push the grease out. the problem is carrying the necessary weight for an hectare (300g to 1,5kg of fluid) and the dispenser mechanism within the drone. so i'm looking for a lightweight dispenser mechanism capable of deliver small amounts of this fluid (3g) distributed on the trees canopy. the grease do not need to be heated as it flows naturally in normal temperatures (like a toothpaste). both pump or syringe-type arrangement would be fine as long as i can control it remotely.\n", "tags": "motor quadcopter design battery actuator", "id": "7881", "title": "how do i dispense a greasy fluid?"}, {"body": "suppose i have a 3 link(1 dimensional) chain in which all the joints are revolutes, the axis of first revolute joint is along z-axis(global) and axis of second joint is along x-axis(global). the first link is along x-axis(global) and second link is along z-axis(global).\n\nnow in order to use dh representation i introduced a local frame for link 1 at joint 1(z axis along z and x axis along x) and another frame at joint 2.here z-axis is along axis of rotation(global x) and here i am clueless how to determine x-axis for joint 2 because the two z axis are intersecting.(standard procedure is to find common normal between two z axis)\nthanks for your time.\n", "tags": "robotic-arm dh-parameters", "id": "7882", "title": "how to determine x-axis if the two z-axis are intersecting in denavit hartenberg representation"}, {"body": "i try to read imu sensor data from an arduino mega 2560 uart with serial receive block of arduino support package for simulink. the imu can send binary packets and also nmea packets and i can configure it to any output. when the serial recieve block output is directly used, it displays just the numbers between 0-255. l need help about how to parse the coming data which contains the euler angles that i want to use.\n\nhere is binary structure ;\n\n\"s\",\"n\",\"p\",packet type(pt),address,data bytes (d0...dn-1),checksum 1,checksum 0\n\nthe pt byte specifies whether the packet is a read or a write operation, whether it is a batch operation, and the length of the batch operation (when applicable). the pt byte is also used by the um7 to respond to commands. the specific meaning of each bit in the pt byte is given below.\n\npacket type (pt) byte;\n\n7 has data,\n6 is batch,\n5 bl3,\n4 bl2,\n3 bl1,\n2 bl0,\n1 hidden,\n0 cf\n\npacket type (pt) bit descriptions;\n\n7...has data: if the packet contains data, this bit is set (1). if not, this bit is cleared (0). \n\n6...is batch: if the packet is a batch operation, this bit is set (1). if not, this bit is cleared (0) \n\n5:2..batch length (bl): four bits specifying the length of the batch operation. unused if bit 7 is cleared. the maximum batch length is therefore 2^4 = 16 \n\n1...hidden: if set, then the packet address specified in the \u201caddress\u201d field is a \u201chidden\u201d address. hidden registers are used to store factory calibration and filter tuning coefficients that do not typically need to be viewed or modified by the user. this bit should always be set to 0 to avoid altering factory configuration.\n\n0...command failed (cf): used by the autopilot to report when a command has failed. must be set to zero for all packets written to the um7.\n\nthe address byte specifies which register will be involved in the operation. during a read operation (has data = 0), the address specifies which register to read. during a write operation (has data = 1), the address specifies where to place the data contained in the data section of the packet. for a batch read/write operation, the address byte specifies the starting address of the operation.\nthe \"data bytes\" section of the packet contains data to be written to one or more registers. there is no byte in the packet that explicitly states how many bytes are in this section because it is possible to determine the number of data bytes that should be in the packet by evaluating the pt byte.\nif the has data bit in the pt byte is cleared (has data = 0), then there are no data bytes in the packet and the checksum immediately follows the address. if, on the other hand, the has data bit is set (has data = 1) then the number of bytes in the data section depends on the value of the is batch and batch length portions of the pt byte.\nfor a batch operation (is batch = 1), the length of the packet data section is equal to 4*(batch length). note that the batch length refers to the number of registers in the batch, not the number of bytes. registers are 4 bytes long.\nfor a non-batch operation (is batch = 0), the length of the data section is equal to 4 bytes (one register). the data section lengths and total packet lengths for different pt configurations are shown below.\nthe two checksum bytes consist of the unsigned 16-bit sum of all preceding bytes in the packet, including the packet header.\n\nread operations;\n\nto initiate a serial read of one or more registers aboard the sensor, a packet should be sent to the um7 with the \"has data\" bit cleared. this tells the device that this will be a read operation from the address specified in the packet's \"address\" byte. if the \"is batch\" bit is set, then the packet will trigger a batch read in which the \"address\" byte specifies the address of the first register to be read.\nin response to a read packet, the um7 will send a packet in which the \"has data\" bit is set, and the \"is batch\" and \"batch length\" bits are equivalent to those of the packet that triggered the read operation. the register data will be contained in the \"data bytes\" section of the packet.\n\nhere is an example binary communication code;\n\n\n", "tags": "arduino electronics embedded-systems matlab", "id": "7883", "title": "how can i get the values of a imu from the serial message received in simulink via uart?"}, {"body": "i was trying to reproduce this youtube tutorial in v-rep and i came across some problems concerning blob detection. there are some complaints on this matter under the video. i don't believe that blob detection stopped working in recent v-rep versions, but i was unable to make it work (as a new v-rep user myself). has anyone any idea how to properly implement it? \n\nmore specifically, i have a vision sensor named  and i want it to follow a red ball. the vision sensor will detect the position of the ball and i will use it to control the joints that steer the sensor ( and ). my script follows\n\n\n\nwhen i run the simulation i can see that the sensor sees the red ball at some point but  is always 0 meaning that no detection takes place.  \n\nhere is my scene\n", "tags": "simulator visual-servoing", "id": "7884", "title": "using blob detection in v-rep"}, {"body": "i'm trying to calculate the jacobian for days now. but first some details. within my master's thesis i have to numerically calculate the jacobian for a tendon-driven continuum robot. i have all homogeneous transformation matrices as i already implemented the kinematics for this robot. due to it's new structure there are no discrete joint variables anymore but rather continuous parameters. therefore i want to compute the jacobian numerically.\nit'd be awesome if someone could provide a detailed way how to compute the numerical jacobian for a 6-dof rigid-link robot (only rotational joints => rrrrrr). from that i can transfer it to the continuum robot.\n\ni've already started computing it. let t be the homogeneous transformation matrix for the endeffector (tip)  with\n\n$$t=\\begin{bmatrix}r &amp; r \\\\ 0 &amp; 1 \\end{bmatrix} $$\n\nwith r = rotational matrix (contains orientation) and $ r = \\begin{bmatrix} x &amp; y &amp; z \\end{bmatrix}^t$ endeffector position.\nmy approach is to compute the first three rows of j by successively increasing the joints, computing the difference to the \"original\" joint values and dividing it by the increment delta, the joint-space is $ q = \\begin{bmatrix} q_1 &amp; q_2 &amp;  q_3 &amp; ... &amp;q_6 \\end{bmatrix}t $\n\n$q_1 = q_1 + \\delta$ => $j(1,1) = (x_{increment} - x_{orig})/\\delta$ \n\n$q_2 = q_2 + \\delta$ => $j(1,2) = (x_{increment} - x_{orig})/\\delta$   \n\nand so on. i do the same for the y and z coordinates. so i get the first 3 rows of j. \n\nnow i don't know how to compute the last three rows as they refer to the rotational matrix r. since it's a 3x3 matrix and no scalar value i don't know how to handle it. \n", "tags": "jacobian", "id": "7886", "title": "how to numerically calculate the jacobian?"}, {"body": "i was wondering whether maybe you could help me with this problem. i have a double pendulum. i have set the origin of cartesian coordinates to be the \"head\" of the first arm, which is fixed. the end of the second arm is attached to a block that slides along the x-axis. what i want to do is to derive the equations relating the pendulum's angles with the distance from the origin to the block. \n\nnow, i know how i could go about deriving the equations without the constraint. \n\n$$x_1 = l_1cos(a_1)$$\n$$y_1 = l_1sin(a_1)$$\n\nwhere $x_1$ and $y_1$ is where the first arm joins the second arm and $a_1$ is the angle between the horizontal and the first arm. \n\nsimilarly, i can derive the equations for the end of the second arm \n$x_2 = x_1 + l_2 cos(a_2)$ and $y_2 = y_1 - l_2 sin(a_2)$\n\nnow then, if i attach a sliding block to the end of my second arm, i don't know whether my equation for $x_2$ would change at all. i don't think it would but  would i have to somehow restrict the swing angles so that the block only moves along the x direction? \n\nwell, basically the problem is finding the equation of $x_2$ if it's attached to a block that only moves along the x- direction. \n", "tags": "forward-kinematics", "id": "7889", "title": "forward kinematics of constrained double pendulum"}, {"body": "so, i need to know a couple of things about soldering. my primary workspace, robotics and otherwise is a desk with a computer and only a little bit of free space (4ft. by 6 in.). i am wondering if it is safe to solder in such a small area. also, what level of ventilation do i need to solder safely? my desk is in a normal house room and my desk is write next to an air vent. my house has heating and a/c? do i need a fan or a fume sucker thing? i plan to only solder a little to get things to stay in my solder less bread board (soldering header pins onto wires and such). so, basically, what are the minimum requirements for soldering safely (space and ventilation). also, if anyone could point me to some hobby/beginner level soldering must-haves on amazon that would be great, thanks.\n", "tags": "beginner", "id": "7894", "title": "beginner soldering question"}, {"body": "i've notice the irobot create 2 does not respond to the app's commands when it has been sleeping.  if i press the clean button and re-run the app then the robot is responsive to the commands.\n\nmy initialization sequence (android/java) using usb-serial-for-android:\n\n\n\nthe physical architecture is irobot create 2 connected by irobot serial cable to google project tango tablet.\n\nhow can my app wake up the roomba from it's sleep?\n", "tags": "irobot-create", "id": "7895", "title": "irobot create 2: powering up after sleep"}, {"body": "i am using 8051 microcontroller and a dc motor.what to do if i have to rotate the motor at any fixed rpm. let's say 120rpm.\n\nand if it is possible by generating pwm,how to do the calculations for the relation between duty cycle and rpm?\n", "tags": "control motor microcontroller", "id": "7896", "title": "how to rotate a dc motor at a fixed rpm"}, {"body": "i want to create a rotating control mechanism that can turn a surface to face any direction in a sphere. my dad (an electrical engineer) said i can probably do it by connecting two servo motors together. \n\ni am looking for a servo motor that can do what i want to do, which is moving the sphere with decent precision (within ~1 degree) but i don't know which kinds of motors are able to handle such precision. \n\nanother challenge is that one servo will have to hold the second servo on top. as i understand it, the torque rating determines the maximum amount of force the servo can exert on its load so i can figure out if the servo is strong enough through some math?\n", "tags": "control servos", "id": "7903", "title": "how can i tell if a servo motor is capable of being controlled degree by degree?"}, {"body": "i'm a software developer not experienced in ai or machine learning, but i'm now interested in developing this kind of software. i want to develop software that recognizes some specific objects, specifically, animals from a video stream (or a sequence of static images).\n\ni saw there's a library called opencv which is often commented in this forum, but what i saw so far is this library is a helper for working with images, i didn't find the object recognition or self learning part.\n\nis opencv a good starting point? better go for some theory first? or there are other already developed libraries or frameworks aimed for object recognition?\n\nedit\nto give some context: i will have ona camera checking a landscape, mostly static but some leaves may move with the wind or some person may step in, and i want to get an alert when some animal is into view, i can reduce the \"animals\" to only birds (not always i will have a nice bird/sky contrast).\n\ni did some work with supervised neural networks some 15 years ago and studied some ai and machine learning theory, but i guess things have improved way too much since then, that's why i was asking for some more practical first steps.\n\nthank you\n", "tags": "computer-vision", "id": "7910", "title": "how can i recognize animals in a video stream or static images with opencv or other library/software?"}, {"body": "project tango development kits come with a mini-dock (see picture below).  \n\ni am controlling the irobot create 2 by the mounted tablet using the usb cable provided plugged into the mini-dock. (see docs).\n\n\n  the usb 3.0 port on the mini-dock is only functional when the tablet is docked. the port can be used to attach an external memory drive or standard peripherals to the tablet.\n\n\ni wish to recharge the tablet using the power from the irobot. the mini dock comes with a port for external charging:\n\n\n  the mini-dock accepts a power adapter for faster charging (not provided). the power adapter output must be 12v, 2a, and the connector must be a barrel plug with 5.5mm outer diameter, 2.1mm inner diameter, center positive.\n\n\nideally the charging would happen only when the irobot is also charging, but charging all the time is acceptable. \n\nis this possible?  if so, how?\n\n\n", "tags": "irobot-create", "id": "7912", "title": "powering a project tango tablet with irobot create 2"}, {"body": "i have a dual (sequential) loop control system controlling the angle of a rotational joint on a robot using an absolute encoder. i have tuned the inner control loop (for the motor) and am now working on tuning the outer loop (for the joint).\n\nexample of a dual loop controller\n\n\nwhen i disturb the system the response isn't what i would expect.\n\nkp = 0.4\n\n\nkp = 0.1 kd = 0.001\n\n\ni didn't add a ki term because i don't have any steady state error.\n\ni'm confused by the fact that the second overshoot in the first plot is larger than the first one. no matter how i adjust the parameters i can't seem to get rid of the oscillation in the velocity of the joint (seen in the second plot). one limitation i have is if i increase both kp and kd too high the gearbox of the becomes very noisy because the noise in the encoder signal creates larger adjustments in the position of the motor. i'm working on adding a filter to the output using the method described here.\n\nthe code i'm using for the outer loop is:\n\n\n\ni'm beginning to think that the system might not be able to be modeled by a first order equation, but would this change the implementation of the control loop at all? any advice is appreciated!\n\nben\n", "tags": "pid", "id": "7913", "title": "non linear control system?"}, {"body": "like the title says.. will it work? i know about the due 3.3 volt limitations.\n\ni want to build a hexapod with 18 servo's.\n\nthe shield i am looking at:\n\nhttp://yourduino.com/sunshop2/index.php?l=product_detail&amp;p=195\n\nif it isn't compatible. is there an alternative shield which will work? i can't seem to find much for the due.\n", "tags": "arduino", "id": "7915", "title": "arduino mega shield v2.0 compatibility with arduino due"}, {"body": "the transmission of telemetry data between the ground base station and apm 2.x (arducopter), using xbee, is not well documented. the only documentation is telemetry-xbee, but it does not specify what xbee version is used. i have been checking and i guess is version 1 (this one has p2p link and the others not), but i am not sure.\n\ni would like to know, what xbee modules people use for flying drones? do they have problems with the apm connection? how can i control the drone remotely using the xbee link with mavlink protocol?\n", "tags": "quadcopter radio-control mavlink", "id": "7919", "title": "telemetry with apm 2.6 and xbee"}, {"body": "my 6 joint robot arm structure doesn't meet the requirements for a closed form solution (no 3 consecutive axes intersecting at a point or 3 parallel axes...).  \n\nwhat would be best method to adopt to get solution in 1ms or less? estimation accuracy of 1mm. i'm assuming the computation is done on an average laptop intel core i3, 1.7ghz, 4gb ram\n", "tags": "inverse-kinematics", "id": "7920", "title": "inverse kinematics solution for 6dof serial arm"}, {"body": "is it possible to localize a robot without any sensors, odometer and servo motors?\n\nassume robot has dc motors and no obstacles.\n", "tags": "localization mapping", "id": "7921", "title": "robot localization without any sensors"}, {"body": "i would like to locate the position of a stationary autonomous robot in x-y-z axis relative to a fixed starting point.\n\ncould someone suggest sensors that would be suitable for this application?\n\ni am hoping to move the robot in 3d space and be able to locate it's position wirelessly. the rate of position update is not important as i would like to stop the robot from moving and relay the information wirelessly.\n\nthe range i am looking for is roughly 2 km + (the more the better) with accuracy of  +/- 1 cm.\n\nis there any system that could do this? thanks for your help.\n", "tags": "sensors imu", "id": "7925", "title": "positioning sensor"}, {"body": "my goal is to move robot in certain points as shown in the figure. it's initial position is (x0,y0) and move along other coordinates.\n\n\n\ni am able to track robot position using a camera which is connected to pc and camera is located at the top of the arena. i've mounted a ir beacon on the robot, camera find this beacon and locates it's coordinate(in cm) in the arena. using this coordinate how can i move my robot to another position, say new position (x1,y1)\n\nmy robot has arduino mega 2560 with two dc motors, communication between pc and robot is done using bluetooth\n\nupdate:\n\nthanks @chuck for the answer, however i still have few doubts regarding turning angle.\n\nmy robot position setup is as shown in the image.\n\n(xc, yc) is the current position and (xt, yt) is the target position.\n\n\nif i want to align robot in the direction of the target coordinates, i've to calculate atan2 between target and current coordinates. but the angle remains same since it's current position is not changing with respect to the target point. so i assume robot simply makes 360' rotation at current position?\n\nupdate:\n\nthe path points is as show below in the image, is my initial heading angle assumption is correct? \n\n'1' is the starting point.\n\n\n\nupdate\n\nthank you for your patience and time, i'm still struck at turning, my code goes like this\n\n\n\nsince angle is always -90' robot only makes right turn in loop at current point, since angle is not changing. i think i'm missing something here.\n", "tags": "arduino navigation", "id": "7928", "title": "robot path planning"}, {"body": "can i charge a lipo nano tech battery over imax b3 charger. 2650mah 35/70c 3s is the battery\n", "tags": "battery lithium-polymer", "id": "7930", "title": "can i charge a lipo nano tech battery over imax b3 charger"}, {"body": "let's say i would like to use an ekf to track the position of a moving robot. the ekf would not only estimate the position itself but also variables affecting the position estimate, for example imu biases, wheel radius, wheel slip and so on.\n\nmy question is, is it better to use one big ekf (state vector containing all estimated variables) or multiple smaller ekfs (each one responsible for tracking a subset of all variables to be estimated)? or is there no difference?\n\nas for the example above, the ekf could be split into one for tracking position, one for estimating wheel radius and slip and one for estimating imu biases. the position ekf would of course use the estimations output from the other concurrent ekfs and vice versa.\n\nto me it seems it would be easier to tune and test multiple smaller ekfs rather than just one big. are there any other advantages/disadvantages (execution time, ease of debugging etc.) assuming the resulting estimates are equal in the two approaches (or close enough at least)?\n\nthanks,\nmichael\n", "tags": "kalman-filter ekf", "id": "7937", "title": "multiple ekfs or one big"}, {"body": "i am new in this field, i am looking for some high precision gyroscopes and accelerometers for attitude measurements.the precision requirement is around 0.2~0.5 deg/s dynamic.\n    i have done some digging myself, not a single integrated mems sensor can do that without costing too much. so some heavy math is needed but that's fine.i need to make sure the prefect sensors are chosen, the budget is less than 100usd.\n    can any one help, thanks in advanced.\n", "tags": "sensors research", "id": "7940", "title": "recommendation for really high precision attitude measurement sensors"}, {"body": "i have a mobile robot and i would like it to follow the walls of a room.\n\ni have:\n\n\na map of the room. \nwheel encoders for the odometry.\na kalman filter for fusing data from wheel encoders and imu. \na hokuyo lidar for localization and obstacle avoidance\na kinect to see obstacles which can not be seen by the hokuyo. \namcl for localization.\na couple of sharp sensors on the side for wall following. \n\n\ni am not planning to use the global or local costmap because the localization of the robot is not perfect and the robot might think that it is closer (or further away) to the wall than it actually is and therefore, wall following might fail. so, i am planning to just use the data from hokuyo lidar and sharp sensors to do wall following and maintain constant distance from the wall (say 10 cm). \n\nnow, i would like to know what is the best technique for doing wall following in this manner? also, how can one deal with the issue of open gaps in the wall (like open doors, etc..) while doing wall following using the above approach?\n\ni know this is a very general question but any suggestions regarding it will be appreciated. please let me know if you need more information from me.\n\nupdate:\ni am just trying to do wall following in a given room (i have the vertices of the room in a global reference frame) for example, lets say i have a map of a room (shown below). i want to make the robot follow the wall very closely (say 10 cm from the wall). also, if there is an open space (on bottom left), the robot should not go in the adjacent room but should keep on doing wall following in the given room (for this, i have the boundary limits of the room which i can use to make sure the robot is within the given room).\nthe approach which i am thinking is to come up with an initial global path (set of points close to the wall) for wall following and then make sure robot goes from one point to the next making sure that it always maintains a certain distance from the wall. if there is no wall, then the robot can just follow the global path (assuming localization is good). i am not sure about its implementation complexity and whether there is a better algorithm/ approach to do something like this.\n\n\n", "tags": "sensors localization navigation", "id": "7942", "title": "wall following using hokuyo lidar and sharp ir sensors"}, {"body": "i'm building my first quadcopter, and these are the components i intend to buy:\n\n\nmotor: emax bl2212 1400 kv brushless outrunner motor around 0.9 kg thrust: \nflight controller: multiwii v2.5 flight controller \npropellers: i don't know which one to get: fut-electronics propellers collection  \ngps: skylab uart gps module skm58   (small form factor)\nradio communication: radio telemetry 915 mhz (3dr), is there an affordable alternative to buying a radio telemetry maybe using wi-fi?  \nescs: 4x1 esc (4x25a) - speed controller for quadcopter \nbattery: i don't know which one to choose\n\n\nmy questions are:\n\n\nare the components compatible?\nwhat battery to choose?\nif i'm not planning to do gps planned missions, would the gps be important for anything else?\n\n\nby the way i intend to attach a camera or a smart-phone to it for video capturing i think it is about an extra 200 grams.\n", "tags": "quadcopter multi-rotor uav", "id": "7944", "title": "compatibilty of my setup?"}, {"body": "why are 'cell decomposition' methods in motion planning given the name, \"combinatorial\" motion planning?\n", "tags": "motion-planning", "id": "7945", "title": "why the name \"combinatorial\"?"}, {"body": "using an imu (gyro, accelerometer and magnetometer), as found in most smartphones, can i detect the differences between tilting the device, say forward, along different (parallel) axis positions? \n\nto clarify, if the axis of rotation is far from the sensor, the the motion contains a translational component.\n\ncan the distance and position of this axis be extracted from the imu data and if so how?\nis there some data fusion algorithm that can do all this?\n", "tags": "imu accelerometer gyroscope magnetometer", "id": "7950", "title": "axis of rotation via imu"}, {"body": "i have a rig for which i have a pretty good estimate of the static transformation between the camera and a joint based off of the cad. it has some errors though and i was hoping to fix it by doing a hand eye calibration. so, i started off with generating some data based off of the transformation that i have already. from the papers that i have been reading, they all want to solve the $$ax = xb$$ problem by either converting $a$, $b$ to dual quaternions or simplifying the equation to something like \n$$ n_a = xn_b $$ where $n_a$, $n_b$ are the eigenvectors corresponding to the eigenvalue of 1 for the $a$ and $b$ rotations.\n\nafter generating the data, i tested if my data collection was correct and i validated it by checking if $ax = xb$ for all of the $a$s and $b$s that i generated. i used the camodocal library to try and solve the problem but i got this -\n\n\n\nthe actual transform is the one that i had based my $a$ and $b$ data on. then i tried implementing the tsai-lenz and horaud and dornaika's nonlinear optimization techniques using lm solver but to no avail. i do not get the correct transformation out of any of the solvers.\n\nso, i was wondering if you could point me to a hand eye calibration library or paper that has worked.\n", "tags": "kinematics calibration", "id": "7953", "title": "hand eye calibration solver"}, {"body": "i wanted to know if there is any sort of archive of mechanisms that contains a brief description of mechanisms like there type of motion and forces involved. not lengthy derivations and other stuff. \n", "tags": "mobile-robot mechanism", "id": "7954", "title": "book on mechanisms"}, {"body": "i am trying to make line following robot. i am using atmega328p mcu, pololu 10:1 motors, pololu qtr6-rc sensor, 2s li-po. here is my code:\n\n\n\nand here is my qtr library:\n\n\n\ni am trying to find kp constant but when it's 7 then my robot just turns off the line always on the same spot. when kp is 8 then it follows staright line but wobbles a lot and can't take corners. i also tried to increase kd 10 to 20 times when my kp was 8 but it didn't change much. how can i get it working?\n\nhere is my robot and the track i want to follow.\n\n\n\n\n", "tags": "pid line-following avr tuning", "id": "7959", "title": "tuning pd for line follower"}, {"body": "i am a student of be taking mega-quadcopter as my final year project.can u please help me with the total hand calculations of the mega-copter i.e its procedure and formulaes? . i wanted to know how to calculate the dimensions of frame,specifications of motor and propeller,the rating of esc's and the power rating of the batteries and its total no.s.i do not want direct answers but its procedure and formulaes.i want to lift aload of around 20-30 kgs .please feel free to help.\n", "tags": "quadcopter", "id": "7964", "title": "total hand calculations procedure & formulaes of mega-quadcopter"}, {"body": "i recently discovered this ros-package: http://wiki.ros.org/laser_ortho_projector .\nwhich is basically exactly what i need. however i am not using ros, so i need to do what is been done in this package myself.\n\nbasically the information i have is the range measurement r and the angle theta for every measurement point of a 360 degree laserscan + i have the orientation in roll, pitch, yaw angles of the laserscanner. however yaw is not important for me and could be ignored.\n\ni really can't get my head around how to project those points to the ground plane. i mean it is easy for the measurement point which align with the roll and pitch axes, but i don't know what to do with the points in between :d\n\none solution i thought of is this:\n\n\nconvert the measurement point (r, theta) in cartesian coordinates (x,y,z) - vector\nuse rotations matrices: create rotation matrix for rotation around roll axis with roll angle, and adequately for the pitch axis. multiplay bot matrices and then multiply it with (x,y,z) - vector.\nnow the orthogonal projection of the of the measurement would be the (x,y,z) - vector with z=0.\nconvert (x,y) - vector back to polar coordinates (r, theta).\n\n\nhowever, especially step 2 is very complicated, because the rotation matrices change according to the sign of the roll and pitch angles, right?\n\ni would like to note that the absolute value of role and pitch angles will always be &lt; 90\u00b0, so there should not be an unambiguity with rotations..\n\nis there an easier (or maybe more elegant) way to solve my problem?\n\nmy guess is, that this problem must have been solved basically for every robot application which uses a 2d-laserscanner that is not fixed to one axis. \nbut i can not find the solution anywhere.\n\nso i would be very glad if anyone of you could point me in the right direction.\n\nkind regards\n", "tags": "quadcopter slam", "id": "7966", "title": "orthogonal projection of laserscanner data"}, {"body": "i have built quadcopter but the problem is of balancing. it doesnt goes up. i am using pid techniqe for balancing. but i am not finding the suitable values for pid tuning. i am using mpu6050 as a sensor. i get the accelerometer values of x and y axis and find the error from them. that is lets say if accel on x is not zero then it error cause it should be zero if balanced. i am using +-2g sensitivity scale of accelerometer. the motors i am using are dji 920 kva. what values for kp, ki and kd should i set. i cant set them while in flight cause it completely out of balance.\n\n\n\nthis is the design. completely home made. i have modified it a little after this photo. accelerometer is at 2g so at balance z will be 32768/2 .\n\n\n\nthere are also few more questions, should i scale error or pid output, because error is from ranging from 0 to 16380 at 2g setting, so i am scaling it from 0 to 42. so should i divide error or pid by some value?\n", "tags": "quadcopter pid balance", "id": "7968", "title": "what pid values should i keep"}, {"body": "i need a basic erector set that the parts will fit with servo motors and dc motors. preferably below $100. i've looked at minds-i basic set and it looks good except i don't know if it will function with my servos without hot glue or extensive modifications. \n\nif it matters, i am making a bipedal robot so i don't require any wheels or anything pre-built. i just need a basic set that i can add on to to build a whole bunch of different robots. \n", "tags": "mobile-robot rcservo", "id": "7969", "title": "what erector sets will function with normal servo motors?"}, {"body": "is there a firmware upgrade for available for the create 2? i had some issues in march when using these for assigning a university of tennessee  programming project. we are getting ready to use them again (we have 10 now) and i'd like to get them all updated to the latest firmware.\n", "tags": "irobot-create", "id": "7970", "title": "firmware upgrade for irobot create 2"}, {"body": "i'm not sure if this is the right place to post this but here goes.\n\nso, as the title states, i'm planning on building a desk that doubles as an air hockey table which has a robot on the other side.\n\nthe robot would be mounted on a rail which should be able to go left and right using a linear actuator. it should be able to \"attack\" the puck using two servos.\n\nthe real problem is how should i detect the puck's location?\n\nmy idea:\n\nsince the table would have tiny holes in the corners of a every square(0.5inx0.5in), i could fit in a laser on the bottom part of the table, a laser for ever 1in so a 1inx1in square, the same location would be reflected on the \"ceiling\" of the table but instead of laser diodes, they would be replaced by an ldr. \n\nso i'm planning on doing a matrix and reading the signals of the ldr's columns and rows then performing some logic to locate the center of the puck.\n\nproblems:\n\nwhile i don't see any performance flaws in my plan, i see tons of flaws when done imperfectly even to the tiniest bit.\n\n\ni have to be exactly accurate regarding the laser diode's position,\nit has to be on the center of the holes, right below the z-axis.\nthis should be easy if i'm just going to place 4 or 5. but i'm not.\naccording to my estimations, i'm going to have to use 300-700 laser\ndiodes, depending on if i'm planning on putting the lasers only on\nthe opponent's side or on the entire board. it would definitely be\ncostly. imagine 300...\nthis isn't really a huge problem, more like a hassle. wiring 300 of\nthese. forget the pcbs, the project area is just to large.\n\n\ni have thought of numerous way to lessen these, like using a color sensor to get the x-axis location and a laser situated on a negative x-axis pointing to the positive x-axis to locate the puck's y location, but i'm still comparing ideas.\n\nadvantages:\n\ni could get a 3d-like graphical representation with 3d-like controls (3d-like in reality but technically 2d since the lasers are only plotted in the x and y axis though facing the z-axis). \n\nsince this project is going to be my room desk, situated in an automated room, i was thinking of making \"desk modes\" which should toggle between a game that takes advantage of the lasers and their controls, a control desk for my room, ordinary desk mode, and an air hockey mode.\n\nmy question: (more like a request)\n\ndoes anyone have another idea regarding how i should be able to locate the puck's x and y location accurately in real time?\n\nedit: the table is roll-able and stored underneath a loft bed which has an under-area height of 5'4\". which means i can't go grande on the a vertical solution.\n\nedit #2: thanks to the helpful people here, i have come to the conclusion of using a camera.\n\nthe camera will be that of a smartphone's, i'll create an app that tracks an object by color and a has fixed size comparison to identify the distance of the robot from the puck. the phone will then process this and send signals via bluetooth.\n\nthe phone is anchored at the end of the robot's moving part so the camera is reminiscent of those games with a first-person view.\n\nincoming problems: i'm looking forward to some delay, given the delay in processing.\n", "tags": "sensors microcontroller design electronics laser", "id": "7972", "title": "air hockey with a robot as an opponent"}, {"body": "i'm building a robotic tea-maker/watchdog robot and have a power problem. i would like to be able to have the robot approach a socket and insert the power cord of a cheap immersion heater (120v, 300w, see links below) to turn the heater on. however, the power and precision required to plug it into the wall is beyond the capabilities of my stepper motors/arduino. \n\nmy solution was a magnetic breakaway power cord like the charger on a mac but at higher voltage. deep fat fryers have suitable ones (120v, high power, see links below). however, the problem is i need both sides of the connector, and i can only find the magnetic breakaway power cord, not the opposite side, which would normally be built into the deep fat fryer. i don't fancy buying a whole fryer just to get one little part...\n\nany ideas? alternatives to a breakaway cord? anyone know of any (cheap) 120v induction chargers? i'll resort to a mechanical on/off switch and just leave the robot plugged in if i have to, but i was hoping for something a bit sleeker.\n\nlinks:\n\n\nimmersion heater\nfryer cord \n\n", "tags": "untagged", "id": "7983", "title": "magnetic, low insertion force connector"}, {"body": "i would like to know how to calculate the distance to each car when i run my application for an autonomous vehicle in real time. in addition i want to know how implement the calculation in c++. \n\nyou can see in the images we can know the distance for each vehicle but i don't know what code i should use to make all these calculations for every vehicle .\n\nplease check the photo to understand more about what i'm trying to achieve.\n\n\n\n\n", "tags": "control sensors localization ros cameras", "id": "7992", "title": "how to calculate vehicle detection distance"}, {"body": "i have two series 1 xbees that won't be in transparent mode because they are in at command mode when i'm not in x-ctu.  i had asked for help elsewhere and no one had the answer except telling me about flow control.\n\nthe xbees had been configured properly with the my and dl settings.  i'm thinking maybe i should shorten the timeout so they supposedly get out of at command mode but they both stay in at command mode.  the only time i can get the two series 1 xbees to talk is under x-ctu.  i need the two series 1 xbees to automatically be in transparent mode when powered on.  \n", "tags": "wireless", "id": "7993", "title": "why can two series 1 xbees only talk in x-ctu?"}, {"body": "having a camera mounted on my robot and looking upwards, i want to estimate the distance of the ceiling as the robot moves and also the position of landmarks observed on the ceiling (lamps for example).\ni know this is a structure from motion problem but i was very confused on how to implement it. this case is a much simpler case than bundle adjustment as the intrinsic calibration of the camera is existing, the camera pose changes just in x and y directions, and the observed thing is a planar ceiling. odometry might also be available but i would like to start solving it without. do you know any libraries that offer a good and simple api to do such a thing? preferably based on levenberg-marquardt or similar optimization algorithms taking in more than just two observations. (python bindings would be nice to have)\n", "tags": "cameras 3d-reconstruction", "id": "7995", "title": "ceiling depth with a monocular camera"}, {"body": "i've been looking at parts for a beginners robotics kit (i teach at a museum) and have been wondering about servos.\n\nyou can buy continuous servos with relative position encoders. but i can't find continuous rotation servos with absolute position encoders. do these exist? if not, why not?\n\ni understand that some forums don't like shopping questions, but i suspect that this part doesn't exist and i'd like to understand why.\n\nalso, i understand that most servos use a potentiometer as a position encoder and that these don't turn more than 1 rotation, but there are other types of encoders that seem like they would do the job.\n\nthanks for the help!\n", "tags": "servos quadrature-encoder", "id": "7997", "title": "why can't you buy continuous servos with absolute positioning?"}, {"body": "this is a simple question that i can't seem to find the answer for but when setting up the weave function how exactly does frequency (hz) determine how fast it moves back and forth? \n\nin other words if i raise frequency will it move quicker or slower and what factors must i consider? \n", "tags": "robotic-arm industrial-robot", "id": "7998", "title": "weave weld lincoln electric mig robot"}, {"body": "is there any way to add a reset button to the create2 that would be the equivalent  of temporarily disconnecting the battery? \n", "tags": "irobot-create", "id": "8001", "title": "add hardware reset button for create2"}, {"body": "i am on the project quadcopter. so i have to use pid for stabalizing it. i think i am going wrong because i am adding the pid output to motors thrust. while the motors thrust means to be its acceleration. the reason of my previous statment is that when the quad is static in air(not goin up nor below), that time the thrust is enough to cancel gravity, means thrust is negative gravity, that is acceleration. so if i add pid output to thrust that is acceleration of motors, it will be wrong. i have to add pid to speed of motors, which is not visible. my quad is not stabalizing the reason i see is this, that i am adding pid to acc, while it should be added to speed(virtually). what should i do. should i derivate the pid output and add to thrust? https://mbasic.facebook.com/photo.php?fbid=1545278952394916&amp;id=100007384772233&amp;set=a.1447457675510378.1073741830.100007384772233&amp;refid=17&amp;ft=top_level_post_id.1545278952394916%3athid.100007384772233%3a306061129499414%3a69%3a0%3a1443682799%3a-1394728329505289925&amp;tn=e\n\nhttps://mbasic.facebook.com/photo.php?fbid=1545281645727980&amp;id=100007384772233&amp;set=a.1447457675510378.1073741830.100007384772233&amp;refid=17&amp;tn=e\n\nthis is the drawing of my circuit. i am giving the current from one esc to whole of the circuit. other esc's has only pwm wire connected to circuit.\n", "tags": "quadcopter pid", "id": "8006", "title": "quadcopter accelerating or not"}, {"body": "i was trying to implement the ibvs algorithm (the one explained in the introduction here) in matlab myself, but i am facing the following problem : the algorithm seems to work only for the cases that the camera does not have to change its orientation in respect to the world frame.for example, if i just try to make one vertex of the initial (almost) square go closer to its opposite vertex, the algorithm does not work, as can be seen in the following image\n\n\n\nthe red x are the desired projections, the blue circles are the initial ones and the green ones are the ones i get from my algorithm.\n\nalso the errors are not exponentially dereasing as they should.\n\n\n\nwhat am i doing wrong? i am attaching my matlab code which is fully runable. if anyone could take a look, i would be really grateful. i took out the code that was performing the plotting. i hope it is more readable now. visual servoing has to be performed with at least 4 target points, because else the problem has no unique solution. if you are willing to help, i would suggest you take a look at the  function to check that the rotation matrix is properly calculated, then verify that the line  in  is correct. the camera orientation is expressed in euler angles according to this convention. finally, one could check if the interaction matrix  is properly calculated.\n\n\n\ncases that work:\n\n\n\ncases that do not work:\nrotation by 90 degrees and zoom out (zoom out alone works, but i am doing it here for better visualization)\n\n\n\n\n", "tags": "control algorithm matlab visual-servoing", "id": "8008", "title": "image based visual servoing algorithm in matlab"}, {"body": "is there a way i can control my arduino robot from anywhere in the world. the robot goes out of range of my home wifi so my wifi shield can't help. is there a way to make sure the robot is always on the internet no matter where it goes? \n", "tags": "arduino mobile-robot raspberry-pi", "id": "8014", "title": "arduino mobile robot"}, {"body": "i'm looking for a complete tutorial textbook for how to build and control a quadrotor (dynamics, control, etc.).\n\ni'm an engineer with a broad background in programming, mechanics, and control but it's been several years and i'm rusty. i was just wondering if anyone knew of a great \"from the ground up\" tutorial for quadrotors? i found this book which looks interesting but thought i'd ask here too.\n\nthanks!\n\nedit\n\nso, assume i've taken a formal course on all necessary topics: system modeling, mechanics, control theory, state estimation, programming, etc.\n\ni'm looking for a book that assumes the reader is familiar with the topics but also goes step-by-step. for example, instead of just stating \"here are the system equations\" i'm looking for \"let's derive the system equations\" (but assumes you are familiar with modeling/kinematics). i'd like to start a quadcopter as a side project but have precious spare time so i'd prefer a single good reference instead of jumping from each individual topic textbook; maybe i'm just being greedy :)\n", "tags": "quadcopter", "id": "8018", "title": "complete quadrotor tutorial (text)book?"}, {"body": "which method is better, in term of accuracy, for detection of indoor localization of a drone. camera based system or wireless techniques like wlan or bluetooth?\n", "tags": "slam", "id": "8019", "title": "indoor positioning system: which is better?"}, {"body": "i want to control the attitude(roll, pitch, yaw) of a vehicle capable of pitching and rolling. to do this i have created a quaternion pid controller. first i take the current attitude of the vehicle converting it to a quaternion qc and do the same for the desired attitude with the quaternion qd. i then calculate the input of my pid controller as qr = qc' x qd. the imaginary parts of the quaternions are then fed as force requests on the roll, pitch, yaw axes of the vehicle. i test on a simulator and the control works but becomes unstable in some cases (request for r: 60 p: 60 y:60). i also want this to work around singularities (i.e. pitch 90)\n\ndoes anyone know why i get this behavior and if so explain (thoroughly) what i'm doing wrong?\n", "tags": "control pid stability", "id": "8024", "title": "pid quaternion contoller"}, {"body": "i'm implementing an extended kalman filter and i'm facing a problem with showing the covariances to the user.\n\nthe covariance matrix estimate contains all the information we have about the current value estimate, but that is too much to display.\ni would like to have a single number that says \"our estimate is really good\" when close to 0 and \"our estimate is not worth much\" when large.\n\nmy intuitive simple solution would be to average all the values in the covariance estimate matrix (or maybe just the diagonal), except that in my case the values have different units and different ranges.\n\nis it possible to do something like this?\n", "tags": "kalman-filter", "id": "8027", "title": "how to sumarize kalman filter covariances for display?"}, {"body": "what subjects are involved in robotics. if i want to build robots then what necessary things i need to learn consecutively as a beginner.\n", "tags": "artificial-intelligence embedded-systems first-robotics", "id": "8028", "title": "what i need to learn to build robots"}, {"body": "as far as i know, a robot sends orders as discrete signals. however, isn't computer simulation based on continuous simulation? do you know if it may happen any important difference when comparing reality to simulation in some cases? i heard that cable-driven robots were quite sensitive.\n", "tags": "control simulation", "id": "8033", "title": "continuous vs discrete simulation in robotics"}, {"body": "i have recently built a raspberry pi based quadcopter that communicates with my tablet over wifi. the problem is that it drifts a lot. at first i thought that the problem was vibration, so i mounted the mpu-6050 more securely to the frame. that seemed to help a bit, but it still drifts. i have tried tuning the pid, tuning the complementary filter, and installing a real time os. nothing seems to help very much. below is my code written completely in java. any suggestions are appreciated.\n\nquadserver.java:\n\n\n\nsensor.java:\n\n\n", "tags": "pid raspberry-pi quadcopter", "id": "8038", "title": "raspberry pi quadcopter drifts like crazy"}, {"body": "i currently have a description of my 22 joint robot in \"classic\" dh parameters.  however, i would like the \"modified\" parameters.  is this conversion as simple as shifting the $a$ and $alpha$ columns of the parameter table by one row?\n\nas you can imagine, 22 joints is a lot, so i'd rather not re-derive all the parameters if i don't have to.  (actually, the classic parameters are pulled out of openrave with the command: .  \n", "tags": "kinematics dh-parameters", "id": "8043", "title": "how to convert between classic and modified dh parameters?"}, {"body": "i am taking part in a robotics competition, where the challenge is to create a pair of robots which successfully navigate a series of obstacles. however, the rules state that of the two robots, only one must have a driving actuator. the other must somehow be moved by the other robot, without physical contact. \n\ni could think of either having sails on the non-driving robot, and moving it with fans on the driving one or electromangnets on the driving one and permanent magnets with the opposite polarity on the non-driving one. however the problem with both is that efficiency falls off drastically with distance. thus, i am looking for possible ways to overcome this problem. thanks :)\n\nalso, the driving robot has a cable power supply, while the non-driving one may only have batteries.\n\nrulebook: http://ultimatist.com/video/rulebook2016_final_website_1_sep_15.zip\n", "tags": "force", "id": "8044", "title": "what are the best ways to transmit force through air efficiently?"}, {"body": "i am the moment learning about rotation matrices.  it seems confusing how it could be that  $r_a^c=r_a^br_b^c$ is the rotation from coordinate frame a to c c to a, and a,b,c are different coordinate frames.\n\n$r_a^c$ must for a 2x2 matrix be defined as \n$$\nr_a^c=\n\\left(\n\\begin{matrix}\nxa\u22c5xb  &amp;  xa\u22c5xb \\\\\nya\u22c5yb &amp;  ya\u22c5yb\n\\end{matrix}\n\\right)\n$$\n\n$x_a, y_a and x_b,y_b$ are coordinates for points given in different coordinate frame.\ni don't see how, using this standard, the multiplication stated above will give the same matrix as for $r_a^c$. some form for clarification would be helpful here.\n", "tags": "frame", "id": "8045", "title": "composition of rotation matrices"}, {"body": "\n\nthe above is my program for my quadcopter, but now i have to tune the pid values, that is kp, ki and kd. my accelome is at 2g. please point to me what is wrong with the program, is the error signal not appropriate? please also give me or help me choose correct pid tuning. my limitation is i always have to connect my arduino to pc and change kp ki or kd values, that is i have no remote control available currently.\n", "tags": "quadcopter pid", "id": "8047", "title": "what kp,ki,kd should i keep"}, {"body": "i don's seem to be able to get any battery power from create 2. i spliced the original cable it came with, and tried to use the power from red/purple(+) and yellow/orange(-) to power a raspberry pi2, with no luck. while the serial-to-usb cable still works, and i am able to command the robot via python, there seems to be no power coming on the red/purple cables. i tried with a multimeter with no luck, even as i moved the device from passive/safe/full modes. there is no power even when create 2 is charging/docked.\n", "tags": "raspberry-pi irobot-create serial roomba", "id": "8050", "title": "irobot create 2 serial battery power"}, {"body": "what is the maximum rotational velocity of miniature ball-screw (diameters up to 12mm) for approximately 1000 thrust cycles, and which type/brand would that be, if the speed is limited by the ball return mechanism? the fastest i could find was 4000 rpm at 3000 n thrust, but this was from a datasheet with a big safety margin (millions of cycles).\n\ni'm looking for either experience and data, or a general method/formula that can be used to find the maximum velocity (and load) as function of cycles or the other way round (similar to those of ball bearings). suggestions and knowledge about faster types and brands of ballscrews than the ones i have been able to find is welcome as well.\n\nsome more background information:\nball screws are very interesting transmissions for electrically actuated legged robotics, since they provide a high-geared rotary-to-linear transmission that is accurate, precise, energy efficient and possibly backlash-free. however, the big downside is their limited rotational speed. the maximum rotational velocity is limited by resonance and the ball return mechanism. the former limit is easy to calculate (eigenfrequency calculation), and mostly not problematic for small spindles. however, the latter is a bigger problem. the balls in a ball screw roll through the threaded spindle and have to be recirculated to the other end of the nut. the recirculation limits the rotational velocity of the ballscrews. the corresponding maximum rotational velocities are not calculate-able (for as far as i know) and are provided by manufacturers in catalogues, either directly in rpm or via a so-called $d_n$-value, where the rotational velocity in rpm is $n=d_n/d$ where d is the diameter of the ball screw. but even then, the maximum rotational velocity of ball screws is capped at 4000 rpm or lower according to datasheets (depending on brand and ball return mechanism). the highest permissible rotational velocities i found were those of steinmeyer ballscrews, at 4000 rpm, using an end-cap-return mechanism. note that for electrical motors (up to 200w) ideal (maximum power) velocities are higher than 4000 rpm, and even more than twice as high for many brushless motors. it appears however that ball screws can run at higher speeds than what they are specified for in reality, because the specifications hold for many millions of cycles. i can only find a single unofficial source where someone claims to have run their ball-screws up to 6000 rpm, and in missiles (one-time-use) up to 7500 rpm. i'm interested in a theory or more experimental data that backs this up.\n", "tags": "driver", "id": "8051", "title": "maximum ball screw speeds"}, {"body": "i am trying to understand how to use, what it requires compute the homogenous transformation matrix. \n\ni know 2 points from 2 different frames, and 2 origins from their corresponding frames. \n\ni how transformation matrix looks like, but whats confusing me is how i should compute the (3x1) position vector which the matrix needs.  as i understand is, this vector a origin of the old frame compared to the new frame.  but how to calculate it, the obvious answer (i think) would be to subtract both ($o_{new} - o_{old}$ ), but it does not feel right. \n\ni know its a simple question but my head cannot get around this issue, and how can i prove it the right way, with the information i know?\n", "tags": "kinematics frame", "id": "8053", "title": "how to use the homogeneous transformation matrix?"}, {"body": "i'm currently undertaking a project to build remote controlled shades from scratch. i currently have every piece figured out except i don't know know much about the motors involved in something like this. i am looking for suggestions on what type of motor to search for. i imagine i need a type that can go forward and back as well as stop when the shade is fully retracted. i don't know what to search for though.\n\nany help is much appreciated. \n", "tags": "motor", "id": "8054", "title": "motor for diy remote controlled shades"}, {"body": "i'm developing a small scale cart-pole balancing robot consisting of two wheels driven by a single motor at the base (essentially like a unicycle, but with two wheels to constrain balance to a one dimensional problem).\n\ni'm not sure what qualities to look for in that motor.  i think the motor should be able to accelerate quickly in directions opposite of motion as dictated by the control system.  however, i'm not sure if this rapid acceleration should correlate with higher torque motors or faster speed motors.  i think higher torque motors would be too slow to react to control commands.  in contrast, fast speed motors may not be able to overcome the momentum of the cart.\n\nare there any design equations or other calculations i can make based on my robot's dimensions and weight to determine the right specs needed for my robot's motor?  how can i determine the right motor specs for this application without resorting to brute-force trial &amp; error experiments?\n", "tags": "motor design balance", "id": "8056", "title": "what factors should i consider when selecting a motor for a free wheeled cart-pole balancing robot?"}, {"body": "say we have a line-following robot that has a moving obstacle in front, that is a one-dimensional problem. the moving obstacle is defined by its initial state and a sequence of (longitudinal) acceleration changes (the acceleration function is piecewise constant). let's say the robot can be controlled by specifying again a sequence of acceleration changes and its initial state. however, the robot has a maximum and minimum acceleration and a maximum and minimum velocity. how can i calculate the sequence of accelerations minimizing the time the robot needs to reach a goal. note that the final velocity must not necessarily be zero.\n\ncan you briefly explain how this problem can be addressed or point me to some references where an algorithm is described? or point out closely related problems?\n\nfurthermore, does the solution depend on the goal position or could the robot just brake as late as possible all the time (avoiding collisions) and still reach any goal in optimal time?\n\na more formal problem description:\ngiven the position of the obstacle $x_b(t) = x_{b,0} + \\int_{t_0}^t v_b(t) dt$, and the velocity of the obstacle $v_b(t) = v_{b,0} + \\int_{t_0}^t a_b(t) dt$, where $a_b$ is a known piecewise constant function:\n\n$$a_b(t) = \\begin{cases} a_{b,1} &amp; \\text{for } t_0 \\leq t &lt; t_1 \\\\\na_{b,2} &amp; \\text{for } t_1 \\leq t &lt; t_2 \\\\\n\\dots &amp; \\\\\n\\end{cases}$$\n\nand given the initial state of the line-follower $x_{a,0}, v_{a,0} \\in \\mathbb{r}$ we search for piecewise constant functions $a_a$, where $a_{min} \\leq a_a(t) \\leq a_{max}$, $v_{min} \\leq v_a(t) \\leq v_{max}$ and $x_a(t) \\leq x_b(t)$ (collision freeness) holds at all times. reasonable assumptions are e.g. $v_b(t) \\geq 0$ and $x_{b,0} \\geq x_{a,0}$. among the feasible solutions i would like to pick those minimizing $\\int_{t_0}^{\\infty} x_b(t) - x_a(t) dt$ or a similar objective. approximation algorithms are also ok.\n\nsome numbers for those who would like a test input:\nhttp://pastebin.com/izsm2uhb\n", "tags": "mobile-robot control motion-planning line-following", "id": "8058", "title": "optimal-time acceleration sequence of a line-following robot following a moving obstacle"}, {"body": "these days i'm trying to build ir 40khz long range receiver. i use ir phototransistor. i don't want to use components like tsop... i need to make\ndaylight filter and intensify filtred signal because out of this sensor i wanna use with some microcontroller. can someone help me? any idea? thanks.\n", "tags": "sensors", "id": "8062", "title": "ir 40khz receiver"}, {"body": "i have a gps module and an imu (gyro, accelerometer and magnetometer) and i need to build an autonomous navigation system for a quadcopter. it must know its position at any time so that it can track a predefined path. i know that, in order to improve precision, i need to merge both sensors data through a kalman filter (or any other technique for that matter, the thing is that the kalman filter is way more common according to my research).\nthe problem is that i am seriously stuck and i know this might be something very simple but i don't seem to find a solution or at least the answer for some of the most basic questions.\nas a start, i know how to get the position from the accelerometer readings. i have some filters that help eliminate noise and minimize the integration errors. i also have the gps readings in latitude and longitude. the first question is, during sensor fusion, how can i make both measurements compatible? the latitude and longitude from the gps won't simply mix with the displacement given by the accelerometer, so what is the starting point for all of this? should i calculate the displacement from the gps readings or should i assume a starting latitude and longitude and then update it with the accelerometer prior to applying the filter?\n\ni have once developed a simple kalman filter in which i could plug the new reading values to obtain the next estimate position of a two wheeled car. now i have two sources of inputs. how should i merge those two together? will the filter have two inputs or should i find a function that will somehow get the best estimate (average, maybe?) from the accelerometer and gps? i am really lost here.\n\ndo you guys have any examples of code that i could use to learn? it is really easy to find articles full of boxes with arrows pointing the direction in which data must flow and some really long equations that start to get confusing very soon such as those presented on this article: http://isas.uka.de/material/samba-papierkorb/vorl2014_15/si/terejanu_tutorialukf.pdf (i have no problems with equations, seriously) but i have never seen a real life example of such implementation.\n\nany help on this topic would be deeply appreciated.\n\nthank you very much.\n", "tags": "kalman-filter sensor-fusion gps", "id": "8065", "title": "solution for ins and gps integration"}, {"body": "i hope you can help me with my project.\n\ni'm using a skid-steering wheeled mobile robot for autonomous navigation and i'd like to find a way to be able to perform path reconstruction in matlab.\n\nby using only the robot encoders (installed on the robot) and the yaw rate information (which come from a very accurate imu sensor mounted on the robot frame), i can successfully do the path reconstruction.\n(i'm using xbow-300cc sensor)\n\nthe problem is that i would like to try to reconstruct the path by using only the imu yaw rate and the imu acceleration values for x and y axis.\n\ni'm able to obtain velocity and distance by integrating two times the imu acceleration values but my problem is that i don't know how to use this data.\n\ndo i have to use a rotation matrix to pass from the imu frame to the robot frame coordinates? \ni'm asking this because i use a rotation matrix for the encoder values which come from the robot encoder.\n\nat the moment, i use these equations for robot encoders and imu yaw rate:\n\n\n\ndo i still have to use r2 matrix?\n\nthank you a lot\n", "tags": "mobile-robot kinematics imu navigation matlab", "id": "8077", "title": "mobile robot path reconstruction by using imu acceleration and yaw angle"}, {"body": "i'm really in doubt whether it is proper to ask this question here, so i'm apologizing if it is not, i'll delete it.\n\ni have a roomba robot which has worked for me for more than three years, and now while it is working it is producing some strange sounds, so i've decided to clean it thoroughly.\n\nbut when i disassembled it down to this point:\n\n \n\ni got stuck with these sort of glass things (marked with the red rectangles at the picture). they are really filthy from the inside and i cannot figure out how to clean them.\n\ndoes anyone know how one can remove dust from the inside on these things? may be there are some roomba creators here.\n\nthanks in advance.\n", "tags": "roomba", "id": "8080", "title": "could anyone tell me what are these things in a roomba robot and how to clean them, please?"}, {"body": "i'm a cs student trying to implement a clustering algorithm that would work for a set of robots in an indoor controlled environment. i'm still starting on robotics and don't have much experiencing in figuring out what will work together.\n\nmy plan is to get 6 of these zumo robots and plug in a wifi module like the wifi shield. then, i would use this to do inter communication and execute my algorithm.\n\nmy question: can the wifi module just be plugged in and would it work? if not, how can i go about achieving this task. i see lots of arduino boards with different names and i'm not sure which works with which, and whether they can be plugged in. any help would be appreciated.\n", "tags": "arduino wifi", "id": "8082", "title": "wifi module for zumo robot"}, {"body": "is it safe to give 5v through 5v pin of arduino uno r3 while the usb cable is inserted? i have escs connected to it which aren't likely to start in other cases. the 5v and gnd is coming from the bec circuit of a connected esc. please help me. thanks\n", "tags": "arduino esc", "id": "8087", "title": "is it safe to give 5v through 5v pin of arduino uno r3 while usb cable inserted"}, {"body": "as in this video: https://www.youtube.com/watch?v=qce5vguj5jg\n\nin this new version (did not see the learning part in the past versions), with three to four trials, cubli can learn to balance on a new surface.\n", "tags": "control", "id": "8094", "title": "what is the learning (control) algorithm inside cubli?"}, {"body": "we do some experiments of real time representation of sensor position on tv. in this experiments, we used sensors for collect real time position in 3d at 250hz and tv for display the sensor position at 60hz. also, we used matlab and c++ for programming with opengl platform.\n\nin programming, every iteration dat display on the tv, erase and draw the circle (object, which is represent real time position on the display). in this program i collect to only 60 points and loose other 190 points in every second, becuase, i think that refresh rate of tv is 60hz.\n\ni have gone through the thread \"how can i control a fast (200hz) realtime system with a slow (30hz) system?\"(how can i control a fast (200hz) realtime system with a slow (30hz) system?), but i don't understand, how to implement two loop on 200hz and 30hz.\n\nmy question is, how can we implement in matlab/c++? so i can store 250 data of sensors as well as 60 points for real time display on the tv.\n\nif you help me through pseudo code, i appreciate your help.\n\nthank you in advance.\n\nplease help me.\n\np.s. code\n\n\n", "tags": "design communication matlab c++", "id": "8096", "title": "how can i control fast real time sensor (250hz) with slow system display(60hz)"}, {"body": "what robotic leg technologies are available.\n\ni'm sorry if this is a basic question i am a software developer looking to get into the field of robotics. i am particularly interested in robotic legs that are similar to those used on boston dynamics atlas robot.\n\nwhat is the mechanism required that allows it to move its joints so quickly. if you see any videos of many of boston dynamics robots they make an engine sound (presumably because it uses an engine), but i cant find any details in the configuration that is being used.\n", "tags": "legged", "id": "8100", "title": "robotic legs technologies"}, {"body": "i have recently purchased my first ever servo, a cheap unbranded chinese mg996r servo, for \u00a33.20 on ebay.\n\n\n\ni am using it in conjunction with a arduino servo shield (see below):\n\n\n\nas soon as it arrived, before even plugging it in, i unscrewed the back and ensured that it had the shorter pcb, rather than the full length pcb found in mg995 servos. so, it seems to be a reasonable facsimile of a bona-fide mg996r.\n\ni read somewhere (shame i lost the link) that they have a limited life, due to the resistive arc in the potentiometer wearing out. so, as a test of its durability, i uploaded the following code to the arduino, which just constantly sweeps from 0\u00b0 to 180\u00b0 and back to 0\u00b0, and left it running for about 10 to 15 minutes, in order to perform a very simple soak test.\n\n\n\nwhen i returned, the servo was just making a grinding noise and no longer sweeping, but rather it seemed to be stuck in the 0\u00b0 position (or the 180\u00b0). i picked the servo up and whilst not hot, it was certainly quite warm. a quick sniff also revealed that hot, burning motor windings smell.  after switching of the external power supply and allowing it to cool, the servo began to work again. however, the same issue occurred a little while later. again, after allowing it to rest, upon re-powering, the servo continues to work.  however, i am reluctant to continue with the soak test, as i don\u2019t really want to burn the motor out, just yet.\n\nis there a common \u201cno-no\u201d of not making servos sweep from extreme to extreme, and one should \u201cplay nice\u201d and just perform 60\u00b0 sweeps, or is the cheapness of the servo the issue here?\n\ni am powering the servo from an external bench supply, capable of 3a, so a lack of current is not the issue.\n\n\n\nplease note that i also have a follow up question, should a mg996r servo's extreme position change over time?\n", "tags": "arduino rcservo", "id": "8103", "title": "overheating/jamming mg996r servo"}, {"body": "this question is a follow on from my previous question, overheating/jamming mg996 servo.\n\ni have recently purchased my first ever servo, a cheap unbranded chinese mg996r servo, for \u00a33.20 on ebay.\n\n\n\nafter mounting the servo horn and the bracket, i realised that i had not mounted the horn in a tout a fait 0\u00b0 orientation, rather the angle between the bracket and the servo side was approximately 20\u00b0. however, after switching the servo on and off a couple of times, with each time allowing the servo to perform, say, about 10 sweeps each time, i quickly noted that the servo\u2019s extreme positions were changing over time, so that the initial extremes and then the extremes after about 5 on and off cycles, had changed by about 15\u00b0, so that now, 0\u00b0 and 180\u00b0 the bracket is now parallel with the body of the servo. \n\ni was quite surprised at this, as i had assumed that the 0\u00b0 and 180\u00b0 positions would be fixed, and not change over time, or vary each time that it was switched on and off.\n\nseeing as there should be a stop peg on the gear connected to the potentiometer inside, how is this even possible?\n", "tags": "rcservo", "id": "8104", "title": "should a mg996r servo's extreme position change over time?"}, {"body": "i am just starting to explore an idea and i am somewhat of a novice in robotics. i am looking to position a mobile robot as accurately as possible on a concrete slab. this would be during new construction of a building and probably not have many walls or other vertical points for reference. the basic premise behind the robot is to print floor plans straight on to the slab. i will have access to the bim (building information models, cad, revit) files of the building. i want the robot to position itself as accurately as possible on the blank slab using the bim files as a map. what would be the best avenue to track and adjust positioning of the robot in the open space of a slab? low frequency, lidar, wifi? lastly what sensors would be best?\n", "tags": "mobile-robot sensors localization", "id": "8106", "title": "2d positioning of mobile robot"}, {"body": "including q, r, and initial states of x and p.\n", "tags": "kalman-filter", "id": "8107", "title": "are there systematic ways to tune the kalman filter in engineering practice?"}, {"body": "attempt to clean up:\n\ni'm trying to use this motor with this esc and an arduino uno r3.\n\ntypically, i used the pwm pins when i use the arduino and an esc, but i can't control the motor even if i use the servo library, and i've also tried sample code from different websites.\n\nthe esc has a beep i can't understand. sometimes it's high-low-high or high for 4 seconds, but i can't find anything on google. \n\nsometimes the motor spins periodically for a short time, but i don't know why. some sites recommend using flash or bootloader, but i'd prefer to use arduino pwm or the servo library. \n\noriginal post\n\nspecific esc is rctimer mini esc16a opto simonk firmware sn16a esc..\n\ni can only using esc(discussed above..) and rctimer 1806-1450kv multi-rotor bldc motor.\n\ntypically, i used pwm pins(3, 9, 10, 11-because similar signal frequency) when using arduino-esc.. but, i can control bldc motor even i used to servo library.. \n\ni've been used usual websites example code.\n\njust esc had unknowable beep .. sometime di-ri-di or di(for 4 seconds).. \ni couldn't find that way.. in google (or my country websites)\n\nsometimes, the motor spins(in a certain value, periodically) for a short time but i don't know why the motor spins\n\nin google sites, just using flash or bootloader, but i'll use arduino pwm or servo..\n\nso.. please! would you please help me?\nthank you for reading my thread.\n\n\n\n", "tags": "arduino brushless-motor esc pwm", "id": "8108", "title": "how to use specific esc,bldc motor through arduino uno r3?"}, {"body": "i'm familiar with using pid to perform closed loop control when there is a single output and a single error signal for how well the output is achieving the desired set-point.\n\nsuppose, however, there are multiple control loops, each with one output and one error signal, but the loops are not fully independent.  in particular, when one loop increases its actuator signal, this changes the impact of the output from other loops in the system.\n\nfor a concrete example, imagine a voltage source in series with a resistor, applying a voltage across a system of six adjustable resistors in parallel.  we can measure the current through each resistor and we want to control the current of each resistor independently by adjusting the resistance.  of course, the trick here is that when you adjust one resistor's resistance, it changes the overall resistance of the parallel set, which means it changes the voltage drop due to the divider with the voltage source's resistance and hence changes the current through the other resistors.\n\nnow, clearly we have an ideal model for this system, so we can predict what resistance we should use for all resistors simultaneously by solving a set of linear equations.  however, the whole point of closed loop control is that we want to correct for various unknown errors/biases in the system that deviate from our ideal model.  the question then: what's a good way to implement closed loop control when you have a model with this kind of cross-coupling?\n", "tags": "control pid", "id": "8111", "title": "multiple control loops with overlapping effects"}, {"body": "does anyone out there know where i can get the original irobot create?\nthe company no longer sells them. \nit was only 2 years ago that it was sold. it is white and its value is the physical design, that it has a large exposed deck for mounting armatures.\nit is preprogrammed to operate in different configurations, eg. spinning, figure 8, following the outline of a wall, etc.\ni have an ongoing art project using this model and as they are in operation everyday, i will eventually need to replace them with new ones.\nto see a video of one of my projects you can go to https://vimeo.com/119486779\ni currently have it working in a spinning motion.\n", "tags": "irobot-create", "id": "8112", "title": "do you know where to get the original irobot create?"}, {"body": "i am trying to build a map containing lamps as landmarks. i drive around with a robot and a monocular camera looking to the ceiling.\n\nthe first step is detect the edges of each observed rectangular lamp and save the position in pixels and also the current position from odometry of the robot. after the lamp disappears from the field of view, there is enough base-line to do a 3d reconstruction based on structure from motion. once this reconstruction is done there will be uncertainty in the position of the lamps that can be modelled by covariance. imagine if the robot was driving for a while, its own position estimated from odometry will also have a relatively high incertitude, how can i integrate all of those incertitudes together in the final covariance matrix of the position of each lamp?\n\nif i understand well there would be the following covariances:\n\n\nnoise from camera\ninaccurate camera calibration matrix\ninaccurate result from optimization\ndrift in odometry\n\n\nmy goal is to manually do loop closure using for example g2o (graph optimization) and for that i think correct covariances are needed for each point.\n", "tags": "slam cameras 3d-reconstruction", "id": "8116", "title": "covariance and optimization"}, {"body": "i'm hesitant to open up a vectornav vn300 to see what's inside.  does anybody here know what underlying sensors it uses?  the accel, gyro, and mag outputs are all very low-noise compared to other \"high end\" 3-axis mems devices (ie kionix kxr94, maxim 21000, st lis3mdl).  everything fits in such a small package so i'm guessing they're using devices with integrated axes and adc's, rather than than \"navigation grade\" devices which tend to be analog, fewer than three axes, and enormous compared to to consumer-level mems.  likewise, automotive mems sensors (which they mention they are using in one of the web pages) tend to be single or dual axis, and not necessarily less noisy than consumer-grade sensors.  \n", "tags": "imu navigation", "id": "8118", "title": "what sensors and mcu does vectornav vn300 use internally?"}, {"body": "i have extremely limited knowledge in the general topic of robotics and therefore this question is a shot in the dark. please let me know if the topic is unsuitable for the site.\n\ni am interested in creating a device that would generate a touchscreen tap. in a nutshell, i would like to replicate on a touchscreen the automated mouse functionality you can obtain with software like autohotkey in windows. since, without jailbreaking the phone, a software solution is basically impossible, it occurs that one of the first components would be a physical device that simulates a tap. do any options for such a component exist?\n\ni recognize that there are philosophical implications with creating such a device. i am assuming the entire conversation to be theoretical and solely related to the hardware design.\n\nthanks,\nalex\n", "tags": "automatic", "id": "8120", "title": "device to generate screen tap response"}, {"body": "a have designed a robot to perform tasks in farms.  but the problem now is i'm not sure on the best way to supply continuous power to my robot.  all the motors are rated at 12v and only arduino and a few sensors work at 5v or less.  \n\ncan i continuously charge a 12v lead acid battery with an adapter (comes with the battery) plugged into the ac output of the generator while the robot is operating? do i have to worry about overcharging the battery?\n\nor should i use the generator's dc output which can supply 12v and up to 8.3amp. or is there any other suggestions?\n\nsome information about the adapter which are stated on the package:\n1. built-in over-charge protection device.\n2. built-in thermal protection device\n3. output: 6v/12v 2amp\n\nthis is the generator that i have: \nhttp://global.yamaha-motor.com/business/pp/generator/220v-60hz/0-1/et950/\n\nthis is my first robot which is quite big that requires a lot of electrical/electronic knowledge to power it. i do not have a lot of experience in this field. so any feedback is greatly appreciated.\n", "tags": "electronics power battery", "id": "8122", "title": "powering my robot with 12v battery which is charged by a gas/petrol generator while the robot is operating?"}, {"body": "i have a system with two inputs (throttle and brake) and one output (speed). how does one design a controller in such a way that the two outputs of the controller (throttle and brake) are never both greater than zero (so that it doesn't accelerate and brake simultaneously)?\n\nthanks\n", "tags": "control automation", "id": "8124", "title": "two exclusive inputs control"}, {"body": "i have some sensors attached to arduino uno r3 and an esc. i start the motor attached to esc through ardiuno with no usb connected to laptop. it starts correctly. there is a must that i will have to start the arduino from non usb supply so that esc is correctly started, means that my motor doesnt start with usb connected to pc. now how can i get the sensor values to laptop. if i connect the usb to pc after starting the motor, will this work.\n", "tags": "arduino esc", "id": "8127", "title": "can i connect the arduino usb to laptop after the arduino is started"}, {"body": "i want to create a virtual quadcopter model, but i am struggling to come up with a satisfying model for the brushless motors &amp; props.\n\nlet's take an example, based on the great ecalc tool:\n\n\n\nlet's say i want to know how much current is consumed by the motor in a hovering state. i know the mass of the quad (1500g), so i can easily compute the thrust produced by each motor:\n\n\n\nthrust is produced by moving a mass of air at an average speed of :\n\n\n\nwhere  (air density) is 1.225kg/m3 and  (propeller disk area) is  (12\" props). so i can compute :\n\n\n\nall right, now i can calculate the aerodynamic power created by the propeller:\n\n\n\nall right, now i can calculate the mechanical power actually produced by the motor. i use the  efficiency term from ecalc:\n\n\n\nhere, ecalc predicts 37.2w. it's not too far from my number, i imagine they use more sophisticated hypotheses... fair enough.\n\nfrom this post, i know that this power is also equal to:\n\n\n\nwhere i know  (0.08 ohms) and  (0.9 a). so, finally, my question: how do you calculate  and  from here? of course, if i knew the rotation speed of the engine i could get  from:\n\n\n\nwhere . but unfortunately i don't know the rotation speed...\n\n(note that vin is assumed to be averaged from the pulse-width-modulated output produced by the esc)\n\nthanks for your help!\n", "tags": "motor quadcopter brushless-motor electronics power", "id": "8128", "title": "how to calculate the current consumed by a brushless motor on a quadcopter"}, {"body": "not sure if this has been asked, but there are lots of simulations of bipedal locomotion algorithms online, some of the evolutionary algorithms converge to very good solutions. so it seems to me that the algorithm part of bipedal locomotion is well-understood.\n\nif you can do well on simulations, you should be able to do it well in the real world. you can model delay and noise, you can model servo's response curve.\n\nwhat i don't understand is then why is it still difficult to make a walking robot? even a robot like the big dog is rare.\n", "tags": "walk", "id": "8129", "title": "why are bipedal robots difficult?"}, {"body": "i have built a mobile robot with several ultrasonic sensors to detect obstacles and an infrared sensor to track a line as a path. i have written a simple algorithm to follow the line which works fine, but avoiding obstacles are a problem because the robot doesn't know the layout of the path, so even if it does move around the obstacle, it is not guaranteed that it will find the path line again(unless the line is perfectly straight). therefore, i think i may need to use a path/motion planning algorithm or find a way to store the layout of the path so that robot could predict where to move and get back to the path line and keep on following after overcoming an obstacle. i would like to hear suggestions or types of algorithms i should focus on for this specific problem.\n\npicture might help specifying the problem i'm facing.\n\n\nthank you.\n", "tags": "motion-planning", "id": "8130", "title": "line following robot path planning"}, {"body": "i am reading \"computer vision: models, learning, and inference\" in which author writes at several points (like on page 428-429) that although matrix a seems to have 'n' degree of freedom but since it is ambiguous up to scale so it only has 'n-1' degree of freedom? can anyone explain what this thing means? why one degree of freedom is decreased?\n", "tags": "computer-vision", "id": "8133", "title": "\"ambiguous up to scale\" , explanation required"}, {"body": "im using two time-of-flight (tof) cameras, a ds325 from softkinetic and a creative senz3d, at the same time with the depthsense-sdk and the point-cloud-library (on ubuntu 15.04). but i got strong interferences.\n\nis there a possibility to control the laser (software side) either to send the light in a special frequency or to turn it off and on alternating?\n\nor is there another way to get rid of the interferences?\n", "tags": "cameras laser 3d-reconstruction 3d-model", "id": "8134", "title": "interferences while using two tof cameras"}, {"body": "i'm trying to build a robot that can be sent into rooms/buildings and detect people using nxt and/or arduino. in addition to this i would like to be able to view what my robot is \"seeing\" in real-time on my pc as an infrared image. the sensors i've shortlisted for this are:\n\n\nthermal infrared nxt sensor from dexter industries - \u00a344\nroboard rm-g212 16x4 thermal array sensor - \u00a394\nomron d6t mems thermal ir sensor - \u00a331\n\n\ni believe the roboard and omron sensors are capable of thermography, so i was wondering if anyone here has experience with these sensors and give me some advice.\n\ni was also thinking about using an idea from this project: www.robotc.net/blog/tag/dexter-industries.\nin this case i'd use the data read from the sensor to plot a graph showing different temperatures.   \n", "tags": "mobile-robot sensors nxt", "id": "8140", "title": "\"thermal imaging\" with arduino and/or lego mindstorm nxt 2.0?"}, {"body": "i am building a sumo-bot and our competitors have thin sticky tires, while we have wider and less sticky tires. the diameter is the same, and the gearbox/motor is the same. who will win?\n\nps: sticky tires: https://www.pololu.com/product/694 &amp; wide tires: https://www.pololu.com/product/62\n\nthanks!\n", "tags": "movement wheel two-wheeled", "id": "8142", "title": "high-traction thin tires vs wide moderate-traction tires? [sumo-bot]"}, {"body": "i am trying to recharge my 12v lead acid battery with a 12v dc motor.  i am using the battery to power the robot when it climbs. when it descends, i notice that i dont need to apply reverse voltage but the dc motor just backdrives instead. this can act as generator to recharge back the battery, am i right?\n\ni know that i need to step up the low voltage that is generated by the backdriven motor to 12v needed to recharge the battery. this is the board that i think can do the job:\nhttps://www.pololu.com/product/799\n\nis this all i need to make it work? with this method, should i be concerned about the 3 stages of battery charging: bulk, absorption and float?\n\nplease advise. any feedbacks are greatly appreciated.\n", "tags": "mobile-robot battery", "id": "8144", "title": "using dc motor as a generator to recharge battery of my robot"}, {"body": "i have a small mobile robot with a lidarlite laser range finder attached to a servo. as of now i have the range finder side-sweeping in a 30 degree arc, taking continuous distance readings to the side of the robot (perpendicular to the robots forward motion).\n\nmy goal is to have the robot drive roughly parallel to a wall, side-scanning the entire time, and create a 2d map of that wall it is moving past. the 2d topography map is created post processing (i use r for much of my data processing, but i don't know is popular for this kind of work).\n\nfrom what i know of it, slam sounds like a great tool for what i want to do. but i have two issues:\n\n1: i know my robot will not have a consistent speed, and i have no way to predict or measure the speed of my robot. so i have no way to estimate the odometry of the robot.  \n\n2: the robot will also move further and closer to the wall as it proceeds down it's path. so i can not depend on a steady plane of travel from my robot.\n\nso given that i don't have any odometry data, and my realtive distance to the wall changes over the course of a run, is it possible to use slam to create 2d maps?\n\ni'm looking into stitching algorithms that are used for other applications, and some of these can handle the variances in relative distance, but i was hoping slam or some other algorithm could be of use here.\n", "tags": "slam servos laser rangefinder", "id": "8146", "title": "using slam to create 2d topography"}, {"body": "on many drones are already external magnetometers. unfortunately, the orientation of such sensors is sometimes unknown. e.g. the sensor can be tilted 180\u00b0 (pitch/roll) or x\u00b0 in yaw. i was wondering, whether one could calculate the rotation of the sensor relative to the vehicle by application of the accelerometer and gyrometer?\ntheoretically, the accelerometer yields a vector down and can be used for calculation of the coordinate system. the discrepancy between magnetometer and gyrometer then, may be used for calculation of the correct orientation of the compass. later the compass should be used for yaw calculation. \nbelow is the starting orientation of both sensors (just an example, the orientation of the compass can be anything). does someone know a good way to figure out the rotation of the compass? \n\n\n", "tags": "magnetometer orientation", "id": "8150", "title": "sensor orientation of an external magnetometer"}, {"body": "i am stuck in adjusting the pid of my quadcopter, i cant adjust them on the fly because it just get out of control. i am adjusting them while attaching my quadcopter to something.\n\nis this method correct. will the pid values required will be different on the fly or same. please suggest me how to attach my quad to some thing.\n", "tags": "quadcopter pid balance", "id": "8152", "title": "how should i tie my quadcopter to some thing, to adjust pid on one axis"}, {"body": "\n", "tags": "c++", "id": "8153", "title": "can someone explaine to me this code?"}, {"body": "i'm a student working on a robotics project, and i'm a complete beginner in robotics.\n\ni'm working on the arducopter structure, using the ardupilot mega associated with the ardupilot imu as my autopilot board. i have an easyvr module with an arduino uno for my vocal recognition stuff.\n\ni don't know how to give order to the autopilot board with my vocal module. do i need to change the arducopter source code? do i have to use mission planner software?\n\nthe final aim is to do this.\n", "tags": "arduino quadcopter ardupilot", "id": "8160", "title": "implement of a vocal interface on a arducopter"}, {"body": "when should you use multiple separate batteries vs a single battery with multiple ubecs?\n\ni'm trying to design the power system for a small 2-wheeled robot. aside from the 2 main drive motors, it also has to power an arduino, a raspberry pi and a couple small servos to actuate sensors.\n\n\nthe motors are each rated for 6v with a peak stall current of 2.2a\nthe arduino uses about 5v@100ma\nthe raspberry pi uses about 5v@700ma\nthe servos each use 6v and have a peak stall current of 1.2a.\n\n\nso the theoretical max current draw would be  = 7.6a.\n\noriginally i was planning to use three separate lipo batteries:\n\n\none 12v using a step-down converter to power the main drive motors for 6v@4.4a peak\ntwo 3.7v lipos each with step-up converter (rated for 5v@3a) to handle the servos and logic separately\n\n\nthen i discovered ubecs, which sound too good to be true, and they seem to be both cheap () and efficient () and able to handle my exact volt/current requirements.\n\nshould i instead use a single high-current 12v lipo with three ubecs to independently power my drive motors, sensor motors and logic? or will this still suffer from brown-out and power irregularities if a motor draws too much current?\n\nwhat am i missing?\n", "tags": "battery bec", "id": "8162", "title": "when to use multiple batteries vs a ubec"}, {"body": "i'm working on project for the autonomous vehicle, and i want to know what's confidence level means and how can we use confidence level for vehicle detection in opencv ?\n", "tags": "opencv statistics", "id": "8165", "title": "what's confidence level? and how can we use it for vehicle detection using opencv?"}, {"body": "for adjusting the pid for quadcopter, how much speed of motors are required before adjusting the pid. do we need to give so much offset speed so that it cancels weight? i am sure we cant start adjusting pid with zero speed of motors initially.\n", "tags": "quadcopter pid", "id": "8168", "title": "how much offset speed of motors on an axis is required before adjusting pid"}, {"body": "given a set of robot joint angles (i.e. 7dof) $\\textbf{q} = [q_1, ... , q_n]$ one can calculate the resulting end-effector pose (denoted as $\\textbf{x}_\\text{eef}$), using the foward kinematic map. \n\nlet's consider the vice-versa problem now, calculating the possible joint configurations $\\textbf{q}_i$ for a desired end-effector pose $\\textbf{x}_\\text{eef,des}$. the inverse kinematics could potentially yield infinitely many solutions or (and here comes what i am interested in) no solution (meaning that the pose is not reachable for the robot).\n\nis there a mathematical method to distinguish whether a pose in cartesian space is reachable? (maybe the rank of the jacobian) furthermore can we still find a reachability test in case we do have certain joint angle limitations? \n", "tags": "mobile-robot robotic-arm kinematics inverse-kinematics jacobian", "id": "8171", "title": "robot arm reachability of a pose in cartesian space"}, {"body": "in an autonomous mobile robot, we're planning on using digital servo motors to drive the wheels. servo motors usually don't rotate continuously. however, they can be modified to do so based on many tutorials online which only mention modifying [analog] servo motors.\n\nmy question is, can the same method(s) or any other ones be used to modify digital servo motors?\n\nthanks\n", "tags": "mobile-robot rcservo", "id": "8181", "title": "can digital servo motors be modified for continuous rotation?"}, {"body": "i am making a robot goalie, the robot is supposed to detect whether a ball has been thrown in its direction , sense the direction of the ball and then stop it from entering the goal post. a webcam will be mounted on top of the goal post. the robot is required to only move horizontally (left or right), it shouldn't move forwards or backwards. the robot will have wheels, the image processing will be performed by raspberry pi which will then send the required information to a micro controller which will be responsible for moving the robot in the required direction(using servo motors). which image processing algorithm will be the best to implement this scenario?\n", "tags": "mobile-robot computer-vision algorithm beagle-bone first-robotics", "id": "8182", "title": "image processing"}, {"body": "people at the reprap 3d-printing project often mention cnc routers or cnc mills.\n\nboth kinds of machines almost always have a motorized spindle with stepper motors to move the spindle in the x, y, and z directions.\n\nwhat is the difference between a cnc router versus a cnc mill?\n\n(is there a better place for this sort of question -- perhaps the woodworking stack exchange?)\n", "tags": "industrial-robot", "id": "8184", "title": "what is the difference between a cnc router versus a cnc mill?"}, {"body": "i'm designing my lawn mower robot, and i am in the perimeter stage. \nthe electronic part is done, and works quite good, now comes the software.\n\ni need an advice on how to deal with the problem of line following. i mean, once the robot is on the line, parallel to the line, that's relatively easy. \nbut how to manage the situation when the robot is driving around and approaches the line (wire)?\n\ni have two sensors, left and right, turned 45\u00b0 with respect to the forward direction. \n\nthe robot could arrive from any angle, so the signal amplitude read from the sensor could be completely random.. \nso i don't understand what to do in order to move it in the right position on the wire...\nwhat's the usual approach? \n\n\n\nthe idea is the same as here:\n\n\n\nthe wire is all around the yard. on the mower there are 2 sensors, left and right, that sense the signal emitted from the wire, a square wave signal at 34 khz. the signal amplitude read from the sensors on the mower is about 2 v when it's above the wire.\n", "tags": "line-following magnetometer", "id": "8186", "title": "robot wire follower + how to position on wire"}, {"body": "i will have a 5 or 6 dof arm build with dynamixel or herculex smart servos. i need  to move the gripper along cartesian trajectory, which i will calculate in my c++ application. i looked at ros, but the learning curve is pretty steep and it looks like a major overkill for this use case. i don't need a distributed system with all the complexity it brings. preferably, i would like to call a standalone c++ library or libraries to get the arm actuated. \n\nwhat are my options? what will be the limitations of not using a full blown robotics framework like ros or yarp in this case.\n\nedit\n\nhere is how i would like to code it:\n\n\n\nthe last line can be spread over several library function calls and intermediate data structures, if needed. the end result should be the gripper physically following cartesian trajectory given by  and .\n", "tags": "robotic-arm ros motion-planning c++", "id": "8187", "title": "is there a simpler way than ros for 5 dof dynamixel arm control"}, {"body": "i am trying to run my 600 series roomba in a large, open space (1700+sf) and it does not recognize the large, open space and throws the error 10 code.  it does not recognize an edge of 2 12\"-3\" either; it will fall off the edge and become stuck.  any suggestions?\n", "tags": "mobile-robot irobot-create roomba", "id": "8188", "title": "how can i avoid roomba error 10 code?"}, {"body": "how do i determine which angle i can negate when gimbal lock occurs. \n\nas i've understood with gimbal lock that it remove one degree of freedom, but how do i determine which degree can be removed when a  value r[1][3]  of a rotation matrix (size 3x3) has the value 1.  is it the roll, pitch or yaw which can be taken out from the equation?\n", "tags": "motion-planning", "id": "8191", "title": "rotation matrix to euler angles with gimbal lock"}, {"body": "i bought this drone frame : q450 glass fiber quadcopter frame 450mm from http://hobbyking.com/hobbyking/store/__49725__q450_v3_glass_fiber_quadcopter_frame_450mm_integrated_pcb_version.html\n\ni'm considering buying 4 ax-4005-650kv brushless quadcopter motor's from http://hobbyking.com/hobbyking/store/__17922__ax_4005_650kv_brushless_quadcopter_motor.html\n\nwill these motor's fit this frame ? how can i determine what motor's will fit the frame ?\n", "tags": "quadcopter", "id": "8194", "title": "choosing motors for quadcopter frame"}, {"body": "i am running ros indigo on ubuntu 14.04. i am doing a mono-camera calibration and trying to follow the camera calibration tutorial on the ros wiki.\n\ni give the following command:\n\n\n  rosrun camera_calibration cameracalibrator.py --size 8x6 --square\n  0.108 image:=/my_camera/image camera:=/my_camera\n\n\ni get the following error:\n\n\n  importerror: numpy.core.multiarray failed to import traceback (most\n  recent call last): file\n  \"/opt/ros/indigo/lib/camera_calibration/cameracalibrator.py\", line 47,\n  in  import cv2 importerror: numpy.core.multiarray failed to\n  import\n\n\ni thought it was to do with updating  and did a  update but no difference.\n\nwhat is a possible way to solve this problem?\n\nupdate:\ni uninstalled and reinstalled ros completely from scratch. i still get the same error. should i have to look somewhere outside ros?\n", "tags": "ros cameras calibration", "id": "8196", "title": "camera calibration fails to run on ros"}, {"body": "background: introductory robotics competition for college freshmen; bot has to open 8 jars (with two balls in each of them) in ten minutes and load the balls into a shooting mechanism.\n\nso, we were doing this project and we hit upon a challenge that the jar is not opening like we originally intended to. so we decided to get a rack-pinion mechanism and use it for unscrewing the lid. however, it is too large and we are unable to fit the bot in the required dimensions\n\nthe actual question: are there any wires or rigid columns/things which can contract ~1 cm when electricity is passed through it? and what would their price range be? our budget is also in the list of constraints for the bot\n\nedit: we can include a wire of length &lt;1m or a column of length &lt;30 cm. also, the wire needs to contract only more than 7mm\n", "tags": "mechanism", "id": "8199", "title": "wires or columns which contract on passing electricity"}, {"body": "i have implemented an ekf on a mobile robot (x,y,theta coordinates), but now i've a problem.\nwhen i detect a landmark, i would like to correct my estimate only on a defined direction. as an example, if my robot is travelling on the plane, and meets a landmark with orientation 0 degrees, i want to correct the position estimate only on a direction perpendicular to the landmark itself (i.e. 90 degrees).\n\nthis is how i'm doing it for the position estimate:\n\n\ni update the x_posterior as in the normal case, and store it in x_temp.\ni calculate the error x_temp - x_prior.\ni project this error vector on the direction perpendicular to the landmark.\ni add this projected quantity to x_prior.\n\n\nthis is working quite well, but how can i do the same for the covariance matrix? basically, i want to shrink the covariance only on the direction perpendicular to the landmark.\n\nthank you for your help.\n", "tags": "mobile-robot slam kalman-filter ekf", "id": "8202", "title": "ekf-slam: shrink covariance matrix on one direction"}, {"body": "the joint velocities are constant and equal to $\\dot{\\theta}_{2}$ = 1 and $\\dot{\\theta}_{1}$ = 1. how to  compute the\nvelocity of the end-effector when $\\theta_{2} =\\pi/2$ and $\\theta_{1} = \\pi/6$\n\n\n", "tags": "robotic-arm", "id": "8204", "title": "velocity of the end-effector"}, {"body": "the question i am asking is that, what is the effect on stability of increasing or decreasing both the sample time and lagging of error signal to pid. does it helps in stability or degrade it?\n", "tags": "quadcopter pid stability", "id": "8207", "title": "quadcopter stability vs (pid error signal lag and sample time)"}, {"body": "i saw one old industrial robot(year 1988) end effector is having 2 dc motor for roll drive. after roll drive, yaw and pitch drives are connected and it has dc motors separately.\n\nbut roll drive has two dc motors. why are they used like this? why not single with higher torque.\n\nall the roll, pitch and yaw motors are same spec. total 4 dc motors.\n\ntwo dc motor connected to single shaft using gears in roll.\n", "tags": "industrial-robot", "id": "8209", "title": "two dc motors and single output?"}, {"body": "i am attempting to use the data underwater simulator (uwsim) provides through the ros interface to simulate a number of sensors that will be running on a physical aquatic robot. one of the sensors detects the current depth of the robot so, i want to simulate this with the data provided by the uwsim simulated pressure sensor. the problem is that nowhere in the uwsim wiki or source code can i find any reference to what units uwsim uses to measure pressure.\n\nthat being said, what units does uwsim use to measure pressure? additionally, i would appreciate general information about what units uwsim uses for the data provided by it's virtual sensors.\n", "tags": "ros simulation underwater", "id": "8212", "title": "uwsim pressure sensor units"}, {"body": "i am trying to find a control model for the system of a balancing robot. the purpose of this project is control $\\theta_2$ by the 2 motors in the wheels i.e. through the torque $\u03c4$ i started with the dynamic equations and went to find the transfer function. \n\nthen i will find the pid gains that will control the robot and keep it balanced with the most optimum response. for the time being i am only interested in finding the transfer function for the dynamic model only.\n\nhere is an example: https://www.youtube.com/watch?v=fdsh_n2yjzk\n\nhowever, i am not sure of my result.here are the free body diagrams for the wheels and the inverted pendulum (robot body) and calculations below: \n\n\n\ndynamic equations:\n\n$$\n\\begin{array}{lcr}\nm_1 \\ddot{x}_1 = f_r - f_{12} &amp; \\rightarrow &amp; (1)&amp; \\\\\nm_2 \\ddot{x}_2 = f_{12} &amp; \\rightarrow &amp;  (2) &amp;\\\\\nj_1 \\ddot{\\theta}_1 = f_r r - \\tau &amp; \\rightarrow &amp; (3) &amp;\\\\\nj_2 \\ddot{\\theta}_2 = \\tau - mgl\\theta &amp; \\rightarrow &amp; (4) &amp; \\mbox{(linearized pendulum)}\\\\\n\\end{array}\n$$\n\nkinematics:\n\n$$\nx_1 = r\\theta_1 \\\\\nx_2 = r\\theta_1 + l\\theta_2 \\\\\n$$\n\nequating (1) and (3):\n$$\nm_1 \\ddot{x}_1 + f_{12} = f_r \\\\\n\\frac{j_1 \\ddot{\\theta}_1}{r} + \\frac{\\tau}{r} = f_r\n$$\n\nyields:\n\n$$\n\\frac{j_1 \\ddot{\\theta}_1}{r} - m_1 \\ddot{x}_1 + \\frac{\\tau}{r} = f_{12} \\rightarrow (5)\n$$\n\nequating (5) with (2):\n\n$$\n\\frac{j_1 \\ddot{\\theta}_1}{r} - m_1 \\ddot{x}_1 + \\frac{\\tau}{r} - m_2 \\ddot{x}_2  = 0 \\rightarrow (6) \\\\\n$$\n\nusing kinematic equations on (6):\n\n$$\n(j_1 - m_1 r^2 - m_2 r^2) \\ddot{\\theta}_1 + m_2 l r \\ddot{\\theta}_2 = -\\tau \\rightarrow (7) \\\\\n$$\n\nequating (7) with (4):\n\n$$\n\\begin{array}{ccc}\n\\underbrace{(j_1 - m_1 r^2 - m_2 r^2) }\\ddot{\\theta}_1 &amp;+&amp; \\underbrace{(m_2 l r + j_2 ) }\\ddot{\\theta}_2 &amp;+&amp; \\underbrace{m_2 gl}\\theta &amp;= 0 \\rightarrow (8) \\\\\na &amp; &amp;b &amp; &amp; c &amp; \\\\\n\\end{array}\n$$\n\nusing laplace transform and finding the transfer function:\n\n$$\n\\frac{\\theta_1}{\\theta_2} = -\\frac{bs^2 + c}{as^2} \\\\\n$$\n\nsubstituting transfer function into equation (7):\n\n$$\n(j_1 - m_1 r^2 - m_2 r^2) \\frac{\\theta_1}{\\theta_2}\\theta_2 s^2 + m_2 lr\\theta_2 s^2 = -\\tau \\\\\n$$\n\nyields:\n$$\n\\frac{\u03b8_2}{\u03c4} = \\frac{-1}{(mlr-b) s^2+c}\n$$\n\nsimplifying:\n$$\n\\frac{\u03b8_2}{\u03c4}=  \\frac{1}{j_2 s^2-m_2 gl}\n$$\n\ncomments:\n\n-this only expresses the pendulum without the wheel i.e. dependent only on the pendulums properties.\n\n-poles are real and does verify instability.\n", "tags": "arduino control pid wheeled-robot", "id": "8213", "title": "balancing robot control model"}, {"body": "i recently bought a imu . i am new at this. \n\nmy question: does the positioning of the imu matter? are there any differences between placing it at the center of the plate or if it is offset from the center?\n\ni am still learning about this topic. so any help would be greatly appreciated.\n\nthanks.\n", "tags": "control sensors imu sensor-fusion", "id": "8220", "title": "balancing a plate with an imu offset from the center"}, {"body": "i have a sensor that gives r, theta, phi (range, azimuth and elevation) as such:\nhttp://imgur.com/hpsqc50\ni need to predict the next state of the object given the roll, pitch yaw angular velocities given the above information. but the math is really confusing me.\nso far all i've gotten is this:\n\n\n\ni worked this out by trigonometry, so far this seems to predict the pitching about the x axis and yawing about my y axis (sorry i have to use camera axis)\nbut i dont know how to involve the roll (angularzvel)\n", "tags": "kalman-filter", "id": "8222", "title": "3d angular velocity to 3d velocity to predict next state"}, {"body": "i am using the ar_track_alvar package in indigo to detect ar tags and determine their respective poses. i am able to run the tracker successfully as i can visualize the markers in rviz. i give the following command to print the pose values\n\n\n  rostopic echo /ar_pose_marker\n\n\nand i get the following output indicating that the poses are determined.\n\n\n\n\n\nnow i want to use these poses in another ros node and hence i need to subscribe to the appropriate ros message('ar_pose_marker\"). but i am unable to get enough information on the web on the header files and functions to use in order to extract data from the published message. it would be great if somebody can point to a reference implementation or documentation on handling these messages. it might be useful to note that ar_track_alvar is just a ros wrapper and hence people who have used alvar outside of rosmay also give their inputs.\n\nupdate:\n\ni tried to write code for the above task as suggested by @ben in the comments but i get an error. the code is as follows\n\n\n\nand i get the following error\n\n\n\nany suggestions?\n", "tags": "ros pose", "id": "8223", "title": "determining pose from ar_track_alvar message in ros"}, {"body": "i am doing research on autonomous car and looking for a sensor to be used along with lidar laser scanner. ladybug could be a very good option but the cost!! too expensive. \ncould you please suggest me options for camera sensors with good fov and which will cost me around $1000. \nthank you so much!!\n\n-chiang chen\n", "tags": "computer-vision", "id": "8225", "title": "choice for camera sensor to be used with lidar"}, {"body": "i'm working on a project that requires me to build a small vehicle (footprint of ~ 14 x 14 inches, less than 6.5 pounds) that can traverse sand. for the steering system, i was thinking of replicating the way tanks and lawn mowers navigate (ability to do zero-point turns), but i want to do this with four wheels instead of tracks like a tank.\n\ni need help with implementing this idea. my preliminary thoughts are to have two motors where each motor power the wheels on one side of the vehicle (i think this would require a gearing system) or to have a motor to power each individual wheel which i'd rather avoid.\n", "tags": "mobile-robot motor wheeled-robot", "id": "8226", "title": "how would i replicate a tank/zero-turn steering system in a small robotic vehicle?"}, {"body": "i want to replace the flight module with smart phone because it has all sensors that are required, like gyroscope, magnetometer, etc. is that possible?\n\ni am using an google nexus 4 android (os model 5.1). i will control using another mobile, i am able write an app, with an arduino acting as a bridge between smartphone and copter. i am using flight controller openpilot cc3d coptercontrol.\n", "tags": "arduino quadcopter", "id": "8228", "title": "quad copter flight module can replace with smart phone?"}, {"body": "in the past i built some simple robot arms at home, using rc servo motors or stepper motors (till 3dof). i would like to build a new arm with 4dof or 5dof with the steppers. until now i used arduino and a4988 stepper drivers and gcode.\nfor calculating inverse kinematics in real time for a 4dof or 5dof i think the arduino is not enough powerful. so i'm searching for a new tool-chain gcode interpreter + inverse kinematics calculation + stepper controller.\ni see linuxcnc + beaglebone black + cnc cape. not too expensive for an hobbyist.\nbut this is the only possibility i found. there are other possibilities for an hobbyist to implement a 4dof or 5dof robot arm working with the stepper motors?\n", "tags": "stepper-motor arm", "id": "8229", "title": "4dof or 5dof robot arm with stepper motors tool-chain for an hobbyist"}, {"body": "my quadcopter's settling time is very large, that is it sets its setpoint in very large amount of time, during which it has covered a large distance. but at settle point, when i gives it a jerk or push its returns to settle in normal duration. doesnt over shoots(little). the problem is with the settling time that is when i move the stick front or back it takes huge amount of time. what could be wrong. i have tried giving more p value and i value to pid but then it overshoots and get unstable. this is my pid program. the pid values are given. i read 6 channels from remote using the command pulsein(). which i guess is taking upto 20ms per command.\n\n\n\n is added and subtracted to esc speeds respectively.\n", "tags": "arduino quadcopter pid", "id": "8231", "title": "my quadcopter settling time is very large"}, {"body": "i'm interested in building a quadcopter. the result i'd like to obtain is an autonomous drone. i'd be interested in a gps to allow it to remain stationary in the air, and also to fly through checkpoints.\n\ncan this be done with a flight controller, or does it need to be programmed? i'm not too sure about what flight controllers really are.\n\ncould someone offer any materials to help me get towards this goal.\n\nthanks, jacob\n", "tags": "arduino quadcopter", "id": "8234", "title": "for a quadcopter: premade flight controller or custom made?"}, {"body": "looking to find a solution to save stills from three cameras. communication protocol can be camera link or gige, but we are looking for a lightweight solution to save stills (don't require video) as the system will be tested for a multirotor application. frames will be saved every 3 seconds so we don't require a lot of bandwidth. we don't require to do anything with the frames other than store them.\n\nthanks\n", "tags": "sensor-fusion uav embedded-systems", "id": "8239", "title": "embedded frame grabber machine vision fusion"}, {"body": "i would like to make a cartesian robot with maximum speed of up to $1ms^{-1}$ in x/y plane, acceleration $2ms^{-2}$ and accuracy at least 0.1mm. expected loads: 3kg on y axis, 4kg on x axis. expected reliability: 5000 work hours. from what i have seen in 3d printers, belt drive seems not precise enough (too much backlash), while screw drive is rather too slow. \n\nwhat other types of linear actuators are available? what is used in commercial grade robots, i.e. http://www.janomeie.com/products/desktop_robot/jr-v2000_series/index.html\n", "tags": "actuator industrial-robot cnc", "id": "8241", "title": "linear actuators in a cartesian robots"}, {"body": "this is a very basic beginner question, i know, but i am having trouble connecting to the hokuyo ust-10lx sensor and haven't really found much in terms of helpful documentation online.\n\ni tried connecting the hokuyo ust-10lx directly to the ethernet port of a lubuntu 15.04 machine. the default settings of the hokuyo ust-10lx are apparently:\nip addr: 192.168.0.10\nnetmask: 255.255.255.0\ngateway: 192.168.0.1\n\nso, i tried going to the network manager and setting ipv4 settings manually, to have the ip addr be 192.168.0.9, netmask of 255.255.255.0, and gateway to 192.168.0.1. i also have a route set up to the settings of the scanner.\n\ni then go into the terminal and run:\n\n\n\nand get this output:\n\n\n\nhow might i fix this? i figure it's just a simple misunderstanding on my end, but through all my searching i couldn't find anything to get me up and running :(\n\nthank you for the help! :)\n\nedit:\n\nhighvoltage pointed out to me that i wasn't running  which was indeed the case. i was actually running into problems before that when i still had  up, and when i tried it again, this was the output of the  command:\n\n\n\nthanks again!\n", "tags": "sensors ros rangefinder linux", "id": "8244", "title": "how to connect ethernet based hokuyo scanner?"}, {"body": "i have a dataset that contains position information from tracking a robot in the environment. the position data comes both from a very accurate optical tracking system (vicon or similar) and an imu. i need to compare both position data (either integrating the imu or differentiating the optical tracking data).\n\nthe main problem is that both systems have different reference frames, so in order to compare i first need to align both reference frames. i have found several solutions; the general problem of aligning two datasets seems to be called \"the absolute orientation problem\".\n\nmy concern is that if i use any of these methods i will get the rotation and translation that aligns both datasets minimizing the error over the whole dataset, which means that it will also compensate up to some extent for the imu's drift. but i am especially interested in getting a feeling of how much the imu drifts, so that solution does not seem to be applicable.\n\nanyone has any pointer on how to solve the absolute orientation problem when you do not want to correct for the drift?\n\nthanks\n", "tags": "sensors localization imu calibration", "id": "8245", "title": "aligning datasets with drift"}, {"body": "let us assume i have an object o with axis $x_{o}$, $y_{o}$, $z_{o}$, with different orientation from the global frame s with $x_{s}$, $y_{s}$, $z_{s}$ (i don't care about the position).\nnow i know the 3 instantaneous angular velocities of the object o with respect to the same o frame, that is $\\omega_o^o = [\\omega_{ox}^o \\omega_{oy}^o \\omega_{oz}^o]$.\nhow can i obtain this angular velocity with respect to the global frame (that is $\\omega_o^s$)?\n\nthank you!\n", "tags": "mobile-robot imu gyroscope", "id": "8247", "title": "angular velocities and rotation matrices"}, {"body": "let us assume we have a gyro that is perfectly aligned to a global frame ($x,y,z$). \n\nfrom what i know the gyro data give me the angular rate with respect to the gyro axis ($x,y,z$). so let's say i got $\\omega_x,\\omega_y,\\omega_z$. since i know that the 2 frames are perfectly aligned i perform the following operations:\n\n\n$\\theta_x = dt * \\omega_x$\n$\\theta_y = dt * \\omega_y$\n$\\theta_z = dt * \\omega_z$ \n\n\nwhere $\\theta_x$ is the rotation angle around $x$ and so on.\n\nmy question is: what is this update like in the following steps? because this time the measurement that i get are no more directly related to the global frame (rotated with respect to the gyro frame).\n\nthank you!\n", "tags": "imu gyroscope frame", "id": "8250", "title": "gyro measurement to absolute angles"}, {"body": "i'm looking for a \"good\" algorithm/model for wheeled odometry estimation. we have encoders on the two back wheels of the tricycle robot, and imu on the controller board. currently we use mems gyro for angular velocity estimation and encoders for linear velocity, then we integrate them to get the pose. but it's hard to calibrate gyro properly and it drifts (due to temperature or just imperfect initial calibration). how can we improve the pose estimation? should we consider model that incorporates both encoders and gyro for heading estimation? model slippage, sensor noise? is there some nice standard model? or should we just use more/better gyro? not considering the visual odometry.\n", "tags": "kalman-filter wheeled-robot odometry", "id": "8255", "title": "accurate wheeled robot odometry"}, {"body": "to avoid wasting your time on this question, you might only want to react on this if you have knowledge of industrial robotic arms specific.\ncommon troubleshooting is unlikely to fix this problem or could take too much time.\n\nwe've started a project with the mitsubishi melfa rv-2aj robotic arm.\neverything went fine until the moment we replaced the batteries.\n\nthe controller displays: \"fail\" and does not respond to any buttons or commands sent through serial connection.\n\nwe did replace the batteries of both the robot and the controller. as it took some time to get the batteries delivered, we've left the robot (and controller) withouth power for the weekend. (which might have caused this problem)\n\nis there anyone with knowledge of mitsubishi robotic arms around here?\ni'm kinda hoping it would be a common problem/mistake and anyone with experience on this subject would know about it?\n", "tags": "robotic-arm", "id": "8261", "title": "robotic arm [\"fail\"] error display. - festo / mitsubishi melfa rv-2aj (controller cr1-571)"}, {"body": "i am working on a project where i want to run some computer vision algorithms (e.g. face recognition) on the live video stream coming from a flying drone. \n\nthere are many commercial drones out there that offer video streams, like\n\n\nhttp://www.flyzano.com/mens/\nhttps://www.lily.camera/\netc..\n\n\nbut none of them seem to give access to the video feed for real-time processing. \n\nanother idea is to have the drone carry a smartphone, and do the processing on the phone through the phone's camera. or just use a digital camera and an arduino that are attached to the drone. \n\nalthough these ideas are feasible, i would rather access the video-feed of the drone itself. so my question is that are there any drones out there that offer this feature? or can be hacked somehow to achieve this? \n", "tags": "computer-vision cameras", "id": "8262", "title": "real-time video processing on video feed from a drone's camera"}, {"body": "i am planning to build a homemade rov, and i wanted to know a couple of things about the motors.  first is: will it be ok, if i use a brushed dc motor,  instead of a brushless motor, and is there any major disadvantages ? second : what rpm dc motor should i aim for ? high rpm or low rpm ? will 600rpm be enough ? the specific motor that i am talking about is    http://www.ebay.ca/itm/37mm-12v-dc-600rpm-replacement-torque-gear-box-motor-new-/320984491847?hash=item4abc2aa747:m:mebeqxxpqmng4-vxmfazp5w\n\nwill this be a good motor for the propellers of the rov.  i am planning to have 4 motors / propellers. two for upward and downward thrusting,  and 2 for forward and side thrusting. the propellers that i plan to use, are basic plastic 3 blade propellers,  with diameter,  between 40mm and 50mm. \n\nmy main question is, what rpm and torque should i aim for when choosing the dc motor ?\n", "tags": "motor", "id": "8271", "title": "dc motors for a rov?"}, {"body": "i do have a robotic application, where a 7dof robot arm is mounted on a omnidirectional mobile platform. my overall goal is to get moveit! to calculate a sequence of joint movements, such that the robot eef reaches a desired goal in cartesian space.\n\nin order to combine a robot platform with a world, the moveit! setup assistant lets you assign virtual joints between the \"footprint\" of the platform and the world it is placed in.\n\ni do have two strategies. either \n\n\nselect a planar joint as a virtual joint. (what are the degrees of freedom or respectively the joint information that i can gather from this joint)\n\n\nor \n\n\nselect a fixed joint and add a (prismatic-x -> prismatic-y -> revolute-z) chain to the robot model.\n\n\nare there any significant differences (advantages/ disadvantages) to either of the approaches?\n", "tags": "mobile-robot robotic-arm ros motion-planning", "id": "8272", "title": "ros moveit!, virtual joints, planar joints, prismatic joints"}, {"body": "i'm using an accelerometer and gyroscope to detect the angle and tilt rate on my two-wheeled cart-pole robot.  is there an optimal height to place the sensors?  should i place them closer to the bottom (near the wheels), the middle (near the center of mass), or the top?  justification for the optimal choice would be appreciated.\n", "tags": "sensors balance", "id": "8273", "title": "where should i put the angle sensor on my cart-pole robot?"}, {"body": "i'm working on modeling and simulation of robotic arm, after i obtained the mathematical model of the robot, i used that to implement some control techniques, to control the motion of the robot. the dimensions and masses of each links are taken from available kit, basically, it's ra02 robot with servo at each joint. after the modeling, different parameters, can be plotted: like the joint angles\\speeds\\torques ... etc. the point now is that, the value obtained for the joint toque is much more higher that the torque limit of the servo, does it mean my design\\modeling is not realizable? is it necessarily to get close (torque)  value for servo's torque? \n\nany suggestion?\n", "tags": "control robotic-arm servomotor dynamics torque", "id": "8276", "title": "should the therotical parameters match the physical setup constraints when modeling a robot?"}, {"body": "the rest of my student team and i are in the process of redesigning an exoskeleton and building it based on an existing one. from the papers that we have been reading there are some references to low, high and zero impedance torque bandwith.\n\nwhat is that? does it have to do with the control system?\n\nit is measured in hz. here is a table from one of the papers:\n\n\n", "tags": "control design mechanism joint", "id": "8277", "title": "what is torque bandwidth in actuated joints and how does it affect the control systems?"}, {"body": "i found not so much literature to the topic, this is why i ask here.\ndoes someone know some ways to estimate the drift rate of the gyrometer.\ni was thinking about basically two approaches. \none would be to use a low pass filter with a low cut-off frequency to estimate the drift of the angular velocity.\nsecond would be to use the accelerometer, calculate the attitude dcm and by this also the angular velocity. the difference between the acc angular velocity and gyrometer would be maybe also a drift rate.\nnevertheless, i am not so sure whether this is a good way to get reliable drift rates :d\n", "tags": "sensors gyroscope", "id": "8279", "title": "ways to estimate the drift rate of the gyrometer"}, {"body": "is it possible to set up communication between an arduino uno and an android phone using a wire that directly connects the android phone and the arduino?\n", "tags": "arduino usb", "id": "8286", "title": "how to connect an arduino uno to an android phone via usb cable?"}, {"body": "i have a 1inch square tube that i would like to place a motor into. \n\nthe motor i have takes up approximately 1/2 of the available space (roughly 3/4 inch i.d.) i would like to find the largest motor that will fit in the space without having to cobble too much of a housing. \n\nwhere/how can i find motors by physical dimensions?\n", "tags": "motor", "id": "8288", "title": "sourcing motors by physical dimensions"}, {"body": "i have a crock pot with an analog knob and would like to find a way to turn the knob by using and appliance timer. i have no idea where to begin. i need help.\nthanks\n", "tags": "control sensors stepper-motor", "id": "8289", "title": "crock pot knob turner"}, {"body": "currently i am developing a control system for an aircraft of a unique design (something in between a helicopter and a dirigible). at this moment i can model only the dynamics of this vehicle without any aerodynamic effects taken into account. for this i use the following work-flow:\n\nmechanical model in solidworks -> msc adams (dynamics) &lt;--> matlab/simulink (control algorithms)\n\nthus, the dynamics of the vehicle is modeled in adams and all control algorithms are in matlab/simulink. unfortunately, adams can not simulate any aerodynamic effects. as a result, i can not design a control system that is capable to fight even small wind disturbances.\n", "tags": "control simulator uav matlab simulation", "id": "8292", "title": "are there any aerodynamics modeling/simulation software that is capable to consume a solidworks model and to interface with matlab/simulink?"}, {"body": "i know how to make a line follower. but in this video what have they done exactly? they are giving the source and destination in the map but how the robot moves based on the instruction given in map?\n\nwhat is the procedure to do it? they have mapped the path. please do watch the video.\n", "tags": "wheeled-robot mapping line-following", "id": "8293", "title": "advanced line following robot of maze solving"}, {"body": "i want a mobile robot to go from a starting position to a goal position. but, i don't want to calculate the pose from encoders. instead i want to know if there exist such a simulator that provides pose function that makes the work easier, like go_to(x_coordinate, y_coordinate). that means, the robot will automatically calculate its current position and leads itself to the goal position. \n", "tags": "mobile-robot odometry simulation", "id": "8294", "title": "need a mobile robot simulator that provides easier odometry funtions"}, {"body": "i am planning to use matlab and gazebo for one of my course projects. however all the tutorials i have seen till now use gazebo by using a virtual machine which has ros and gazebo installed. i have already installed ros and gazebo on this machine (os ubuntu). i also have matlab installed on it. is it possible to use the gazebo on this machine itself with the matlab toolbox? thanks.\n", "tags": "ros matlab gazebo", "id": "8296", "title": "using gazebo installed on same machine in matlab"}, {"body": "what are the main differences between motion planning and path planning?\nimagine that the objective of the algorithm is to find a path between the humanoid soccer playing robot and the ball which should be as short as possible and yet satisfying the specified safety in the path in terms of the distance from the obstacles. \n\nwhich is the better terminology? motion planning or path planning?\n", "tags": "mobile-robot motion-planning humanoid", "id": "8302", "title": "what is the difference between path planning and motion planning?"}, {"body": "i am currently planning on building a robotic arm. the arm's specs are as follows: \n\n\n3 'arms' with two servos each (to move the next arm)\nsingle servo clamp\nmounted on revolving turntable\nturntable rotated by stepper motor\nturntable mounted on baseplate by ball bearings to allow rotation\nbaseplate mounted on caterpillar track chassis\nbaseplate is smaller in length and width than caterpillar chassis\n\n\nwhat are the required formulas in determining how much torque each servo must produce, keeping in mind that the arm must be able to lift weights of up to 1 kilogram? also, considering that the ball bearings will take the load of the arm, how strong does the stepper have to be (just formulas, no answers)?\n\nas far as overall dimensions are concerned, the entire assembly will be roughly 255mm x 205mm x 205mm (l x w x h). i have not finalized arm length, but the aforementioned dimensions give a general estimate as to the size. \n", "tags": "arduino robotic-arm stepper-motor rcservo torque", "id": "8310", "title": "what is the required theory behind building a robotic arm?"}, {"body": "as someone who is new and is still learning about robotics, i hope you can help me out.\n\nlet's say i have two systems: \n\n\n(a) inverted pendulum (unstable system)\n(b) pole climbing robot (stable system)\n\n\nfor system (a), i would say that generally, it is a more dynamic system that produces fast motion. so, in order to effectively control it, i would have to derive the equations of motions (eom) and only then i can supply the sufficient input to achieve the desired output. eventually, the program will implement the eom which enables the microcontroller to produce the right signal to get the desired output.\n\nhowever for system (b), i assume that it is a stable system. instead of deriving the eom, why cant i just rely on the sensor to determine whether the output produced is exactly what i want to achieve? \n\nfor unstable system, controlling it is just difficult and moreover, it does not tolerate erratic behavior well. the system will get damaged, as a consequence. \n\non the contrary, stable system is more tolerant towards unpredictable behavior since it is in fact stable.\n\nam i right to think about it from this perspective? what exactly is the need for deriving the eom of systems (a) and (b) above? what are the advantages?  how does it affect the programming of such systems?\n\nedited:\nsome examples of the climbing robot that i'm talking about: \n\n\ni.ytimg.com/vi/gf7hibl5m2u/hqdefault.jpg\nece.ubc.ca/~baghani/academics/project_photos/utpcr.jpg\n\n", "tags": "mobile-robot control", "id": "8313", "title": "is modelling a robot and deriving its equations of motions more applicable to a system that is inherently unstable?"}, {"body": "i would like to filter angular velocity data from a \"cheap\" gyroscope (60$). these values are used as an input of a nonlinear controller in a quadcopter application. i am not interested in removing the bias from the readings.\n\nedit:\ni'm using a\n l2g4200d gyroscope connected via i2c with an arduino uno. the following samples are acquired with the arduino, sent via serial and plotted using matlab.\n\nwhen the sensor is steady, the plot shows several undesired spikes.\n\n\n\nhow can i filter these spikes?\n\n1st approach: spikes are attenuated but still present...\n\nlet's consider the following samples in which a couple of fast rotations are performed. let's assume that the frequency components of the \"fast movement\" are the ones i will deal with in the final application.\n\n\n\nbelow, the discrete fourier transform of the signal in a normalized frequency scale and the second order butterworth low pass filter.\n\n\n\nwith this filter, the main components of the signal are preserved. \n\n\n\nalthough the undesired spikes are attenuated by a factor of three the plot shows a slight phase shift...\n\n\n\nand the spikes are still present. how can i improve this result?\nthanks.\n\nedit 2:\n\n1./2. i am using a breakout board from sparkfun. you can find the circuit with the arduino and the gyro in this post: can you roll with a l3g4200d gyroscope, arduino and matlab? \n    i have added pullup resistors to the circuit. i would exclude this option because other sensors are connected via the i2c interface and they are working correctly.\n    i haven't any decoupling capacitors installed near the integrated circuit of the gyro. the breakout board i'm using has them (0.1 uf). please check the left side of the schematic below, maybe i am wrong.\n\n\n\nmotors have a separate circuit and i have soldered all the components on a protoboard.\n\n\nthe gyro is in the quadcopter body but during the test the motors were turned off.\nthat is interesting. the sampling frequency used in the test was 200hz.  increasing the update freq from 200 to 400 hz doubled the glitching.\n\n\ni found other comments on the web about the same breakout board and topic. open the comments at the bottom of the page and ctrl-f \n", "tags": "quadcopter gyroscope filter", "id": "8319", "title": "filtering angular velocity spikes of a cheap gyroscope"}, {"body": "first off, sorry if my question is too naive or not related to the forum (this is the best matching one i've found on stackexchange).\n\ni have some amount of sim-cards. i can programmatically access a single sim-card if it is inserted into a usb-modem. i want to be able to access the specified card in the set. the best way to achieve this i can think of is to create a device that would somehow replace the current card in the modem with one in the set. i can not use several modems for this because i don't really know the amount of cards and i would like to automate this process anyway.\n\ni am more of a programmer than an engineer so everything that follows (including the entire concept of switching cards) looks pretty weird to me. there probably is a better solution, but this is the best i've come up with. for now i consider building some sort of conveyor that would move cards and insert the ones i need with some sort of a feed device. this looks like an overkill to me that would be both expensive to build and uneffective to work with.\n\ni want an idea of a device that would replace sim-cards into the modem (or maybe a better solution to the problem). any disassembly of a modem needed is possible.\n\nthis is required to automate receiving sms from clients that have different contact phones. unfortunately, a simple redirection of sms is not an option.\n", "tags": "automatic automation", "id": "8322", "title": "need an idea: automated sim card switcher"}, {"body": "i have the following system here:\n\nhttp://imgur.com/utqswoi\n\n\nbasically, i have a range finder which gives me $r_s$ in this 2d model. i also have the model rotate about the centre of mass, where i have angular values and velocities beta ($\\beta$) and betadot ($\\dot{\\beta}$).\n\ni can't see, for the life of me, how to figure the formula for the angular velocity in the range finder frame. how am i supposed to do this? i have all the values listed in those variables. the object there doesn't move when the vehicle/system pitches. it's stationary.\n", "tags": "robotic-arm sensor-fusion", "id": "8327", "title": "transforming angular velocity?"}, {"body": "i try to measure euler angles from an imu, but some discontinuities happens during measurement, even in vibrationless environment, as shown in the images below. \n\ncan someone explain which type of filter will be the best choice to filter this type discontinuities?\n\n\n\n\n\n", "tags": "sensors imu matlab noise filter", "id": "8329", "title": "filtering imu angle discontinuities"}, {"body": "i am a beginner at robotics. \ni recently stumbled across this robotic clock on youtube.\n\ni am an electrical engineering student and am interested in submitting it as my minor project.\n\ni have studied the basics on forward and inverse kinematics, greubler's equation, four bar linkage but this robot seems to be a 5 bar linkage. i want to know how to implement it in a 5 bar linkage.\n\nhow to use the inverse kinematics solutions described in combined synthesis of five-bar linkages and non-circular gears for precise path\ngeneration, to make the robot follow desired trajectory?\n\ni have been stuck at this for days... any sort of help would be appreciated.\n", "tags": "robotic-arm beginner first-robotics", "id": "8331", "title": "designing a 5 bar linkage robot: plot clock"}, {"body": "i'm given an assignment in which i have to design a full state feedback controller by pole placement. the state space system is fully controllable and i've been using matlab/simulink to determine the required feedback gain k using the place() command for several sets of poles, however once i use poles that are \"too negative\", for example p=[-100,-200,-300,-400,-500], my controlled system starts showing bounded oscillatory behaviour. \n\nis it possible that too negative poles can cause marginal stability? and if so, why? i've read that this is only possible when the real part of one or more poles equals 0, which certainly isn't the case here. \n", "tags": "control stability", "id": "8333", "title": "relation between pole placement and marginal stability?"}, {"body": "i am trying to establish the fri connection for kuka lbr iiwa. i know how to configure the fri connection as there are example programs available in the sunrise.workbench. \n\na sample code is given below. my question is 'how to pass' the joint torque values (or joint position or wrench) to the controller using 'torqueoverlay' as mentioned in the code below. since i could not find any documentation on this, it was quite difficult to figure out. any sample code with explanation or any clues would be more than helpful.   \n\njava code:\n\n\n", "tags": "robotic-arm programming-languages", "id": "8337", "title": "kuka fri program using java"}, {"body": "i am trying to calculate the thrust my 4 quadcopter motors will have.\ni am not sure how to do it. here are the parts i am using\n\n4s 6600mah 14.8v lipo pack \n\n15x5.5 prop\n\n274kv motor max output is 28a\n\nesc 35 amp\n\nthank you\n", "tags": "quadcopter", "id": "8340", "title": "trying to calculate the thrust of my quadcopter motors"}, {"body": "when i send several commands in a row some don't get executed. for example i have a script which starts the roomba driving in a circle and plays the john cena theme song through its speakers but sometimes it will only play the music and not drive. i have noticed that in all the guides there are pauses after every command. is there any documentation which describes when pauses are needed?\n", "tags": "irobot-create", "id": "8343", "title": "how much of a pause should there be between messages? (irobot create-2)"}, {"body": "so i am building a quadcopter and i already have the frame which is about 39 inches(3.25 feet) and 680 grams. i want to run these multistar elite 5010-274kv multi-rotor motor. \n\n\nkv(rpm/v): 274kv\nlipo cells:  6~8s  \nmax current: 630w\nmax amps:  28a\nno load current: 0.43a/10v\nweight: 211g\n\n\ni want use a multistar high capacity 4s 6600mah multi-rotor lipo pack\n\n\nminimum capacity: 6600mah\nconfiguration: 4s1p / 14.8v / 4cell\nconstant discharge: 10c\npeak discharge (10sec): 20c\npack weight: 537g\n\n\nthe props i am thinking of using are 15x5.5props. so if anyone knows what is wrong with this setup, or if it will work, or maybe even have some tips and pointers for me since i am a beginner drone builder and i really don't want to spend the $1,044 it will take to build it and have it not work so any advice will be appreciated.\n\ntotal estimated weight = 1,919 grams(4.23 pounds)\n\nalso here is a chart for the motor that my be helpful. the chart is near the price under the files tab\n\nmultistar elite 5010-274kv multi-rotor motor\n\nthanks in advance\n", "tags": "battery multi-rotor", "id": "8345", "title": "trying to figure out what parts to buy for my quadcopter"}, {"body": "i have a stm32f072rb nucleo board which has a 64pin microcontroller.\n\nfor my application i chose the stm32f103rg which has a bigger ram size and flash size too.\n\ncan i remove an f072r from a nucleo board put a f103r on top of it?\n\ni am testing my code with a f103c, but the flash and ram size is not meeting my requirement. i have a f072r nucleo board lying around so for a quick developmental test could i swap it for the 103r ? the r series is pin compatible!\n\nanyone has done microcontroller swapping before? \n", "tags": "microcontroller", "id": "8348", "title": "changing stm32 nucleo board's microcontroller"}, {"body": "i am currently interested in scara arm designs and i have few, beginner questions for which i didn't find answers yet. \n\n1/ while comparing professional arms (made by epson, staubli...) i noticed that the actuator used for the translation on the z axis is at the end of the arm. on \"hobby\" arms like the makerarm project on kickstarter they use a leadscrew with the actuator at the beginning of the arm.\ni thought it was smarter to put the actuator handling this dof at the begining of the arm (because of its weight) and not at the end, but i assume that these companies have more experience than the company behind the makerarm. so i'm probably wrong, but i would like to understand why :)\n\n2/ also i would like to understand what kind of actuators are used in these arms. the flx.arm (also a kickstarter project) seems to be using stepper motors but they also say they are using closed loop control, so they added an encoder on the stepper motors right?\n\nwouldn't it be better to not use stepper and, for instance, use dc brushless motors or servos instead ?\n\n3/ i also saw some of these arms using belts for the 2nd z axis rotation, what is the advantage ? it only allows to put the actuator at the begining of the arm ?\n", "tags": "robotic-arm design actuator", "id": "8350", "title": "differences between scara arm design"}, {"body": "i'm trying to make a quadcopter with arduino.\ni already have the angles (roll pitch and yaw) thanks to an imu. they are in degrees and filtered with a complementary filter.\ni want to apply a pid algorithm for each axis but i dont know if the inputs should be angles (degrees) or angular velocities in degrees per second so as to calculate the errors with respect referencies.\nwhich will be the difference? which will be the best way?\u00a0\n\nfinally, another question about a pid code: i have seen that many people don't include time in their codes. for example, their derivative term is kd\u00d7(last error-actual error) instead kd\u00d7(last error-actual error)/looptime and something similar with the integrative term. which is the difference?\n\nthank you in advanced.\n", "tags": "quadcopter pid", "id": "8354", "title": "need help for a quadcopter pid"}, {"body": "i am using arduino and l298n motor driving ic to drive 4 12v dc motors (150rpm).\nalso i am using 11.1v lipo battery (3cell, 3300 mah, 20c).i have connected two pwm pins of l298n to digital high from arduino.battery positive terminal is connected to the 12v input of ic.battey negative terminal and arduino ground is connected to the ground input of ic.also a 5v input is given from arduino to ic and ground from arduino is connected to other gnd pin which is adjacent to int3 pin.motor1 pins from l298n are connected with two motors (connected parallely on right side of bot) and motor2 pins are connected with other two motors (connected parallely on left side of bot).appropriate inputs are given to int1,int2,int3,int4 to drive the bot in forward direction.but the bot is moving too slowly.the voltage measured across motor1 pins is only 5v.i have connected the battery directly to the motors,then it is running very fast.how to run it fast.please help.....\n\n\n", "tags": "arduino motor", "id": "8356", "title": "very low output voltage at the output of l298n?"}, {"body": "i am participating in a robotics competition. i am supposed to design and build two robots. out of these, one cannot have a driving actuator (it can have a steering actuator though, fed by a line following circuit). the other is supposed to drive the non-driving robot through an obstacle course, without touching it. this is kind of driving me crazy since at one point the separation between the two robots is 60 cm (23 inches).\n\nways i've considered:\n\n\nwind energy (wont work, need huge sails)\nmagnetic repulsion of some sort\n\n\nnow repulsion i've spent a lot of time studying. \n\nmy solution was to use strong permanent magnets on the non-driving robot (neodymium,n52) and electromagnets on the driving robot.\n\nbut, after doing a huge load of calculations came to the conclusion that not enough force can be transmitted over the distance as magnetic fields fall off too quick.\n\nrulebook: http://ultimatist.com/video/rulebook2016_final_website_1_sep_15.zip\n\ni am really looking for even a pointer here. is there a trick somewhere that i am missing?\n", "tags": "line-following", "id": "8357", "title": "how to drive robot without driving actuator?"}, {"body": "i have a 6 dof arm whose velocities i'm controlling as a function of force applied to the end effector. the software for the robot allows me to input either the desired end effector velocity or the desired joint angular velocities, which i know can be found using the inverse jacobian.\n\nare there any benefits of using one scheme over the other? would, for example, one help avoid singularities better? does one lead to more accurate control than the other does?  \n", "tags": "robotic-arm jacobian force-sensor", "id": "8361", "title": "6dof robot arm: velocity of end effector vs. joint velocities"}, {"body": "this might be a dumb question. i have started to play with this robot with raspberry pi two days ago. i did some simple stuff, like- move around and sensor reading etc. but since yesterday night, it seems like i cannot send any command. the built in clean, dock functions are working perfectly but i cannot do anything using the same python code that i already used before. its behaving like nothing is going through the rx.\n\ncan you suggest what might go wrong? thanks\n", "tags": "irobot-create", "id": "8363", "title": "cannot command irobot create 2"}, {"body": "im working on a robot that needs image processing to analyze data recieves from cameras. \n\nas i searched for arm and avr libraries i found that there is no dip library for these micros and their limited ram is hard for image data process. i want to know is there any hardware that connects to a win or android or... devices and make it possible to that device connect to actuators and sensors?\n\nthank you or helping.\n", "tags": "microcontroller", "id": "8365", "title": "using a device with os instead of microcontrollers"}, {"body": "i am trying to read navdata from ardrone using following callback function,\n\n\n\nbut this is always returing 0 value, but when i echo navdata in linux terminal, these topics are publishing non-zero value. where is the error?\n\nedit: after some debugging i found that this callback function is not being called at all. i wrote my subscriber node like this ros::subscriber sub = ardrone.subscribe(\"/ardrone/navdata\",1,callback); what is going wrong here?\n", "tags": "quadcopter", "id": "8367", "title": "ardrone navdata reading error"}, {"body": "given three sets of joint angles in which the end effector is in the same position, is it possible to find the dh parameters?\n\nif the robot has 2 dof in shoulder, 2 dof in elbow, and 1 dof in wrist, with dh parameters as upper arm length, elbow offset in 1 axis, lower arm length, can this be solved, if so how?\n\ni tried iterating through dh parameters to minimize position of end effector with forward kinematics, but this doesnt seem to work as dh parameters of 0 for everything makes 0 minimal distance.\n\nreason for this; given a physical robot, no dh parameters are known, and measuring by hand is not accurate.\n", "tags": "dh-parameters", "id": "8368", "title": "solving for dh parameters"}, {"body": "i have arduino talk with create 2 via serial interface. but before sending commands to the robot, i have to power it on by manually pushing the power button on the robot. how to make the robot turned on via that mini din 7 port, instead of pushing that power button? \ni notice when plugin irobot serial-2-usb cable into that port on the robot, the robot is immediately turned on, ready for received the first command (command 128), so apparently there is way to turn on the robot via that port.\n", "tags": "irobot-create", "id": "8369", "title": "power on irobot create 2 via serial port"}, {"body": "i have 28  pmac motors (3-ph, 230 volt of 0.5 kw, 1.05kw an 1.21kw) in motor control center. please suggest a time staggered switching scheme in order to avoid tripping due to voltage sag, swell , flicker etc\n", "tags": "power", "id": "8371", "title": "switching scheme for vector controlled pmac drive"}, {"body": "can anybody help figure out hd parameters for the case where two links with a revolute joint are in the same plane, thus that the variable angle is 0, but the twist is not 0. this is a simple drawing. i think that x-axis that is perpendicular to both z-axis, points away and goes through the intercection of z-axis. the link length is 0, the twist is a and the offset is d. whould it be correct?\nthanks.\n\n\n\n", "tags": "dh-parameters", "id": "8372", "title": "denavit hartenberg parameters"}, {"body": "as i dont know that what kind of radiation animals emit. as humans emit ir radiations so pir sensors help to identify humans. pls suggest me if someone have knowledge about sensors which detects animals.\n", "tags": "arduino raspberry-pi first-robotics", "id": "8375", "title": "can we detects animals through pir(passive infrared sensors)"}, {"body": "i have a quadcopter controlled through kk2.1.5 flight controller. i have been flying with it without problems, but now i am facing a problem. when i start and arm the kk2.1.5, by giving throttle it starts turning towards some direction with acceleration. i double checked the motor pins locations and all other things, they are correct. when i took a look to the gyro bubble of kk2.1.5 it wasnt at mid of the crosshair. i turned the quad off and then on. i check bubble again it was at centre. now again when i gave it throttle it started turning towards some direction. i checked the again, and it wasnt on centre this time too. so at the armed state the gyro bubble moves away from centre by giving throttle. due to which quad overcorrects itself. now i have understood that the gyro is off centre due to vibration  of the fc. what should i do to antivibrate it. what material should i keep in between so that vibrations are almost zero.\n", "tags": "quadcopter", "id": "8376", "title": "kk2.1.5 gyro bubble is not at centre"}, {"body": "in most papers about ibvs the camera velocity is computed and then used as a pseudo-input for the manipulator. (e.g. this one) is there any work in which the dynamic lagrange model $h(q) \\ddot q +c(q,\\dot q)\\dot q+g(q)=\\tau$ of the manipulator is taken into consideration in order to compute the torque required to move the joints accordingly?\n", "tags": "dynamics torque visual-servoing", "id": "8378", "title": "torque control in eye-in-hand visual servoing"}, {"body": "i tried this coding and its working. \n\n\n\nbut the problem is that if the above coding is placed inside a loop such as 'if loop', its not working. can anyone help me in figuring out this problem. i am using accelstepper library for the above coding.\n\n\n", "tags": "arduino stepper-motor", "id": "8382", "title": "how to make a stepper motor to rotate and come to a position of certain degress (say for 90 degrees) from any initial position?"}, {"body": "i'm trying to implement direct-multiple shooting method to my problem.\n\n\n\nas i understand from the theory, i have to divide the variables as state variables and control variables.\n\n\n\ncould you help me how i will implement it? i don't understand what is the ode here and how i should construct the algorithm?\n\nare there any example about it?\n\nedit to make the equations clear i'll rewrite them here again:\n\nbased on the link\n\nstate variables:\n and .  and derivatives of the state variables are equal to  where f is\n \n\ni don't know how to insert cubic polynomials in that equation system and how to solve ode will it be like  \n", "tags": "manipulator", "id": "8387", "title": "optimal trajectory for manipulators using optimal control"}, {"body": "i am currently trying to implement an inverse kinematics solver for baxter's arm using only 3 pitch dof (that is why the ygoal value is redundant, that is the axis of revolution). i for the most part copied the slide pseudocode at page 26 of http://graphics.cs.cmu.edu/nsp/course/15-464/fall09/handouts/ik.pdf .\n\n\n\nhere is my logic in writing this:\ni first calculated the jacobian 3x3 matrix by taking the derivative of each equation seen in the forwardkinematics method, arriving at:\n\n[370cos(theta1) + 374cos(theta1+theta2) .....   \n\n0                                               0                      0\n\n-370sin(theta1)-374sin(theta1+theta2)-......                            ]\n\nin order to arrive at numerical values, i inputted a delta theta change for theta1,2 and 3 of 0.1 radians. i arrived at a jacobian of numbers:\n\n[0.954  0.586   .219\n\n0.0000          0.000         0.0000\n\n-.178   -.142   -0.0678]\n\ni then input this matrix into a pseudoinverse solver, and came up with the values you see in the invjacob matrix in the code i posted. i then multiplied this by the difference between the goal and where the end effector is currently at. i then applied a tenth of this value into each of the joints, to make small steps toward the goal. however, this just goes into an infinite loop and my numbers are way off what they should be. where did i go wrong? is a complete rewrite of this implementation necessary? thank you for all your help.\n", "tags": "inverse-kinematics python joint jacobian", "id": "8389", "title": "3 dof inverse kinematics implementation: what's wrong with my code?"}, {"body": "i am having a thesis right now regarding a robot. my research requires the robot to be attached to linear guide rail. a robot has to detect human in a very close range (of about 2 meters distance). what easiest and efficient method or components shall i use?\n", "tags": "sensors wheeled-robot industrial-robot", "id": "8391", "title": "what is the easiest and efficient way to detect human in close range distance and make the robot follow it?"}, {"body": "i am using this sensor to make self balancing robot.at first i have soldered the header(only to vcc,gnd,scl,sda ) on the imu borad at the opposite side where there is no component mounted.then connecting it to arduino uno r3(vcc to vcc 3.3v/5v,gnd to 1 of 3 gnd,scl to scl and sda to sda(first time at those next to aref, second time a5,a4) ) i uploaded the sketch https://github.com/adafruit/adafruit_adxl345/blob/master/examples/sensortest/sensortest.pde then when i opened the serial monitor i got\n\n\n  accelerometer test\n  \n  ff ooops, no adxl345 detected ... check your wiring!\n\n\ni thought may be i have soldered the header in wrong direction(as in picture and videos at internet,they are so) so i desolder(with solder iron,no other technique) the header,but there were still some solder around the hole which i could not remove.then while checking the continuity between pins with multimiter(in resistance mode) i found the resistance to be 20k(scl-sda),220k(scl-gnd),220k(sda-gnd),between vcc and 3 other pins multimieter shows 1(range 2000k). then i soldered it on opposit side(this time where other components are mounted).the serial monitor still shows same output,and so does the muiltimeter.so where is the problem? is it with soldering ?do i need to disolder the header again and clean left out solder(with chip quik type desoldering technique ) on the opposite side(no component mounted)?is there any hope that i won't need to buy it again?\n\npicture of opposite side where no component is mounted and this is after desoldering and resoldering\n", "tags": "imu accelerometer gyroscope", "id": "8394", "title": "connecting mpu-9250 gy-9250 sensor module to arduino uno"}, {"body": "i am facing problems in updating my kk2 board. i have used usbasp header for connecting it to the pc, and kk2firmware software. but it fails. it says not valid vid and pid values etc. please help me. if any idea on updating firmware other than this. i have used usbasp header connecting kk2 board to pc.\n", "tags": "arduino", "id": "8399", "title": "updating firmware of kk2"}, {"body": "i've recently been trying to use gazebo to do some modelling for a couple tasks. i have a robot that's effectively able to locate a ball and get x,y coordinates in terms of pixels using a simple rgb camera from the kinect. i also have a point cloud generated from the same kinect, where i hope to find the depth perception of the ball using the x,y coords sent from the circle recognition from my rgb camera. my plan earlier was to convert the x,y coordinates from the rgb camera into meters using the dpi of the kinect, but i can't find any info on it. it's much, much harder to do object recognition using a point cloud, so i'm hoping i can stick to using an rgb camera to do the recognition considering it's just a simple hough transform. does anybody have any pointers for me?\n", "tags": "localization computer-vision kinect cameras gazebo", "id": "8400", "title": "using an rgb + depth camera to locate x,y,z coordinates of a ball"}, {"body": "how would you vertically tilt a camera 180 degrees using mirrors?\n\ni'm trying to add a pan/tilt mechanism to a raspberry pi's camera. the camera uses one of those flat cables with unstranded wires, and even with a strain gauge, i don't trust it to handle repeated bending, so i'm trying to design a tilt mechanism that allows the camera to be rigidly mounted so no wires move. the tilting also has to happen very quickly, so i'm trying to minimize the amount of mass i need to move.\n\nthen i saw the oculus kit that actuates a mirror to effectively tilt a laptop's fixed webcam. i'm trying to extend this idea, but i having trouble working out the mechanics that would allow the tilt to extend to 180 degrees. the layout in the oculus's mechanism only supports a tilt angle of about 90 degrees, and the mirrors are relatively large. is it possible to modify this to support 180 degrees?\n\nare there other ways to \"bend\" the view of a camera without having to move the actual camera?\n", "tags": "mechanism cameras", "id": "8403", "title": "how to tilt a camera 180 using mirrors"}, {"body": "can we use wifi direct feature of samsung ip cameras to connect them directly to laptop wifi adapter without the need of any additional router? i am working on an opencv project and i want to read stream from 3 cameras simultaneously, so i was thinking that i could connect 3 usb wifi adapter to my laptop and connect them directly with cameras. is this scenario possible?\n", "tags": "opencv", "id": "8406", "title": "samsung ip camera, wifi direct feature"}, {"body": "can a one propeller drone work efficiently for a good flight and stable camera footage in a drone flight\n", "tags": "quadcopter", "id": "8407", "title": "one propeller drone?how well it works?hope"}, {"body": "i would like to clarify my self on singularity configurations. if i am moving the robot in joint space only one joint at a time, can i come to a singular configuration? if so how?\nthanks\n", "tags": "joint jacobian", "id": "8411", "title": "joint space singularities"}, {"body": "for a mod on the dynamixel rx-24f i need to remove the enclosed pcb. i removed all screws but the pcb doesn't come out easily (without applying more force than i'm comfortable with). it seems to be stuck on the three large solder points in the white area. has someone experience with this particular servo? \n\nit might be glued/soldered to the case, but i'm not quite certain. any help is appreciated.\n\n\n", "tags": "dynamixel", "id": "8417", "title": "removing pcb from a dynamixel rx-24f servo?"}, {"body": "how can i compute the relative pose between two robots?\n\ni have these robots (matlab code):\n\n\n\nand the corresponding covariance matrix of the probability distribution of each one (the movement of each robot is asociate with a gaussian distribution)\n\n\n\nauxiliar functions to solve\n\nfunction to compose two poses ([x ; y ; theta])\n\n\n\nfunction to get the inverse pose of a pose\n\n\n\nsolution:\n\nto get the relative pose, we need to compute two things:\n\n\nthe pose to be composed with poser1 to get poser2.   it means that we need to get the \"pose_inc\" that satisfy with this equality\n\n\n\nthe next code compute that relative pose that allows to poser1 reach poser2\n\n\nthe covariance matrix of the relative pose. the uncertainty of r1 after reach the position poser2\n\n\n\n\nroughly,\n\n\n\n\n\nc_p12 is the covariance matrix of r1 after move to poser2 \n", "tags": "sensors movement pose first-robotics", "id": "8418", "title": "how to compute the relative pose between two robots?"}, {"body": "i asked a question similar to this earlier, but i believe i have a new problem. i've been working on figuring out the inverse kinematics given an x,y,z coordinate. i've adopted the jacobian method, taking the derivative of the forward kinematics equations with respect to their angles and input it into the jacobian. i then take the inverse of it and multiply it by a step towards the goal distance. for more details, look at http://www.seas.upenn.edu/~meam520/notes02/introrobotkinematics5.pdf page 21 onwards. \n\nfor a better picture, below is something:\n\n\nbelow is the code for my matlab script, which runs flawlessly and gives a solution in under 2 seconds:\n\n\n\nbelow is my python code, which goes into an infinite loop and gives no results. i've looked over the differences between it and the matlab code, and they look the exact same to me. i have no clue what is wrong. i would be forever grateful if somebody could take a look and point it out.\n\n\n", "tags": "kinematics inverse-kinematics matlab python jacobian", "id": "8419", "title": "running my 3 dof inverse kinematics code: works in matlab, not in python"}, {"body": "i have two servo motors that i rigged up to use as a telescope remote focuser. the idea is to turn one servo by hand and use the power generated to turn the other, which is geared to a telescope focuser knob. i noticed that when the two servos are electrically connected, it is noticeably harder to turn a servo compared to turning it by itself. i tried changing the polarity of the connection hoping it would help, but it is still harder to turn the servo when they are connected. does anyone know why this is?\n", "tags": "servos servomotor", "id": "8420", "title": "why does it require more force to turn a servo if it is electronically connected to another servo?"}, {"body": "i want to turn some motors using my raspberry pi. i am able to turn an led on and off using the 3.3v gpio pin. for the motors, i tried using a l293d chip as per the instructions on this link.\n\nwhat happened is that the very first time i set the circuit up for one motor, it worked perfectly. but then, i moved the pi a little and the motor has since refused to work. i even bought a new pi and still no luck with the circuit. i then bought a l298n board that fits smugly on top of the gpio pins of the pi and followed the instructions on the this video\n\nstill no luck, the motor just won't run with either pi. i am using four aa batteries to power the motor and a connecting the pi to a power supply from the wall. what could possibly be the problem here?\n", "tags": "motor raspberry-pi", "id": "8421", "title": "i can't get motors to turn with raspberry pi"}, {"body": "for a quadcopter, what is the relationship between roll, pitch, and yaw in the earth frame and acceleration in the x, y, and z dimensions in the earth frame? to be more concrete, suppose roll ($\\theta$) is a rotation about the earth frame x-axis, pitch ($\\phi$) is a rotation about the earth frame y-axis, and yaw ($\\psi$) is a rotation about the z-axis. furthermore, suppose $a$ gives the acceleration produced by all four rotors, i.e. acceleration normal to the plane of the quadcopter. then what are $f, g, h$ in\n\n$$a_x = f(a,\\theta,\\phi,\\psi)$$\n$$a_y = g(a,\\theta,\\phi,\\psi)$$\n$$a_z = h(a,\\theta,\\phi,\\psi)$$\n\nwhere $a_x$, $a_y$, and $a_z$ are accelerations in the $x$, $y$, and $z$ dimensions.\n\ni've seen a number of papers/articles giving the relationship between x,y,z accelerations and attitude, but it's never clear to me whether these attitude angles are rotations in the earth frame or the body frame.\n", "tags": "quadcopter dynamics", "id": "8425", "title": "relationship between earth frame attitude and acceleration for a quadcopter"}, {"body": "i am trying to solve a forward kynematics problem for a 3dof manipulator.\n\ni am working with the robotics toolbox for matlab created by peter corke and after calculte the dh parameters and introduce them into matlab to compute the fordward kynematics the plotted robot is not what it should be.\n\ni guess i made some mistakes calculating the dh parameters.\n\nattached is the file where you can see what are the dh frames calculated for each joint and the dh parameters for each frame.\n\nanyone could give me a clue whether this is the correct answer?\n\nhere is the image with the frames calculated by me.\n\n\nand here the robot i get from matlab (using the robotics toolbox by p.corke)\n\n", "tags": "robotic-arm inverse-kinematics forward-kinematics", "id": "8426", "title": "denavit hartenberg parameters - 3dof articulated manipulator"}, {"body": "hello i am trying to build a quadcopter for a school project and i need to finish quick before our mission trip because i was asked to finish it before then so that i could take it. but i am having a problem figuring out which charger would work with my zippy compact 6200mah 4s 40c lipo pack \n\n\n\nhere are the specs on the battery\n\n\ncapacity: 6200mah\nvoltage: 4s1p / 4 cell / 14.8v\ndischarge: 40c constant / 50c burst\nweight: 589g (including wire, plug &amp; case)\ndimensions: 158x46x41mm\nbalance plug: jst-xh\ndischarge plug: hxt4mm\n\n\n\n\nalso i will be running the tarot t4-3d brushless gimbal for gopro (3-axis)\nand if anyone can tell me a good battery to run it off of or maybe it would be better to run it off my main battery.\n\nthanks in advance\n", "tags": "quadcopter battery lithium-polymer", "id": "8427", "title": "what charger to use with my zippy compact 6200mah 4s 40c lipo multirotor battery"}, {"body": "i have salvaged a moatech bl55k-m01 stepper motor from an old printer and i'd like to play around with it, but i'm unsure of how to use the pinout listed on the board. pinout reads:\n\n\n24v\n24v\ngnd\ngnd\nsgnd\n5v\nst/sp\nrd\nclk\ngain\n\n\nwhich is all well and good, but i am not sure what sgnd, st/sp, rd, and gain should be used for. i understand clk, but also don't know what frequency this expects. moatech e-mail bounced back, so i was hoping someone might have a coherent datasheet or know what these designations mean. \n", "tags": "stepper-motor", "id": "8428", "title": "moatech stepper motor salvage - unsure of pinout"}, {"body": "what 2d slam implementations (preferably included in ros) can be used with simple distance sensors like ir or ultrasonic rangefinders?\n\ni have a small mobile platform equipped with three forward facing ultrasonic sensors (positioned at 45 degrees, straight ahead, and -45 degrees), as well as a 6-dof accel/gryo and wheel encoders, and i'd like to use this to play around with a \"toy\" slam implementation. i don't want to waste money on a kinect, much less a commercial laser rangefinder, so methods that require high-density laser measurements aren't applicable.\n", "tags": "slam ros", "id": "8432", "title": "how to use slam with simple sensors"}, {"body": "\n\nhow can we achieve this kind of rotation to enable maximum trapping of solar rays during the day?\n", "tags": "mobile-robot", "id": "8438", "title": "solar panel set rotation: how to achieve both vertical and horizontal rotation?"}, {"body": "i am a new learner of irobot. i am trying to program it to control the movement of the create 2. after glancing through the existing project, i find most of them are based on sending commands to roomba through a cable. \n\nis there anyway to embed the code in and let the roomba behave accordingly? if there is not such method, which kind of api tool do you think is easiest for beginner? \n", "tags": "irobot-create", "id": "8443", "title": "irobot create 2: can i load the code in instead of connecting cable? (new learner)"}, {"body": "i am working with an irobot create 2 and i work with others around me. whenever i turn the robot on, send it an oi reset command, etc., it makes its various beeps and noises. i would like to not have this happen since i find it a little annoying and i'm sure those who have to work around me would like to have things quiet so they can concentrate on their work. is there a way to accomplish turning off the beeps (while still being able to easily re-enable them), or am i out of luck?\n", "tags": "irobot-create roomba digital-audio", "id": "8444", "title": "is there a way to turn the sound off of a roomba?"}, {"body": "i am starting to assemble a quadrotor from scratch.\n\ncurrently, i have this:\n\n\nstructure;\nan imu (accelerometer, gyro, compass);\n4 escs and dc motors;\n4 propellers;\nraspberry pi to control the system, and;\nlipo battery.\n\n\ni have calibrated the escs and the four motors are already working and ready.\nbut now i am stuck.\n\ni guess the next step is to dive deeply in the control system, but i am not sure where to begin. i read some articles about the control using pids, but i don't know how many should i use, or whether i need to model the quadrotor first to compute kinematic and dynamic of the quadrotor inside the rpi.\nsorry if the question is too basic!\n\nmore details\n\nthe structure is from a kit. well, all i have now is the escs calibrated, although i do not have documentation of them to adjust the cut off voltage for the lipo battery. i have been made tests with some python code i found to have pwm outputs for the motors and to control i2c bus to communicate with imu. \n\none of my problems now is that i need rpio library for pwm and the  to work with the i2c libraries from the mit to control my imu but as far as i know rpio works with python2 and  works with python3 so i don't know how to manage this.\n\nso actually, i have no code yet to control the four motors in parallel, only have testing code to test them separately and also with the imu.\n\nabout the imu, i am still learning how to work with it and how to use the mit library. the unit includes those sensors:\n\n\nadxl345\nhmc5883l-fds\nitg3205\n\n\nyou can see a picture of the quadrotor below,\n\n\n\nso as i said before, i would like to know how to handle the control system and how it is implemented inside the raspberry pi, and then start to work with the python code to assemble the motors, the imu and the control.\n", "tags": "control pid raspberry-pi quadcopter beginner", "id": "8448", "title": "quadrotor - control system, where to begin?"}, {"body": "i am working on a robotic application, and i want to control the torque (or current) of brushless dc motors. there are many bldc speed controllers but i could not find anything related to torque or current. \n\ninstead of continuously spinning, the motor is actuating a robotic joint, which means i need to control the torque at steady-state, or low-speed, finite rotation. \n\ni am looking for a low-cost, low weight solution, similar to what texas instruments drv8833c dual \nh-bridge motor drivers does for brushed dc motors.\n", "tags": "brushless-motor", "id": "8450", "title": "torque / current control for bldc motors"}, {"body": "i am computer science student and i have no knowledge on robotics. \n\nin my project, i am trying to find controllers for modular robots to make them do specific tasks using evolutionary techniques. for the moment i am doing this in a simulator, but if i want to make physical robots i have to know a priori the components to add to the robot, where do i place them, especially if modules of robot are small (cubes of 5*5*5cm)...\n\nso my questions are:\n\n\nwhat are must have components to make physical robot ? (arduino, batteries, sensors, ...)\nfor a small robot how many batteries do i need ?\nif modules have to communicate with wifi, do i have to put a wifi card on each module?\ni want to add an imu. is its position important, i mean do i have to put it in the middle of the robot ?\n\n\nthank you very much.\n", "tags": "arduino mechanism", "id": "8451", "title": "what are hardware components to build a modular robot which consists of several 5x5x5cm modules?"}, {"body": "i would like to put a train on a track and control its movement with high precision left and right using a wireless controller.\n\nwhat is the best way to do it?\n", "tags": "arduino", "id": "8452", "title": "what is the easiest yet precise method one can make a track and a train"}, {"body": "can anyone recommend an ir distance sensor that works on black surfaces? i'm looking for something to use as a \"cliff\" sensor, to help a small mobile robot avoid falling down stairs or off a table, and i thought the sharp gp2y0d805z0f would work. however, after testing it, i found any matte black surface does not register with the sensor, meaning the sensor would falsely report a dark carpet as a dropoff.\n\nsharp has some other models that might better handle this, but they're all much larger and more expensive. what type of sensor is good at detecting ledges and other dropoffs, but is small and inexpensive and works with a wide range of surfaces?\n", "tags": "sensors", "id": "8455", "title": "small ir distance sensor that works on black surfaces"}, {"body": "i found cad files for the create on the ros turlebot download page (.zip),\nand shells on the gazebo sim page. \n\nany ideas where the files for the create 2 could be found?\n", "tags": "design irobot-create", "id": "8457", "title": "create 2 cad files"}, {"body": "i'm trying to make artificial muscles using nylon fishing lines (see http://io9.com/scientists-just-created-some-of-the-most-powerful-muscl-1526957560 and http://writerofminds.blogspot.com.ar/2014/03/homemade-artificial-muscles-from.html)\n\nso far, i've produced a nicely coiled piece of nylon fishing line, but i'm a little confused about how to heat it electrically.\n\ni've seen most people say they wrap the muscle in copper wire and the like, pass current through the wire, and the muscle acuates on the dissipated heat given the wire resistance.\n\ni have two questions regarding the heating:\n\n1) isn't copper wire resistance extremely low, and thus generates very little heat? what metal should i use? \n\n2) what circuit should i build to heat the wire (and to control the heating)? most examples just \"attach a battery\" to the wire, but afaik that is simply short-circuiting the battery, and heating the wire very inneficiently (and also may damage the battery and it could even be dangerous). so what's a safe and efficient way to produce the heat necessary to make the nylon muscle react? (i've read 150 centigrads, could that be correct?) for example with an arduino? or a simple circuit in a breadboard?\n\nthanks a lot!\n", "tags": "arduino electronics actuator", "id": "8461", "title": "electronic circuit for heating nylon fishing line muscle"}, {"body": "i want to build an automatic sliding window shutter and need help\nwith part selection and dimensioning.\n\nsome assumptions: \n\n\nwindow width 1.4 m\nsliding shutter weight 25 kg\nmax speed 0.07 m/s\nmax acceleration 0.035 m/s^2\npulley diameter 0.04m.\n\n\nleaving out friction i need a motor with about 0.02 nm of torque and a rated speed of 33 rpm.\n\nwhat i would like to use:\n\n\nmotor controller with soft-start and jam protection,\ndc motor 24v,\npulleys and timing belts.\n\n\nwould you suggest other components or a different setup?\nhow do i connect motor and pulleys (clamping set?)?\ndo i need additional bearings because of the radial load?\n\n\n\n(m motor, p pulley, b bearing, = shaft)\n\nif so i have to extend the motor shaft. what would i use for that (clamp collars, couplings?)?\nwhat width do i need for the belts? which belt profile (t, at, hdt) should i use?\n\nupdate\n\nthe construction i am aiming for resembles the one which can be seen on page 6 (pdf numbering) here.\n", "tags": "motor design microcontroller automation", "id": "8462", "title": "automatic sliding window shutter"}, {"body": "i have a 3d point in space with it's xyz coordinates about some frame a. i need to calculate the new xyz coordinates, given the angular velocities of each axis at that instant of time about frame a\n\ni was referring to my notes, but i'm a little confused. this is what my notes say:\nhttp://imgur.com/yjd8oui\n\nas you can see, i can calculate the angular velocity vector w given my angular velocities. but i'm not sure how this translates to how to calculate my new xyz position! how can i calculate the rpy values this equation seems to need from my xyz, and how can i calculate my new position from there\n", "tags": "mobile-robot", "id": "8463", "title": "angular velocity to translational velocity"}, {"body": "i'm a researcher in a lab that's starting work on some larger humanoid/quadruped robots as well as a quadcopter. currently, we have several power supplies that have a max rating of 30v/30a and our modified quadcopter easily maxes out the current limit with only half of its propellers running. it seems like most power supplies are meant for small electronics work and have fairly low current limits. i think that i want to look for power supplies that are able to provide between 24-48v and higher than 30a for an extended period of time. \n\n1.) is this unreasonable or just expensive? \n2.) do most labs just connect psus in series to get higher voltages?\n\nthanks for the input.\n", "tags": "quadcopter power humanoid", "id": "8466", "title": "sizing high current power supplies for large robots"}, {"body": "i am using the kit version oculus prime mobile platform sold by xaxxon for a project and was planning to use and odroid xu4 running ros. but, the problem i faced was that the odroid xu4 has a 32bit processor whereas the server application and ros packages written for oculus prime run only on 64bit. here are the links for the documentation and server application.\n\ncould anyone tell me if:\n\n\nthere is a 32-bit variant or any other way to run the server application and ros packages on the odroid.\nwill any single board computer with a 64-bit architecture be able to run the required packages or is there any special configuration in the motherboard that they sell. (is this the case with all such platforms in general)\nwhich single board computers would be ideal for this situation so that i can purchase them?\n\n", "tags": "ros ugv platform", "id": "8470", "title": "xaxxon oculus prime platform: using a different sbc than the one sold by manufacturer"}, {"body": "i have a preliminary design for a legged robot that uses compliant elements in the legs and in parallel with the motors for energy recovery during impact as well as a pair of flywheels on the front and back that will oscillate back and forth to generate angular momentum. i'd like to create a dynamic simulation of this robot in order to be able to test a few control strategies before i build a real model. what simulation package should i be using and why? \n\ni have heard good things about msc adams, namely that it is slow to learn, but has a lot of capability, including integration with matlab and simulink. i have also heard about the simmechanics toolbox in matlab, which would be nice to use since i already am decent with cad and know the matlab language. i am not yet familiar with simulink, but have used labview before.\n", "tags": "mobile-robot design dynamics matlab simulation", "id": "8471", "title": "dynamic simulation of compliant elements in quadruped robot"}, {"body": "is kinematic decoupling of a 5dof revolute serial manipulator also valid?\nthe three last joints is a spherical joint. most literatures only talks about decoupling of 6dof manipulators.\n\nthanks in advance,\noswald\n", "tags": "kinematics", "id": "8474", "title": "kinematic decoupling"}, {"body": "i have a motor with a stall current of up to 36a. i also have a motor controller which has a peak current rating of 30a. is there any way i could reduce the stall current or otherwise protect the motor controller?\n\ni realize the \"right\" solution is to just buy a better motor controller, but we're a bit low on funds right now.\n\ni thought of putting a resistor in series with the motor and came up with a value of 150m\u03a9, which would reduce the maximum current draw to 25a (given the 12v/36a=330m\u03a9 maximum impedance of the motor). is there any downside to doing this? would i be harming the performance of the motor beyond reducing the stall torque?\n", "tags": "motor microcontroller current", "id": "8475", "title": "how can i reduce a motor's maximum current draw?"}, {"body": "i'm reading from astrom &amp; murray (2008)'s feedback systems: an introduction for scientists and engineers about the difference between feedback and feedforward.  the book states:\n\n\n  feedback is reactive: there must be an error before corrective actions are taken.  however, in some circumstances, it is possible to measure a disturbance before the disturbance has influenced the system.  the effect of the disturbance is thus reduced by measuring it and generating a control signal that counteracts it.  this way of controlling a system is called feedforward.\n\n\nthe passage makes it seem that feedback is reactive, while feedforward is not.  i argue that because feedforward control still uses sensor values to produce a control signal, it is still reactive to the conditions that the system finds itself in.  so, how can feedforward control possibly be any different from feedback if both are forms of reactive control?  what really separates the two from each other?\n\na illustrative example of the difference between the two would be very helpful.\n", "tags": "control", "id": "8480", "title": "what's the difference between feedback and feedforward control?"}, {"body": "the title pretty much says it all.  i'm on a team that is currently building a robotic arm for the capstone project of my engineering degree, our design is similar to the dobot (5 degrees of freedom). we purchased our 6 servomotors, and each one requires 2a at 6v.  \n\nfrom my preliminary research, i haven't been able to find a power source that could satisfy this.  we'd rather not purchase six individual ac/dc power source for each servo, and we've heard that these can introduce problems, as they aren't necessarily voltage-regulated.  another suggestion we've received is to buy a computer power source, and modify it to output our the voltage and amperage we need. this raises some concerns, since our professor running the course might find this dangerous.\n\nwe'd like some input into how we can power our servos effectively, without going overboard on costs (we are students, after all).\n\nthanks!\n", "tags": "power servos servomotor arm", "id": "8483", "title": "powering 6 servomotors requiring 6v and 2a each"}, {"body": "im currently in a (risky) proyect that involves me building the fastest quad i can afford.\n\nim trying to get something close to this extremely fast warpquad\n\nafter reading a lot about quadcopters, as i know i can buy all this and it should fit together and fly without any problem.\n\n\n\nmy questions are, first if im wrong or im missing something as i had only read about it (thinking this is a common build for racer quad).\n\nthen:\n\nwill this overheat (bad consecuences) if i let it drain the full battery at 100% throttle?\n\nwill this fly at least 4 minutes under the previous conditions?\n\nshould i get a higher c-rating battery?\n\nas i can't find better motors of that size, is the only way to improve its speed by putting a 6s battery? and what would happen if i do it?\n\nshould i put the 6inch props or 4inch? i know 4inch should get faster rpm changes but will it be noticeable at this sizes?\n\nand in general any tips to make it faster will be welcome.\n\nthanks. \n", "tags": "quadcopter", "id": "8486", "title": "how to build a fast quadcopter"}, {"body": "i am working with a create 2 and i am executing a simple sequence like (in pseudocode):\n\n\n\nthe outcome when i run the above program repeatedly is that it works the first time, i see sensor data (that seems to not change or be reactive) printing to the screen, but on future runs no serial can be read and the program crashes (because i am throwing an exception because i want to get this problem ironed out before getting to far along with other things). if i unplug and replug my usb cable from my macbook, then the program will work for another run, and then fall back into the faulty behavior.\n\ni do not experience this issue with other things like driving the robot, i am able to run programs of similar simplicity repeatedly. if i mix driving and sensor streaming, the driving works from program run to program run, but the data streaming crashes the program on the subsequent runs.\n\ni have noticed that if i want to query a single sensor, i need to pause the stream to get the query response to come through on the serial port, and then resume it. that is why i am so inclined to pause/restart the stream.\n\nam i doing something wrong, like pausing the stream too often? are there other things i need to take care of when starting/stopping the stream? any help would be appreciated!\n\nedit:\ni should note that i am using python and pyserial. i should also note, for future readers, that the irobot pushes its streamed data to the laptop every 15ms where it sits in a buffer, and the data sits there until a call to serial.read() or to serial.flushinput(). this is why it seemed that my sensor values weren't updating when i read/polled every half second, because i was reading old values while the current ones were still buried at the back of the buffer. i worked around this issue by flushing the buffer and reading the next data to come in.\n\nedit 2:\nsometimes the above workaround fails, so if i detect the failure, i pause the stream, re-initialize the stream, and read the fresh data coming in. this seems to work pretty well. it also seems to have solved the issue that i originally asked the question about. i still don't know exactly why it works, so i will still accept @jonathan 's answer since i think it is good practice and has not introduced new issues, but has at least added the benefit of the robot letting me know that it has started/exited by sounding tones.\n", "tags": "mobile-robot irobot-create", "id": "8487", "title": "is there a way to disconnect and reconnect from a create 2 that was streaming sensor readings without having to unplug/replug my usb-serial cable?"}, {"body": "given two robot arms with tcp (tool center point) coordinates in the world frame is:\n\n$x_1 = [1, 1, 1, \\pi/2, \\pi/2, -\\pi/2]$\n\nand\n\n$x_2 = [2, 1, 1, 0, -\\pi/2, 0]$\n\nthe base of the robots is at:\n\n$base_{rob1} = [0, 0, 0, 0, 0, 0]$\n\n$base_{rob2} = [1, 0, 0, 0, 0, 0]$\n\n(the coordinates are expressed as successive transformations, x-translation, y-translation, z-translation, x-rotation, y-rotation, z-rotation. none of the joint axes are capable or continuous rotations.)\n\nhow many degrees does the tcp of robot 2 have to rotate to have the same orientation as the tcp of robot one?\n\nis the calculation \n\n$\\sqrt{(\\pi/2 - 0)^2 + (\\pi/2 - (-\\pi/2))^2 + (-\\pi/2 - 0)^2}$\n\nwrong? if yes, please specify why.\n\nupdated:\nis the relative orientation of the two robots [\u03c0/2,\u03c0/2,\u2212\u03c0/2]\u2212[0,\u2212\u03c0/2,0]=[\u03c0/2,\u03c0,\u2212\u03c0/2]? but the euclidean distance cannot be applied to calculate angular distance?\n\nin other words:\n\n\nwhile programming the robot, and tool frame is selected for motion, to match the orientation of the other one, i would have to issue a move_rel($0, 0, 0, \\pi/2, \\pi, -\\pi/2$) command, but the executed motion would have magnitude of $\\pi$?\nwhile programming the robot, and world frame is selected for motion, to match the orientation of the other one, i would have to issue a move_rel($0, 0, 0, \\pi, 0, 0$) command, and the executed motion would have magnitude of $\\pi$?\n\n", "tags": "robotic-arm kinematics geometry", "id": "8491", "title": "relative orientation of two robots"}, {"body": "i have a robotic arm and a camera in eye-in-hand configuration. i know that there is a relationship between the body velocity $v$ of the camera and the velocities $\\dot s$ in the image feature space that is $\\dot s=l(z,s) v$ where $l$ is the interaction matrix. i was wondering if one can find a mapping (a so called diffeomorphism) that connects the image features' vector $s$ with the camera pose $x$. all i was able to find is that it is possible to do that in a structured environment which i don't fully understand what it is.\n", "tags": "mapping visual-servoing", "id": "8499", "title": "mapping between camera pose and image features in visual servoing"}, {"body": "i am not quite sure if i quite understand the difference between these two concepts, and why there is a difference between these two concept. \n\nyesterday i was trying to compute the jacobian needed for an inverse kinematics, but the usual input i provided my transformation in the forward kinematics being the points p and xyz could not be applied, the transformation matrix was given a state vector q, at which the the tool position could be retrieved... \n\ni am not sure if understand the concept quite well, and can't seem to the google the topics, as they usually  include terminologies which makes the concepts too simple (angle calc and so on.. )\n\ni know it might be pretty much to ask, but what form of input is needed to compute the jacobian ?, and what and why is there a difference between forward and inverse kinematics?.. \n", "tags": "inverse-kinematics forward-kinematics jacobian", "id": "8500", "title": "forward kinematic and inverse kinematic... when to use what?"}, {"body": "note before i start: i have not actually put anything together yet, i'm still just planning, so any changes that require a shape change or anything like that are accepted.\n\ni'm working on making a walking robot with my arduino and 3d printing all the pieces i need. it will have four legs, but since it needs to be mobile, i didn't want the power supply to be huge. i've decided it would be best if i can get each leg to only require 1 servo, at 5v each. i know how to get the leg to move back and forth, but i want to be able to lift it in between; before it brings the leg forward, it needs to lift up the foot.\nthe only thing i can think of is the rotation maybe locking some sort of gear.\n\nwhen a motor begins rotating clockwise, how can i have it power a short motion to move an object toward itself, and when it begins moving counterclockwise to power the same object a short distance away from itself?\n\nthe servos i am using have 180* of rotation, so they don't go all the way around in a loop.\n\nalso: don't know if it will be important or not, but because of the peculiar construction of the foot, it would be best if it was lifted straight up, rather than up at an angle, but it isn't 100% necessary. \n\nare there any robots that already do this? if so, i'm unaware of them. thanks for your time.\n", "tags": "mechanism motion-planning servomotor legged gearing", "id": "8501", "title": "lifting robotic leg with only one servo"}, {"body": "i understand that most of the self-driving cars solutions are based on lidar and video slam.\n\nbut what about robots reserved for indoor usage? like robot vacuums and industrial agvs? i see that lidar is used for irobot and their latest version uses vslam. agvs also seem to use lidar.\n", "tags": "slam lidar", "id": "8510", "title": "what are the approaches for indoor robot positioning?"}, {"body": "ok, let's say we have a tech request for a robotic system for peeling potatoes, and a design is as follows:\n\n\none \"arm\" for picking up a potato and holding it, rotating when needed.\nanother \"arm\" for holding a knife-like something which will peel the skin from the potato.\narm picks up a potato from first container, holds it over trash bin while peeling, then puts peeled potato in second container.\nfor simplicity a human rinses peeled potatoes, no need to build automatic system for it.\nin first iteration even 100% spherical peeled potatoes are ok, but ideally would be good to peel as little as possible, to minimize the wastes.\n\n\nquestion:\ni know that we're very, very far away from building such a system. nevertheless, what are the purely technical difficulties which needs to be solved for such a robot to be built? \n\nedit\n\nlet's assume we stick to this design and not invent something radically different, like solving the problem with chemistry by dissolving the skin with something. i know that the problem of peeling the potatoes is currently being solved by other means - mainly by applying friction and a lot of water.\n\nthis question is not about it. i am asking specifically about the problems to be solved with the two-arms setup using the humanlike approach to peeling.\n", "tags": "robotic-arm", "id": "8511", "title": "what engineering problems needs to be solved to build a potato-peeling robot?"}, {"body": "\n\n\n\nhi,\nhere i have added 2 options for connecting encoder on shaft.\nmotor, gearhead and shaft is connected using coupling. but where will be best place for encoder (to avoid backlash from coupling and gearhead).\nwhether through hollow encoder is available? (see option 1).\ni dont know which one will be best for this kind of system. \nwhich one is widely using arrangements?\n\noptions 3 is encoder will be placed before the motor.\n", "tags": "motor", "id": "8512", "title": "how to connect absolute encoder on the rotating shaft. please see the three options?"}, {"body": "i've calculated a dh parameter matrix, and i know the top 3x3 matrix is the rotation matrix. the dh parameter matrix i'm using is as below, from https://en.wikipedia.org/wiki/denavit%e2%80\n\nabove is what i'm using. from what i understand i'm just rotating around the z-axis and then the x-axis, but most explanations from extracting euler angles from rotation matrixes only deal with all 3 rotations. does anyone know the equations? i'd be very thankful for any help.\n", "tags": "kinematics forward-kinematics dh-parameters", "id": "8516", "title": "getting pitch, yaw and roll from rotation matrix in dh parameter"}, {"body": "my work has an older fanuc robot ( arc mate 100-ibe rj3ib, fanuc awe2 teach pendant with  powerwave 355m) and the old operator/programmer has left. i have taken over his job and cant find out how to turn down the voltage and wire feed speed because it occasionally burns through parts. i tried manually putting in voltage and wire feed speed but it seems it will only accept the previous weld schedules 1-8 and if i mess with them that will affect other programs using those. i just need someone to please point me in the right direction. \np.s. typed on phone , sorry if sloppy.\n", "tags": "industrial-robot", "id": "8519", "title": "fanuc robot \"heat\" control"}, {"body": "i'm working on a robotic hand and i would like to simulate different joints and tendon insertion points before starting to actually build it.\n\ni've been googling and found things like solidworks and autodesk, that seem very costly for a hobbyst like me but also i don't quite fully understand their capabilities (just cad? 3d modelling but not simulation? simulation but not interactive?). i've also found things like freecad which seem to me somehow abandoned or just for cad and not for simulation.\n\nanother requirement would be interactivity of the simulation, not just rendering.\n\ni don't have a problem with commercial software, but i'm looking for a reasonable cost for a hobbyst, not an engineering company.\n\nis there a software out there that meets all this requirements? or should i use several programs each for a specific purpose?\n\nthanks!\n", "tags": "design mechanism software simulator 3d-model", "id": "8522", "title": "recommendation for 3d mechanism modeling and simulation software"}, {"body": "i'm building quadcopter from scratch, software is implemented on stm32f4 microcontroller. frequency of main control loop equals 400hz.\n\ni've though everything is almost finished but when i've mounted everything and started calibration of pids i faced a problem.\nit was impossible to adjust pid parameters properly.\nso i started test with lower power (not enough to fly) and i've managed quite fast adjust pid for roll but when i've increased power problems with control came back.\n\nafter that i've done more measurements.\n\n\n\ni didn't make test with blades but probably this is even worse and that is why i cannot calibrate it.\n\n\nif problem is due to vibration how can i fix it?\nif something else is cause of that symptom, what is it?\ncan i solve this through better controls and data fusion algorithms?\nnow i use complementary filter for acc and gyro sensors data fusion in roll and pitch.\n\n", "tags": "control imu accelerometer gyroscope", "id": "8525", "title": "quadrocopter problem with stability"}, {"body": "i am new to robotics,\ni will be controlling dc motors from android device through usb\n\nfor this i have selected l298n motor controller(after watching youtube videos )\nand got some dc motors\n\ni have no idea how do i connect this to android device via usb cable\n\nhelp appreciated\n\nref:\nhttps://www.bananarobotics.com/shop/l298n-dual-h-bridge-motor-driver\nhttps://youtu.be/xrehsf_9yq8\n\nps: all i know is programming android\n", "tags": "motor usb", "id": "8526", "title": "which usb interface for android device i can use for motor driver"}, {"body": "i have a small bot(around 4-5kg with wheels) which is to be pushed without contact by another bot. i plan to do this using a bru and a propeller. i am having problems selecting the right combination. please help me with these questions:-\n\n\nshould the bldc be high kv or low kv(will i need high rpm or low rpm)\nwhat is the  ideal propeller to use with the motor so that i can create enough thrust to get the 'small' bot in motion and keep it in motion?\nwhat are the other criteria i should keep in mind while selecting.\n\n", "tags": "brushless-motor", "id": "8527", "title": "which brushless dc and propeller to choose?"}, {"body": "i just bought the wookng-m for my multi copter and i was wondering if it is possible to have a two man system where one controls the main craft with one transmitter and the other controls the camera and gimbal with another transmitter.\ni know you can set up a gimbal with the wookong-m but i wasn't if you could use a separate transmitter to control it or even control it at all. i am also waiting for my 3-axis gopro gimbal to arrive but can i even control the pan or do i have to control the gimbal completely separate and not use the wookong-m for the gimbal and camera.\n\ni am a beginner and this is my 1st time working with the wookong-m so please keep the answers understandable.\n\nthanks in advance\n", "tags": "quadcopter", "id": "8538", "title": "setting up a gimbal with the dji wookong-m but using a separate transmitter and receiver"}, {"body": "say i have a motor and i want it to spin at exactly 2042.8878 revolutions per minute.  say i have a very precise sensor to detect the rpm of the motor to a resolution of 1/1000th of a revolution per minute.  \n\n\ncan i produce a pwm signal which can match the speed to that degree of \nprecision?  \nwhat variables in the signal parameters would i have to adjust to get the precision if possible?  \nwould i have to use additional circuitry between the motor and the driver?  \nwould i have to design the signal/circuitry around the specific specifications of the motor?\nshould i just use a stepper motor?\n\n\nthis is assuming i am using a microcontroller to measure the motor's speed and adjust the signal in real-time to maintain a certain speed.\n", "tags": "motor microcontroller stepper-motor pwm stepper-driver", "id": "8539", "title": "how can i increase the resolution of a pwm signal?"}, {"body": "not sure if i am posting this question in the correct community, as it relates primarily to reinforcement learning. apologies early on if this is not so.\n\nin reinforcement learning many algorithms exist for 'solving' the cart-pole problem; that of balancing a mass on the edge of a stick, connected to a cart on a hinge, which has 1 dof. there is td learning, q-learning and many other on and off-policy methods. there is also the more recent, model-based policy search method pilco.\n\nwhat i am really wondering, i suppose is more of a physics question: is there a need for active control? why is it not possible to find the one point for the cart, which prevents the mass to move, even incrementally, left or right as it sits atop the pole? why does it always 'fall'?\n", "tags": "control", "id": "8544", "title": "pole-balancing / inverted-pendulum; is there a need for active control?"}, {"body": "i am currently applying path planning to my robotic arm (in gazebo) and have chosen to use an rrt. in order to detect points of collision, i was thinking of getting a point cloud from a kinect subscriber and feeding it to something like an octomap to have a collision map i could import into gazebo. however, there is no gazebo plugin to import octomap files and i do not have enough experience to write my own. the next idea would be to instead feed this point cloud to a mesh generator (like meshlab) and turn that into a urdf, but before starting i'd rather get the input of somebody far more experienced. is this the right way to go? keep in mind the environment is static, and the only things moving are the arms. thank you. below is just a picture of an octomap.\n", "tags": "robotic-arm localization slam kinect gazebo", "id": "8546", "title": "modelling point clouds for collision detection in gazebo"}, {"body": "is it possible to decouple a 5dof manipulator? \nthis question i asked earlier and i believe i got the right answers but i never show the drawings of the manipulator and now i'm hesitating during setup of the dh parameters for forward kinematics. see drawing depicted here. \n\n\n", "tags": "kinematics forward-kinematics dh-parameters", "id": "8549", "title": "dh parameters and kinematic decoupling"}, {"body": "in order to perform a cyclic task, i need a trajectory planning algorithm. this trajectory should minimize jerk and jounce.\n\nwhen i search for trajectory planning algorithms, i get many different options, but i haven't found one which satisfies my requirements in terms of which values i can specify. an extra complicating factor is that the algorithm should be used online in a system without too much computing power, so mpc algorithms are not possible...\n\nthe trajectory i am planning is 2d, but this can be stripped down to 2 trajectories of 1 dimention each. there are no obstacles in the field, just bounds on the field itself (minimum and maximum values for x and y)\n\nvalues that i should be able to specify:\n\n\ntotal time needed (it should reach its destination at this specific\ntime) \nstarting and end position\nstarting and end velocity\nstarting and end acceleration\nmaximum values for the position.\n\n\nideally, i would also be able to specify the bounds for the velocity, acceleration, jerk and jounce, but i am comfortable with just generating the trajectory, and then checking if those values are exceeded.\n\nwhich algorithm can do that?\n\nso far i have used fifth order polynomials, and checking for limits on velocity, acceleration, jerk and jounce afterwards, but i cannot set the maximum values for the position, and that is a problem...\n\nthank you in advance!\n", "tags": "control algorithm", "id": "8555", "title": "which trajectory planning algorithm for minimizing jerk"}, {"body": "i'm building a line following robot. i have made different chassis designs. the main prototype i'm using is a rectangle base. at one side motors are placed. on the other side of the rectangle caster wheel is placed in the middle. look at the following image.\n\n\n\nby varying the values of , i have seen that the stability of the robot is varying rapidly. \n\ni'm driving the robot using . i have seen that for some chassis designs it is very hard(sometimes impossible) to calculate correct constant values. and for some chassis it is very easy. by the word stability i meant this. i have a feeling that the robot dimensions,  values and that stability has a relationship..\n\nis there an equation or something that can be used to estimate the value of the  when the width of the robot is known..?\n\nother than that is there a relationship between robot weight and diameter of the wheel or robot dimensions and the diameter..?\n\nthanks for the attention!!\n", "tags": "arduino motor pid line-following", "id": "8556", "title": "length and width of a line following robot"}, {"body": "so i'm really interested in robotics.. i'm not really a robot expert as i have no experience on creating one. i just like them.  anyway, i am always wondering if its possible to build a robot that can transfer itself to different devices and still function. i mean, if you want that robot to transfer itself(the data that making it function or whatever you call it) to your laptop so you can still use it while you are away or anything.. does creating one require advanced computing and knowledge? is it kind of creating a artificial intelligence?. when it think of this i would always thought of j.a.r.v.i.s since he can go to stark suit and communicate with him.\n\n\n  translated into robotics terminology by a roboticist:\n  \n  is it possible to create software for controlling robot hardware that can transfer itself to different devices and still function. could it transfer itself to your laptop and collaborate with you using information it gathered while it was in it's robot body? \n  does creating software like this require advanced knowledge and computing? is software like this considered to be artificial intelligence?\n\n\ni am serious about this question sorry to bother or if anyone will be annoyed./ \n", "tags": "mobile-robot", "id": "8559", "title": "is it possible to design robot software or ai to function in different devices?"}, {"body": "i see there are things like glass and mirror in autodesk inventor professional 2016 but is there a possibility to have venetian mirror? so that from one side it would look like a mirror and from the other side it would look like a transparent glass?\n", "tags": "design mechanism 3d-printing 3d-model visualization", "id": "8561", "title": "is venetian mirror possible in autodesk inventor?"}, {"body": "i'm currently working on humanoid robot. i've solved the forward &amp; inverse kinematic relations of the robot, and they turn out to be fine. now i want to move onto walking. i've seen tons of algorithms &amp; research papers but none of them make the idea clear. i understand the concept of zmp &amp; what the method tries to do, but i simply can't get my head around all the details that are required to implement it in a real robot. do i have to plan my gait &amp; generate the trajectories beforehand, solve the joint angles, store them somewhere &amp; feed it to the motors in real-time? or do i generate everything at run-time(a bad idea imo)? is there a step-by-step procedure that i can follow to get the job done? or do i have to crawl all my way through those research papers, which never make sense(at least for me).\n", "tags": "mobile-robot stability humanoid", "id": "8562", "title": "humanoid balancing"}, {"body": "i read several textbooks but could not find a good explanation.\ncan anybody tell me why velocity of link (i+1) with respect to frame (i+1) is not zero?\nmy argument is: since the velocity of the link (i+1) is the velocity of origin of the frame (i+1) it should be zero with respect to itself.\n\n!from \"introduction to robotics\nmechanics and control\"\n", "tags": "kinematics forward-kinematics", "id": "8565", "title": "velocity of link (i+1) with respect to frame (i+1)"}, {"body": "i have a task that involves implementing robot behaviour that will follow wall and avoid obstacles along it's path. the robot must stay at desired distance from the wall but also stick to it so it should not loose sight of it. robot is sensing it's surrounding with ultrasonic sensor that is oscillating from left to right and filling an array of small length (10 values) with detected distances (every 10 degrees). from this reading i would like to calculate heading vector that will result in robot path similar to one shown in bottom picture:\n\nblack(walls), red(obstacles), blue(robot), green(desired path)\n\n", "tags": "wheeled-robot navigation", "id": "8570", "title": "calculate robot heading to follow wall and avoid obstacles"}, {"body": "i'm currently developing a 6 dof robotic arm. the arm is vibrating when it stop moving and i want to reduce it. another thing is that arm is so heavy (because there is a projector inside it, lol) and i have to use spring between joints. so, can anyone tell me 1. how to select springs because my supervisor told me that proper selection of springs can reduce vibration? 2. how do i tune the pid parameters? all the joints are dynamixel servos and their pid parameters are tunable. i read article about tuning for a single servo. how do i tune these parameters for the whole arm?\n", "tags": "control pid robotic-arm", "id": "8575", "title": "pid tuning for 6 dof robotic arm"}, {"body": "i have a motor with an encoder. when i set the speed of the motor it should change its speed so that encoder readings per second should fit an equation $y = ax^2 + bx + c$ where  is  value that is given to the motor and  is the encoder readings per second that should get with motor.\n\nencoder reading is counted in every  and if it is not equal to the value of the encoder output should get from motor (it is calculated using the equation), the pwm input to the motor should vary in-order to get desired encoder output.\n\ni want to control this value using a  controller but i'm confused in writing equations. any help would be appreciated..\n", "tags": "motor pid pwm", "id": "8579", "title": "change pwm values according to encoder output"}, {"body": "i'm shopping for my first arduino with a specific goal in mind. i need to attach 3 standard servo motors, an arducam mini 2mp camera, and several leds. i'm trying to figure out power requirements. i assume that usb power won't be sufficient. i'm looking at 12v ac-to-dc outlet adapters and i noticed that amps vary from ~500ma to 5a. i don't want to use batteries.\n\nwhat would you recommend as minimum amperage for this setup? is there a maximum amperage for arduino boards? i don't want to plug it in and burn it out. if i plug in both the usb cable and a power adapter at the same time, is power drawn from both cables?\n\nthanks!\n", "tags": "arduino power", "id": "8580", "title": "arduino power adapters"}, {"body": "i'd like to get rgb and depth data from a kinect, and i found a little tutorial here: http://wiki.ros.org/cv_bridge/tutorials/convertingbetweenrosimagesandopencvimagespython. it's fine, but what i'd like is to be able to get the data on demand, and not as whenever the callback is triggered, assuming i won't try to get the data faster than it can be available. i'd appreciate any help - do go easy on the ros jargon, i'm still learning...thanks.\n", "tags": "ros kinect", "id": "8585", "title": "ros and kinect data without callbacks"}, {"body": "good day to all.\n\nfirst of all, i'd like to clarify that the intention of this question is not to solve the localization problem that is so popular in robotics. however, the purpose is to gather feedbacks on how we can actually measure the speed of the robot with external setup. the purpose is to be able to compare the speed of the robot detected by the encoder and the actual speed, detected by the external setup.\n\ni am trying to measure the distance traveled and the speed of the robot, but the problem is it occasionally experiences slippage. therefore encoder is not accurate for this kind of application. \n\ni could mark the distance and measure the time for the robot to reach the specified point, but then i would have to work with a stopwatch and then transfer all these data to excel to be analyzed. \n\nare there other ways to do it? it would be great if the external setup will allow data to be automatically sent directly to a software like matlab. my concern is more on the hardware side. any external setup or sensors or devices that can help to achieve this?\n\nthanks.\n", "tags": "mobile-robot control wheeled-robot", "id": "8586", "title": "how can i measure the actual speed and distance traveled of the robot with an external setup?"}, {"body": "i have one sharp sensor and i have to use it to measure the height of a block (6cm - 12 cm). how can i accomplish this ?\nactually it is to be connected to a robot which will move near the box and determine its height.\n\nabout gp2y0a21yk0f:\nhttp://www.sharpsma.com/webfm_send/1489\n\nthe robot is like this: http://i.imgur.com/8qt8zeq.jpg\n\n\n\nif possible please suggest a solution that doesn't require moving the sensor.\nbut any method will do fine.\n", "tags": "mobile-robot first-robotics", "id": "8590", "title": "how can i measure the height of an object with a single sharp sensor (gp2y0a21yk0f)?"}, {"body": "i have the following problem:\n\ngiven 3 points on a surface, i have to adjust a manipulator end-effector (i.e. pen) on a baxter robot, normal to that surface.\n\nfrom the three points i easily get the coordinate frame, as well as the normal vector. my question is now, how can i use those to tell the manipulator its supposed orientation.\n\nthe baxter inverse kinematics solver takes a $(x,y,z)$-tuple of cartesian coordinates for the desired position, as well as a $(x,y,z,w)$-quaternion for the desired orientation. what do i set the orientation to? my feeling would be to just use the normal vector $(n_1,n_2,n_3)$ and a $0$, or do i have to do some calculation?\n", "tags": "inverse-kinematics orientation", "id": "8592", "title": "manipulator end-effector orientation with quaternions"}, {"body": "it's unclear as to how one goes about integrating occupancy grid mapping and monte carlo localization to implement slam.\n\nassuming mapping is one process, localization is another process, and some motion generating process called exploration exist. is it necessary to record all data as sequenced or with time stamps for coherence? \n\nthere's motion: $u_t$, map: $m_t$, estimated state: $x_t$, measurement: $z_t$\n\nso..\n\n\neach estimated state, $x_t$, is a function of the current motion, $u_t$, current measurement, $z_t$, and previous map, $m_{t-1}$;\neach confidence weight, $w_t$, of estimated state is a function of current measurement, $z_t$,  current estimate state, $x_t$, and previous map, $m_{t-1}$;\nthen each current map, $m_t$ is a function of current measurement, $z_t$, current estimated state, $x_t$,  and previous map, $m_{t-1}$.\n\n\nso the question is, is there a proper way of integrating mapping and localization processes? is it something you record with timestamp or sequences? are you suppose to record all data, like fullslam, and maintain full history. \nhow can we verify they are sequenced at the same time to be referred to as current (i.e. measurement) and previous (measurement).\n", "tags": "slam occupancygrid", "id": "8595", "title": "in order to integrate mcl and occupancy grid to implement grid-based fastslam, do you have to record all data?"}, {"body": "i've implemented a model of a ball-on-plate plant and am controlling it over a network. below is the open loop output when excited by successive sinusoidal inputs with increasing frequencies. i know that the plant is open loop unstable, and it is cool that this figure so nicely captures the instability. \n\nwhat i'd like to know is if there is other information that i can glean about the plant from the relationship between the input and the output state.\n\n(the state is clipped at 3.1 units.)\n\n\n", "tags": "control balance distributed-systems", "id": "8596", "title": "what can this picture/data tell?"}, {"body": "what kind of systems can be used to make a torso lifting system like the one used by this robot (the black part) :\n\n\n\n\nrack and pinion\nlead screw\nscissor lift\ncan a triple tree help ? \n\n\nwhat are the pro and cons of each system ?\nhow do they ensure stability ?\nand finally, is there a way to draw current when lowering instead of drawing current when lifting ?\n", "tags": "mechanism", "id": "8598", "title": "comparison of lifting systems"}, {"body": "i am trying to get depth data from a kinect in a ros project. it currently looks like this:\n\n\n\nto arrive at this, i've done:\n\n\n\ni also launch openni.launch from the openni_launch package, which publishes the depth data.\n\ni also get this weird warning from the node (can be seen in the image):\n    complexwarning: casting complex values to real discards the imaginary part.\n\nbut as i understand it the data type is an array of 32-bit floats. yet some of the values appear as .\n\ni would like a depth image that directly corresponds to a rgb image array of the same size. i will be doing some tracking in the rgb space, and using the tracked coordinates (x,y) from that to index into the depth array. thanks. \n\nedit:\nturns out,  is published as an array of uint8s, but the actual data is 32bit floats (which is not listed anywhere, had to hunt it down in other people's code). thus an array of 480x640 uint8s, interpreted as 32bit floats, in effectively \"quartered\" in the number of data points. which could explain how the image is 4 times smaller (and hence accessing datapoints out of bounds = nan?), but not why there are two of them.\n", "tags": "ros kinect", "id": "8600", "title": "ros + kinect depth data duplication"}, {"body": "i have a robotic system i'm controlling with arduino, is there an heuristic way to determine a proper sampling time for my pid controller? considering i have some other things to compute on my sketch that require time, but of course a good sampling time is crucial.\n\nbasically i have a distance sensor that needs to detect at a constant rate an object that is moving, sometimes slow, sometimes fast. i don't have a good model of my system so i can't actually tell the physical frequency of the system.\n", "tags": "pid", "id": "8601", "title": "choosing a proper sampling time for a pid controller"}, {"body": "how mechanically robust are lipo batteries? how much force or acceleration can they maximally withstand before failure? what is their (mechanical) shock resistance?\n\nfor some electrical components used in robots, such as imu's, it can be found in datasheets that they can suffer mechanical failure if accelerated or loaded beyond given values. for imu's, this is typically somewhere between $2000g$ and $10000g$ (where $1g = 9.81 m/s^2$).\n\ni'm wondering if similar values are known for lipo batteries, since they are known to be vulnerable components. but, is there any quantification known for their claimed vulnerability?\n", "tags": "battery", "id": "8605", "title": "mechanical robustness/shock resistance lipo batteries"}, {"body": "are there any open source implementations of gps+imu sensor fusion (loosely coupled; i.e. using gps module output and 9 degree of freedom imu sensors)? -- kalman filtering based or otherwise.\n\ni did find some open source implementations of imu sensor fusion that merge accel/gyro/magneto to provide the raw-pitch-yaw, but haven't found anything that includes gps data to provide filtered location and speed info.\n", "tags": "kalman-filter imu sensor-fusion gps", "id": "8607", "title": "open source implementations for gps+imu sensor fusion?"}, {"body": "i have an r.c car and there is a program in my computer in which i can code the car to perform movements.i would like to have an application with a visual design.where it shows the cars path.\n\nis there available software code for this? saves me lots of time.\n", "tags": "software research", "id": "8611", "title": "iphone controlled rc car"}, {"body": "how can one control a combustion engine using a remote control.\nor how would you make a car controlled using a remote.\n", "tags": "robotic-arm first-robotics", "id": "8612", "title": "combustion engine controlled with a remote"}, {"body": "i am preparing for an exam in neural networks. as an example for self-organizing maps they showed the inverted pendulum problem where you want to keep the pole vertical:\n\n\n\nnow the part which i don't understand:\n\n\n  $$f(\\theta) = \\alpha \\sin(\\theta) + \\beta \\frac{\\mathrm{d} \\theta}{\\mathrm{d} t}$$\n  let $x= \\theta$, $y=\\frac{\\mathrm{d} \\theta}{\\mathrm{d} t}$, $z=f$.\n  \n  solution with som:\n  \n  \n  three-dimensional surface in $(x,y,z)$\n  adapt two-dimensional som to surface\n  method of control\n  \n  \n  for a given $(x,y)$ find neuron $k$ for wich $w_k = [w_{k1}, w_{k2}, w_{k2}, w_{k3}]$\n  $f(\\theta)$ is then $w_{k3}$\n  \n  \n\n\ni guess we use the som to learn the function $f$. however, i would like to understand where $f$ comes from / what it means in this model.\n", "tags": "control stability machine-learning", "id": "8617", "title": "what is $\\alpha \\sin(\\theta) + \\beta \\frac{d \\theta}{d t}$ in the inverted pendulum problem?"}, {"body": "i want to build robots, and right now i aim to work with arduino boards\ni know that they are compatible with c and c++, so was wondering which language is better for robotics in general?\n\ni know how to write in java, and the fact that c++ is object oriented makes it look like a better choice for me\n\ndoes c have any advantages over c++?\n", "tags": "arduino c++ c", "id": "8618", "title": "does c have advantages over c++ in robotics?"}, {"body": "i am the moment trying to compute the transformation matrix for robot arm, that is made of 2 joints (serial robot arm), with which i am having some issues. l = 3, l1 = l2 = 2, and q = ($q_1$,$q_2$,$q_3$) = $(0 , \\frac{-\\pi}{6},\\frac{\\pi}{6})$\n\nbased on this information i have to compute the forward kinematic, and calculate the position of each joint. \n\nproblem here is though, how do i compute the angle around x,y,z.. for the transformation matrix.  using sin,cos,tan is of course possible, but what do their angle corresponds? which axis do they correspond to?\n\n\n\ni tried using @steveo answer to compute the $p_0^{tool}$ using the method he provided in his answer, but i somehow mess up something, as the value doesn't resemble the answer given in the example.. \n\n \n", "tags": "kinematics forward-kinematics orientation", "id": "8621", "title": "forward kinematic computing the transformation matrix"}, {"body": "hi i'm using \"minimu 9\" 9 dof imu (gyro, accelerometer and compass) sensor and it gives pitch roll and yaw values with a slope on desktop (no touch, no vibration, steady). y axis is angle in degree and x axis is time in second. x axis length is 60 seconds. how can fix this?\n\npitch\n\n\n\nroll\n\n\n\nyaw\n\n\n\nnote1: minimu code\n", "tags": "sensors imu sensor-fusion", "id": "8631", "title": "imu sensor and compensation"}, {"body": "i'm trying to implement a pid controller by myself and i've a question about the sum_error in i control. here is a short code based on the pid theory.\n\n\n\nnow, i start my commands:\n\nphase 1, if at t=0, i set target=1.0, and the controller begins to drive motor to go to the target=1.0,\nphase 2, and then, at t=n, i set target=2.0, and the controller begins to drive motor to go to the target=2.0\n\nmy question is, in the beginning of phase 1, the error=1.0, the sum_error=0, and after the phase 1, the sum_error is not zero anymore, it's positive. and in the beginning of phase 2, the error=1.0 (it is also the same with above), but the sum_error is positive. so, the iterm at t=n is much greater than iterm at t=0.\n\nit means, the curves between phase 2 and phase 1 are different!!!\n\nbut to end-user, the command 1, and the command 2 is almost the same, and it should drive the same effort.\n\nshould i set the sum_error to zero or bound it? can anyone tell me how to handle the sum_error in typical?\n\nany comment will be much appreciated!!\n\nkevin kuei\n", "tags": "pid", "id": "8636", "title": "sum_error in pid controller"}, {"body": "why is there a discontinuity in the quaternion representation of my device orientation?\n\ni'm using a sentral+pni rm3100+st lsm330 to track orientation. i performed the following test:\n\n\nplace the device in the center of a horizontal rotating plate (\"lazy susan\").\npause for a few seconds.\nrotate the plate 360\u00b0 clockwise.\npause for a few seconds.\nrotate the plate 360\u00b0 clockwise again.\n\n\ni got this output, which appears discontinuous at sample #1288-1289.\n\n\nsample #1288 has  , but sample #1289 has .\n\nplugging in the formulas on page 32 of this document, this corresponds to a change in orientation from  to .\n\nthe graph of (heading, pitch, roll) is also not continuous mod 90\u00b0.\n\ndoes this output make sense? i did not expect a discontinuity in the first plot, since the unit quaternions are a covering space of so(3). is there a hardware problem, or am i interpreting the data incorrectly?\n\nedit: the sensor code is in central.c and main.c. it is read with this python script.\n", "tags": "sensors calibration orientation", "id": "8642", "title": "discontinuity in device orientation"}, {"body": "i found this website http://robotbasic.org/ and it talks about a language used for programming things related to robotics, and i want to make sure whether or not it's worth investing any time or energy into compared to other languages before i just wipe it from my browser bookmarks for good. nowadays, are there better languages and methods for going about the same things that it talks about? \n\ni mean, the site looks pretty old, like something from the late 90s or pre-2010, plus i never heard of it anywhere except for this site, so i wonder if it's just not relevant any more if it ever was.\n", "tags": "control software", "id": "8644", "title": "is robotbasic outdated?"}, {"body": "my professor gave us an assignment in which we have to find the cubic equation for a 3-dof manipulator. the end effector is resting at a(1.5,1.5,1) and moves and stops at b(1,1,2) in 10 seconds. how would i go about this? would i use the jacobian matrix or would i use path planning and the coefficient matrix to solve my problem. i'm assuming coefficient matrix but i am not given the original position in angle form. i was only taught how to use path planing when the original angles are given.\n", "tags": "motion-planning inverse-kinematics motion jacobian", "id": "8654", "title": "finding cubic polynomial equation for 3 joints"}, {"body": "good day,\n\ni am currently working on a project using complementary filter for sensor fusion and pid algorithm for motor control. i viewed a lot of videos in youtube as well as consulted various blogs and papers with what to expect with setting the p gain to high or too low.\n\np gain too low\n\n\n  easy over correction and easy to turn by hand\n\n\np gain too high\n\n\n  oscillates rapidly\n\n\ni have a sample video of what i think a high p gain (3 in my case) looks like. do this look like the p gain is too high?  https://youtu.be/8rbqkcmvs1k \n\nfrom the video:\n\ni noticed that the quad sometimes corrects its orientation immediately after turning few degrees (4-5 deg). however, it does not do so in a consitent manner.\n\nit also overcorrects.\n\nthe reason behind my doubt is because the quadcopter doesn't react immediately to changes. i checked the complementary filter. it updates (fast) the filtered angle reading from sudden angular acceleration from the gyro as well as updates the long term filtered angle changes from the accelerometer (albeit slowly). if i am right, is the the p gain is responsible for compensating the \"delay\"?\n\nthe formula i used in the complementary filter is the following:\n\n\n\nhere is a video for a p gain of 1: https://youtu.be/rsbrwulkun4\n\nyour help would be very appreciated :)\n", "tags": "quadcopter pid sensor-fusion tuning filter", "id": "8657", "title": "p gain tuning for quadcopter (is my perception for a p-gain too high correct?)"}, {"body": "i have a 2dof robot with 2 revolute joints, as shown in the diagram below. i'm trying to calculate (using matlab) the torque required to move it but my answers don't match up with what i'm expecting.\n\n\ndenavit-hartenberg parameters:\n$$\n\\begin{array}{c|cccc}\njoint &amp; a &amp; \\alpha &amp; d &amp; \\theta \\\\\n\\hline\n1 &amp; 0 &amp; \\pi/2 &amp; 0 &amp; \\theta_1 \\\\\n2 &amp; 1 &amp; 0 &amp; 0 &amp; \\theta_2 \\\\\n\\end{array}\n$$\n\ni'm trying to calculate the torques required to produce a given acceleration, using the euler-lagrange techniques as described on pages 5/6 in this paper.\nparticularly,\n$$ t_i(inertial) = \\sum_{j=0}^nd_{ij}\\ddot q_i$$\nwhere\n$$ d_{ij} = \\sum_{p=max(i,j)}^n trace(u_{pj}j_pu_{pi}^t) $$\nand\n$$ \nj_i = \\begin{bmatrix}\n{(-i_{xx}+i_{yy}+i_{zz}) \\over 2} &amp; i_{xy} &amp; i_{xz} &amp; m_i\\bar x_i \\\\\ni_{xy} &amp; {(i_{xx}-i_{yy}+i_{zz}) \\over 2} &amp; i_{yz} &amp; m_i\\bar y_i \\\\\ni_{xz} &amp; i_{yz} &amp; {(i_{xx}+i_{yy}-i_{zz}) \\over 2} &amp; m_i\\bar z_i \\\\\n m_i\\bar x_i &amp; m_i\\bar y_i &amp; m_i\\bar z_i &amp; m_i \\end{bmatrix}\n$$\n\nas i was having trouble i've tried to create the simplest example that i'm still getting wrong. for this i'm attempting to calculate the inertial torque required to accelerate $\\theta_1$ at a constant 1 ${rad\\over s^2}$. as $\\theta_2$ is constant at 0, i believe this should remove any gyroscopic/coriolis forces. i've made link 1 weightless so its pseudo-inertia matrix is 0. i've calculated my pseudo-inertia matrix for link 2:\n$$\ni_{xx} = {mr^2 \\over 2} = 0.0025\\\\ i_{yy} = i_{zz} = {ml^2 \\over 3} = 2/3\n$$\n$$\nj_2 =\\begin{bmatrix}\n1.3308 &amp; 0 &amp; 0 &amp; -1 \\\\\n0 &amp; 0.0025 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0.0025 &amp; 0 \\\\\n-1 &amp; 0 &amp; 0 &amp; 2 \\\\\n\\end{bmatrix}\n$$\n\nmy expected torque for joint 1:\n$$ \nt_1 = i\\ddot \\omega \\\\\nt_1 = {ml^2 \\over 3} \\times \\ddot \\omega \\\\\nt_1 = {2\\times1\\over3}\\times1 \\\\\nt_1= {2\\over3}nm\n$$\n\nthe torque calculated by my code for joint 1:\n\n\n\n$$\nt_1={4\\over3}nm\n$$\n\nso this is my problem, my code $t_1$ doesn't match up with my simple mechanics $t_1$.\n\nthe key functions called are shown below.\n\n\n\ni realise this is a fairly simple robot, and a complicated process - but i'm hoping to scale it up to more dof once i'm happy it works. \n", "tags": "dynamics matlab torque", "id": "8661", "title": "calculating required torques for a given trajectory using lagrange-euler"}, {"body": "i am using a mindstorm robot with an nxt brick, using the graphical interface to create the program. part of the course my robot will take includes a black line on a white background. at the end of the line there is a gap, and after the gap there is a semi circular line. there is a ball that the robot has to hit soon after the robot crosses the gap. \n\nthe robot has a small code to follow the black line for a certain amount of time, enough time so that it stops just before the gap. the the robot just runs forward for 1 second across the gap, then the robot swings an arm to hit the ball, and after that i have the code for line-following again. however, once it crosses the gap, the robot stops in a different place every time, so the arm usually misses the ball. is it possible to program the robot to hit the ball every time/almost every time in the gui?   \n\namong other things i have tried using the ultrasonic sensor to detect the ball, but the sensor does not pick it up.\n", "tags": "nxt mindstorms", "id": "8663", "title": "how to program an nxt brick to always hit a ball?"}, {"body": "i am making a white line follower. i am using an ir sensor module based on tcrt5000. i am directly taking the 8bit adc reading from arduino uno and printing the values on serial monitor. i observe that the values for white are around 25-35, which is ok. the problem arises when i try detecting an orange (158c) surface. the sensor gives me values very close to that of white which is around 25-40. \n\ni can use a color sensor but they are bulky and i am not sure how i can get readings faster with them since they take a finite time for sampling 'r','g' and 'b' pulses. can someone please tell me an alternate approach to detecting the colours or any other possible solution to my problem.\n\nedit: i would like to add that the line i wish to follow is 3cm in width. hence i plan to use three sensors. two just outside the line on either sides and one exactly at the centre. the sampling frequency of arduino uno is around 125khz. sampling ir is not an issue because it is quick but using a color sensor takes a lot of time.\n", "tags": "sensors line-following", "id": "8665", "title": "color sensor alternatives"}, {"body": "i am trying resolve some issues i am having with some inverse kinematics. \n\nthe robot arm i am using has a camera at the end of it, at which an object is being tracked. i can from the the camera frame retrieve a position, relative to that that frame but how do i convert that position in that frame, to an robot state, that set all the joint in a manner that  the camera keep the object at the center of the frame?...\n\n-- my approach -- \n\nfrom my image analysis i retrieve a position of where the object i am tracking is positioned => (x,y) - coordinate.\n\ni know at all the time the position (a) of the end tool by the t_base^tool - matrix, and from the image analysis i know the position (b) of the object relative to the camera frame for which i compute the difference as such c = b - a. \n\ni then compute the image jacobian, given the c, the distance to the object and the focal length of the camera. \n\nso... thats where i am at the moment.. i am not sure whether the position change retrieved from the cam frame will be seen as position of the tool point, at which  the equation will become un undetermined as the length of the state vector would become 7 instead of 6.  \n\nthe equation that i have must be \n$$j_{image}(q)dq = dp$$\n\n\nj_image(q)[2x6]: being the image jacobian of the robot at current\nstate q \ndq[6x1]: wanted change in q-state \ndp[2x1]: computed positional change...\n\n\nsolution would be found using linear least square.. \nbut what i don't get is why the robot itself is not appearing the equation, which let me doubt my approach.. \n", "tags": "inverse-kinematics visual-servoing", "id": "8667", "title": "visual servoing - tracking a point"}, {"body": "i have started working on robotic manipulators and got into a project which deals with control of robotic manipulator using artificial neural networks (solution of inverse kinematics and trajectory generation, to be precise!).\ncan someone please suggest me where to start as i have no prior knowledge about robotic manipulator and ann and how to code them?\n", "tags": "arduino robotic-arm inverse-kinematics machine-learning", "id": "8670", "title": "robotic manipulator"}, {"body": "a couple years ago robocup competitions seems to be quite vivant issue. now when i'm looking for some info about it, it seems to be some kind of insignificant but this may be only my first impression (i was looking for 2d simulator league and it seems that it does not even exist anymore).\n\nso is robocup still alive and significant robotic issue?\n", "tags": "soccer", "id": "8671", "title": "is robocup still vivant and significant robotic issue?"}, {"body": "i'm a software developer and i work for a company that i think could use some automation in its warehouse. i thought it would be fun to put together a prototype of a conveyor system that automates a manual sorting process that we do on our warehouses. i'm primarily a .net developer so i'm wondering if there is an .net sdk for conveyor automation. \n\nany other information on where to start would be helpful but is not my main question here. \n", "tags": "automation", "id": "8673", "title": "is there a .net library for conveyor belt automation?"}, {"body": "please help me with the following task. i have mpu 9150 from which i get acceleration/gyro and magnetometer data. what i'm currently interested in is to get the orientation and position of the robot. i can get the position using quaternions. its quite stable. rarely changes when staying still.\nbut the problem is in converting accelerometer data to calculate the displacement.\n\nas i know its required to to integrate twice the accel. data to get position.\nusing quaternion i can rotate the vector of acceleration and then sum it's axises to get velocity then do the same again to get position. but it doesn't work that way. first of all moving the sensor to some position and then moving it back doesn't give me the same position as before. the problem is that after i put the sensor back and it stays without any movement the velocity doesn't change to zero though the acceleration data coming from sensors are zeros.\n\nhere is an example (initially its like this):\nthe gravity: -0.10  -0.00   1.00\nraw accel: -785 -28 8135\naccel after scaling to +-g: -0.10   -0.00   0.99\nthe result after rotating accel vector using quaternion: 0.00   -0.00   -0.00\n\nafter moving the sensor and putting it back it's acceleration becomes as:\n 0.00   -0.00   -0.01\n 0.00   -0.00   -0.01\n 0.00   -0.00   -0.00\n 0.00   -0.00   -0.01\nand so on.\nif i'm integrating it then i get slowly increasing position of z.\n\nbut the worst problem is that the velocity doesn't come back to zero\n\nfor example if i move sensor once and put it back the velocity will be at:\n-0.089 for vx and\n0.15 for vy\n\nafter several such movements it becomes:\n-1.22 for vx\n1.08 for vy \n-8.63 for vz\n\nand after another such movement:\n\nvx -1.43\nvy 1.23\nvz -9.7\n\nthe x and y doesnt change if sensor is not moving but z is changing slowly.\nthough the quaternion is not changing at all.\n\nwhat should be the correct way to do that task?\n\nhere is the part of code for integrations:\n\n\n\ncurrently set speed to 1 just to test how it works.\n\nedit 1: here is the code to retrieve quaternion and accel data, rotate and compensate gravity and get final accel data.\n\n\n\n\n  edit 2:\n\n\ndmpgetquaternion, dmpgetaccel functions are just reading from the fifo buffer of mpu. \n\ndmpgetgravity is:\n\n\n\nedit 3:\nthe library for using mpu 9150:\nhttps://github.com/sparkfun/mpu-9150_breakout\n\nedit 4: another example \n\ngravity vector: -1.00   -0.02   0.02\nraw accel data: -8459   -141    125 \naccel data scaled (+-2g range): -1.03   -0.02   0.02\ngravity compensation and rotation of accel data: -0.01  0.00    0.33    \n", "tags": "accelerometer algorithm", "id": "8680", "title": "calculating position based on accelerometer data"}, {"body": "i am thinking of a project proposal for my robotics course and we are required to make one that has a potential application on physical therapy or medical fields. one thing that came across my mind is a motorized wheelchair that moves when a stress ball control is squeezed by the user. as a robotics novice, i wonder if i could integrate a sensor circuit with a rubber ball so that when it is pressed, perhaps by a stroke patient, it triggers some driver circuit. is this possible? if so, how? my experience with robotics is limited to arduino, servo motors and basic sensors.\n", "tags": "wheeled-robot", "id": "8684", "title": "is there a way to use a stress-ball-like device as acceleration control interface"}, {"body": "this is from the icreate 2's document:\n\"pins 1 and 2 (vpwr) are connected to the roomba battery through a 200 ma ptc resettable fuse. the continuous draw from these two pins together should not exceed 200 ma. do not draw more than 500 ma peak from these pins, or the fuse will reset.\"\nmy project just need to draw a bit more than that number. so is there anyway to disable - short circuit - that fuse or replace that fuse with a bigger one? where does that fuse reside on the bot's circuit board?\nif the above 2 is not possible (because the fuse is embedded inside some chip or too difficult to access for example), is it safe to run a small wire from the battery pole to the pin to by pass that fuse?\n\n\ni know i can run a wire directly from battery pole to my circuit or draw power from the motor wires, but i love running through the serial port with keep things simple.\n\n", "tags": "power irobot-create serial", "id": "8686", "title": "increase current draw from serial port of icreate 2"}, {"body": "if i build my quadcopter with the following components, will it take off vertically smoothly?\n\nframe: f450 glass fiber and polyamide nylon [280g]\n\nlanding gear: high landing gear for f450 [90g]\n\nbattery: 3000mah 20c (30c burst) 3s/1p [260g]\n\nmotor: xxd a2212 1400kv brushless outrunner motor [4x50g]\n\nesc: 30a (40a burst) brushless with bec - 2a/5v [4x25g]\n\nprop: 1045 propeller set\n\n\n  assume that the total weight of the quad is ~1.1kg and i would like to have payload upto 400g, making the quad weigh ~1.5kg\n\n\nfrom all that i've learnt, the thrust:weight ratio should never fall below 1.7:1 (and a 2:1 is recommended to have better control) which if does creates problems in lifting my quad vertically smoothly. i'm neither planning to have very high maneuverability nor cruise the sky pushing my quad's limits. i just want it to fly.\n\n\n  here's the motors pull(g)-amps(a)-voltage(v)-power(w)-specificthrust(g/w) information for a2212 1400kv with 10 x 4.5 props\n\n\n\n\nsince my battery's discharge rate is limited to 20c, if i'm not wrong, it can give out only 60a (since its 3000mah) for the entire circuit with four motors, each of which can take 15a max, which from the above table would produce ~600g thrust, which is 2400g from all four motors.\n\n\n  does this mean i can get 1.6:1 thrust:weight at 100% throttle? can my quad still take off vertically smoothly?\n\n\ni'm also confused about the efficacy of my motor, meaning if i buy a bigger motor, can get more than 600g thrust at 15a/11.1v with same/bigger props? if not, is this the most efficient combination of motor and prop? \n\n\n  if yes, what is the maximum thrust (practical pulling force in g, not in n) i can get outta 15a/11.1v? which equation tells defines that exact relation, provided i fly my quad at usual conditions (1013hpa/25degc exttemp/80%-90% relhumidity/max15malti)?\n\n\nps: i tried ecalc, prop power thrust and efficiency calculations, and a few other online stuff.\n\nupdate: will replacing the 3000mah-20c/30c battery with the 5000mah-20c/30c 3s1p keeping everything same solve some problems and increase the flight time, provided i keep everything else the same?\n", "tags": "quadcopter brushless-motor battery lithium-polymer", "id": "8689", "title": "how do i if know my quad is powerful enough to take off vertically smoothly and how do i calculate max thrust(g) for given current(a) and voltage(v)?"}, {"body": "i am a newbie,\ni got a 30w motor here,\nhttp://www.rcbazaar.com/product.aspx?productid=1915\n\n\n\nthey claim to have 300g thrust,however compaired to other motors like on hobbyking,\nthis motor seems to have too good ((power input )/thrust) ratio.\ncould it be true?\nbesides,esc of 6 amp (30/7.4) seems sufficient,why did they recommend 20amp?\nalso,what is the max thrust i can get from a motor w.r.t. power consumed in decent price.\n", "tags": "quadcopter brushless-motor", "id": "8692", "title": "what is the max thrust a 30w brushless motor can produce? can it produce 300g?"}, {"body": "i am using a differential wheel robot for my project. i need to know the current coordinates of the robot with respect to it's initial position taken as the origin. i m doing the computation on an arduino uno and the only sensory input that i get is from the two encoders. i have the function updateodomenty() called in the loop() and this is it's corresponding code:\n\n\n\nthis is the code that me and my team mates made after reading this paper. i am wondering if this is the best possible way to solve this problem with an arduino. if not, how is odometry done in differential wheeled systems?\n", "tags": "arduino kinematics odometry differential-drive", "id": "8693", "title": "how to perform odometry on an arduino for a differential wheeled robot?"}, {"body": "first, i don't see any manufacturer or online store telling whether an esc is ppm controlled or not. second, i have also been googling and asking comments in all about escs youtube videos for long, but i couldn't find anything useful.\n\n\n  why do i need ppm controlled escs?\n\n\ni'm doing a project based on androcopter and its clearly mentioned that it specifically requires the use of ppm controlled escs.\n\n\n  can i use any escs available in the market for this project?\n\n\nit's also mentioned in the github repo that ppm controlled escs are the most common ones. however from some who explained escs in youtube video has commented back for my doubt telling that most common escs are pwm controlled which is contradicting the previous statement.\n\nps: i need to use arduino mega to control the four escs. and arduino mega is programmed to send out ppm signals which is exactly why i need ppm controlled escs. correct me if i made any mistakes.\n", "tags": "arduino quadcopter esc", "id": "8698", "title": "what exactly are ppm controlled escs? are most escs available to build quadcopters ppm controlled?"}, {"body": "i am the moment having some issues with an jacobian going towards a singularity (i think)as some of its values becomes close to zero, and my robot oscillates, and therefore thought that some form of optimization of the linear least square solution is needed. \n\ni have heard about the interior point method, but i am not sure on how i can apply it here. \n\nmy equation is as this.. \n\nj(q)dq = du \n\nhow would i have to implement the optimization, and would be needed?\n", "tags": "inverse-kinematics c++", "id": "8700", "title": "inverse kinematics osciliations.."}, {"body": "i have purchased the undermentioned robot chassis with dc motors supported with plastic gears from a local store. there is a 3a battery holder and when i connect robot to that and put it on the ground both the motors working fine and smooth. but when i connect the l298n motor controller which was made by myself and tested with proteus, only one motor is working. when i switch the wires of both motors, still the motor which worked before is working and the other motor never runs unless i manually give a little rotation to the wheel.\n\ni use a pic18f4431 to control the motor controller and tried usb power and a 5v regulator created by myself and the result is the same in both occasions. what could be the issue here? if i tried oiling the malfunctioning motor will it work? i heard one of my friends having the same issue, one motor is not working. but both motors work with just two pen torch batteries and can't think of a valid reason that the motor is faulty. may be my motor controller? but when i swap the wires from faulty to the working one, the working one still works as i've mentioned above.\n\n\n\n\n", "tags": "mobile-robot", "id": "8702", "title": "malfuction in motors using l298n"}, {"body": "i have a project that is going to involve capturing ~80 images of a 35m x 50m agricultural study area for image processing.\n\ni am wondering whether to use a naza v2 or pixhawk controller to outfit my ship (on a dji f550 flamewheel airframe).\n\nmy understanding is that naza is limited as far as the amount of waypoints i can have during a particular mission. can i break my imaging mission up into 8 or 10 sub-missions?\n\ni also understand that pixhawk is in many ways superior but getting it up and running can be more finicky - i am on a limited time frame for this project.\n", "tags": "gps uav radio-control", "id": "8703", "title": "pixhawk or naza m v2 for aerial imaging of small study area with hexacopter"}, {"body": "i want to make pc controlled quadrotor. all the tutorials/projects made with rc receiver. i want to use arduino or xbee instead of rc receiver for pc control of quadrotor. how can i do this. \n\nnote: i have arduino, beaglebone, xbee, hc-05, kk2 and multiwii parts. \n", "tags": "pid quadcopter", "id": "8704", "title": "using another device instead of rc transmitter"}, {"body": "i have three hc-sr04 sensors connected to a pic18f4431 with the schematic provided below. before building the pcb i've tried testing all three sensors with a bread board and they worked fine. but now i have my pcb and when i connect them to that and tried testing, they work only for a few seconds and then stop working.\n\ni set timers and a set of leds to lit if an item is in between 40cm from each sensor. as i've tried from the bread board, when i cross my arm withing that range, the appropriate bulbs are lit and when i take my arm off the other bulbs are lit. but using the pcb, when i upload the code through a pickit2 they work fine for a few seconds and then they freeze. if i reset the mclr pin it works again for another few seconds and freeze again. and sometimes randomly if i touch the receiving part of the sensor it works but that happens randomly. not always working. what could be the issue? \n\nis my oscillator burnt while i was soldering it? once i connected two 0.33uf polar capacitors and found out that for one second, it takes one minute or more to blink a bulb.\n\n\n\n", "tags": "mobile-robot", "id": "8705", "title": "problem with hc-sr04 sensor"}, {"body": "i've kind of finished implementing a birrt for a 7 dof arm, using a kd-tree from numpy.spatial in order to get nearest queries. a picture is below:\n\n\n\ni'm currently having trouble with the fact that it is impossible to retrieve a path from the start node to a particular node using a kd-tree, and while i do have an array of of all the nodes, and there are edges that can be calculated between subsets of the array, but the edges are not in any useful order. can anyone give me some tips on how i'd retrieve a path from the starting node in the first array, and the ending node in the second array? are there any useful data structures that would let me do this? below is my code\n\n\n", "tags": "inverse-kinematics motion-planning planning rrt", "id": "8709", "title": "birrt: getting path from an array of 7 dof angle configurations"}, {"body": "before i ask my question, i'd better confirm that i've read the most prominent post about running ros on raspberry pi devices.\n\nthat post contains some valuable information, but it's a bit dated, and ros support for arm devices is much better these days. in fact, ros 2.0 is evidently going to have excellent support for running on embedded devices like the raspberry pi.\n\ni just got a pi model 2 for my birthday, and i'm really eager to get ros running on it so i can build a robot i've been working on, which is based on the wild thumper 6wd platform.\n\nfrom my perspective, here are a few pros &amp; cons regarding ubuntuarm and rosberrypi:\n\nubuntuarm\n\npros:\n\n\nubuntu is the official ros distro and is the most well-supported ros os \nthe best documentation on the ros wiki for running on arm devices is written for ubuntuarm\n\n\ncons:\n\n\nraspbian (on which rosberrypi is based) is the official distro for rasbperry pi and thus has the best support for the board.\n\n\n\nrosberrypi\n\npros:\n\n\nraspbian (on which rosberrypi is based) is the official distro for rasbperry pi and thus has the best support for the board.\ncons:\nros is not well supported on os's other than ubuntu\nto use the rosberrypi distro, you must build ros from source.\n\n\nmy question is: can anyone provide any further insight into this dilemma? if you've been running ros on your raspberry pi 2 (model 2 only please; the model b+ has completely different issues, like not being well-supported by ubuntu), what's your experience been? \n\nwhich distro would/did you choose, and why?\n", "tags": "ros raspberry-pi arm", "id": "8710", "title": "ros on raspberry pi model 2: ubuntuarm vs rosberrypi"}, {"body": "eigen library (http://goo.gl/cv5ly), which is used extensively in ros and pcl.\n\nthank you.\n", "tags": "ros", "id": "8711", "title": "how to convert rotation matrix in to equivalent quaternion using eigen library."}, {"body": "\n\ni have got a robot that exactly looks like as shown in the figure above. i have worked out the inverse kinematics analytical solution without the base rotation (considering 2 dof alone) but i am not able to find the analytical solution including the base(3 dof). how do i find the anlytical solution for this robot ?? \n", "tags": "robotic-arm", "id": "8717", "title": "3 degrees of freedom analytical solution"}, {"body": "i am studying an article about estimating facial points using forests of regressions (random forests). in order to train the trees of the forest, the training algorithm is initialized by a mean shape. i don't understand what a mean shape is. i looked it over the internet and came across it in articles that talk about face detection and facial shape estimation, but there was no clear explanation of it. can you please tell me what exactly is a mean shape?\n", "tags": "computer-vision artificial-intelligence", "id": "8719", "title": "mean shape for initializing the training of a face shape regression tree"}, {"body": "i am working on proposal of autonomous fire fight robots but i'm little bit confused about its sensor and algorithms. my friend suggested there are some path finding  algorithms like , , , and  which are used in robots, but i didn't believe it. \n\ni want to ask: are these algorithms used in real world robots or some other genetic algorithms? how does a robot discover path to detect, and differentiate, a human from fire?  i only want some explanation that gives knowledge.\n", "tags": "algorithm machine-learning", "id": "8720", "title": "which algorithms are used in autonomous robot"}, {"body": "can a micro controller flash itself?\nwhat i mean to say is, i have an stm32f103rg with 1mb flash size. \ni have a uart communication modem connected to it. can i send a firmware (.hex or .bin) to the microcontroller via the radio verify checksums, on sucess the microcontroller saves the file into a sd card ( via spi ) and then restarts itself and start flashing itself reading from the file ( in the sd card )?\n\ncan something like this be done or an external mcu is required to carry out the flashing?\n\nthe purpose is the microcontroller and radio will be sitting at a remote location and i need a way to change the microcontroller's firmware by sending it a firmware update file remotely. \n", "tags": "microcontroller", "id": "8722", "title": "microcontroller flashing itself"}, {"body": "what should a quadcopter have, or have access to, in order to make this 'return home' feature work? is gps enough? what is the approach needed to make this happen?\n\ni used a arduino mega 2560 with imu to stable my quadcopter.\n", "tags": "arduino quadcopter sensors localization gps", "id": "8726", "title": "how to have a 'auto go home' feature, like the dji phantom 3, on a project built quadcopter?"}, {"body": "i'm hoping to use a dc motor to drive a cog bar horizontally along a track. afaik, i'd need to install a (plastic) cog on the motor shaft, which itself grips on the (plastic) cog bar. does anyone know how to prevent the cog from shifting on the shaft? the shaft is 10mm long and i'd like to make sure the cog cog sits at 5mm, where the cog bar is.\n\nany help will be appreciated.\n\nthanks\n", "tags": "motor", "id": "8728", "title": "dc motor shaft and gear installation"}, {"body": "i have a irobot create model 4400 and i need to send commands to the open interface through ubuntu. i'm using gtkterm at 57600 baud but when i press play button, it only drives around itself. i have tried to send commands as raw data and as hexadecimal data but it doesn't work.\n\nwhat am i doing wrong?\n", "tags": "irobot-create roomba linux", "id": "8731", "title": "sending commands from ubuntu"}, {"body": "i bought a 3-axis magnetometer (similar to this one ) and plugged into an arduino in order to read the heading value.\ni mounted it on my robot and i drove with the robot for around 30 meters, turned 180 degrees and drove back to my starting position. i plotted the heading value and it shows inconsistent values. the 180 degrees turn started at sec 55, the rest is driving in one direction using a joystick and following a wall as reference so small deviations are expected but not that big as in the image.\n\n\n\nwhen the robot is turning in-place, there is no such problem and the heading variation follows the rotation of the robot. the robot (turtlebot) is a little bit shaky such that the magnetometer doesn't always have the x and y axes parallel to the floor but i don't think few degrees of offset can cause such a huge difference. i calculate the heading as follows:\n\n\n  heading = atan2(y field intensity, x field intensity)\n\n\nwhy does this happen? could it be form some metals or electric wires under the floor?\ncan you suggest a more robust method/sensor for estimating the heading in indoor environments?\n\nedit:\n\ni drove the same path again and the pattern similarity is making it even weirder\n\n", "tags": "mobile-robot navigation magnetometer", "id": "8732", "title": "weird magnetometer values"}, {"body": "i'm looking at the n20 dc motor which is fairly popular. does anyone know if the shaft could be swapped out for a threaded shaft?\n", "tags": "motor", "id": "8742", "title": "could a motor shaft be swapped for a threaded shaft?"}, {"body": "guys my robot looks like this,\n\nthanks to @croco i came to know that it looks much similar to phantom omni. since it looks similar to the phantom omni i am trying to get the inverse kinematics geometric solution for it. using this inverse kinematics solution i can build my fpga design. there is a very good research paper on this\n\nhttp://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6318365&amp;url=http%3a%2f%2fieeexplore.ieee.org%2fxpls%2fabs_all.jsp%3farnumber%3d6318365\n\nbut i do not understand in page 2122 how they find the l3 and l4 (shown in the image below). if i find this for my robot then my project is almost done. how do i find l3 and l4 for my robot ? should i just bring my end effector to the (0,0,0) position as shown in fig2  amd then measure the l3 and l4, would this work ?? would be great if you guys could help. cheers.\n\n", "tags": "inverse-kinematics geometry", "id": "8748", "title": "phantom omni type robot inverse kinematics solution"}, {"body": "i am thinking about building a small robotic arm with 4 small servo motors and an arduino uno to apply basic neural networks concepts.\n\nis it  a good idea to use a hand made robotic arm to learn more the power of neural networks?\n\nthank your for your time and merry christmas\n", "tags": "control", "id": "8749", "title": "bulding a robot arm for neural networks understanding"}, {"body": "i am currently in the process of writing a pose estimation algorithm using image data. i receive images at 30 fps, and for every image, my program computes the x,y,z and roll, pitch, yaw of the camera with respect to a certain origin. this is by no means very accurate, there are obvious problems such as too much exposure in the image, not enough feature points in the image, etc., and the positions go haywire every once in a while; so i want to write a kalman filter that can take care of this part.  \n\ni have read through the basics of kf, ekf etc. and then i was reading through an opencv tutorial that has an implementation of a kalman filter inside an algorithm for the pose estimation of an object. while this matches my use case very well, i don't understand why they are using a linear kalman filter while explicitly specifying parameters like (dt*dt) in the state transition matrix. for reference, the state transition matrix they are considering is \n\n\n\ni'm a little confused, so my main question can be broken down into three parts:\n\n\nwould a linear kalman filter suffice for a 6dof pose estimation filtering? or should i go for an ekf?\nhow do i come up with the \"model\" of the system? the camera is not really obeying any trajectory, the whole point of the pose estimation is to track the position and rotation even through noisy movements. i don't understand how they came up with that matrix.\ncan the kalman filter understand that, for instance, if the pose estimation says my camera has moved half a meter between one frame and other, that's plain wrong, because at 1/30th of a second, there's no way that could happen?\n\n\nthank you!\n", "tags": "kalman-filter pose", "id": "8751", "title": "how do i go about implementing a kalman filter for a pose estimation algorithm?"}, {"body": "i would like to wirelessly control the button on a spray can such that on pressing it the spray comes. for example, a deodorant bottle.\n\nwhat is the best way to do this ?\n\nwhat thing can i mount on the bottle to do this?\n", "tags": "robotic-arm", "id": "8752", "title": "automatic activation of a spray can"}, {"body": "i have studied claus brenner's lectures on how to implement the fastslam 1.0 algorithm, where each particle maintains the robot pose, and maintains ekf's of the landmarks.\n\nhowever, i'd like to implement fastslam 2.0. which i understand uses particle filters completely. is this the same as fs 1.0, but instead of each particle maintaining an ekf of the landmark, each particle maintains yet another array of particle filters for landmarks?\n", "tags": "slam", "id": "8757", "title": "fastslam 2.0 implementation?"}, {"body": "good day,\n\ni have a robot with an imu that tells yaw rate, and magnetic heading. it also tells xvelocity and yvelocity at that instance of the vehicle, on the vehicle frame. (so irrespective of heading, if the robot moved forward, yvelocity would change for example)\n\nassuming my robot starts at position (0,0) and heading based on the magnetic heading, i need to calculate the next position of the robot based on some world frame. how can i do this?\n", "tags": "mobile-robot", "id": "8758", "title": "2d robot motion"}, {"body": "i have to program an autonomous robot to traverse through a grid given in the following figure.\nbut the main problem is that the nodes to visit is not known beforehand, it will be received by the bot in real time.\ne.g.- after reaching the node 19, the bot now has to go to node 6. the shortest path(19-17-7-6) can be calculated by dijkstra algo but i don't know how to make the robot traverse that path.\nany idea ?\n\n\n\nedit: sorry for not making the question clear enough.\ni am facing the problem in determining the current position and the direction the robot is facing so i can't define the set of commands (turn left/right/forward) to traverse to the next desired node.\ni am thinking about an extra array for previously visited nodes and the current node and an extra variable for facing direction.\nbut for that i will have to define command sets for each node from each and every node.\nany better idea ?\n", "tags": "mobile-robot automatic dynamic-programming", "id": "8761", "title": "completely autonomous traversal of a planar graph"}, {"body": "i just got a create 2 for christmas, and while i'm planning to create with it (obviously), i'd like to use it around the house as a vacuum if at all possible. i've heard that you can buy parts for the roomba and throw them on to this chassis, but i wanted to confirm/refute that before i bought anything. is that possible or am i crazy?\n", "tags": "irobot-create", "id": "8763", "title": "irobot create 2 to vacuum?"}, {"body": "i recently googled about pepper robot and i wonder how one could write apps for it and get money for them. as far as i know they have app store, but does it sell apps or give them for free? (all info i googled myself is rather incomplete and old - probably outdated)\n\nalso i believe that apps for such (or similar) robots is the potential multibillion-dollar market. what do you think about that?\n", "tags": "mobile-robot", "id": "8765", "title": "apps for pepper robot"}, {"body": "i'm hoping to move a plate (3mm x 45mm) back and forth using a dc motor. here's my idea so far:\n\n\n\nthe motor drives a threaded shaft which is attached on one side of the plate. to help alignment, a rod is added to the other side of the plate (red). my guess is that if it's just a rod in a through hole, it could potentially jam.\n\nafaik, usually, in bigger setups, a linear bearing would come in handy. however, given that the plate is just 3mm thick, are there better ways to help alignment? could making the edge around the through hole like the inside of a donut help? something like\n\n\n\nis it easy to make? in fact, is my concern actually valid?\n\nthanks\n\nedit the centre area of the plate needs to be kept clear. this is intended to be part of a (~10mm thick) pole climber, where several guide rollers are fitted on the left side of the plate and a motor driven roller is on the left of the part (not depicted). so the idea is the press the guide roller against the pole until the two rollers have a good grip on the pole. the whole car is fairly light, so the force expected is around 30n.\n\nhere's a more complete depiction:\n\n\n\nthe rollers are spring loaded, but they need to be released and retracted - and adjusted for different pole widths.\n", "tags": "motor linear-bearing tracks", "id": "8768", "title": "moving a small plate back and forth"}, {"body": "does anyone know of a good (low profile) way to attach the threaded (chamfered) end of a m5 screw shaft to a 30mm (30t 1m) plastic cog wheel? would it be a good idea to widen the shaft hole and just let the threads grip the hole?\n\nedit the torque will be around 6kg-cm and the plastic i'm looking at is pom.\n", "tags": "mechanism torque gearing", "id": "8775", "title": "attaching a m5 screw shaft to a cog wheel"}, {"body": "i have a panasonic msmo22f2g servo motor that i'm using as a stepper motor. the motor has 4 wires coming out of a port farther to the front, and 11 wires coming out of a port towards the rear of the unit (presumed to be the encoder). i am trying to drive the motor with an arduino motor shield. \n\nmy question is this, how do i hook it up?\n\ni have read that a stepper with 4 leads is a bipolar stepper, and that you group same coil wires together. i found that 3 wires on my stepper were on the same coil and the fourth seems to have no effect on the stepper. my checking process was using an ohm meter to see what was connected to what, as well as connecting wires and feeling the resistance.\n", "tags": "arduino stepper-motor", "id": "8779", "title": "panasonic msmo22f2g (stepper motor hook-up)"}, {"body": "just a short question: the irobot create 2 open interface spec says:\n\nserial sequence: [167] [days] [sun hour] [sun minute] [mon hour] etc.\n\ncan somebody explain to me, what \"days\" stands for?\n", "tags": "irobot-create", "id": "8785", "title": "roomba schedule opcode: 167, byte 1"}, {"body": "i have a friend who is getting into quadcopters and being the good techie buddy, i'm trying to find the right technology for battery monitoring so his expensive machine does not fall out of the sky unexpectedly. \n\nso far the only technology for hobbyists that i am seeing is voltage monitors, which aren't really useful for this battery chemistry. with the flat voltage curve liion has i'd expect a voltage monitor to falsely report a low battery when you draw extra current and indeed i'm seeing exactly this when my buddy does fast maneuvers mid flight.\n\nin my day job we use charge counting battery monitors (bms) for this battery chemistry. usually custom designed for the battery pack (just like for laptop batteries, etc). sometimes built into the battery pack, or sold by cell suppliers.\n\nhave i missed a product for electric aircraft? are hobbyists in the battery dark ages?\n", "tags": "battery lithium-polymer", "id": "8786", "title": "are there any lithium ion battery monitors designed for hobbyists (quadcopters)?"}, {"body": "i've built quadruped robot which is using 12 servos (towerpro sg90 servo) and raspberry pi (model b 1). right now i'm \"feeding\" it with 5v 2.5a charger. \n\nhow can i make it un-tethered? what should i look out for when selecting batteries? also, i think that i need to separate powering of rpi and servos because power is \"jumping\" when it moves and that isn't good for rpi.  \n\na little video - testing walking algorithm\n", "tags": "mobile-robot raspberry-pi power battery walking-robot", "id": "8787", "title": "best power solution for my robot"}, {"body": "i have a rather simple setup for my robotic extender. the dc motor turns a shaft with a worm on it. the worm connects to a small worm gear (green) which itself has a small gear (red) on the same shaft connecting to a large gear (blue):\n\n\n\nthe dc motor's gearbox gives around 50-100rpm and has a stall torque of around 2kg-cm. the small gear should be around 15mm tall and the large gear should be around 45mm tall.\n\nif the load (on the large gear) has a maximum torque of 5kg-cm and a typical torque of 3kg-cm, could the two gears be made from plastic and be of module (metric form for pitch, as @chuck pointed out) 0.5? is a higher module needed? how about the worm, could it be made of plastic (nylon?)?\n\nany help will be appreciated.\n\nedit fixed typos and updated diagram.\n", "tags": "motor torque gearing", "id": "8797", "title": "suitable gear construction for a robotic extender - plastic?"}, {"body": "just received my first hobby-grade quadcopter.\n\nits the eachine racer 250 and comes preassembled with transmitter and receiver also included.\n\nit comes with some kind of cc3d flight board, most people say its not the original one, but can be configured with the same software.\n\nit is actually flying very well right out of the box so im not sure if i want to touch the fc config.\n\nim mostly interested in learning to fly in manual/acro mode, the transmitter seems to have a switch with 3 flight modes, first 2 looks like low/high rates in self-level mode, im expect the third to be the acro mode, but im not sure right now, i couldnt test it because of the weather, it could be a third higher rate?.\n\nso, is there any way i can look at the actual fc config without changing anything? witch software do i need? and are the flight modes actually set on the fc or transmitter so i could be able to see and edit them?\n\nthanks in advance.\n", "tags": "quadcopter", "id": "8804", "title": "see cc3d actual configuration"}, {"body": "i'm looking at this setup: \n\n\n\nwhere the pom bevel gears are fitted with some kind of metal (bronze?) tube inside which fits over the shaft. what benefits does this method provide? is it to allow the shaft to free-spin? the metal fitting wouldn't be able to grip on the shaft - right?\n\nis it supposed to be a canonical approach to fitting a pom gear for (relatively) high-load applications?\n", "tags": "motor mechanism gearing", "id": "8807", "title": "pom gears and metal fittings"}, {"body": "i'm now considering to choose gazebo or simmechanics for simulating my quadruped robot.\n\ni set some standards for the simulation:\n\n\nsupport real-time application with ros\nsimulate contact impact well(with ground) (deformable if possible)\ngood rendering quality.\n\n\ni have learned gazebo for months, and see it has some limits to meet my requirements, especially the contact and friction problems. \n\ni didn't use simmechanics, but when i'm very impressed on it when i see this video.\n\nanyone who has experiences on quadruped simulation can share me some advice?\n\nthank you so much.\n", "tags": "simulator gazebo", "id": "8810", "title": "gazebo or simmechanics"}, {"body": "i am building a quadcopter and i was wondering if a 5200mah 30c 22.2v 6s lipo battery will work with a 40amp esc's, mt4108 370 kv motors, and gemfan 1470 carbon fiber props. the over all payload will be about 5-6 pounds.\n", "tags": "quadcopter", "id": "8811", "title": "will a 5200mah 30c 22.2v 6s lipo battery work with"}, {"body": "http://www.irobotweb.com/~/media/mainsite/pdfs/about/stem/create/create_2_wheel_hack.pdf?la=en\n\nscroll to page 3.\n\ni'm trying to interface the roomba's preloaded navigation system with a pair of motors not attached to the roomba itself- however, to do this i need an interface board of the same dimensions as the one pictured in the above document. it has 0.050\" (1.27) contact centers, which don't seem to be commercially available. can anyone provide any help locating pcbs of this size?\n", "tags": "irobot-create electronics navigation roomba", "id": "8816", "title": "create 2: wheel interface board replacement"}, {"body": "what is the most accurate way to locate the position and orientaion of the body during some motion (rotation and translation)?\n\ni need to track the body very precisely, the required accuracy is 100-200 microns, and with rather high frequency - at least 1khz. the body has one rotation axis. this axis can translate along the path. normally the track has ellipse like shape, but translation path can change, that's why i need to track the body. the limit of motion is 50 cm on any direction. maximum velocity is 5 m/s. \n\nrequirments about sensors: it's possible to place any sensor on the surface, but it's impossible to change the construction. so it's impossible to use encoders at the rotation axis to measure the angle. \n\ni tried to do it with mems 9dof sensors, but because of the noise it's very difficult to understand when there is a motion and when it's a noise.\nanother idea is to use magnet and magnetomemter, but how is it possible to measure the resolution in this way? \n", "tags": "kinematics navigation tracks orientation", "id": "8818", "title": "accurate technique to locate position?"}, {"body": "i am working on an arduino robot project. this project requires a base with 4 wheels where the back wheels are attached to two dc motors that can be controlled independently of each other. i am thinking that the robot will turn by giving power to just one of the motors but i am having trouble with how the front axis should look like. would it be possible to have a solid front axis with 2 wheels and still possible for the robot to turn or would the friction be too great?\n", "tags": "arduino wheeled-robot brushless-motor", "id": "8820", "title": "arduino project: turning with a fixed front wheel axis"}, {"body": "as part of a sorting machine, i need to orient a pile of plastic brick-shaped objects (which are all identical in size - about 3cm x 2cm x 1.5cm) so that they always end up with the white side facing up:\n\n\n\nthese will then be fed into a bowl feeder type of machine for further re-orienting. \n\nhow can i accomplish this, preferably without optical sensors? i was thinking about cutting into the bricks and putting magnets inside, but is there a more elegant solution?\n", "tags": "mechanism orientation", "id": "8822", "title": "orienting rectangular plastic bricks"}, {"body": "if a lipo battery has more mah will it be slower to run out of energy or will it have larger power output?\n\nthanks in advance\n", "tags": "power battery", "id": "8823", "title": "does the mah of a battery mean longer power or more power?"}, {"body": "are joint-state vectors $q$, which define the position and orientation of a set of joints, limited somehow? \n\ni know they are used for the rotation part of the transformation matrix => therefore i would think they were limited within 0 and 2$\\pi$\n", "tags": "joint", "id": "8831", "title": "are joint state vectors limited?"}, {"body": "i am using arduino mega to run 4 motors via 4 motor controllers. i am using a ps2 controller as the remote control. when the joystick is at rest, the motor should stop.\n\nthe problem is the motor still moves randomly when the joystick is at rest. sometimes, the joystick can only produce forward motion on the motor but not backward. is this the grounding issue or the ps2 remote control issue ir others.. does the gnd from the arduino board have to be connected to the gnd from the external battery? \n\nhow can i troubleshoot this? \n\nthanks.\n", "tags": "mobile-robot control power", "id": "8833", "title": "erratic motor behavior. is it due to the faulty remote control or grounding or something else?"}, {"body": "how can it be used to determine the transformation matrix?\n\nan example could be at computing the inverse kinematics for small displacements:\nj(q)$\\delta$q = $\\delta$u\n\n$\\delta$u is a vector defining the difference between current and desired position. the desires position can always be computed, but if keep solving this in such manner that every time you solve $$j(q)\\delta q = \\delta u$$\n\nyou do this \n\n\nq:= q + $\\delta$q\ncompute $t_{base}^{tool}(q)$\ncompute the difference between $[t^{tool}_{base}]_{desired~position} $ and $t_{base}^{tool}(q)$.\nif change is less than 10^-5 finish and output q, if not resolve. \n\n\nhow would you compute the transformation matrix based on q state vector. \n", "tags": "inverse-kinematics jacobian", "id": "8835", "title": "q-state vector used to define the transformation matrix? how?"}, {"body": "i have just disassembled a rc car (a bbr 1:18 scale ferrari enzo from 2005). attached to the stepper motor that controls the steering of the car was a small circuit board with a cog wheel attached to it. i am trying to figure out what this circuit does. my idea is that it is responsible of keeping track of the wheels position but i am not certain. here is a picture of it:\n\n\n\n\n\nas you can see there are 5 wires coming out of it and i know that the green one is gnd. does anyone have an idea of what its function might be?\n", "tags": "stepper-motor radio-control circuit", "id": "8840", "title": "rc car circuit: what does it do?"}, {"body": "is there any general problem in using precise positioning (centimeter-accurate gnss rtk system meant) for autonomous car guidance given i have a predefined path the car should follow? i mean, autonomous cars were the topic #1 at ces 2016 yet no such simple system seems to have been introduced to date... of course the \"path planning\" is only a part of the \"autonomous package\" and other problems need to be solved (collision prevention etc.) but i really wonder whether something simple like rtk guidance could be used.\n\nan rtk system relies on very little amount of live correction data (about 1 kb/s) and mobile networks are really ubiquitous today so i can not really see a technical problem in such solution given there are enough rtk base stations around.\n\nedit:\n\nthis question is only about using precise positioning to follow a predefined track in obstacle-free environment. i am not asking about other systems that need to be implemented in an autonomous car like collision prevention etc. (such systems may employ lidar or a stereo camera). surely a collision prevention is a must for an autonomous system but i consider a theoretical case only.\n\nan updated question may be: is precise satellite positioning accurate enough to guide/navigate a full-scale passenger car in an obstacle-free outdoor environment in the speed of about 100 km/h given i have a precise-enough path prerecorded that is to follow?\n\nsome answers below already say yes, this is a solved problem. it would be nice to have the answers elaborated in more detail regarding the existing solutions (accuracy, possible problems etc.). one of the solutions may probably be the open source apm autopilot which work for rovers too (example by emlid) but that does not seem to use rtk so the accuracy may be rather low.\n", "tags": "navigation gps precise-positioning", "id": "8842", "title": "path following with precise positioning system (rtk)"}, {"body": "i want to open a door using a dc motor. i've estimated that the required power in the worst case would be around 35-40w (considering a ~80% efficiency). the whole is controlled by a particle photon.\n\ni was thinking to use a l298n to control the output of current to the motor. however, when i looked for powerful enough motors, they would all consume too much current when stalling (> 4a part of the l298n datasheet).\n\ndo you have ideas of how to overcome this? maybe there's another dual-bridge that can handle more current, maybe there exists a dc motor that is ok for a l298n, or maybe i need to have simultaneous dc motors?\n\nedit: this part should be a question by itself. i'll keep it here so that future visitors know what the sub-question was about, but please ignore it from now on if answering.\n\n\n  as a sub-question, would it be better to use a brushed or a brushless dc motor?\n\n", "tags": "motor h-bridge", "id": "8844", "title": "dc motor to open a door"}, {"body": "my application is composting with worms outdoors inside an a styrofoam cooler.  i use a heat lamp and a thermo-electric cooler to maintain the temperature in the bin when the temperature outside is out of bounds for healthy worms.  when the temperature outside is in bounds, i'd like to exchange the air in the bin with fresh air from outside, but i don't want to permanently compromise the insulating properties of my bin with lots of air holes.  so i'm looking for actuator solutions that would allow me to open/close a window of sorts.  i'm considering a solenoid air valve but i don't necessarily need/want an air compressor - a simple fan is sufficient to circulate the air. any suggestions?\n", "tags": "actuator valve", "id": "8845", "title": "exchange air and maintain thermal insulation"}, {"body": "i am working on a project where i should perform object tracking using the camera of parrot ar drone 2.0. so the main idea is, a drone should be able to identify a specified colour and then follow it by keeping some distance.\n\ni am using the cvdrone api to establish communication with the drone. this api provides function:\n\n\n\nwhich moves the ar.drone in 3d space and where \n\n\nvx: x velocity [m/s]\nvy: y velocity [m/s]\nvz: z velocity [m/s]\nvr: rotational speed [rad/s]\n\n\ni have written an application which does simple image processing on the images obtained from the camera of the drone using opencv and finds needed contours of the object to be tracked. see the example below:\n\n\nnow the part i am struggling is finding the technique using which i should find the velocities to be sent to the move3d function. i have read that common way of doing controlling is by using pid controlling. however, i have read about that and could not get how it could be related to this problem.\n\nto summarise, my question is how to move a robot towards an object detected in its camera? how to find coordinates of certain objects from the camera? \n", "tags": "control pid quadcopter tuning opencv", "id": "8849", "title": "tracking objects from camera; pid controlling; parrot ar drone 2"}, {"body": "our thesis is about multicopter using two batteries wherein the first battery is used and when the power goes out, a switch is placed use to charge the first battery while the second battery is powering the multicopter, the process repeats again and again until all the power of the battery is drained. is the process possible to achieved?\n\nedit 1:\njust to clear everything up again. the thesis is about a quadcopter that uses two batteries as its power source. these two batteries are attached to a \"switching circuit system\" that will allow one battery to be drained then the quadcopter will be transferred to the other battery. the solar panels serve as the \"charger\" of whatever battery is in its standby mode (the discharged battery). so while one battery is being used by the quadcopter, one will be connected to a solar panel which will be charging that battery. once the other battery is drained, the quadcopter will switch to the charged battery. this will continue to go on until both batteries are drained. is this achievable?\n", "tags": "battery multi-rotor", "id": "8852", "title": "quadcopter charging"}, {"body": "i'm planning to build a small (probably around 30 centimeters in diameter, at rest) hexapod robot, but, currently, it would only be able to walk on even ground. to improve this, i would have to, somehow, detect when each leg collides with the ground. \n\nideally, i would be able to know how much weight each leg is supporting, so that i could both balance the weight and adapt to moving (up or down) terrain (so, if you put a finger below one leg and lifted it, the leg would go up); however, if that's not possible, a simple binary signal would do.\n\nis there a simple and compact method to do this? \n", "tags": "sensors hexapod", "id": "8855", "title": "how can i detect ground collision, for an hexapod robot?"}, {"body": "i have a imu that has 3-axis accelerator, 3-axis magnetometer, 3-axis gyroscope and row, yaw, pitch value. i want to get the location of the imu coordinate(the beginning point is (0,0,0)) but i know just using double integration will has dead reckoning problem. and i found a lot of paper just talking about combining imu with gps or camera by using kalman filter. is it possible that i just use a imu to get a slightly precise position data? because in the future work i will use multiple imus bounded on human arms to increase the accuracy. \n", "tags": "imu gps", "id": "8857", "title": "imu position without gps or camera"}, {"body": "i\u2019ve made a datalog from a mpu6050 (imu: gyroscope and accelerometer) at 500hz sample rate. now i want to calculate the characteristics from the gyro to evaluate the sensor. \n\nfor the gyro i\u2019ve found following values in the datasheet:\n\ntotal rms noise = 0.05 \u00b0/s\n\nlow-frequency rms noise = 0.033 \u00b0/s \n\nrate noise spectral density = 0.005 \u00b0/s/sqrt(hz)\n\nnow i want to ask how i can calculate these values from my dataset?\n\nat the moment i\u2019ve the following values from the dataset:\n\nstandard deviation = 0.0331 \u00b0/s\n\nvariance = 0.0011\n\nangular random walk (arw) = 0.003 \u00b0/sqrt(s) (from allan deviation plot)\n\nbias instability = 0.0012 \u00b0/s\n\nis the arw equal to the rate noise spectral density mentioned in the datasheet? and also is the rms noise from the datasheet equal to the standard deviation? \n\nedit:\ni found following website: http://www.sensorsmag.com/sensors/acceleration-vibration/noise-measurement-8166\nthere is the statement:  \"...because the noise is approximately gaussian, the standard deviation of the histogram is the rms noise\" so i guess the standard deviation is the rms noise from the datasheet. but how about the arw?\n", "tags": "sensors imu gyroscope sensor-fusion statistics", "id": "8860", "title": "angle random walk vs. rate noise density (mpu6050)"}, {"body": "my quadcopter has an onboard gps unit; and in a crash today i happen to come down on top of it.\n\ni split apart the broken case and noticed that the chip antenna has cracked; i am unfamiliar with how this type of antenna functions, will this be an issue?\n\n\n\nthis is a closed source drone so i have little insight as to the number of gps locks acquired, and the signal strengths, would a broken antenna cause any type of signal degradation, or will i not even be able to acquire a lock?\n", "tags": "gps", "id": "8863", "title": "cracked gps chip antenna"}, {"body": "i have built my differential drive mobile robot in solidworks and converted that to urdf file using soliworks2urdf converter. i successfully launched and robot and simulated with tele-operation node. since i am intended to use navigation stack in i viewed the transform of the robot in rviz which resulted as below.\n\n\n\nas you can see the base plate is the one which supports the wheels and castors but the tf of base plate is shown away from the actual link and even odom is away from the model. where have i gone wrong and how to fix this. refer the urdf of model below.\n\n\n", "tags": "mobile-robot ros navigation odometry gazebo", "id": "8866", "title": "tf frame origin is offset from the actual base_link"}, {"body": "please have a look at this youtube video.\n\nwhen this mobile robot operates, the rack (which is said to weight up to 450 kg) can have its center of mass (com) distributed at any location. for example, this com can be located 5 or 10 cm off from the center of the robot. because of this, when the robot revolves, the center of rotation will not be at the center of the robot anymore. however as you see in the video, it can still rotate many circles perfectly around its up-right axis.\n\nso what do you think about this? is this possible by mechanical design only? or did they use some kind of advanced feedback control system to counter the effect of the off-center com?\n", "tags": "mobile-robot control mechanism", "id": "8874", "title": "how can this mobile robot rotate so perfectly?"}, {"body": "i've got a project that will require my drone to perform a circle turn while the drone is always facing the tangent of the turning curve. similiar to a car that is performing a frictionless banked turn. \njust wondering which method should i use to achieve it, the throttle control can be ignored since i already have a pid on height control.\n\nany suggestions would be appreciated. \n", "tags": "quadcopter pid multi-rotor", "id": "8877", "title": "need the uav to perform a circle turn"}, {"body": "i want to make a drone. but my budget is very low. brushless motors are very expensive. i want use the brushed cheap ones. can i us them ?\n", "tags": "quadcopter motor brushless-motor", "id": "8879", "title": "can i use dc brushed motors for building a drone?"}, {"body": "where can i find a general implementation of canny's roadmap algorithm(or silhouette method) for robot motion planning?\n", "tags": "motion-planning", "id": "8882", "title": "canny's roadmap algorithm"}, {"body": "hello i am new to the field of robotics but have some knowledge of raspberry pi, arduino, python.\ni want to make robotic arm which can be used to find the centre of any disk.\nthere may be disk of different diameter coming one after another on conveyor.\ni need to make hole at the center of disk using robotic arm. how can i do this ?\nwhat techniques and sensors i should use to implement the mechanical and electronic part.\n(i don't want to use camera and opencv). thanks in advance.\n\n\n", "tags": "robotic-arm mechanism electronics movement", "id": "8883", "title": "how to find center of a disk using robotic arm"}, {"body": "basically, how does the kirobo, which went on the iss and spoke some sentences, think? does it talk to itself?\n\ndoes it learn (become more intelligent with time) to have better conversations with \"experience\"?\n", "tags": "mobile-robot humanoid", "id": "8887", "title": "how does the kirobo robot make up sentences / know how to respond to humans?"}, {"body": "i am building a quadrotor with a cc3d (openpilot) and a rpi. \n\nmy first idea is trying to communicate the cc3d with the rpi using the main port of the first one but i can't find any information about how to communicate the board with any device (or other information about the commands or serial configuration).\n\nanybody knows whether this is possible and where to find that info??\n", "tags": "raspberry-pi quadcopter", "id": "8888", "title": "cc3d openpilot - communication port"}, {"body": "i have all the specs from a quadcopter, everthing, would it be possible to figure out the pid from those specs?\n", "tags": "quadcopter pid", "id": "8891", "title": "figure out pid values from drone specs"}, {"body": "this is my first week with gazebo.  the tutorials are clear (except for my dearth of c++ knowledge) but now that i'm working to move out on my own things are getting cloudy.  i made a model comprising two boxes and a revolute joint.  the file one_r_test.world loads this model.  a plugin is \"loaded\" (?) in model.sdf and that plugin, modelcontrol, comes from model_push.cc in the \"model plugins\" tutorial (http://gazebosim.org/tutorials?tut=plugins_model&amp;cat=write_plugin), which uses setlinearvel to move a box.  i can get this same behavior out of model_control.cc if i just copy the tutorial code (and change the plugin name as appropriate), but that's not what i want.  i'm seeking to eventually simulate joint control of robotic manipulators and what's not working in this basic simulation is my attempt to move the model joint via the modelcontrol plugin.  it moves in the gui if i set the velocity (or torque) that way.  the model_control.cc code is pasted below in hopes that you can identify a problem.\n\nmodel_control.cc\n\n\n\nedit: if i change\n\n\n\nto\n\n\n\nthen the joint moves (yes, very, very quickly).  what's wrong with setparam vs setvelocity?\n", "tags": "simulator joint gazebo", "id": "8893", "title": "gazebo: moving joint with model plugin"}, {"body": "i would like to know how the header in this makerbot printer moves in the vertical up /down direction.  is there a detailed explanation of it including the parts involved?\n\n\n", "tags": "mechanism 3d-printing", "id": "8894", "title": "how a 3d printer moves the header vertically in a makerbot printer"}, {"body": "good day,\n\ni am a student currently working on an autonomous quadcopter project, specifically the stabilization part as of now. i am using a tuned propeller system and i also already considered the balancing of the quadcopter during component placements. i had been tuning the pid's of my quadcopter for the past 3 1/2 weeks now and the best i've achieved is a constant angle oscillation of the quadcopter by +-10 degrees with 0 degrees as the setpoint/desired angle. i also tried a conservative 7 degrees setpoint with the same results on the pitch axis.\n\nas of now my pid code takes in the difference of the angle measurement from the complementary filter  and the desired angle.\n\n\n\ni have read somewhere that it is impossible to stabilize the quadcopter utilizing only angle measurements, adding that the angular rate must be also taken into consideration. but i have read a lot of works using only a single pid loop with angle differences (pitch yaw and roll) as the input. \n\nin contrast to what was stated above, i have read a comment from this article (https://www.quora.com/what-is-rate-and-stabilize-pid-in-quadcopter-control) by edouard leurent that a single pid control loop only angle errors and a cascaded pid loop (angle and rate) that utilizes both angle errors and angular velocity errors are equivalent mathematically.\n\nif i were to continue using only the single pid loop (angle) method, i would only have to tune 3 parameters (kp, ki &amp; kd).\n\nbut if i were to change my code to utilize the cascaded loop (angle and angular velocity),\n\n\nwould i have to tune two sets of the 3 parameters (kp, ki &amp; kd for angle and kp, ki &amp; kd for the angular velocity)?\nwould the cascaded pid control loop give better performance than the single pid control loop?\nin the cascaded loop, is the set point for the angular velocity for stabilized flight also 0 in deg/sec? what if the quadcopter is not yet at its desired angle?\n\n\nthank you :)\n", "tags": "control quadcopter pid raspberry-pi stability", "id": "8895", "title": "quadcopter pid control: is it possible to stabilize a quadcopter considering only angle measurements?"}, {"body": "consider the system \n$$\n\\tag 1\nh\\delta x=-g\n$$\n where $h$ and $g$ are the hessian and gradient of some cost function $f$ of the form $f(x)=e(x)^te(x)$. the function $e(x)=z-\\hat{z}(x)$ is an error function, $z$ is an observation (measurement) and  $\\hat{z}$ maps the estimated parameters to a measurement prediction. \n\nthis minimization is encountered in each iteration of many slam algorithms, e.g.one could think of $h$ as a bundle adjustment hessian. suppose $x=(x_1,x_2)^t$, and let $x_2$ be some variables that we seek to marginalize. many authors claim that this marginalization is equivalent to solving a smaller liner system $m\\delta x_1=-b$ where $m$ and $g$ are computed by applying schur's complement to (1), i.e. if\n$$h=\n\\begin{pmatrix}\nh_{11} &amp; h_{12}\\\\\nh_{21} &amp; h_{22}\n\\end{pmatrix}\n$$\nthen\n$$\nm=h_{11}-h_{12}h_{22}^{-1}h_{21}\n$$ \nand \n$$\nb=g_1-h_{12}h_{22}^{-1}g_2\n$$\n\ni fail to understand why that is equivalent to marginalization... i understand the concept of marginalization for a gaussian, and i know that schur's complement appears in the marginalization if we use the canonical representation (using an information matrix), but i don't see the link with the linear system. \n\nedit: i understand how schur's complement appears in the process of marginalizing or conditioning $p(a,b)$ with $a,b$ gaussian variables, as in the link supplied by josh vander hook. i had come to the same conclusions, but using the canonical notation: if we express the gaussian $p(a,b)$ in canonical form, then $p(a)$ is gaussian and its information matrix is the schur complement of the information matrix of $p(a,b)$, etc. now the problem is that i don't understand how schur's complement appears in marginalization in bundle adjustment (for reference, in these recent papers: c-klam (page 3 if you want to look) and in this (part titled marginalization). in these papers, a single bundle adjustment (ba) iteration is performed in a manner similar to what i initially described in the question. i feel like there is a simple connection between marginalizing a gaussian and the marginalization in ba that i am missing. for example, one could say that optimizing $f$ (one iteration) is equivalent to drawing a random variable following a denstiy $$e^{-\\frac{1}{2}(x-\\mu)^t\\sigma^{-1}(x-\\mu)}$$  where $\\sigma$ is the inverse of the hessian $h$ of $f$, and $\\mu$ is the true value for $x$ (or an approximation of that value), and that marginalizing this density is equivalent to using schur's compelement in the bundle? i am really confused...  \n", "tags": "slam computer-vision", "id": "8900", "title": "slam : why is marginalization the same as schur's complement?"}, {"body": "how do i read real time values from the gy-85 imu sensor at simulink connected via arduino? also, i intend to interact with the virtual reality environment at simulink using this gy-85 imu sensor.\nis this possible?\nhow do i make matlab read real time values from this gy-85 imu sensor connected to arduino via i2c communication ?\nplease help!\n", "tags": "imu matlab", "id": "8902", "title": "real-time gy-85 imu sensor interfacing with simulink"}, {"body": "we're working on a quadcopter that will carry a solar panel on top that will continually charging the lipo battery of the quad. what's the smallest and easiest way to recreate a charger that will allow safe charging for the lipo battery?\n", "tags": "quadcopter battery", "id": "8906", "title": "how do i create a portable solar panel lipo charger?"}, {"body": "i am trying to find the inverse kinematics formulation of dlr/hit ii hand. till now i had success in finding an analytical method described in the thesis of mavrogiannis, christoforos i. named grasp synthesis algorithms for multifingered robot hands, given in appendix b. \nmy question is regarding the a.28 where the author calculates q3 but has mentioned previously in the text that q3 is equal to q2. \n\nnote: q here denotes joint angles\n", "tags": "inverse-kinematics", "id": "8907", "title": "inverse kinematics of dlr/hit ii hand"}, {"body": "i am working on building a line follower robot using atmega2560 and i want its movement to be more precise. i am facing a very typical problem.\nit consists of three(3) ir sensors. the thickness of the line to be followed is 1.2cm and the gap between the sensors is more than that, around 1.8cm. \n\nso if the black line comes between the center and any of the side sensors, all the three sensors are on white and it stops.\nand i need the robot to stop over white, due to my application. \nso please can anyone suggest me a good algorithm to tackle this situation.\ni think pid control can be of good use, as i searched on google. but i don't understand how to implement it with three sensors.\nplease help\n", "tags": "microcontroller line-following avr", "id": "8909", "title": "typical problem in simple line follower using 3 sensors"}, {"body": "i've been trying to understand image rectification. most of the time the result of image rectification is illustrated by the original image (i.e the image before rectification) and the rectified image, like this:\n\nthe original image:\n\n\n\nthe rectified image:\n\n\n\nto me the original image makes more sense, and is more 'rectified' than the second one. i mean, why does the result of rectification looks like that? and how are we supposed to interpret it? what information does it contain? \nan idea has just dawned on me : could it be that this bizarre shape of the rectified image is dependent on the method used for rectification, because  here polar rectification was used (around the epipole)?\n", "tags": "mobile-robot stereo-vision 3d-reconstruction", "id": "8910", "title": "how to interpret the result of image rectification?"}, {"body": "i'm working on a little project where i have to do some simulations on a small robot.\n\ni my case i'm using a differential-drive robot as one of the wheels of a bigger robot platform (which has two differential-drive casters), and i really do not understand how to find its kinematics in order to describe it in a model for finding the speed v_tot of the platform.\n\n\n\nthis is my robot and i know the following parameters\n\n\nd is the distance between a joint where my robot is costrained\nblue point is the joint where the robot is linked to the robot platform\nl is distance between the wheels\nr the radius of the wheel\nthe robot can spin around the blue point and with and theta angle\n\n\nas i know all this dimensions, i would like to apply two velocities v_left and v_right in order to move the robot.\n\nlet's assume that v_left = - v_right how do i find analitically the icr (istantaneous center of rotation) in this costrained robot?\n\ni mean that i cannot understand how to introduce d in the formula.\n", "tags": "mobile-robot kinematics wheeled-robot differential-drive two-wheeled", "id": "8911", "title": "how to find kinematics of differential drive caster robot?"}, {"body": "\n  is it possible to speed up execution time of a c++ program in raspberry pi solely by increasing the i2c baudrate and increasing the sampling frequency of the sensors? \n\n\ni have the issue of sudden jerkiness of my quadcopter and found the culprit which is the frequency at which my loop excecutes which is only about 14hz. the minimum requirement for a quadcopter is 100-200hz. it is similar to the issue he faces here raspberry pi quadcopter thrashes at high speeds\n\nhe said that he was able to increase his sampling rate from 66hz to 200hz by increasing the i2c baudrate. i am confused on how that is done.\n\nin the wiring pi library, it says that we can set the baudrate using this command:\n\n\n\nwhat i am curious about is how to set this baudrate to achieve my desired sampling rate?\n\ni plan on optimizing it further to achieve at least a 100hz sampling rate\n\nas of now, the execution time of each loop in my quadcopter program is at 0.07ms or 14hz.\n\nit takes 0.01ms to 0.02ms to obtain data from the complementary filter.\n\ni have already adjusted the registers of my sensors to output readings at 190hz (gyroscope l3gd20h) and 200hz (accelerometer lsm303) and 220hz (magnetometer lsm303).\n", "tags": "quadcopter pid raspberry-pi sensor-fusion c++", "id": "8916", "title": "quadcopter program execution time optimization using raspberry pi by increasing i2c baudrate"}, {"body": "my aim is to actuate a pneumatic muscle based on signals received by emg sensors placed on the biceps. is there any matlab code which can process the received emg signals and convert them into any form which can be useful for muscle actuation?\n\nthe linked video gives better insight to my question: exoskeleton arm pneumatic muscle\n", "tags": "raspberry-pi matlab", "id": "8917", "title": "how to actuate pneumatic muscle by the signals received by emg sensors interfaced with raspberry pi?"}, {"body": "how to construct silhouette curves (roadmap) in canny's silhouette method, when 3d model of the environment (obstacles and robot) is given? \n\n\n\ni want to specifically implement cannys roadmap method, which works for any n dimensional configuration space.\n", "tags": "motion-planning", "id": "8918", "title": "canny's silhouette method"}, {"body": "is the notation of the geometry of robots from khalil and kleinfinger be considered as one of the probably \"many\" modified dh parameters?\n", "tags": "dh-parameters", "id": "8930", "title": "modified dh parameters?"}, {"body": "i'm working with a differential-drive robot that has odometry measurements from wheel shaft encoders and heading information from an imu (i'm using bno055 in imu mode to get euler angles, primarily the heading angle).\n\ni'd like to use the imu header angle to augment the odometry which is prone to slipping and other errors, but the imu lags the odometry by up to 100ms.\n\nhow can i combine these measurements to get the best estimate of the robot pose?\n\nthanks for any word on this.\n", "tags": "mobile-robot imu odometry", "id": "8931", "title": "how to combine odometry information with time-shifted information from imu"}, {"body": "i ask this question to clear my concept about hardware structure of humanoid autonomous fire robot, here is scenario a fire robot detect humans from fire, there are some vision cameras some temperature and smoke sensors which help to perform this task. now a days in market there are many processors like  and  which process tasks in any device and control the system i don't think autonomous fire robot use some kind of processors.\n\ndoes autonomous robots like fire robot use processors or micro controllers. does it require  or  environment? like any computer system which use these kind of things.\n", "tags": "embedded-systems humanoid", "id": "8934", "title": "autonomous robots hardware structure planning"}, {"body": "i have added hokuyo laser plug-in to mu urdf file. i successfully launched the robot in gazebo and done required changes to visualize it in rviz. i didn't get any error but the laser scan results are not published.\n\nfine my results below \n\nterminal while launching gazebo model in world\n\n\n\nresult for \n\n\n\ni have also verified the tf of the model in rqt and its fine\n\naslo find my urdf file below, can someone help me to fix this?\n\nlink &amp; joint:\n\n\n\ncontroller:\n\n\n", "tags": "mobile-robot ros navigation simulation gazebo", "id": "8935", "title": "gazebo laser plug-in fails to publish scan results"}, {"body": "can someone tell me where can i find some specifications about the irobot create 2 ir bump light sensors?\n\nwe have an sdf model of the create 2 that uses the hoyuko laser range finder sensor and we have to simulate the behavior of the ir sensors starting from the laser scan data. \n\nhence we would like to have some additional information on the ir sensors which we can't find anywhere on the web - such as their exact position in the robot chassis, their maximum range, the shape of the obstacle detection field and so on.\n", "tags": "sensors irobot-create", "id": "8938", "title": "irobot create 2 ir bump light sensor specifications"}, {"body": "i've taken a class and started a thesis on robotics and my reference for calculating the jacobian by product of exponentials seems incorrect, see:\n\nhttp://www.cds.caltech.edu/~murray/books/mls/pdf/mls94-complete.pdf\n\nspecifically the resulting jacobian matrix for the scara manipulator on page 118 would have us believe that the end effector translational velocity depends on joints 2 and 3 rather than 1 and 2.\n\ncould someone please explain me why?\n", "tags": "jacobian manipulator product-of-exponentials", "id": "8940", "title": "robotic manipulator jacobian by product of exponentials"}, {"body": "what will be the best approach to get the most localization accuracy out of only an accelerometer and gyroscope?\n", "tags": "localization imu accelerometer gyroscope algorithm", "id": "8941", "title": "localization with only imu"}, {"body": "is the notation of the geometry of robots from khalil and\nkleinfinger be considered as one of the probably \"many\"  modified dh parameters?\n", "tags": "dh-parameters", "id": "8946", "title": "modified dh parameters"}, {"body": "i have been playing with mbed since few week ago and would like to create a test program for dcmotor speed control. im using a 24v dc motor with encoder attach.. then test,\nat first i test with supplying a 5v and 3.3v from mbed to the motor. the thing work fine giving a 16 and 10rpm respectively.\nlater i try using 12v it give 40rpm. \nthen i try a 24v input, now the speed wont come out. and also the counter that count each on/off tick not counting up.\n\nany possible reason?\nmy though that maybe when it move so fast that the isr routine couldn't catch up to the speed..\n\n\n\n\n", "tags": "control motor", "id": "8949", "title": "mbed dc motor speed control using optocoupler encoder"}, {"body": "i am building a simple robot with two driving wheel.\ni want to control the wheel rotation using a wheel encoder like this one.\n\nhere is a code i have on arduino to try to understand the problem i'm facing:\n\n\n\n\n\nwhat i notice is:\nthe interrupt is triggered multiple times when the sensor beam is cut once.\nbut when i digitalread the pin, then there is only one change.  \n\ni also noticed that the interrupt is also triggered when going from high to low.\n\nhere is an example of the ouput i have:\n\n\n\nthe only way i can explain that is that during a change of state, the signal received from the sensor is not really square, but somehow noisy.\nlike in this image :\n\ntherefore we would have, indeed, many rising on one change....\n(however reading the output of the sensor on an analog pin shows a direct variation from 880(high) to 22(low))\n\ndoes anyone have another explanation? or a solution to this problem ?\n\n\n\nedit\n\nthanks to @tobiask i know that this is called a bouncing effect. by doing further research i came across this solution:\nplayground.arduino.cc/main/rotaryencoders (ctrl+f for rafbuff).\ni'm trying it and i'll let you know.\n", "tags": "arduino wheel two-wheeled interrupts", "id": "8950", "title": "wheel encoder triggers interrupt too many times"}, {"body": "good day\n\ni am currently implementing an autonomous quadcopter with stereo vision using raspberry pi. one (let's call this pi_1) is responsible for stereo vision the other is responsible for motor control and trajectory planning (pi_2). i was looking for a way to transfer a 480 element float vector via gpio from pi_1 to pi_2. pi_1 stereovision program runs at 2hz while pi_2 motor control runs at 210hz. \n\n\n  is there any protocol fast enough to deliver this amount of information to the second raspberry pi via gpio? \n\n\ni am currently looking at spi but i saw that the raspberry pi cannot be turned to a slave making it not an option. i also looked at uart however it is too slow for my needs. all the i2c ports on the pi are currently being used by the stereo vision cameras and the imu's. if the gpio option is not feasible, i am also open for other suggestions such as using other hardware (middle man) or wireless options.   \n", "tags": "quadcopter raspberry-pi communication", "id": "8951", "title": "establishing data transfer between two raspberry pi's using gpio"}, {"body": "i would like to know what software is available to control a timing belt with stepper motors for arduino board.much like how its done in 3d printing.\nbut in this case i wont be making a 3d printer.just one simple setup.\n", "tags": "arduino robotic-arm stepper-driver", "id": "8954", "title": "software to control an arduino setup with a timing belt and stepper motors"}, {"body": "what are strategies used when trajectories, which are applied to a robotic joint, are interrupted? say a robotic arm hits an obstacle, the controller just keeps applying the trajectory. then at the end, the error gets so large, the torque can get quite strong and damage the robot or snap.\n", "tags": "control robotic-arm joint", "id": "8960", "title": "what to do when position control with trajectories is interrupted?"}, {"body": "a lot of the create 2's interior space is taken up by the brush assembly and the aerovac bin.  i'd like to take these out and put in my own stuff, but i'm concerned that the roomba might get confused by the fact that i've unplugged these items.  is there anything special i need to do, aside from adding an appropriate amount of weight in that area?\n", "tags": "irobot-create", "id": "8962", "title": "using the brush assembly and aerovac bin space?"}, {"body": "i am trying to control a quadcopter using te openpilot cc3d board and a raspberrypi. the main idea was first replace the signals from the rc emitter to the cc3d rc receiver for an rpi connected directly to the rc receiver inputs of the cc3d.\n\nas far as i know the rc signals to the cc3d are pwm so the rpi should be able to control the channels using rpio library to create the pwm by software. \n\nbut after make some tests i haven't find any way to move the motors. i am using the ground control system (openpilot software) to configure the cc3d.\n\ni am not sure whether i need to send the pwm signals in any order or something like that. i am also not sure how the flight mode switch works, i suposse it works the same way as the other channels, using pwm.\n\nanyone have made anything similar to this? \n", "tags": "raspberry-pi quadcopter uav", "id": "8965", "title": "cc3d - replacing rc emitter with an rpi"}, {"body": "i was looking at these dc motors and i converted the torque to force at a meter distance and i got that two of them should be able to lift over 130 pounds, is this right? http://www.active-robots.com/high-torque-dc-servo-motor-10rpm-with-step-dir-drive there has to be some catch i am not seeing\n", "tags": "motor torque", "id": "8968", "title": "do these motors really have enough torque to lift 130 pounds"}, {"body": "i've used ros for a while, my environment is raspberry pi + ubuntu + opencv + ros + c/c++. i also use several ros packages (tf2, usb_camera, slam related, and laser scanner capture.) also, in my projects, nodes are in multiple devices, and i'm using multiple master package for one project.\n\ni did review some tutorials about alljoyn, but no handon experience so far.\n\nthe questions are:\n\n\nregarding to message (especially, ros image) passing cross devices, is alljoyn a good ros alternative? (devices are connected by wifi or bluetooth.)\nfor alljoyn, does it still need single master (like roscore in ros) to coordinate the nodes (or similar)? \n\n\nthanks.\n", "tags": "ros alljoyn", "id": "8970", "title": "is alljoyn a good ros alternative regarding message passing cross multiple devices?"}, {"body": "an aerial vehicle captures images of the ground using its down facing camera. from the images, multiple targets are converted from their pixel position to the camera reference frame using the pinhole camera model. since the targets are static and there is information of the vehicle attitude and orientation, each sample is then converted to the world referencial frame. note that all targets are on a flat, level plane.\n\nthe vehicle keeps \"scanning\" for the targets and converting them to the world referencial frame. due to the quality of the camera and detection algorithm, as well as errors on the altitude information, the position of the \"scanned\" targets is not constant (not accurate). a good representation might be a gaussian distribution around the target true position, however it will also be influenced by the movement of the aerial vehicle.\n\nwhat's the best approach to estimate the position of the targets from multiple readings?\nthis basically resumes to a problem of noise removal (as well as outlier removal) and estimation, so i would like to know what algorithms and strategies could solve the problem. in the end i expect to implement and test a collection of different approachs to understand their performance on this specific problem.\n\nfurthermore, this system is implemented using ros, so if you know of packages that already do what i'm searching for i would be glad to hear. you can also cite papers on the topic that you think might be of my interest.\n", "tags": "localization ros computer-vision algorithm uav", "id": "8973", "title": "how to estimate the position of multiple static ground targets captured from a down facing camera?"}, {"body": "i am working with the cjmcu build of cleanflight on a small drone. as of now, the algorithm for altitude hold uses a first order complementary filter to combine data from the barometer and the accelerometer (after integrating the accelerations twice). however, i have noticed a considerable lag in the altitude readings and this seems to be hampering the control algorithm's performance.\n\nthe filter in question has been implemented in https://github.com/diydrones/ardupilot/blob/db8a2f7e8bb2183e6d281e7a348d455d855cf5e1/libraries/ap_tecs/ap_tecs.cpp\nhowever, i'm unable to understand how this works.\n\npardon me for any errors i may have committed.\n", "tags": "quadcopter sensors sensor-fusion", "id": "8975", "title": "how does one implement a third order complementary filter for estimating altitude using data from an accelerometer and a barometer?"}, {"body": "i'm using the delphi example to command my create 2, i just adapted the demo code to unicode (delphixe). i use the original irobot usb to serial cable. \n\nmy create 2 seemed to be responding fine to all the commands send via serial yesterday and correctly received all sensor data back this morning, until i recharged the battery. now when i send \"7\" \"soft reset\" the robot attempts every time to start a clean cycle. it also attempts to start the clean cycle when i press the clean button. it tells me to move the roomba to a new location, which is normal in cleaning mode because my wheels are not touching my desk. communication via serial seem to be fine because i still get the soft reset response texts in the log memo of my app when i use the 2 buttons method to soft reset my create 2, so there is still communication both ways.\n\ni must say i had the same yesterday after charging but after a while unexpectedly, don't know why, the robot responded again fine to my commands.\n\nit really seems to me the create 2 is stuck in the cleaning mode, or am i missing something?\n\nbtw, i also tried to fix the problem by removing the battery.\n", "tags": "irobot-create", "id": "8976", "title": "irobot create 2 stuck in clean mode?"}, {"body": "my computer will not recognize the serial to usb cable.  i have tried it on a mac and an hp.  \n\nis there a driver that i need to install?  if it is supposed to install automatically, it is not.\n", "tags": "irobot-create serial usb", "id": "8981", "title": "how do i get the create 2 to communicate with a laptop via the serial to usb cable?"}, {"body": "i have recorded the rosbag data by simulating the robot in gazebo. i played back the logged bag file and tried to build the map using slam_gmapping node and i ended up in error below  \n\n\n  [ warn] [1453398305.145461344]: laser has to be mounted planar!\n  z-coordinate has to be 1 or -1, but gave: -0.03982\n\n\nafter few iterations, i was able to build the map by modifying laser scanner joint origin in my urdf file \nfrom \n\n\n\nto\n\n\n\nbut my robot model become weird as below (laser scanner is away from the robot).\n\n\nhow to get around the issue without relocating the laser scanner.\nfind my urdf file here \n", "tags": "slam ros navigation mapping simulation", "id": "8987", "title": "error while building map in ros slam_gmapping"}, {"body": "before i start, i am a 13 year old, i would like to apologise because i am a beginner to all this and i wouldn't really understand any professional talk, i am only doing this for a hobby.\n\ni am building a quadcopter,\n\n\nflight controller: kk 1.2.5\nesc: q brain 25amp\nframe: kk 260 / fpv 260\nframe addon: kk/fpv 250 long frame upgrade kit\ntx &amp; rx: hobbyking 6ch tx rx (mode 2)\nbattery: turnigy nano-tech 2200 mah 3s\n\n\ni am not sure about what motor and propellers i should use.\nall i know is: for the frame the motor mounts are: 16mm to 19mm with m3 screws\ni am not sure what 1806 and 2208 means.\n\nhere are my questions:\n\n\nwhat calculations should i do to find out how much thrust the quad needs to produce / any other useful calculations\nusing the calculations what would be the best and cheapest motors i could have\nand finally, what propeller would be best suited for the motor.\n\n\np.s: i am looking for a durable and really cheap motors also, i live in london, so shipping might be a problem if there is an immense bill.\n\nthanks a lot for your time,\nsid\n", "tags": "quadcopter motor", "id": "8988", "title": "building a quadcopter, what motors, props and what are the calculations?"}, {"body": "i am writing a code in arduino ide for nodemcu board to control a differential drive 2 wheeled robot.\n\ni am able to steer only one direction for some reason and the steering response time is a little awkward.\n\nis there perhaps a better strategy for the code that i am using?` \n\ni am using an app called blynk that has a virtual joystick that controls that feeds the data through virtual pins. v1 param 0 and 1 are x and y. x would be left to right on the joystick and y would be forward and back. \n\ninformation about the app is available here: http://www.blynk.cc/. i have it working for the most part, but there is some latency since it is through a cloud service.\n\nthe main problem i am stuck on is steering while driving forward and backward. \n\n\n\nany help would be appreciated. thanks!`\n", "tags": "differential-drive", "id": "8990", "title": "standard equation for steering differential drive robot"}, {"body": "i am reading slam for dummies, which you can find on google, or at this link: slam for dummies - a tutorial approach to simultaneous localization and mapping.\n\nthey do some differentiation of matrices on page 33 and i am getting different answers for the resulting jacobian matrices. \n\nthe paper derived \n\n$$\n\\left[ {\\begin{array}{c}\n\\sqrt{(\\lambda_x - x)^2 + (\\lambda_y - y)^2} + v_r \\\\ \\tan^{-1}\\left(\\frac{\\lambda_y - y}{\\lambda_y - x}\\right) - \\theta + v_\\theta \\end{array}} \\right]\n$$\n\nand got\n$$\n\\left[ {\\begin{array}{ccc}\n\\frac{x - \\lambda_y}{r},&amp; \\frac{y - \\lambda_y}{r},&amp; 0\\\\ \\frac{\\lambda_y - y}{r^2},&amp; \\frac{\\lambda_y - x}{r^2},&amp; -1 \\end{array}} \\right]\n$$\n\ni don't get where the $r$ came from. i got completely different answers. does anybody know what the $r$ stands for? if not, is there a different way to represent the jacobian of this matrix?\n", "tags": "slam", "id": "8992", "title": "in the slam for dummies, why are there extra variables in the jacobian matricies?"}, {"body": "i see that in small robots tracked chassis is implemented with 2 motors, each powering one side of the vehicle, like this: \n\n(image stolen from here)\n\nbut in real scale tanks i assume there is only one motor so there must be some way of applying power to both sides independently.\n", "tags": "tracks gearing chassis", "id": "8994", "title": "how to implement transmission in tracked chassis with one motor?"}, {"body": "good day,\n\ni am currently implementing a single loop pid controller using angle setpoints as inputs. i was trying out a different approach for the d part of the pid controller. \n\nwhat bought this about is that when i was able to reach a 200hz (0.00419ms) loop rate, when adding a d gain, the quadcopter seems to dampen the movements in a non continous manner. this was not the case when my algorithm was running at around 10hz. at an angle set point of 0 degrees, i would try to push it to one side by 5 degrees then the quad would try to stay rock solid by resisting the movements but lets go after while enabling me to get it of by 2 degrees (the dampening effect weakens over time) then tries to dampen the motion again.\n\nthis is my implementation of the traditional pid:\n\n\n  derivative on error:\n\n\n\n\nwhat i tried to do now is to implement a derivative on measurement method from this article to remove derivative output spikes. however the derivative part seems to increase the corrective force than dampen it.\n\n\n  derivative on measurement:\n\n\n\n\nmy question now is:\n\n\n  is there something wrong with my implementation of the second method? \n\n\nsource: http://brettbeauregard.com/blog/2011/04/improving-the-beginner%e2%80%99s-pid-derivative-kick/\n\nthe way i've obtained the change in time or dt is by taking the timestamp from the start of the loop then taking the next time stamp at the end of the loop. their difference is obtained to obtain the dt. gettickcount() is an opencv function.\n\n\n\nhere's my data:\n\n\n\n\n\n\n", "tags": "control quadcopter pid raspberry-pi stability", "id": "8998", "title": "quadcopter pid controller: derivative on measurement / removing the derivative kick"}, {"body": "i am using 2 identical dc motors and a castor wheel. the motors are connected to l293d motor driver and are controlled by rpi. \n\nthe robot is not going straight. it veers off to the right. i am running both the motors at 100% pwm.\n\nwhat i tried to correct the error:\n\n\ni adjusted the pwm of the wheel going faster to 99%, but the robot just turns to the other side; \ni adjusted the weight on the robot and the problem still persists.\n\n\ni once tried to run the motor without any load. is that the cause of this, as i was later told that, running a dc motor without any load damages them?\n\nif that is not the cause, then please tell me how to solve this problem without using any sensors for controlling it. \n", "tags": "control motor wheeled-robot raspberry-pi", "id": "9001", "title": "robot never goes straight"}, {"body": "if i'm limited to 4 motors, for four legs, how could the robot rotate if they all only go up/down directions? i'd have to use 2 motors on a leg? correct?\n", "tags": "stepper-motor", "id": "9007", "title": "is 2 motors needed for a arm that can move anyway not just circular?"}, {"body": "i am controlling a robot via usb from an android phone running the robot's code. this phone has a poor battery and i need to extend its life with a usb charger (can't change phones). how can i charge an android phone via usb, while maintaining a usb connection to the robot? i can solder wires together if needed, or can buy adapters as needed.\n\n\n", "tags": "mobile-robot usb", "id": "9009", "title": "how to connect phone to robot while charging phone from external battery?"}, {"body": "i have a computer-vision application i made to localize a robot in a room; the software has been in use for a while and is working fine.\n\nwhen i calibrated the camera and got the intrinsics and lens distortion coefficients there was a lens protector on the lens, mounted on the robot's lid.\n\nif i take off the robot's lid (and thus the lens protector) the localization solution becomes erratic and inaccurate, so i think the lens protector might be changing the distortion properties significantly.\n\ntoday the lens protector became detached and it was replaced shortly after. so now the calibration may no longer be valid and the localization solution is much more noisy.\n\ncan a lens protector can greatly effect the distortion properties of the image, or can someone offer another explanation?\n\ni intend to recalibrate and super-glue the lens protector down to the robot's lid, but i am curious if this is my problem, and if anyone else has encountered this with lens protectors.\n", "tags": "computer-vision cameras calibration", "id": "9010", "title": "is the distortion introduced by a lens protector significant in practice?"}, {"body": "i understand that to be able to define point in 3d space, you need three degrees of freedom (dofs). to additionally define an orientation in 3d space, you need 6 dofs. this is intuitive to me when each of these dofs defines the position or orientation along one axis or an orthogonal x-y-z system.\n\nhowever, consider a robot arm such as this: http://www.robotnik.eu/robotics-arms/kinova-mico-arm/. this too has 6 dofs, but rather than each dof defining a position or orientation in an x-y-z system, it defines an angular rotation of one joint along the arm. if all the joints were arranged along a single axis, for example, then these 6 dofs would in fact only define one angular rotation.\n\nso, it is not true that each dof independently defines a single position or orientation. however, in the case of this robot arm, it can reach most positions and orientations. i'm assuming this is because the geometry of the links between the joints make each dof define an independent position or orientation, but that is a very vague concept to me and not as intuitive as simply having one dof per position or orientation.\n\ncan somebody offer some help in understanding these concepts?\n", "tags": "kinematics", "id": "9013", "title": "how many dofs required to define a 3d pose"}, {"body": "the target is in the shape of a u where the horizontal segment is 20 inches, and the two vertical segments are 14 inches. we are using a camera to image the target, and then using vision processing to isolate the target from the rest of the image. we know the vertical field of view, and the horizontal field of view of the camera. the resolution of the camera is 640x480 pixels.\n\nthe vertical distance between the camera on the robot and the target is constant but as of yet unknown because the robot hasn't been constructed yet. it is known, however, that the target will always have a higher elevation than the camera.\n\nhow can we use this data to calculate in real time the robot's distance to the target, and the angle to the target?\n", "tags": "computer-vision real-time", "id": "9020", "title": "how does one calculate distance and angle using a target with known measurements?"}, {"body": "so i have multi-rotor with a basic pid controller, that keeps its axis stable through the gyroscope. however, the multi rotor, does not keep its height or position. so i would like to use an accelerometer for keeping its rough position (auto level). i want to use both the gyro and accelerometer, but how would the accelerometer values be used, is it implemented through the pid the same ways the gyro values are (degrees per second, which is the rate i used to calculate pid)? and then adjusting the esc through that?? i am confused at that part (the basic logic for using the accelerometer values) \n", "tags": "quadcopter pid imu accelerometer", "id": "9022", "title": "the logic of implementing an auto-level function in a pid flight controller"}, {"body": "i have bought an stm inemo evaluation board in order to monitor the inclination of a separate magnetic sensor array as it moves in a linear scan outside of a (non-magnetic) stainless steel pipe. i want to measure the inclination of the sensor along the scan and ensure that it does not change. the problem i have found is that the measured magnetic field from the integrated magnetometer varies greatly with position along the pipe, and in turn, causes a large, position dependent error in one axis of the inclination reported by the inemo imu. in fig. 1 below i show the set up of the test, i measured the inclination from the imu while moving it along the length of the pipe and back again. the board did not change inclination throughout the measurement. in fig 2 i show the magnetometer and inclination measurements recorded by the \"inemo application\" showing the large error in one of the inclinations.\n\nmy question is whether you know if there is any way of correcting for the magnetic field variation so that i can still accurately determine the inclination in all three directions? my data suggests to me that the magnetic field variation measured from the magnetometer is much greater than the geomagnetic field, so the inclination measurement will always be inaccurate. a follow up question i then have is: is there a way to measure 3 axis orientation without using a magnetometer?\n\n\n\n\n", "tags": "imu gyroscope magnetometer orientation", "id": "9023", "title": "is there a way to measure 3 axis orientation without a magnetometer?"}, {"body": "can someone please share the typical cost of material to 3d print an object like a raspberry pi case?  thank you.\n", "tags": "3d-printing", "id": "9025", "title": "cost of material to 3d print"}, {"body": "my understanding of walking robots (e.g. https://www.youtube.com/watch?v=xjlkbbdybyi) is that they use a gyroscope to determine the current orientation of the robot, or each joint of the robot. this is because if you just put encoders on each joint, the cumulative error over the entire robot will be too large to maintain stability. therefore, a gyroscope measures the \"real\" orientation, and this is used for feedback when the robot is walking.\n\nhowever, i'm also aware that some walking robots use accelerometers to maintain stability. what would be the benefit of using an accelerometer in this case? would it be used instead of a gyroscope, or together with a gyroscope?\n\nmy guess is that gyroscopes do not measure acceleration directly (unless you were to numerically calculate this based on lots of orientation readings), but accelerometers do measure it directly (and more reliably than this numerical method). know the acceleration as well as the position then enables to robot to more accurately predict its future position, and hence the feedback loop is more robust. is this correct, or am i missing the point?\n", "tags": "control sensors accelerometer dynamics walking-robot", "id": "9027", "title": "do walking robots use accelerometers?"}, {"body": "hello i'm trying to figure out how modular arm joints are designed and what kind of bearings/shafts are used for a modular-type robotic arm. take \"ur arm\" for example. i believe those 't-shaped pipes' include both a drive and bearing system. and as you can see from second image, it can be detached easily. so i think it's not just a simple \"motor shaft connecting to the member that we want to rotate\" mechanism. i'm wondering which type of mechanism and bearing system is inside of those t-shaped pipes. how can i transfer rotational motion to a member without using shafts? \n\n\n\n\n", "tags": "mechanism joint arm", "id": "9033", "title": "how modular arm joints work"}, {"body": "i am looking at this page that describes various characteristics of gyroscopes and accelerometers. close to the end (where they speak about imus), the names of the items have something like this:\n\n\n9 degrees of freedom\n6 degrees of freedom\n\n\ncan anyone explain what does this mean?\n", "tags": "accelerometer gyroscope", "id": "9034", "title": "what does \"6 degrees of freedom\" mean?"}, {"body": "i have a pr2 robot in an environment, which can be seen on the gui of openrave. \n\nnow, how can i load a puma robot arm in the same environment?\n", "tags": "robotic-arm motion-planning", "id": "9038", "title": "how to load a puma robot in the existing environment in openrave 0.9"}, {"body": "i want to switch from usercontrol to autonomous. when i have the program running for 120 seconds, how come it wont automatically switch in autonomous mode? thanks!\n\n\n", "tags": "wheeled-robot robotc vex", "id": "9039", "title": "how can i switch between autonomous mode and usercontrol?"}, {"body": "i have a small pom bevel gear with these dimensions:\n\n\n\nit has a 6mm hole for the shaft and a m4 hole for the set screw.\n\nsuppose this bevel gear is meshed with a 45t bevel gear and give a max. output torque of 0.4kg/cm. how should the design of the 6mm shaft be? should the diameter be precisely 6mm? should it be flattened into a 'd' shape (so that the set screw can hold the shaft)? i'm planning to use a metal shaft.\n\nany help will be appreciated.\n\nthanks\n", "tags": "mechanism torque gearing", "id": "9040", "title": "metal shaft design for a 6mm plastic bevel gear"}, {"body": "i am a programmer for my school's frc robotics team and have received the request from our hardware/driving department to limit the speed at which the robot's motors can accelerate given a joystick input telling it to increase the speed of the motor. for example, when the robot first starts up and the driver decides to move the joystick from the center to the fully up position (0 to full motor power), we don't want it to literally go from 0 to full motor power in an instant - it obviously creates some rather jerky, unstable behavior. how might i receive the target joystick position from the joystick, save it, and build up to it over time (and if any other inputs are sent in this process \u2014 like telling it to turn around \u2014 stop the current process and enact the new one)?\n\ni am using java with wpilib's 2016 robotics library: here's the api http://first.wpi.edu/frc/roborio/release/docs/java/, and here's the tutorials http://wpilib.screenstepslive.com/s/4485/m/13809.\n\ni am using the \"iterativerobot\" template class, and teleop is being run in the method teleopperiodic(), which is continuously called every few milliseconds in the program (it's where i'm receiving joystick input and calling the method robotdrive.tankdrive() with the inputs).\n\ni realize this is more of a programming question than a robotics question, but i figured it would be better to put it here than in stack overflow, etc. if someone could give me some simple pseudocode or just a conceptual idea of how this might be done (not necessarily as it pertains directly to the library or the language i'm using), that would be great.\n", "tags": "software first-robotics", "id": "9043", "title": "joystick rate limit filter for frc java programming"}, {"body": "for the last few months i have been playing with ros on an nvidia jetson tk1 development board. up until this point, it has mostly been playing with the gpio header, an arduino uno, a couple physical contact sensors, and a few custom motor and servo boards that i slapped together. but lately i've been eyeing an old 700 series roomba that has been gathering dust (was replaced by an 800 series).\n\ndoes anyone know if the communication cable for create 2 will work with a 700 series roomba?\n\ni know there are diy designs out there, but i have always been a fan of using off-the-shelf components if they exist - you rarely save more money than your time is worth if it is something like a cable or similar component. so if the create 2 cable will work, i'll use that. if not, i'll see what i can do to make my own.\n", "tags": "ros irobot-create roomba", "id": "9044", "title": "create 2 cable with 700 series roomba"}, {"body": "my small robot has two motors controlled by an l293d and that is controlled via a raspberry pi. they will both go forwards but only one will go backwards. \n\ni've tried different motors and tried different sockets in the breadboard, no luck. either the l293d's chip is broken (but then it wouldn't go forwards) or i've wired it wrong. \n\ni followed the tutorial, controlling dc motors using python with a raspberry pi, exactly.\n\nhere is a run down of what works. let the 2 motors be a and b:\n\nwhen i use a python script (see end of post) both motors go \"forwards\". when i change the values in the python script, so the pin set to high and the pin set to low are swapped, motor a will go \"backwards\", this is expected. however, motor b will not move at all. \n\nif i then swap both motors' wiring then the original python script will make both go backwards but swapping the pins in the code will make motor a go forwards but motor b won't move.\n\nso basically, motor a will go forwards or backwards depending on the python code but motor b can only be changed by physically changing the wires.\n\nthis is \n\n\n\nand this is \n\n\n\nif you see this diff https://www.diffchecker.com/skmx6084, you can see the difference:\n\n\n  \n\n\nbelow are some pictures. you can use the colour of the cables to link them between pictures\n\n\nenter image description here\n\n\n", "tags": "wheeled-robot raspberry-pi", "id": "9048", "title": "l293d won't turn motor backwards"}, {"body": "i am working on a remote control project that involves using node.js and socket.io to transmit joystick data from a webpage to my beaglebone black.\n\nhowever, i am somewhat disappointed with the beaglebone - it seems like what should be such simple tasks such as connecting to wi-fi can be quite tricky...\n\nmy question is: are there alternative boards i should be looking at? boards that also have node.js libraries with pwm support, could stream video from a webcam, but are easier to set up and have a larger developer community?\n", "tags": "control microcontroller pwm beagle-bone", "id": "9052", "title": "alternative to beaglebone black for node.js based remote control project?"}, {"body": "i want to find the equations of motion of rrrr robot.i have studied about it a bit but i am having some confusion.\n\nhere in one of the lectures i found online it describes as inertia matrix of a link as ii which is computed by tilde of i also described in picture below???\n\n\nso tilde of i is computed wrt to fixed frame attached to the centre of mass.\n\nhowever in another example below from another source there is no rotation matrix multiplication with ic1 and ic2 as shown above.am i missing something??\n\n\nwhat is the significance of multiplying rotation matrix with ic1 or tilde of i?\n\ni am using former approach and getting fairly large mass matrix. is it normal to have such long terms inside mass matrix?? still need to know though which method is correct?? \n\n\n\n\n\nthe equation i used for mass matrix is\n", "tags": "robotic-arm dynamics", "id": "9054", "title": "mass matrix in lagrange equation"}, {"body": "i'm looking at the assembly of a tail rotor that should look like this\n\n\noriginal image\n\ni wonder if the \"tail output shaft stopper\" (circled in red) is meant to be a bearing or just a piece of metal stopper:\n\n\n\nmy reading is that, since it's held by 2 set screws, the whole part should rotate with the rod. while rotating, it'd rub against the bevel gear on the tail drive though. am i missing something?\n", "tags": "mechanism torque gearing", "id": "9057", "title": "is this supposed to be a bearing?"}, {"body": "i've looked everywhere i can think of to find this information, but haven't come across anything. does anyone know what kind of screws i can use to replace the ones on top of my roomba 530? \n\ni realize that the create 2 is technically a 600 series, but i would expect they were the same.\n\ni'd like to replace the screws on my roomba with standoffs so i can stick a mounting plate on top of it. (additional sensors, cpu, etc.)\n", "tags": "irobot-create roomba", "id": "9063", "title": "irobot create 2/roomba 530 screw size/thread?"}, {"body": "i have fpv camera which outputs analog video (rca, pal).\n\ni want to capture video and do image processing, therefore i need some way to convert the analog video to digital.\n\ncan some one recommend me how to do it? is there advice or a shield which can assist?\n\nplease note:\n\n\ni want to convert the frames with minimum latency, because it is a real time flying drone.\ni don't need to convert the image to some compressed format (which encoding/ decoding may take time), if i can get the rgb matrix straight, it is preferred.\ni thought about digital output camera, but i need one which weighs few grams and i haven't found yet.\n\n", "tags": "quadcopter cameras", "id": "9069", "title": "analogue video to digital"}, {"body": "scenario\n\ni have 2 roaming robots, each in different rooms of a house, and both robots are connected to the house wifi. each robot only has access to the equipment on itself.\n\nquestion\n\nhow can the robots be aware of each other's exact position using only their own equipment and the house wifi?\n\nedit: additional info\n\nright now the robots only have:\n\n\nrgbdslam via kinect\nno initial knowledge of the house or their location (no docks, no mappings/markings, nada) \ncan communicate via wifi and that part is open ended\n\n\ni'm hoping to be able to stitch the scanned rooms together before the robots even meet. compass + altimeter + gps will get me close but the goal is to be within an inch of accuracy which makes this tough. there is freedom to add whatever parts to the robots themselves / laptop but the home needs to stay dynamic (robots will be in a different home every time).\n", "tags": "mobile-robot localization precise-positioning", "id": "9073", "title": "determine robot's position in a nearby room"}, {"body": "as far as i know, a hardware real-time robot control system requires a specific computing unit to solve the kinematics and dynamics of a robot such as interval zero rtx, which assigns cpu cores exclusively for the calculation, or a dsp board, which does exactly the same calculation. this configuration makes sure that each calculation is strictly within, maybe, 1 ms. \n\nmy understanding is that ros, which runs under ubuntu, doesn't have a exclusive \ncomputing unit for that. kinematics and dynamics run under different threads of the same cpu which operates the ubuntu system, path plan, and everything else. \n\nmy question is that how does ros achieve software-real time? does it slow down the sampling time to maybe 100ms and makes sure each calculation can be done in time? or the sampling time changes at each cycle maybe from 5ms, 18ms, to 39ms each time in order to be as fast as possible and ros somehow compensates for it at each cycle?\n", "tags": "microcontroller ros real-time", "id": "9074", "title": "software real-time of ros system"}, {"body": "good day,\n\ni am working on an autonomous flight controller for a quadcopter ('x' configuration) using only angles as inputs for the setpoints used in a single loop pid controller running at 200hz (pid implementation is here: quadcopter pid controller: derivative on measurement / removing the derivative kick). for now i am trying to get the quadcopter to stabilize at a setpoint of 0 degrees. the best i was able to come up with currently is +-5 degrees which is bad for position hold. i first tried using only a pd controller but since the quadcopter is inherently front heavy due to the stereo cameras, no amount of d or p gain is enough to stabilize the system. an example is the image below which i added a very small i gain:\n\n\n\nas you can see from the image above (at the second plot), the oscillations occur at a level below zero degrees due to the quadcopter being front heavy. this means that the quad oscillates from the level postion of 0 degrees to and from a negative angle/towards the front. to compensate for this behaviour, i discovered that i can set the dc level at which this oscillations occur using the i gain to reach the setpoint. an image is shown below with [i think] an adequate i gain applied:\n\n\n\ni have adjusted the pid gains to reduce the jitters caused by too much p gain and d gain. these are my current settings (which are two tests with the corresponding footage below):\n\n\n\ntest 1: https://youtu.be/8jsraze6xgm\n\ntest 2: https://youtu.be/zzte6vqerq0\n\ni can't seem to tune the quadcopter to reach the setpoint with at least +-1 degrees of error. i noticed that further increasing the i-gain no longer increases the dc offset. \n\n\n  when do i know if the i-gain i've set is too high? how does it reflect on the plot?\n\n\nedit:\nthe perr in the graphs are just the difference of the setpoint and the cf (complementary filter) angle.\nthe derr plotted is not yet divided by the deltatime because the execution time is small ~ 0.0047s which will make the other errors p and i hard to see.\nthe ierr plotted is the error integrated with time.\n\nall the errors plotted (perr, ierr, derr) are not yet multiplied by the kp, ki, and kd constants\n\nthe 3rd plot for each of the images is the response of the quadcopter. the values on the y axis correspond to the value placed as the input into the gpiopwm() function of the pigpio library. i had mapped using a scope the values such that 113 to 209 pigpio integer input corresponds to 1020 to 2000ms time high of the pwm at 400hz to the esc's\n\nedit:\n\nhere is my current code implementation with the setpoint of 0 degrees:\n\n\n", "tags": "pid raspberry-pi quadcopter tuning", "id": "9077", "title": "pid tuning for an unbalanced quadcopter: when do i know if the i-gain i've set is too high?"}, {"body": "i'm building an inverted pendulum to be controlled by dc motors, but i've run across a conundrum.  personal life experience tells me that it's better to have a lower center of mass to maintain balance.  on the other hand, the greater the moment of inertia (e.g. the higher the center of mass), the easier it is to maintain balance as well.\n\nthese two views both seem plausible, and yet also seem contradictory.   for an inverted pendulum, is there an optimal balance between the two perspectives?  or is one absolutely right while the other absolutely wrong?  if one is wrong, then where is the error in my thinking?\n", "tags": "dynamics balance", "id": "9082", "title": "optimal location of the center of mass for an inverted pendulum"}, {"body": "i have a broke my xbee xtend 900mhz module. while it was doing firmware-writing job i suddenly restarted the windows.\n\ni've tried recovery procedure of xctu new version and it does not work, therefore i'm still working with old version.\n\nhere is the error i'm facing \n\n\n", "tags": "radio-control", "id": "9090", "title": "xbee xtend recovery/troubleshoot"}, {"body": "i want to implement my own pose graph slam following [1]. since my vehicle is moving in 3d-space i represent my pose using a 3d-translation vector and a quaternion for orientation. [1] tells me that it's necessary to adapt their algorithm 1 by using manifolds to project the poses into euclidean space.\ni also studied the approach of [2]. in section \"iv.b. nonlinear systems\" they write that their approach remains valid for nonlinear systems. i conclude that for their case it's not obligatory to make use of a manifold. but i don't understand how they avoid it. so my questions are:\n\n\nis it correct that there is an alternative to manifolds?\nif yes, how does this alternative look like?\n\n\n\n\n[1] grisetti, g., kummerle, r., stachniss, c., &amp; burgard, w. (2010). a tutorial on graph-based slam. intelligent transportation systems magazine, ieee, 2(4), 31-43.\n\n[2] kaess, m., ranganathan, a., &amp; dellaert, f. (2008). isam: incremental smoothing and mapping. robotics, ieee transactions on, 24(6), 1365-1378.\n", "tags": "slam pose", "id": "9096", "title": "is there an alternative to manifolds when using quaternions for orientation representation in pose graph slam?"}, {"body": "to plot any curve or a function on a paper we need points of that curve, so to draw a curve, i will store a set of points in the processor and use motors, markers and other mechanism to draw straight lines attaching these points and these points are so close to each other that the resultant will look an actual curve.\n\nso i am going to draw the curve with a marker or a pen.\n\n\nyes to do this project i need motors which would change the position of a marker but which one?\n\n\nwith my knowledge stepper motor and servo motors are appropriate but not sure whether they are appropriate since i have never used them, so will they work?\n\nthe dimension of paper on which i will be working on is 30x30 cms.\n\ni have two ideas for this machine \n\na. a rectangular one as shown \n\ni would make my marker holder movable with help of rack and pinion mechanism but i am not sure that this would be precise and i may have to alter to some other mechanism and if you know such then that can really help me.\n\nb. a cylindrical one \n\nhere i would roll a paper on this cylinder and this paper will get unrolled as the cylinder rotates and even the marker holder is movable but only in x direction and the rolling of paper is nothing but change of y position.\n\n\nwhich one of the above two methods is good?\ni know about microcontrollers and i want to control the motors using them so i decided to go with atmega 16 microcontroller. but here i might need microstepping of signals how would i be able to do that with microcontrollers?\n\n\nif you know the answer to atleast one of the questions then those answers are always welcomed. \nif you need any clarifications about these then please leave a comment.\n\nthankyou for your time.\n\nyour sincerely,\njasser\n\nedit : to draw lines of particular slope i would have to know the slope between two points and the depending on the slope i would rotate motors with particular speed so that marker will move in a straight fashion with that slope.\n", "tags": "microcontroller stepper-motor servomotor", "id": "9097", "title": "a simple function plotter project"}, {"body": "i am currently reviewing a path accuracy algorithm. the measured data are points in the 7 dimensional joint space (the robot under test  is a 7 axes robot, but this is not of importance for the question). as far as i know path accuracy is measured and assessed in configuration (3 d) space. therefore i am wondering if a path accuracy definition in joint angle space has any practical value. sure, if one looks at the joint angle space as a 7 dimensional vector space in the example (with euclidean distance measure) one can do formally the math. but this seems very odd to me. for instance, an angle discrepancy between measured and expected for the lowest axis is of much more significance than a discrepancy for the axis near the actuator end effector.\n\nso here is my question: can anyone point me to references where path accuracy in joint space and/or algorithms for its calculation is discussed ?  \n\n(i am not quite sure what tags to use. sorry if i misused some.)\n", "tags": "algorithm industrial-robot joint", "id": "9099", "title": "reference request: path accuracy algorithm in the joint angle space"}, {"body": "i've been working on humanoid robot, and i face the problem of finding the center of mass of the robot which will help in balancing the biped. although com has a very simple definition, i'm unable to find a simple solution to my problem.\n\nmy view: i have already solved the forward and inverse kinematics of the robot with torso as the base frame. so, if i can find the position(and orientation) of each joint in the base frame, i can average all of them to get the com. is this approach reasonable? will it produce the correct com?\n\ncan anyone offer any series of steps that i can follow to find the com of the biped? any help would be appreciated. \n\ncheers! \n", "tags": "mobile-robot inverse-kinematics humanoid balance", "id": "9100", "title": "finding center of mass for humanoid robot"}, {"body": "i want to get telemetry data in my raspberry pi that will be connected to a cc3d board either via usb cable or serial communication. how can i get the data? i plan to have wifi communication between the pi and my laptop. also oplink modems will be used both in the pi and the cc3d for the telemetry. does anyone have a python example that may help to build an interface or output in the linux shell to get raw telemetry data in rpi? \n", "tags": "serial communication", "id": "9103", "title": "connecting a cc3d board with raspberry pi to get telemetry data"}, {"body": "i am fusing data from the barometer and accelerometer using a complementary filter. however, there is a considerable lag in the readings which is affecting the alt-hold performance. \naccelerometer : mpu 6050 \nbaro : ms 5611. \nhere's the code : https://github.com/cleanflight/cleanflight/blob/master/src/main/flight/altitudehold.c\n\nyou will find the filter being implemented in the function calculateestimatedaltitude()\ndoes anyone have any suggestions to improve the measurements ?\n", "tags": "quadcopter sensors sensor-fusion", "id": "9105", "title": "lag in altitude measurements using a barometer and an acclerometer"}, {"body": "i am working on a robotics application that involves moving objects (e.g. books) between several (around 10) stacks. to measure the performance, i'd like to be able to measure which book is located on each of the stacks. the order is not important i just want to know if a book is on one of the stacks. \n\nthe stacks are separated by at least one meter and the height of the stacks is less than 30cm (&lt; 8 books). \n\nif have thought of putting an rfid card in every book and fixing rfid readers above (or below) the stack positions. several readers could be attached via spi or i2c to some arduinos or rpis. \n\nwhat to you think about this approach? is there a simpler way? could someone maybe recommend a sensor that could solve this problem? \n\n// update:\ni can modify the books (e.g. add a qr-marker) to some extent, but can't guarantee that the orientation on the stack is fixed. \n", "tags": "untagged", "id": "9106", "title": "sensors for identifying stacked books"}, {"body": "i'm currently a robotics hobbyist and am full fledged in arduino and i have used the raspberry pi to make some robots and pcs. currently, i am thinking of making my own raspberry pi, from scratch, on a breadboard or a pcb or something. i surfed the web quite a bit and i did not get the answer i was hoping for. by making a pi, i mean like instead of buying an arduino, i can make one myself by buying the atmega328, crystal oscillators, etc. i am asking for this because my school requires me to do a project in which i make a computer or a gaming console or something like that and i would hate to look at the disappointed face of the tester all because i just bought a pi , an connected some devices to it. thanks in advance!\n", "tags": "arduino raspberry-pi", "id": "9111", "title": "how to make a raspberry pi?"}, {"body": "suppose i have a dc motor with an arm connected to it (arm length = 10cm, arm weight = 0), motor speed 10rpm.\n\nif i connect a 1kg weight to the very end of that arm, how much torque is needed for the motor to do a complete 360\u00b0 spin, provided the motor is placed horizontally and the arm is vertical?\n\nis there an simple equation where i can input any weight and get the required torque (provided all other factors remain the same)?\n", "tags": "motor torque", "id": "9114", "title": "simple equation to calculate needed motor torque"}, {"body": "i'm willing to make my first robot, and i'd like to make one similar to the sphero.\n\ni know i have to add 2 motors in it, and make it work as a hamster ball, but i don't understand how i can make it rotate on the x axis aswell and not only on the y axis, if we assume that the y one is in front of the robot and the x one on its sides.\n\nany ideas?\n", "tags": "design two-wheeled", "id": "9115", "title": "sphero's logic, how does it work"}, {"body": "this is not really a problem but something strange is going on.\n\nwhen create2 is connected to a pc via the usb original connector lead, when you start-up the computer the create2 is activated by the baud rate change (brc) pulling to ground. if i understand correctly, normal behaviour.\n\nmy create2 is connected to a xbee via a buck converter, i added a switch so the buck converter and the xbee should not drain the battery continuously so as mentioned in the specs.\n\ni followed the bluetooth pdf for the connections, its working well for sending commands but i still just have a few problems with streaming the return data but that will be resolved.\n\nbut now, with the xbee switched off my create2 still activates when i start-up my pc, how is that possible, how can the brc be pulled to ground?\n\nthere can be no communication between the pc xb and the create2 xb since the create2 xb is switched off, only the pc xb is switched on when starting the computer.\n\nits not a problem, its just that i am puzzled. can anyone explain why this is happening?\n", "tags": "irobot-create", "id": "9119", "title": "weird behaviour with a create2 connected via xbee"}, {"body": "i'm developing a robotic hand, and decided to place motors inside joints (as in picture) and i'm stuck with finding a stepper motor that can fit there. approximate size of motor body is radius - 10mm, length - 10 mm. \n\nany suggestions?\n\n\n", "tags": "stepper-motor", "id": "9124", "title": "choosing stepper motor for hand"}, {"body": "given a pose $x_i = (t_i, q_i)$ with translation vector $t_i$ and rotation quaternion $q_i$ and a transform between poses $x_i$ and $x_j$ as $z_{ij} = (t_{ij}, q_{ij})$ i want to compute the error function $e(x_i, x_j) = e_{ij}$, which has to be minimized like this to yield the optimal poses $x^* = \\{ x_i \\}$:\n\n$$x^* = argmin_x \\sum_{ij} e_{ij}^t \\sigma^{-1}_{ij} e_{ij}$$\n\na naive approach would look like this:\n\n$$ e_{ij} = z_{ij} - f(x_i,x_j) $$\n\nwhere $z_{ij}$ is the current measurement of the transform between $x_i$ and $x_j$ and $f$ calculates an estimate for the same transform. thus, $e_{ij}$ simply computes the difference of translations and difference of turning angles:\n\n$$ e_{ij} = \\begin{pmatrix} t_{ij} - t_j - t_i \\\\\\ q_{ij} (q_j q_i^{-1})^{-1} \\end{pmatrix} $$\n\nis there anything wrong with this naive approach? am i missing something?\n", "tags": "slam errors", "id": "9129", "title": "how to compute the error function in graph slam for 3d poses?"}, {"body": "could someone help me understand the logic behind choosing a particular state space vector for an ekf?\n\ncontext: say there is a 4 wheeled robot that operates only in 2d. it is equipped with an inertial unit (a/g/m) and wheel encoders (i understand that these alone might not satisfy accuracy constraints, but consider this as a hypothetical case).\n\nnow, some literature has the state as [q, x, y, vx, vy]' while a few others as [q, q_dot, x, y, vx, vy]'. my question is, what is the advantage with having certain 'rate terms' as opposed to only the normal parameters? also, what about including bias terms in there?\n\nhow do i go about selecting an appropriate state space vector for any use-case (in general)? is there a set of intuitive/mathematical steps to consider/follow?\n\nthanks!\n", "tags": "wheeled-robot kalman-filter ekf pose", "id": "9130", "title": "choosing the state vector for an ekf"}, {"body": "i have a robot with 3 rotational joints that i am trying to simulate in a program i am creating. so i have 4 frames, one base frame, and each joint has a frame. i have 3 transformation functions to go from frame 1 or 2 or 3 to frame 0.\n\nby using the transformation matrix, i want to know how much each frame has been rotated (by the x,y and z axis) compared with the base frame. any suggestions?\n\nthe reason i want this is because i have made some simple 3d shapes that represent each joint. by using the dh parameters i made my transformation matrices. when ever i change my \u03b8 (it does not mater how the \u03b8 changes, it just does), i want the whole structure to update. i take the translation from the last column. now i want to get the rotations.\n", "tags": "robotic-arm dh-parameters", "id": "9132", "title": "find orientation through transformation matrix"}, {"body": "with dc motors, it is common to put a freewheel diode and/or a capacitor in order to protect the equipment as the motor can induce current into the system.\n\ni plan to use this board to control a 24v dc motor with a arduino-like microcontroler. in an example in their documentation, they don't put such protection, so i wanted to know if it's unsafe, or is it that the board already protects the system?\n\nthe example in question:\n\n", "tags": "arduino motor protection", "id": "9135", "title": "freewheel diode / capacitor with this board?"}, {"body": "i need to know if the irobot create 2 can be controlled with a ni myrio that has been programmed through labview. \n\nthe goal is to program an autonomous robot for real-time tracking using a kinect sensor. \n", "tags": "mobile-robot irobot-create labview", "id": "9139", "title": "can i control irobot create 2 with ni myrio and labview codes?"}, {"body": "what is an intuitive understanding for homotopy?  at what stage is homotopy (i understand it as stretching or bending of path) in a planning algorithm?  is homotopy involved, for example, while implementing an algorithm like rrt?\n", "tags": "motion-planning rrt", "id": "9143", "title": "how is homotopy used in planning algorithms?"}, {"body": "i'm trying to work a car that's being controlled by an arduino. i'm using the following chassis: new 2wd car chassis dc gear motor, wheels easy assembly and expansion and an l298n motor driver.\n\nthe problem is that it's hard to make the car go straight. giving the same pwm value to the motors still makes them spin in different speeds, trying to calibrate the value is hard and every time i recharge my batteries the value changes.\n\nwhat are my options on making the car go straight when i want (well, sometimes i'll want to turn it around of course)?\n\ni've thought about using an encoder but i wish to avoid that since it will complicate the whole project, is there any other viable option? and even when using an encoder, does it means i will need to keep track all the time and always adjust the motors value continuously? is there some built-in library for that?\n", "tags": "arduino wheeled-robot calibration two-wheeled", "id": "9144", "title": "making a car go straight"}, {"body": "what?\n\nput together here a list of books (like the one for c/c++ on stackoverflow) that are spiritually similar to sebastian thrun's probabilistic robotics for robotic manipulation and mechanics.\n\nwhy?\n\nthrun's book is a wonderful resource for implementable algorithms while also dealing with the mathematics/theory behind them. in somewhat similar vein for robotic mechanics there is \"a mathematical introduction to robotic manipulation - s.sastry, z.li and r.murray\" which has a lot of mathematical/theoretical content. what is missing however in this book are the algorithms concerned with how should/would one go about implementing the theoretical stuff.\n\nrequirements\n\n\nideally list books dealing with diverse areas of robotics.\nthe books have to present algorithms like what thrun does in his book.\nalgorithms presented have to be language agnostic and as much as possible not be based on packages like matlab in which case they should be categorized appropriately.\n\n", "tags": "kinematics inverse-kinematics algorithm dynamics books", "id": "9145", "title": "list of books similar to thrun's probabilistic robotics for robot mechanics and manipulation"}, {"body": "i am working on a 6dof robot arm project and i have one big question. when i first derived the inverse kinematics (ik) algorithm after decoupling (spherical wrist), i could easily get the equations based on nominal dh values, where alpha are either 0 or 90 degrees and there are many zeros in $a_i$ and $d_i$. however, after kinematics calibration, the identified dh parameters are no longer ideal ones with a certain small, but non-zero, bias added to the nominal values. \n\nso my question is, can the ik algorithm still be used with the actual dh parameters? if yes, definitely there will be end-effector errors in actual operation. if not, how should i change the ik algorithm? \n\np.s. i am working on a modular robot arm which means the dh bias could be bigger than those of traditional robot arms. \n", "tags": "inverse-kinematics calibration", "id": "9152", "title": "inverse kinematics after calibration"}, {"body": "in catia .stl format is available only for part file not for assembly file. please help how to import asembly in simmechanics\n\n.catproduct to .stl\nor is there any other way to do?\n", "tags": "matlab", "id": "9155", "title": "how to import catia assembly to matlab. simmechanics"}, {"body": "i am looking for a sensor which is able to read the displayed data from a lcd display (4 digits). the output of this sensor must be fetched to a  microcontroller.\ncan anyone suggest me such sensor please soon?\n", "tags": "sensors hall-sensor", "id": "9157", "title": "regarding finding out sensor"}, {"body": "context: i have an imu(a/g/m) + wheel odometry measurement data that i'm trying to fuse in order to localize a 2d (ackermann drive) robot.\n\nthe state vector .\ni'm using the odometry data to propagate the state through time (no control input).\nthe update step includes the measurement vector .\n\ni have two questions:\n\n1.does it make sense to use the odometry data(v_linear, omega) in both the prediction as well as update steps?\n\n2.how do i account for the frequency difference between the odometry data(10hz) and the imu data(40hz)? do i run the filter at the lower frequency, do i dynamically change the matrix sizes or is there any other way?\n\nthanks!\n", "tags": "localization wheeled-robot imu ekf odometry", "id": "9159", "title": "multi-rate sensor fusion using ekf"}, {"body": "i am programming a robot to drive over variable terrain obstacles autonomously. the variable terrain could potentially knock the robot off of its initial heading, but i would like to design an autonomous sequence to correct for any change in direction. i am using a very accurate sensor with compass and yaw. what is the best way to have it correct for any changes and maintain its heading? side to side motion does not have to stay perfect, but the heading needs to stay the same.we are currently correcting it by overpowering one side of the wheels (depending on direction of correction needed) until the heading is correct again, but this seems to be a slightly antiquated method, so i'm looking for a cleaner and more smooth method.\n", "tags": "compass automation", "id": "9161", "title": "robot autonomous variable terrain with yaw sensor"}, {"body": "is there any way to make the central controller for all vex parts without using the vex cortex (or anything by vex)? \n\ni am wondering whether you can make your own custom controller for vex parts.\n", "tags": "vex", "id": "9163", "title": "making a vex central controller"}, {"body": "i have a robot arm in an environment. how can i check for collision between this robot arm and the environment?\n", "tags": "robotic-arm motion-planning python", "id": "9167", "title": "check collision between robot and environment in openrave"}, {"body": "are there still applications where memory is still a criteria with respect to motion planning algorithms. are memory efficient motion planning algorithms still relevant?\n", "tags": "mobile-robot quadcopter motion-planning", "id": "9168", "title": "memory as a benchmarking criteria for motion planning algorithm"}, {"body": "good day,\n\ni had been recently reading up more on pid controllers and stumbled upon something called integral wind up. i am currently working on an autonomous quadcopter concentrating at the moment on pid tuning. i noticed that even with the setpoint of zero degrees reached in this video, the quadcopter would still occasionally overshoot a bit:  https://youtu.be/xd8wgvffesm\n\nhere is the corresponding data testing the roll axis:\n\n\ni noticed that the i-error does not converge to zero and continues to increase:\n\n\n\n  is this the integral wind-up?\n  \n  what is the most effective way to resolve this?\n\n\ni have seen many implementations mainly focusing on limiting the output of the system by means of saturation. however i do not see this bringing the integral error eventually back to zero once the system is stable.\n\nhere is my current code implementation with the setpoint of 0 degrees:\n\n\n", "tags": "control quadcopter pid stability tuning", "id": "9169", "title": "pid control: integral error does not converge to zero"}, {"body": "context: i am working with the sfu mountain dataset [http://autonomylab.org/sfu-mountain-dataset/]\n\nthe ugv image - via the sfu mountain dataset website:\n\n\ni have used the following state update equations (husky a200 - differential drive)\n\nstate update - from prob. robotics, thrun et. al \n\n\n\n\nafter plotting the x and y positions based on just the wheel encoder data (v_fwd and w -> the dataset provides these directly, instead on the vr and vl), the curve seems to be quite weird and unexpected.\nwheel odometry data - http://autolab.cmpt.sfu.ca/files/datasets/sfu-mountain-workshop-version/sfu-mountain-torrent/encoder-dry-a.tgz\n\nblue - wheel odom  |  red - gps\n\n\nactual path!\n\n\nquestion: is the above curve expected (considering the inaccuracy of wheel odometry) or is there something i'm missing? if the wheel encoder data is that bad, will an ekf (odom + imu) even work?\n\nps: i'm not worried about the ekf (update step) just as yet. what concerns me more is the horrible wheel odometry data.\n", "tags": "localization wheeled-robot ekf odometry differential-drive", "id": "9171", "title": "plotting location using wheel encoder data"}, {"body": "what kind of sensor i can use to identify which fruit it is (like mango or apple). moreover, is there any sensor to identify different varieties of apples or mangoes. \n", "tags": "sensors", "id": "9174", "title": "what kind of sensor i can use to identify which fruit it is (like mango or apple)?"}, {"body": "i am trying to make custom parts that fit directly onto a servo. doing this has proved more difficult than i've expected so far.\n\ni was hoping to avoid incorporating the provided servo horns into the 3d printed part, so i've been trying this method out. below are images of my current test - a 3d printed attachment to the servo, with an indentation for an m3 nut (the servo accepts an m3 bolt) for attachment to the servo. the plastic ring doesn't have the spline (i can't print that level of detail i think) but is tight around it. the top piece attaches to a 3/8\" nut for use with the 3/8\" threaded rod i had laying around.\n\n\n\n\n\nso far, i'm having difficulty of this setup working with any level for torque and not just spinning in place.\n\nso... is this the correct approach? am i going to have to design a piece with the servo horn inside of it to get the servo to connect? are there better approaches i haven't considered?\n", "tags": "servos 3d-printing", "id": "9175", "title": "what is the best way to attach a 3d printed part to a servo for robotics use?"}, {"body": "i've recently come across the concept of using information gain (or mutual information criteria) as a metric for minimizing entropy on a map to aid in robotic exploration. i have somewhat of a basic question about it. \n\na lot of papers that talk about minimizing entropy consider an example case of something like a laser scanner and try to compute the 'next best pose' so that the maximum entropy reduction is achieved. usually this is mentioned like \"information gain based approaches help finding the best spot to move the robot such that the most entropy is minimized using raycasting techniques, as opposed to frontier based exploration which is greedy\" etc. but i don't understand what the underlying reason is for information gain/entropy based exploration being better. \n\nlet's say a robot in a room with three walls and open space in front. because of range limitations, it can only see two walls: so in frontier based exploration, the robot has two choices; move towards the third wall and realize it's an obstacle, or move towards the open space and keep going. how does an information gain based method magically pick the open space frontier over the wall frontier? when we have no idea what's beyond our frontiers, how can raycasting even help?\n", "tags": "mapping exploration information-gain", "id": "9180", "title": "how does information gain based exploration differ from frontier based?"}, {"body": "prediction of new landmarks are commonly expressed as:\n\n\n\nhowever this is only true for point landmarks. what if i am extracting line feature?\n", "tags": "slam ekf", "id": "9183", "title": "ekf slam (prediction of new landmarks)"}, {"body": "i am looking to make a non contact hydraulic flow meter. i was wondering, which ultrasonic sensor to use? i don't have any specific ideas on how to go forward. are there any articles or documented builds out there about this subject?\n", "tags": "ultrasonic-sensors", "id": "9185", "title": "which ultrasonic sensor can be used for detecting hydraulic flow?"}, {"body": "i've read some papers for controlling nonlinear systems (e.g. nonlinear pendulum). there are several approaches for targeting nonlinear systems. the most common ones are feedback linearizaing, backstepping, and sliding mode controllers. \n\nin my case, i've done the theoretical and practical parts of controlling nonlinear model of a simple pendulum plus other manipulators problems in c++. for the pendulum, i've utilized a backstepping controller for solving the tracking task for the angular displacement and velocity. the results are \n\n$$\n\\ddot{\\theta} + (k/m) \\dot{\\theta} + (g/l) \\sin\\theta= u \n$$\n\nwhere $m=0.5, k=0.0001, l=.2$ and $g=9.81$.\n\n\n\n\n\nthe results are good. however, tuning the controller is time consuming. the majority of papers use genetic algorithms for tuning their controllers such as pd, pid, and backstepping controllers. i'm clueless in this field and i hope someone sheds some light on this concept, preferable if there is a matlab sample for at least controlling a simple pendulum.\n\nso far i've designed a simple gui in c++/qt in order to tune the controller manually. in the below picture, the response of the controller for step function. \n\n \n", "tags": "control", "id": "9186", "title": "using genetic algorithm for tuning controllers"}, {"body": "the goal is to have a non-invasive flow meter that i can clamp over hydraulic lines.\n\nas a student of hydraulics, i ended up looking and poking around for a good way to make an ultrasonic flow sensor with arduino and possibly the hc-sr04. not married to either idea. \n\nso, i admit, i know nothing, but is it possible to do this?\nis there an easier way?\n", "tags": "ultrasonic-sensors", "id": "9187", "title": "ultrasonic flow sensor"}, {"body": "how to design a quadrotor which travels at particular maximum speed? and how to determine the power required for a quadrotor to hover?\n", "tags": "mobile-robot quadcopter power", "id": "9194", "title": "what determines the speed of quadrotor"}, {"body": "i have created a robot model in solidworks and exported in solidworks to urdf plug-in. when exporting the co-ordinates of the model is misaligned which is causing problem while using in ros.\n\n\n\nas you could see in picture the z-axis is horizontal in assembly whereas vertical in solidworks. how to align these co-ordinates. the generated co-ordinate system must be similar to solidworks' co-ordinates\n\nps: i have mated the assembly origin and base_link origin\n", "tags": "mobile-robot ros navigation odometry gazebo", "id": "9195", "title": "how to align solidworks global origin with assembly origin while exporting in solidworks to urdf"}, {"body": "i am trying to implement the vector field histogram as described by borenstein, koren, 1991 in python 2.7 using the scipy stack.\n\ni have already been able to calculate the polar histogram, as described in the paper, as well as the smoothing function to eliminate noise. this variable is stored in a numpy array, named .\n\nhowever, the function , pasted below, which computes the steering direction, is only able to compute the proper direction if the valleys (i.e. consecutive sectors in the polar histogram whose obstacle density is below a certain threshold) do not contain the section where a full circle is completed, i.e. the sector corresponding to .\n\nto make things clearer, consider these two examples: \n\n\nif the histogram contains a peak in the angles between, say,  and , with the rest of the histogram being a valley, then the steering direction will be computed correctly.\nif, however, the peak is contained between, say,  and , then the valley will start at , go all the way past  and end in , and the steering direction will be computed incorrectly, since this single valley will be considered two valleys, one between  and , and another between  and .\n\ndef computetheta(self, goal):\n\n\n\n\nhow can i address this issue? is there a way to treat the histogram as circular? is there maybe a better way to write this function?\n\nthank you for your time.\n", "tags": "motion-planning python planning", "id": "9200", "title": "a vector field histogram implementation in python 2.7"}, {"body": "i've hacked a rc helicopter, and i am able to control it by running a program on my computer. i am interested in writing algorithms that will stabilize the helicopter. for instance, the helicopter is hovering, and then if it is shoved off balance it can return to its previous position in a stable state. any help on an algorithm would be awesome.\n", "tags": "algorithm stability", "id": "9203", "title": "helicopter stabilization algorithm"}, {"body": "my irobot create is playing a tune about every 30 seconds and continuously flashing a red light when i attempt to charge it. what is the issue?\n", "tags": "irobot-create", "id": "9206", "title": "irobot create: making noise and flashing red light while charging"}, {"body": "i am working with a sampling based planning library. when i looked into the implementation, i found for kinematic car a se2 state space(x, y, yaw), for dynamic car a se2 compound state space (space allowing composition of state spaces), for blimp and quadrotor a se3 compound state space was used. i could understand the design of se2 and se3 state spaces but the compound state spaces of dynamic car, blimp and quadrotor i could not comprehend or differentiate.\n\nwhat is the difference in terms of state space for motion planning for kinematic car, dynamic car, blimp and quadrotor?\n", "tags": "mobile-robot quadcopter motion-planning", "id": "9210", "title": "what is the difference between planning for kinematic car, dynamic car, blimp and quadrotor"}, {"body": "i am looking for a mini, wireless, chargeable, camera that can stream video in real time to my computer. i will put the mini camera on my helicopter and send the live video feed to my pic and make certain calculations to make the helicopter navigate in a certain way. any help would be great.\n", "tags": "cameras", "id": "9214", "title": "wireless mini camera"}, {"body": "i have to program an autonomous bot (using an atmega2560). it has a 4-digit seven segment display attached to it. i have to make the bot traverse through arena while continuously displaying the time in seconds on the seven segment display.\n\ni can't use the code to display on seven segment display in my  function.\n\nany help? \n", "tags": "interrupts", "id": "9218", "title": "programming 4-digit seven segment display using interrupts only"}, {"body": "i have a few robotic manipulators from nakkanippon electric and i'm trying to communicate with them using rs232 without success. the robot model are microrobots 88-4 or 88-5.\n\ni'm sending commands via com port, but i can't received anything from the box. i'm using a usb-to-db9 converter (ftdi) with a db9-db25 cable.\n\non the net, the only reference i have for the robot is from an old post from year 2000 that was from user @peterkneale.\n\nif it can help, here is the link to the scanned pdf manual.\n\nyou can see the commands on page 23-24 of the pdf (page 21-22 in document). \n\nany advice would be grateful\n", "tags": "robotic-arm", "id": "9225", "title": "how to communication with old robotic arms from nakkanippon electric?"}, {"body": "i'm trying to determine the wind flow diagram around a quad-copter when it is in action. i looked up on internet but couldn't find any reliable source.\n\nby wind flow diagram what i mean is when my quad-copter is in mid-air hovering at some fixed position. how the air is moving around it. all the directions are needed to be kept in mind, from top to bottom ( vertical direction) and also the horizontal direction.\nthank you.\n", "tags": "quadcopter dynamics", "id": "9226", "title": "wind flow diagram of a quadcopter"}, {"body": "is there a way to charge a li-po battery using solar panels to increase the flight time of a quadcopter during its flight?\n", "tags": "quadcopter power", "id": "9227", "title": "solar cells charging a li-po battery"}, {"body": "i am working on the baxter robot where i have a first arm configuration and a bunch of other arm configurations, where i want to find the closest arm configuration to the first among the many other arm configurations. the trick here is that the end effector location/orientation is the exact same for all the arm configurations, they are just different ik solutions. can anyone point me towards the right direction towards this? thank you.\n", "tags": "robotic-arm inverse-kinematics", "id": "9228", "title": "evaluating the similarity of two 7 degree of freedom arms"}, {"body": "background: i'm using the l3gd20h mems gyroscope with an arduino through a library (pololu l3g) that in turn relies on interrupt-driven i2c (wire.h); i'd like to be able to handle each new reading from the sensor to update the calculated angle in the background using the data ready line (drdy). currently, i poll the status register's zyxda bit (which is what the drdy line outputs) as needed.\n\ngeneral question: with some digital output sensors (i2c, spi, etc.), their datasheets and application notes describe using a separate (out-of-band) hardware line to interrupt the microcontroller and have it handle new sets of data. but on many microcontrollers, retrieving data (let alone clearing the flag raising the interrupt line) requires using the normally interrupt-driven i2c subsystem of a microcontroller. how can new sensor data be retrieved from the isr for the interrupt line when also using the i2c subsystem in an interrupt-driven manner?\n\npossible workarounds:\n\n\nuse nested interrupts (as @hauptmech mentioned): re-enable i2c interrupt inside of isr. isn't this approach discouraged?\nuse non-interrupt-driven i2c (polling)--supposedly a dangerous approach inside of isrs. the sensor library used depends on the interrupt-driven wire library.\n[edit: professors' suggestion] use a timer to interrupt set to the sample rate of the sensor (which is settable and constant, although we measure it to be e.g. 183.3hz rather than 189.4hz per the datasheet). handling the i2c transaction still requires re-enabling interrupts, i.e. nested interrupts or performing i2c reads from the main program.\n\n\n[edit:] here's a comment i found elsewhere on a similar issue that led me to believe that the hang encountered was from i2c reads failing inside an interrupt handler: https://www.sparkfun.com/tutorials/326#comment-4f4430c9ce395fc40d000000 \n\n\n  \u2026during the isr (interrupt service routine) i was trying to read the\n  device to determine which bit changed. bad idea, this chip uses the\n  i2c communications which require interrupts, but interrupts are turned\n  off during an isr and everything goes kinda south.\n\n", "tags": "arduino microcontroller gyroscope i2c interrupts", "id": "9232", "title": "how to interrupt on a data ready trigger when communications to the sensor are interrupt driven?"}, {"body": "i need some help here because i can't figure how the unscented kalman filter works.\ni've searched for examples but all of them are too hard to understand.\n\nplease someone can explain how it works step by step with a trivial example like position estimation, sensor fusion or something else?\n", "tags": "kalman-filter", "id": "9233", "title": "uncented kalman filter for dummies"}, {"body": "i am not sure if this is the best place to ask this question, but hopefully someone here can give me some advice. i have a device hooked up to a data acquisition system that can provide sync out signal and record sync in signals. i need to synchronize my recordings with this device to a video feed. i am having trouble finding a camera that can provide a sync signal or any other good way to accomplish this. thanks for your help.\n", "tags": "sensors cameras", "id": "9234", "title": "syncing camera with other signals"}, {"body": "i have the crazyflie usb dongle and it works with the python crazyflie software, and the usb dongle can control the crazyflie drones as well as other drones that operate on 2.4 ghz. how can i get the dongle and software to work with the hubsan x4? any help would be great.\n\nthe link for the usb dongle and software:\n\nhttp://www.seeedstudio.com/depot/crazyradio-pa-long-range-24ghz-usb-radio-dongle-with-antenna-p-2104.html\n\nhttps://github.com/bitcraze/crazyflie-clients-python\n", "tags": "python", "id": "9236", "title": "controlling hubsan x4 with crazyflie usb dongle"}, {"body": "i am working with a 6 dof manipulator. currently i have implemented a simple velocity controler along a fixed direction on xyz space. i control the xyz space velocity (xdot) by using a predefined velocity profile against time. \njoint values are updated based on the defined velocity profile.\n\nassume i want to move robot along a direction parallel to z axis, i define a trapezoidal velocity profile (z dot) over time as following, \n\nin the robot controller program, i convert this (z dot) to velocity at a time in joint space by multiplying by inverse of jacobian. in this way i can move robot as needed.\n\nmy question is how we can define the above velocity profile over time, so that the total jerk in joints over time is minimized ?\n\nyour help is really appreciated.\n", "tags": "control robotic-arm manipulator", "id": "9239", "title": "cartesian space velocity profile to minimize jerk"}, {"body": "i have been working with kuka lbr iiwa 7 r800 robot, with the kuka's ide, which is the 'sunrise.workbench'. since it does not have any virtual platform to verify the code (simulate), it's been quite difficult, as i need to test each code by deploying to the robot.\n\ncan anyone suggest if there is any simulation software available where i can test the code written using the robotics api in sunrise.workbench?\n\ni came across v-rep simulation software, but, not sure if i can use my code in the workbench platform.\n\nappreciate if anyone can shed some light on it.\nthanks in advance.\n", "tags": "robotic-arm simulation", "id": "9240", "title": "simulation software for kuka lbr iiwa robot?"}, {"body": "the matlab compiler sdk allows to create a wrapper for a matlab function which can be accessed by java software. based on my understanding, kuka's sunrise.workbench ide uses most of the standard java functions. \n\ni was trying to read the package generated using the matlab compiler sdk (the new version of matlab builder ja) into the workbench platform. i could successfully read the package into eclipse ide, but not into workbench.\n\nthe reason for using the compiler sdk is that, i have some functions is matlab, and i want to use the same in the workbench programming.\n\ndoes anyone have experience with the same? appreciate any help.\n", "tags": "matlab robotc", "id": "9241", "title": "use matlab compiler sdk generated package in kuka sunrise.workbench"}, {"body": "here there is a mimo transfer function with size of 3*7 (3 inputs and 7 outputs).\n\n\n\nis it possible to decouple the interaction between the loops and how can we get multi siso system from g ?\n", "tags": "control", "id": "9243", "title": "how is it possible to decouple mimo transfer function of robot to multi siso system"}, {"body": "has anyone ever run into a case where a fresh install of ros cannot run its tutorial packages?\n\ni am running ros indigo on an nvidia jetson tk1, using the nvidia-supplied ubuntu image. i just did a fresh install, ubuntu and ros, just to keep things clean for this project. i am building a kind of 'demo-bot' for some students i will be teaching; it will use both the demo files and some of my own code. now, after setting things up, i try to run the talker tutorial just to check to make sure that everything is running, and rospack is pretty sure that the tutorials don't exist.\n\nfor example, inputting this into the terminal\n\n\n\noutputs\n\n\n\nthis is the case for every tutorial file; python and c++. now, i am sure the tutorials are installed. i am looking right at them in the file system, installed from the latest versions on github. so i think it is something on ros' side of things.\n\nhas anyone ever bumped into something similar before, where a ros package that supposedly was installed correctly isn't found by ros itself? i would rather not have to reinstall again if i can avoid it.\n\nedit #1\n\nafter playing with it some more, i discovered that multiple packages were not running. all of them - some turtlebot code, and some of my own packages - returned the same error as above. so i suspect something got messed up during the install of ros.\n\nroswtf was able to run, but it did not detect any problems. however, going forward.\n\nedit #2\n\ni double checked the bashrc file. one export was missing, for ros directory i was trying to work within. adding it did not solve the problem. \n\ni am still looking for a solution, that hopefully does not involve reflashing the tk1.\n\nedit #3\n\nalright, so i've been poking at this for a few days now and pretty much gave up trying to get ros to work correctly, and decided a re-flash was necessary. but i think i found something when i booted up my host machine. in my downloads folder, i have the v2.0 and the v1.2 jetpack. i know i used the v2.0 for this latest install, and it has been the only time i have used it (it provides some useful updates for opencv and bug fixes, among other things). i'm going to re-flash using the v1.2 jetpack this time, see if things behave better with ros under that version. its a long shot, but it is all i have to work with at the moment, and it shouldn't lose any ros capabilities (aside from some of the stuff i wanted to do with opencv). i'll update everyone if that seems to work.\n\nedit #4\n\nok, everything seems to be working now. the problem does seem to be an issue with jetpack v2.0. i suspect that some change, somewhere between v1.2 and v2.0 (made to accommodate the new tx1 board), messes with running ros indigo on a tk1. i'm going to be a more detailed explanation in an answer to this question.\n", "tags": "ros", "id": "9244", "title": "ros tutorials no longer working"}, {"body": "i am designing a new mechanism similar to robot arm. it would be a 6 or 7 axis with arms but not the same with traditional articulated arms. as a result, new dh matrix,and inverse kinematics involve. i would like to consult the robot professionals in this forum that do you suggest any simulation tool of this mechanism?\n\ni plan to start with start and end points. then i will do a trapezoid velocity plan and take sample points with sampling time along the path. after that, i would like to transfer these sampling points to motor joints by dh matrix and inverse kinematics. finally i would do some basic 3d animation to visualize the movement temporally. i do not plan to simulate controller behavior because in my application motor drivers deal with it. i only need to focus on sending reasonable commands to motor drivers. \n\nin my opinion, matlab, octave, vc++, and some third-party tools are candidates. starting from ground zero would be a time-consuming work. i would appreciate if any experts can share a tool or open source code from his or her experience. i did some search on matlab robotics toolbox but i am not sure if it fits my need because it is expensive and optimized for ros. in octave there are also some robotics toolbox but i am not sure about what it can do and what it cannot. \n", "tags": "robotic-arm matlab simulation c", "id": "9246", "title": "simulation of robots"}, {"body": "good day,\n\ni am currently creating an autonomous quadcopter using a cascading pid controller specifically a p-pid controller using angle as setpoints for the outer loop and angular velocities for the inner loop. i have just finished tuning the roll pid last week with only +-5 degrees of error however it is very stable and is able to withstand disturbances by hand. i was able to tune it quickly on two nights however the pitch axis is a different story.\n\nintroduction to the problem:\nthe pitch is asymmetrical in weight (front heavy due to the stereo vision cameras placed in front). i have tried to move the battery backwards to compensate however due to the constraints of the dji f450 frame it is still front heavy.\n\nin a pid controller for an asymmetrical quadcopter, the i-gain is responsible for compensating as it is the one able to \"remember\" the accumulating error.\n\nproblem at hand\ni saw that while tuning the pitch gains, i could not tune it further due to irregular oscillations which made it hard for me to pinpoint whether this is due to too high p, i or d gain. the quadcopter pitch pid settings are currently at prate=0.0475 irate=0.03 drate=0.000180 pstab=3 giving an error from the angle setpoint of 15degrees of +-10degrees. here is the data with the corresponding video.\n\nrate kp = 0.0475, ki = 0.03, kd = 0.000180 stab kp=3\nvideo: https://youtu.be/nmbldhrzp3e\n\nplot:\n\n\nanalysis of results\nit can be seen that the controller is saturating.\nthe motor controller is currently set to limit the pwm pulse used to control the esc throttle to only 1800ms or 180 in the code (the maximum is 2000ms or 205) with the minimum set at 155 or 1115ms (enough for the quad to lift itselft up and feel weightless). i did this to make room for tuning the altitude/height pid controller while maintaining the throttle ratio of the 4 motors from their pid controllers.  \n\n\n  is there something wrong on my implementation of limiting the maximum throttle?\n\n\nhere is the implementation:\n\n\n\npossible solution\ni have two possible solutions in mind\n\n\ni could redesign the camera mount to be lighter by 20-30 grams. to be less front heavy\ni could increase the maximum throttle but possibly leaving less room for the altitude/throttle control.\n\n\n\n  does anyone know the optimum solution for this problem?\n\n\nadditional information\nthe quadcopter weighs about 1.35kg and the motor/esc set from dji (e310) is rated up to 2.5kgs with the recommended thrust per motor at 350g (1.4kg). though a real world test here showed that it is capable at 400g per motor with a setup weighing at 1600g take-off weight \n\nhow i tune the roll pid gains\n\ni had set first the rate pid gains. at a setpoint of zero dps\n\n\nset all gains to zero.\nincrease p gain until response of the system to disturbances is in steady oscillation.\nincrease d gain to remove the oscillations.\nincrease i gain to correct long term errors or to bring oscillations to a setpoint (dc gain).\nrepeat until desired system response is achieved\n\n\nwhen i was using the single loop pid controller. i checked the data plots during testing and make adjustments such as increasing kd to minimize oscillations and increasing ki to bring the oscillations to a setpoint. i do a similar process with the cascaded pid controller.\n\nthe reason why the rate pid are small because rate kp set at 0.1 with the other gains at zero already started to oscillate wildy (a characteristic of a too high p gain). https://youtu.be/scd0hda0fty\n\ni had set the rate pid's such that it would maintain the angle i physically placed it to (setpoint at 0 degrees per second) \n\ni then used only p gain at the outer loop stabilize pid to translate the angle setpoint to velocity setpoint to be used to control the rate pid controller.\n\nhere is the roll axis at 15 degrees set point https://youtu.be/voaa4ctc5ru\nrate kp = 0.07, ki = 0.035, kd = 0.0002 and stabilize kp = 2\n\n\nit is very stable however the reaction time/rise time is too slow as evident in the video.\n", "tags": "quadcopter pid stability", "id": "9250", "title": "quadcopter pid: controller is saturating"}, {"body": "i recently bought a set of escs, brushless outrunner motors and propellers. i'm trying to perform a calibration on the esc, but i can't find how i can do that without using components other than the arduino uno itself. the setup i've managed to make is the one shown in the picture. the escs are a mystery, as there is no manual to be found. if it helps, the buy link is this : http://www.ebay.co.uk/itm/4x-a2212-1000kv-outrunner-motor-4x-hp-30a-esc-4x-1045-prop-b-quad-rotor-/111282436897\nthere might also be a problem with the battery (lipo 3.7v, 2500mah).\n\n\ncan andybody figure out what i'm doing wrong?\nthe sample arduino code i found was this:\n\n\n", "tags": "arduino quadcopter esc", "id": "9254", "title": "pure arduino quadcopter"}, {"body": "i am working on a micro dispensing system, using syringe pump. the design involves a syringe on top to be moved by stepper motor. there would be one liquid reservoir form which the syringe would pull liquid from, and push it to eject liquid from other end. \n\nwhen we pull the syringe, the liquid is sucked into the syringe, while the other opening is shut. when the syringe is pushed, the liquid is ejected from the other end.\n\nthe quantity of liquid to be dispensed would be very small (400mg) so i am using small syringe of 1 or 2 ml .. as per my measurement, after every 100 dispensing operations, 1 ml syringe would be empty and we would need to pull liquid from the reservoir into the syringe, and do the dispensing again. \n\nmy question is, i am unsure about the check valve here. is there a 'single' check valve available which would allow this kind of flow to happen ?\n", "tags": "motor design", "id": "9256", "title": "3 way check valve"}, {"body": "i want to build a low cost robot, running ros for educational purposes. it can be a simple line follower using raspberry pi and an ir sensor. is it overambitious as a beginner project? how difficult is it to make ros run on custom hardware?\n\np.s. i am newbie in both robotics and programming and i am more interested in building actual robots than running simulations. also, i cant afford to buy ros compatible robots.\n", "tags": "ros raspberry-pi electronics", "id": "9260", "title": "how difficult it is to build simple robots (for example line follower) using raspberry pi and ros?"}, {"body": "i started to use ros hydro (robot operating system) on ubuntu, using the simulator \"gazebo\" and roscpp library, in order to program some robots. \n\nin case of pick up and place known objects by robots, what are the topics of object perception for pr2 in ros??\n", "tags": "ros gazebo", "id": "9262", "title": "topics of object perception for pr2"}, {"body": "some questions about this, my friends and i argued with this problem.\n\nare operational space and joint space dependent on each other?\n\ni know that $x_e$ (end effector's pos.) and $q$ (joint var.) can be expressed by an equation with non-linear function $k$:\n\n$x_e = k(q)$\n\nbut i don't think that it tells us operational space and joint space are dependent. \n", "tags": "kinematics", "id": "9263", "title": "are operational space and joint space dependent on each other?"}, {"body": "i started to use ros hydro (robot operating system) and roscpp.\ni tested some examples to move the gripper of pr2 in gazebo (especially the code in :  http://wiki.ros.org/pr2_gripper_sensor_action/tutorials/grab%20and%20release%20an%20object%20using%20pr2_gripper_sensor_action ) with catkin package.\n\ni launch : roslaunch pr2_gazebo pr2_empty_world.launch \n\nand when i run the node of code with : rosrun pack_name node_name, i get :: waiting for the r_gripper_sensor_controller/gripper_action action server to come up ... waiting for the r_gripper_sensor_controller/gripper_action action server to come up ...\n\ni want to know the cause of those lines in order to see the results. what should i do??\n\nit is notable that when i launch : roslaunch pr2_gripper_sensor_action pr2_gripper_sensor_actions.launch\n      in the previous link, i get : \n\n[pr2_gripper_sensor_actions.launch] is neither a launch file in package [pr2_gripper_sensor_action] nor is [pr2_gripper_sensor_action] a launch file name\n", "tags": "ros gazebo", "id": "9265", "title": "waiting for the r_gripper_sensor_controller/gripper_action action server to come up"}, {"body": "i have to know where a multi-rotor is, in a rectangular room, via 6 lasers, 2 on each axis.\n\nthe problem is like this: \n\ninputs :\n\n\nroom : square => 10 meters by 10 meters\n6 positions of the lasers : fixed on the frame\n6 orientations of the lasers : fixed on the frame\nthe 6 measurements of the lasers\nthe quaternion from the imu of my flight controller (pixhawk).\nthe origin is centered on the gravity center of the multi-rotor and defined as if the walls are perpendicular to each axes (the normal of the wall in x is (-1,0,0)) \n\n\noutput :\n\n\nposition in 3d (x,y,z)\nangular position (quaternion)\n\n\nsince i got the angular position of the multi-rotor, i rotated the laser positions and orientations via the quaternion, then extrapolate via the 6 measurements and i got the 3 walls. (orientations of the walls are trivial, then only one point is enough to determine its position.\n\nbadly, i noticed that the yaw (rotation about z) measurement from the pixhawk is unreliable. then i should measure the yaw from the lasers, but i do not success to do it. event if the 2d problem is easy, i am lost in 3d.\n\ndoes someone know if it [algorithm to know xyz position and quaternion from 6 measurments] exists somewhere ? or what is the right way to go on this problem ? \n\nthe question : how could i get the yaw from 2 measurements from 2 lasers which i know the original position, orientation and the pitch and roll. \n\nnote : green pointers are the origin position, red pointers are the \"final\" position, but could be rotated around the red circle (due to yaw).\n\n\n", "tags": "algorithm geometry", "id": "9267", "title": "6d localization with 6 lasers"}, {"body": "\n  update\n  i have aded 50 bounty for this question on the stackoverflow\n\n\ni am trying to implement object tracking from the camera(just one camera, no z info). camera has 720*1280 resolution, but i usually rescale it to 360*640 for faster processing.\n\nthis tracking is done from the robots camera and i want a system which would be as robust as possible. \n\ni will list what i did so far and what were the results.\n\n\ni tried to do colour tracking, i would convert image to hsv colour space, do thresholding, some morphological transformations and then find the object with the biggest area. this approach made a fair tracking of the object, unless there are no other object with the same colour. as i was looking for the max and if there are any other objects bigger than the one i need, robot would go towards the bigger one\nthen, i decided to track circled objects of the specific colour. however, it was difficult to find under different angles\nthen, i decided to track square objects of specific colour. i used this\n\n\n\n\n\n\nand then i checked this condition\n\n\n  if (approx.size() >= 4 &amp;&amp; approx.size() &lt;= 6)\n\n\nand afterwards i checked for\n\n\n  solidity > 0.85 and aspect ratio between 0.85 and 1.15\n\n\nbut still result is not as robust as i would expect, especially the size. if there are several squares it would not find the needed one.\n\n\n\nso, now i need some suggestions on what other features of the object could i use to improve tracking and how? as i mentioned above several times, one of the main problems is size. and i know the size of the object. however, i am not sure how i can make use of it, because i do not know the distance of the object from the camera and that is why i am not sure how to represent its size in pixel representation so that i can eliminate any other blobs that do not fall into that range.\n\nupdate\n\nin the third step, i described how i am going to detect squares with specific colour. below are the examples of what i am getting. \n\ni used this hsv range for the red colour:\n\n\n  scalar(250, 129, 0), scalar(255, 255, 255), params to opencv's inrange function\n  \n  hmin = 250, hmax = 255; smin = 129, smax = 255; vmin = 0, vmax = 255;\n  (would like to see your suggestions on tweaking this values as well)\n\n\nso, in this picture you can see the processing; gaussian blurring (5*5),\nmorphological closing two times (5*5). and the image with the label \"result\" shows the tracked object (please look at the green square).\n\n\n\non the second frame, you can see that it cannot detect the \"red square\". the only main difference between these two pics is that i bended down the lid of the laptop (please look closer if you cannot notice). i suppose this happens because of the illumination, and this causes the thresholding to give not desired results. \n\n\n\nthe only way, i can think of is doing two separate processing on the image. first, to do thresholding based on the colour as i was doing above. then if i find the object to move to the next frame. if not to use this opencv's find squares method.\n\nhowever, this method will involve doing too much of processing of the image. \n", "tags": "quadcopter cameras opencv", "id": "9269", "title": "suggestions on object types (features) to track from ardrone 2 camera"}, {"body": "what action cost should be used to get a smooth path? like we use distance traversed to get the shortest path. will the cost to get a smooth path will be something related to rate of change of slope of the path?\n", "tags": "motion-planning", "id": "9274", "title": "action cost to get smooth path"}, {"body": "i'm a cs student and i need to give a 30-minute lecture about 1-2 papers describing 1-2 algorithms for any of the main problems in robotics (navigation, coverage, patrolling, etc.).\n\ni have no background in robotics specifically, but i did take classes such as algorithms and ai (including some basic ai algorithms such as a*, ids, ucs, and subjects such as decision trees, game theory, etc.).\nthe difference between simply describing one of the above is that i need the paper to refer to actual physical robots and their algorithms, with real problems in the physical world, as opposed to ai \"agents\" with more theoretical algorithms.\n\ni am required to lecture on 1-2 academic papers, published from 2012 onward, with a \"respectable\" amount of citations. any suggestions of such papers would be greatly appreciated!\n", "tags": "navigation algorithm theory coverage", "id": "9276", "title": "papers on algorithms in robotics"}, {"body": "let's say i have a 6-dof flying camera and i want to make it move through a circular tube autonomously and let's suppose that the camera and the system that makes it fly are considered to be just a point in space. which feature of the image i get from the camera can i use to move the camera appropriately, that is to get in one end of the tube and get out from the other? \n\nfor example, i thought i could use edge detection. as the camera moves forward through the tube, due to the fact that its far plane is not infinitely away, there is a dark circle forming where the camera sees nothing surrounded by the walls of the tube. i think that \"preserving\" this circle might be the way to go (for example if it becomes an ellipse i have to move the camera accordingly for it to become a circle again), but what are the features that will help me \"preserve\" the circle?\n\ni would like to use image-based visual servoing to do that. however, what troubles me is the following. in most visual-servoing applications i have seen, the control objective is to make some features \"look\" in a certain way from the camera point of view. for example, we have the projections of 4 points and we want the camera to move accordingly so that the projections' coordinates have some specific values. but the features are actually the same. \n\nin my case i thought that for example i could say that i want the projections of the 4 \"edge points\" of the circle/ellipse to take specific values so that they define a circle centered at the fov of the camera. but if the camera moves to achive this setup of features, then the 4 new \"edge points\" will correspond to the projections of 4 different real points of the pipe and the theory collapses. am i right to think that? any way to get past it?\n\nany oher ideas or relevant literature?\n", "tags": "localization cameras motion-planning visual-servoing exploration", "id": "9277", "title": "how to guide a camera through a circular tube?"}, {"body": "i want to find the general coordinates q=[alpha,beta,gamma] (3 revolute joints) that minimizes the norm ||rgoal - r||_2 with rgoal not included in the manipulator workspace. \n\nthe problem is already solved for coordinates rgoal inside the manipulator workspace, but i really dont know how to do that for singularities.\n\n\n\na stopping criteria i can use is:\n\n\n\nbut that does not really give me the correct answer.\nhow can this be solved in general for a rgoal outside of the workspace?\n", "tags": "inverse-kinematics", "id": "9280", "title": "inverse kinematics with singularity in matlab"}, {"body": "for my robotics project i would like to utilise readily available mobile phone 'power banks' to simplify the power system for my robot. however, such power banks output 5v, great for the logic systems but not for the motors. \n\ni was wondering if i could wire the outputs of two power banks in series and get 10v or is this a very bad idea? should i wire them in parallel and use a boost converter? is a custom solution using 'ordinary' li-po batteries and associated charging circuit the best answer?\n\nadditional information:\n\n\nthis will be a two wheeled robot.\n5v logic\n7+v motor driver\npower banks: 5v 2.1amp 2100mah\n\n", "tags": "mobile-robot power battery", "id": "9282", "title": "mobile phone power packs"}, {"body": "i am in process of designing a micro powder doser for metallic powder into plastic capsules (the capsule volume would be bigger than what we require, so traditional capsule filling wont work). the quantity i need to dose is 400 mg .. what in your opinion would be the best approach for this? \n\nas per my research, i have found out that there can be 3 approaches\n\n\nauger based solution, where an auger is used to control the powder drop gravitationally. \n\n\n\n\n\nvolumetric\nas powder quantities would be small, a metal rods with groove of proper volume can be designed to dispense powder. for keep powder flowing we can use a vibration motor to pour powder in the groove. a stepper motor can rotate the rod for dispensing the powder. i have made the rod bigger to show the concept, and then how it would be mounted in a cap so when we rotate it that much volume of powder is dropped from the cap.\n\n\n\n\n\nweight based measurement.\ndropping powder on to a weighing balance, and control it via feedback .. i think it would be a difficult thing to do, plus time consuming, provided we'd have to fill thousands of capsules.\n\n\nprio # 1 is accuracy, with an error margin of 1 -2 % .. secondly cost .. i'd like it not to be too costly .. 300 - 500 $. \nthe metal powder is not magnetic .. is very fine .. and doesn't clump .. i have read articles and it appears by continues tapping the powder flow can be improved a lot.\n", "tags": "arduino motor", "id": "9290", "title": "micro powder dosing"}, {"body": "i have built an r/c lawnmower.  i call it the honey badger, because it tears stuff up (that's a good thing).  well, i used used batteries to get the project going and now it's long past time to get the honey badger going again.\n\nthe honey badger is built on an electric wheelchair frame, and originally used wheelchair deepcycle batteries.  u1 if i recall.  there are 4 of them wired in 2 banks in series and parallel to give 24v for the 24v motors.\n\ngoing down to the used wheelchair parts place is about an hour drive and requires a weekend visit and will get me used batteries of unknown condition.  \n\ncontrast that with harbor freight, which is 20 minutes away and has solar batteries the same physical dimensions and comparable (?) electrical characteristics.  i think with coupons, tax, and after playing the game, i can get a battery for ~$50, about the same price as a used u1.\n\ni found that amazon also has u1 batteries, and they can be had for ~\\$120 for 2 with shipping.\n\nbatteries plus will sell me some deepcycle auto batteries of greater ah capacity for ~$100 each.\n\ngross for each solution winds up being around the same: ~\\$240 - ~\\$300.\n\nis there a difference in technology between a \"solar battery\" and a \"wheelchair battery\"?  is that difference substantial?  given that i'm pretty rough with this thing, is any particular technology any better suited to these tasks?  is there a benefit or drawback to using an automotive battery?\n\ni have the charger from the original wheelchair and if i recall, it's good for the capacity and has room to spare.  i think it can put out 5 amps.\n\n\n", "tags": "battery", "id": "9294", "title": "choosing a battery: is a harbor freight solar battery ok for a r/c lawnmower?"}, {"body": "i'm seeing a behavior in my roboclaw 2x7 that i can't explain.  i've been trying to manually tune the velocity pid settings (i don't have a windows box so i can't use ionmc's tuning tool) by using command 28 to set the velocity pid gains, then command 55 to verify that they're set correctly, then 35 to spin the wheel at half of its maximum speed.  the problem is that no combination of pid gains seems to make any difference at all.  i've set it to 0,0,0 and the motor still spins at roughly the set point.\n\ni must be doing something wrong, but i'm pouring over the datasheet and i just don't see what it is.  by all rights the motor shouldn't spin when i use 0,0,0!  any ideas?\n", "tags": "control pid", "id": "9296", "title": "why does my roboclaw seem to be ignoring the pid gain settings?"}, {"body": "i am trying to use a quaternions for robotics and there is one thing i don't understand about it. most likely because i don't understand how to define position with quaternions and how to define rotation with quaternions if there is any difference..\nplease watch my \"understanding steps\" and correct if i am wrong somewhere.\n\nlets assume i we have 2 vehicle positions described by 2 rotation quaternions:\n$$\nq_1 = w_1 + x_1i + y_1j +z_1k = \\cos(\\pi/4) + \\sin(\\pi/4)i\n$$\nthis quaternion is normalized and represents rotation over the $x$ axis for $\\pi/2$ angle as i understand it.\n$$\nq_2 = w_2 + x_2i + y_2j + z_2k = \\cos(\\pi/4) + \\sin(\\pi/4)k\n$$\nand this one represents rotation for the same angle $\\pi/2$ over the $y$ axis.\n\n$q_1*q_2 = q_3$ which would be the same rotation as if we made $q_1$ first and $q_2$ second. \n$$q_3 = \\frac{1}{2} + \\frac{i}{2} +\\frac{j}{2} +\\frac{k}{2}$$\n\nquestion 1: given $q_2$ and $q_3$ how can i find $q_1$?\n\nquestion 2: how to find a rotation angle over some other vector, given rotation quaternion? for example i want to find on what angle does $q_3$ turned over $2i+j-k$ quaternion.\n", "tags": "kinematics", "id": "9297", "title": "finding rotation quaternion"}, {"body": "i am new working with robotic arms but i am having trouble finding the correct servo for the base of the arm. \n\nit is a 2 link robot - each link weighs 1.2 kg and is 40 cm long. i have a gripper of 10 centimeters. the servo in the gripper can hold a max of 4kg. the whole robotic arm, including the maximum load it will carry and the servos and other accessories, is 8.3 kg. the maximum load it needs to carry is 4 kg at the end of the arm at 90 cm. \n\nwhat servo could i use to move the rotary base and what servo could i use to lift the arm in the base? the last one is to move the link so it would be preferable to have a 2 axis servo.\n\nthe only specification i need right now is what servo to use my energy supply are two 12 volts dc batteries connected in series with 18ah. i need the servo to be dc. the other things can be worked around the servo that can best do the work.\n", "tags": "robotic-arm servomotor", "id": "9298", "title": "finding high torque servo for robotic arm"}, {"body": "i was reading these papers on visual inertial odometry from iros 15:\n\nsemi-direct ekf-based monocular visual-inertial odometry\n\nrobust visual inertial odometry using a direct ekf-based approach\n\n\ni would appreciate if someone could explain how semi-direct and direct methods vary exactly? as far as i understand, direct methods use pixel intensities in their framework. however, both these papers listed above use photometric intensities/pixel intensity values and yet one is semi direct and the other's direct. \n", "tags": "ekf", "id": "9301", "title": "direct vs semi-direct methods for visual inertial odometry"}, {"body": "what is your preferred time-domain method to simulate ideal position of a motor given an electrical-current input?\n\nassume that the goal is to plot the position output of a motor, based only on a constant electrical-current input.\n\nfor example, x amps are given to the motor. plot the output angle y as a function of time, using only time-based tools (not laplace or matlab outputs).\n\n========= two methods ===========\n\nhere are two methods, which yield different results for some reason. any ideas why they are not identical?\n\ninput is a constant current .\n\nmethod 1) find updated velocity given an acceleration\n\n\n\n\n\n\n\nso, as desired, this is just a function of 'i'. this version has a less stable, oscillating output, regardless of which order  and  are computed in.\n\nmethod 2) find velocity using difference between positions. this could obviously be a noisy result, in practice would need to be filtered.\n\n\n\n\n\n\n\nas before, this position equation is then ultimately just a function of . however, this version has a smoother output.\n\n=====================\n\nappendix 1: for method (1), assume an ideal current-driven dc motor laplace model:\n;  is motor constant,  is rotor inertia, and  is damping.\n\nthen , so . as a reminder,  was found using the previous acceleration via  as noted above in method #1.\n", "tags": "motor kinematics servomotor simulation", "id": "9302", "title": "output angle of dc motor - with current input"}, {"body": "good day, i would like to ask how is it possible to use an ultrasonic sensor for altitude hold in a quadcopter if the sampling rate of the ultrasonic sensor (hc-sr04) is only 20hz before incurring any errors through polling when i had tested it. i have seen this sensor being implemented on other projects however i could not find any papers that explain the use of this sensor in better detail. i have seen possible solutions on the raspberry pi one using interrupts and the other using linux's multithreading.\n\nif my understanding is right, to use interrupts, i need a some sort of data ready signal from the ultrasonic sensor. however this is not available in this particular sensor. is it possible to use the echo pin as the positive edge trigger for the interrupt service routine (read sonar/calculate distance function). but would this not introduce inconsistent loop execution times which is bad for a consistent pid loop.\n\nanother approach is to use multithreading using the wiring-pi library which enables me to run a function, let's say a function that triggers the sonar and calculates the distance along side the pid control loop. how would this affect the pid control loop rate?\n\n\n  which is the best way to implement sonar sensor based altitude hold?\n\n", "tags": "quadcopter pid stability real-time sonar", "id": "9303", "title": "ultrasonic sensor's lag (20hz) effect on pid contol loop rate (150hz)"}, {"body": "i am working on a project with the create 2. just recently i have run into a problem with the battery state. the create 2 has been charging all night so its clean light shows green. however, when i unplug it and press the clean button, it shows red and will not consistently run commands from my arduino that i have hooked up to it. \n\nwhat could be the problem?\n", "tags": "arduino irobot-create battery", "id": "9305", "title": "create 2 light red/green"}, {"body": "i have a chance to develop a user interface program that lets the user control a kuka robot from a computer. i know how to program stuff with the kuka utilities, like orangeedit, but i don't know how to do what i want to do. i don't even know what's the \"best\" language to talk to the robot.\n\nmy idea is to control the robot with the arrow buttons, like up/down controls the z axis and left/right controls the x/y axes.\n\ncan someone help me here? i know there's a lot of libraries to control the robot even with an xbox controller, but if i limit the robot to 3 axes i might be able to control with simple buttons. \n\nedit: now imagine that i have a routine that consists on going from p1 to p2 then to p3. i know i can \"touch up\" the points to refresh its coordinates using the console, but can i do it in a .net application? like modifying the src/srcdat files?\n", "tags": "robotic-arm kuka", "id": "9312", "title": "kuka delimiter .net"}, {"body": "im currently working on an autonomous indoor quad-rotor. for this purpose i'm using opencv to enable computer vision in my drone. i need to be able to detect wall corners, fans (both stationary and rotating), lights and lamps, wall paintings and any other object associated with the walls and ceiling of an indoor environment. until now i have come up with two ideas to achieve this. \n\n1) establish ml (machine learning). use feature descriptors like sift, surf to collect a set of feature descriptors from a training set and try detect the objects of interest. the main issue with this is the access to sift and surf algorithms as they are not available in opencv 3. \n\n2) implement slam algorithm and map the environment and then use the information returned to identify the wall-corners. of course this way i will be not able to detect fans and lights. \n\nso the question is, is there are any other methods i could use other than ones listed above in order to achieve my goal. am i missing something on image segmentation, clustering or image transforms (hough line/circle) which could be utilised in my situation? \n\nthanks\n", "tags": "computer-vision machine-learning opencv", "id": "9313", "title": "how to detect wall-corners, fans, lights in an indoor using cv?"}, {"body": "i am trying to understand the effect of drift in simultaneous localization and mapping (slam). my understanding is that drift occurs because the robot tracks its position relative to a set of landmarks it is storing, but each landmark has a small error in its location. therefore, an accumulation of these small errors over a long trajectory causes a large error by the end of the trajectory.\n\nhowever, what i am confused about is what would happen when the robot tracks its way back to its starting positions. suppose the robot starts in position a, and then starts to move along a path, mapping the environment as it does so, until it reaches position b. now, the robot will have some error in its stored position of b, due to the drift during tracking. but then suppose the robot makes its way back to a, by tracking relative to all the landmarks it created during the first path. when it reaches a, will it be back at the true position of a, i.e. where it started the first path? or will it have drifted away from a?\n\nmy intuition is that it will end up at the true position of a, because even though the landmarks have errors in them, as long as the error is not too large then the robot will eventually get back to the position where it stored the landmarks for a. and once it is there, those landmarks are definitely correct, without error, because they were initialized before any drift errors had started to accumulate.\n\nany help? thanks!\n", "tags": "mobile-robot localization slam navigation mapping", "id": "9318", "title": "understanding drift in simultaneous localization and mapping (slam)"}, {"body": "with introduction of incremental sampling algorithms, like prm and rrt planning in higher dimensional spaces in reasonable computation time has become possible though it is pspace-hard. but why is a quadrotor motion planning problem still difficult even with simplified quadrotor model? \n\ni was solving a dynamic car problem with ompl, which produced solution within 10s but i set a planning time of 100s for quadrotor, but it still does not find a solution.\n", "tags": "mobile-robot quadcopter motion-planning planning rrt", "id": "9320", "title": "why is quadrotor motion planning hard?"}, {"body": "reading this paper on visual odometry, where they have used a bearing vector to parameterize the features. i am having a hard time understanding what the state propagation equation for the bearing vector term means :\n\n\n\nthe vector n is not mentioned in the equations, so its not very clear what it does. would really appreciate if someone would help me understand it :)\n", "tags": "localization slam navigation ekf mapping", "id": "9321", "title": "having a hard time understanding this equation in monocular ekf slam"}, {"body": "looking at pictures of existing designs for quadropod robots, the servos in the legs seem to usually be mounted inside the chassis, with a second attachment at back of the servo as well, such as this:\n\n\n\nrather than putting what looks like an asymmetrical load, like the knees here:\n\n\n\nis this for aesthetics or are there real structural reasons to minimize the lateral load on the axle on a robot of this size?\n", "tags": "joint", "id": "9323", "title": "lateral load on a servo motor"}, {"body": "good day, i have just finished tuning the pitch and roll pid's. i did this by setting the throttle such that the quad is weightless. i did the tuning of each axes separately.\n\ni would just like to ask what is the best way to tune the pid for maintaining an altitude setpoint.\n\n\n  is it best to turn off the pitch and roll pid controllers while tuning the altitude pid or is it best to have them already active while tuning the latter controller?\n\n\ni am going to use a cascaded pid controller using the velocity along the z-axis calculated from the accelerometer output for the inner pid loop (150hz) and the altitude measurement of the hc-sro4 ultrasonic sensor (20hz) for the outer pid loop.\n", "tags": "quadcopter pid stability real-time sonar", "id": "9324", "title": "quadcopter pid tuning for altitude hold/position hold along z axis"}, {"body": "as a subtask inside a main project i need to compute the position (x,y,z) of a quadrotor using an homography. \n\nto do this i will use a camera (attached to the quadrotor) pointing down to an artificial landmark on the floor. basically i need to compute the extrinsic parameters of the camera to know the pose with respect to the landmark. i know the projective points of the landmark in the camera, and the intrinsic matrix of the camera but i also need the real landmark position [x, y, z].\n\ni suppose that z coordinate is equal to 0 because the landmark is plane, but i am not sure how to compute the real [x,y] coordinates.\n\nany idea how to do that?\n\ni am also interested in put the (x,y,z) position of the quadrotor into a control path, anybody knows where i can find info about the most common controllers for do this kind of task?\n", "tags": "control computer-vision quadcopter uav visual-servoing", "id": "9327", "title": "vision-based position estimation for a quadrotor"}, {"body": "out of curiosity, it is possible to control my x5c from a computer? i think that i can buy a transmitter to attach to my laptop. do you think that the communications between my transmitter and the drone be over some proprietary protocol? or would they adhere to some standard?\n\nif you have any links/advice that could point me in the right direction, that would be appreciated.\n", "tags": "quadcopter", "id": "9334", "title": "controlling syma x5c from a laptop"}, {"body": "quaternion has four parameters. calculating jacobian for inverse-kinematics, 3 positions and four quaternion parameters make jacobian $7\\times7$ instead of $6\\times6$. how to reduce jacobian to $6\\times6$ when using quaternion?\n", "tags": "robotic-arm inverse-kinematics jacobian", "id": "9336", "title": "jacobian for inverse kinematics with quaternion of end effector"}, {"body": "i am working on a non-holonomic motion planning problem of a mobile robot in a completely unknown environment. after going through some research papers, i found that d-star algorithm is widely used in such conditions. but there are many d-star variants like focused d*, d*-lite, field d* etc... so which of these variants is suitable in this case? also please suggest any other better approach for this problem?\n", "tags": "mobile-robot motion-planning algorithm", "id": "9337", "title": "suitable d star variant is for non-holonomic motion planning of mobile robots"}, {"body": "in my research project i deal with a mobile robot that perceives through stereo vision. as the stereo input data i currently use several datasets taken from a passenger vehicle that contain real world photos. the datasets are good to get started but have a limited content so i would need to model my own traffic situations to further work on the stereo vision system.\n\ni am thinking about using some kind of synthetic graphics simulation as the input for the stereo system. what are my options? i can imagine a 3d graphics rendering engine whose output would be fed as the input for the stereo vision could probably be used.\n\ni found there are general robotic simulators available like gazebo but since i am all new to robotic simulation i do not really know where to begin.\n\nedit:\n\ni forgot to write that all my code is a pure c++. i use opencv and libelas for stereo vision and point cloud library (pcl) for visualization. all glued together into a single c++ project and compiles into single binary.\n", "tags": "mobile-robot simulator stereo-vision", "id": "9341", "title": "computer stereo vision simulator"}, {"body": "zigbee is not designed for video transmission. i need a mesh network which contains multiple nodes like zigbee for networking robots fpv. is there any solution? \n", "tags": "communication", "id": "9342", "title": "zigbee like network for fpv"}, {"body": "i would like to make a little survey regarding the (geo)spatial projection that you use when elaborating your gps and movement data for the spatial awareness of your robots. \n\nmoving all gps coordinates to a planar projection seems to be the more reasonable choice since not only distances (for which several formulas and approximations exist), but bearings must be computed.\n\ngenerally, although scales are pretty small here, avoiding equirectangular approximation seems a good idea in order to keep a more consistent system.\n\navoiding working in the 3d world (haversine and other great-circle stuff) is probably a good idea too to keep computations low-cost.\n\nmoving the world to a 2d projection is hence what seems to be the best solution, despite reprojection of all input gps coordinates is needed. \n\ni would like to get opinions and ideas on the subject \n(...if ever anyone is interested in doing it u_u).\n", "tags": "odometry geometry", "id": "9344", "title": "which geo-projection to use for odometry"}, {"body": "i have to simulate a pick and place robot (3 dof). i tried with matlab. it should pick and place different objects according to their geometry. \n\nwhere can i find similar m-codes and algorithms?\n", "tags": "robotic-arm matlab", "id": "9346", "title": "pick and place robot"}, {"body": "i have a kinect sensor, and ipi software i use to create motion capture data to use in film editing. i am looking at creating a small, raspberry pi driven bipedal robot just for fun, and i was wondering if it was possible to use the mocap to control the robot? it will only be 20-30 cm tall, with six servos (hips, knees, ankles). is it possible to apply the movement from these six joints on the human body to my robot, like having a string directly from my left knee joint to its left knee servo? it could either be in real-time, like following my actions, or using pre-recorded data.\n\n(note: if needed, i can plug it directly to my apple/windows computer, if the pi could not support this. also, it will have no upper torso at the moment.)\n", "tags": "mobile-robot motion-planning servos", "id": "9349", "title": "applying mocap data to real life robot"}, {"body": "i am doing a project on an automated grain dispenser system using a plc control. i need a valve for dispensing grain from hopper to packet. i should be able to control the flow of the grain. \n\nso what kind of valve should i use for flow control of the grain? there are different types of grains like rice, wheat, etc., and the valve should be controlled by the plc (opening and closing of valve).\n", "tags": "automation", "id": "9351", "title": "which kind of valve is used for dispensing food grains?"}, {"body": "i've built a quadcopter with four brushless motors and escs (30a). i'm using an arduino to control them. i haven't written any complex code; just enough to get them running. everything is fine until i send a number over 920 to the serial. then, for some reason, all the motors stop spinning. i'm using three freshly bought and charged lipo cells (v = 11.1v). here is the link for the site that i bought them from (i cannot seem to find any other resource about them) : 4x a2212 1000kv outrunner motor + 4x hp 30a esc + 4x 1045 prop (b) quad-rotor. \n\nwhen i tried turning on only one motor, i could write up to about 1800 microseconds, while both with 4 and with 1 motor, the minimum that it works is 800. \n\ncan somebody explain why this happens and i how i can fix it? \n\nhere is my code: \n\n\n", "tags": "arduino quadcopter brushless-motor esc", "id": "9353", "title": "why does the esc stop?"}, {"body": "i'm having a hard time trying to understand how to obtain the dynamic model for a system similar to the image.  \n\nthe balloon is a simple helium balloon, however the box is actually an aerial differential drive platform (using rotors). now there's basically a model for the balloon and another for the actuated box. however i'm lost to how to combine both. \n\nthe connection between both is not rigid since it is a string.\nhow should i do it? is there any documentation you could point me to, in order to help me develop the dynamics model for this system? \n\nsince i'm so lost, any help will be useful. thanks in advance!  \n\n\n", "tags": "mobile-robot kinematics dynamics", "id": "9355", "title": "dynamic model of a robot lifted by a balloon (multibody system)"}, {"body": "what's the difference between an underactuated system, and a nonholonomic system? i have read that \"the car is a good example of a nonholonomic vehicle: it has only two controls, but its configuration space has dimension 3.\". but i thought that an underactuated system was one where the number of actuators is less than the number of degrees of freedom. so are they the same?\n", "tags": "kinematics", "id": "9365", "title": "difference between an underactuated system, and a nonholonomic system"}, {"body": "i am working on designing and building a small (1 1/2 lbs), 2-wheeled, differential drive arduino-controlled autonomous robot. i have most of the electronics figured out, but i am having trouble understanding how much torque the motors will actually need to move the robot. i am trying to use the calculations shown here and the related calculator tool to determine what speed and torque i will need. i will be using wheels 32mm in diameter and one of pololu's high-power micro metal gearmotors. i performed the calculations for a robot weight of 2 lbs to be safe and found that the 50:1 hp micro metal gearmotors (625 rpm, 15 oz-in) should theoretically work fine, moving the robot at 3.43 ft/s with an acceleration of around 29 ft/s^2 up a 5-degree incline. \n\nhowever, i have not found an explanation for several things that i think would be very important to know when choosing drive motors. when the robot is not moving and the motors are turned on at full power, they should need to deliver their full stall torque. based on the calculations, it seems that any amount of torque can get the robot moving, but the more torque, the faster the robot's acceleration. is this true? also, if the power source cannot supply the full stall current of the motors, will the robot not be able to start moving? in my case, i am powering the robot through a 7.2v (6s) 2200mah nimh battery pack that can provide around 2.6a continuously, and when it does that the voltage drops to less than 1v. will this be able to power my motors? once the robot reaches full speed and is no longer accelerating, theoretically the motors will not be providing any torque, but i do not think this is the case. is it, and if so, how will i know how much torque they will be providing? will the motors i chose have enough torque to move my robot?\n", "tags": "mobile-robot motor", "id": "9367", "title": "relationship between motor torque and acceleration"}, {"body": "so i am planning on building a robot that turns on when it detects some kind of heat source, i am currently looking at thermal imaging cameras, but am not sure as to how to go about writing code to send a ping or some sort of message when the camera detects a heat source.\n\ndoes anyone know of any way to do this?\n\nthanks \n", "tags": "wheeled-robot", "id": "9370", "title": "thermal imaging camera activation upon detection"}, {"body": "i've started tinkering with a create 2, but i'm having issues reliably getting it to accept my commands.  i can occasionally get it right, but sometimes, it just seems to ignore me.  i'm guessing my cleanup code isn't getting the state fully reset or something.  is there a good pattern to follow for fail-safe initialization code?\n\nhere's what i'm doing right now:\n\n\npulse brc low for 1 second\nwait 1 second\nsend 16x 0 bytes (to make sure if it's waiting for the rest of a command, this completes it - seemed to help a bit when i added this)\nsend 7 (reset)\nwait 10 seconds\nsend 128 (start)\nwait 2 seconds\nsend 149 35 (ask for the current oi state)\nwait 1 second\nsend 131 (safe mode)\n\n\nsometimes i'm then able to issue 137 (drive) commands and have it work.  most times it doesn't.  the times when it doesn't, i'm seeing a lot of data coming from the create 2 that i'm not expecting, that looks something like this (hex bytes):\n\n\n\nthere's more, but my logging cut it off.  i get the same pattern a couple of times, and it seems to be at least partially repeating.  i thought maybe it's the 16 0-bytes i sent followed by , but i still don't know how to interpret that.\n\nsometimes it will make some noise for the reset command, but usually ignores the start/safe mode commands completely (i can tell because the green light stays on).\n", "tags": "irobot-create", "id": "9371", "title": "reliably establishing communication and oi mode with create 2"}, {"body": "i am designing an indoor autonomous drone. i am currently writing an object classification program in opencv for this purpose. my objects of interests for classification are: ceiling fans; ac units; wall and ceiling lamps, and; wall corners. i am using bow clustering algorithm along with svm classifier to achieve this (i'm still in the process of developing the code, and i might try other algorithms when testing).\n\nthe primary task of the drone is to successfully scan (what i mean by scanning is moving or hovering over the entire ceiling space) a ceiling space of a given closed region while successfully avoiding any obstacles (like ceiling fans, ac units, ceiling and wall lamps). the drone's navigation, or the scanning process over the ceiling space, should be in an organised pattern, preferably moving in tight zig-zag paths over the entire ceiling space.\n\nhaving said that, in order to achieve this goal, i'm trying to implement the following to achieve this:\n\n\non take off, fly around the given closed ceiling space and use slam to localise and map its environment.\nwhile running slam, run the object classier algorithm to classify the objects of interests and track them in real time.\nonce obtained a detail map of the environment and classified all objects of interest in the local environment, integrate both data together to form an unified map. meaning on the slam output, label the classified objects obtained from the classifier algorithm. now we a have fun comprehensive map of the environment with labeled objects of interest and real-time tracking of them (localization).\nnow pick random corner on the map and plan a navigation pattern in order to scan the entire ceiling space. \n\n\nso the question now here is, is using object classification in real-time will yield successful results in multiple environments (the quad should be able to achieve the above mentioned tasks in any given environment)?. i'm using a lot of train image sets to train my classifier and bow dictionary but i still feel this won't be a robust method since in real-time it will be harder to isolate an object of interest. or, in order to overcome this, should i use real-situation like train images (currently my train images only contain isolated objects of interests)?\n\nor in my is using computer vision is redundant? is my goal completely achievable only using slam? if, yes, how can i classify the objects of interest (i don't want my drone to fly into a running ceiling fan mistaking it for a wall corner or edge). furthermore, is there any kind of other methods or sensors, of any type, to detect objects in motion? (using optical flow computer vision method here is useless because it's not robust enough in real-time).\n\nany help and advice is much appreciated.\n", "tags": "mobile-robot quadcopter slam opencv", "id": "9372", "title": "real-time object classification for an indoor autonomous quad-rotor"}, {"body": "i am working on rrt planner to remove the goal_bias in it and introduce a new_method to direct the planner towards the goal. i find this method good for lower dimension. when the dimension increases, especially for a dynamic car, though the new_method works better, i observe that the new_method along with goal_bias works even better. \n\nso, what are the possible advantages of removing goal_bias will i be loosing by adding it along with the new_method? \n", "tags": "mobile-robot motion-planning algorithm rrt", "id": "9373", "title": "drawbacks of goal bias in rrt"}, {"body": "assuming a quality industrial servo, would it possible to calculate the weight/resistance of a load? maybe by comparing current draw in a holding position, or by the time it takes to lift/lower an object. could it accurately measure grams, kilograms? what kind of tolerance could be achieved?\n\ni'm trying to eliminate the need for a dedicated weight measurement sensor.\n", "tags": "sensors robotic-arm servomotor", "id": "9374", "title": "measure weight of an object using a servo"}, {"body": "i am trying to make a real time simulink model of leddar sensor (ledddar sensor evaluation kit) in matlab using usb port. but i can't use 'data acquisition toolbox' and 'instrument control toolbox' because the leddar vendor isn't listed in these toolboxes. i would really appreciate if someone could help me with this because i am new to real time simulink.  \n", "tags": "sensors matlab simulator real-time", "id": "9377", "title": "real time simulation model of sensors with matlab"}, {"body": "transformation of frame from global to local is crucial for measurement update but how do we keep track of the direction ? eg: robot location [10,2] and [-10,2] requires different sign. is there a way to not have to set a if else case and have a general expression ?\n", "tags": "slam", "id": "9380", "title": "slam map quardrant"}, {"body": "i have been stabilizing my quadcopter. i tuned my angle pidies and my quadcopter tries to stabilize itself, but there is some overshooting. which is, i think, due to gyro rates. i have read that we have to use two pidies on an axis. i'm having problems to attach these two pidies. \n\ncan anyone help me in cascading angle pid and rate pid? will i have to tune rate pid after tuning angle pid?\n", "tags": "quadcopter pid", "id": "9381", "title": "pid tuning for quadcopter"}, {"body": "i plan to build a mechanism with multiple axis, which is similar to a robot. to start, i need to define some specifications such as repeatable precision, speed, acceleration, and payload. then the motor and structure is selected and designed based on these parameters. after that, i need to choose methods to manufacture these components. i would like to consult experienced experts in this forum that is there any suggested books, textbooks, or website resources i can learn these knowledge? \n", "tags": "mechanism manufacturing books", "id": "9382", "title": "any books or web resources for robotics mechanical design?"}, {"body": "first of all hope this is not a stupid question but i couldn't find any ware a solution.\n\ni have constructed a 3 dof robot arm. i want it to follow a trajectory on a 2d plane (xy). tha shapes i want to follow are lines, cycles and splines. i now the math behind these 3 shaped (how they are defined). i have the kinematics, the inverse kinematics the jacobian and the whole control system (with the pid controller). the system receives as inputs, xd(position), xd'(velocity) and xd''(acceleration) over time.\n\ni found the following image that shows (more or less) my system.\n\n\nso here is were i am stuck. how do i translate the shape to the position, velocity and acceleration that each joint needs to make so the end effector moves in the cartesian space according to that shape?\n", "tags": "pid robotic-arm line-following", "id": "9383", "title": "how to make a robot arm follow a shape/path"}, {"body": "i have recently started reading about pid tuning methods and algorithms, and i encountered the particle swarm optimization algorithm and genetic algorithm.\n\nthe problem is, that i don't understand how each particle/chromosome determines his fitness. on real physical system, each particle/chromosome checks his fitness on the system? wouldn't it take a really long time? i think that i am missing something here... can those algorithms be implemented on an actual physical system? if so, then how?\n", "tags": "pid algorithm", "id": "9384", "title": "pid tuning with methods like ga and pso"}, {"body": "i want to simulate the detection of a moving object by a unicycle type robot. the robot is modelled with position (x,y) and direction theta as the three states. the obstacle is represented as a circle of radius r1 ( in my code). i want to find the angles  and from the robot's local coordinate frame to the circle, as shown here:\n\n\n\nso what i am doing is trying to find the angle from the robot to the line joining the robot and the circle's centre (this angle is called  in my code), then find the angle between the tangent and the same line (called ). finally i would find the angles i want by adding and subtracting  from . the diagram i am thinking of is shown:\n\n\n\nthe problem is that i am getting trouble with my code when i try to find the alpha angles: it starts calculating the angles correctly (though in negative values, not sure if this is causing my trouble) but as both the car and the circle get closer,  becomes larger than  and one of the alphas suddenly change its sign. for example i am getting this:\n\n$$\\begin{array}{c c c c} \n\\text{aux_t} &amp; \\text{phi_c} &amp; \\text{alpha_1} &amp; \\text{alpha_2} \\\\ \\hline\n\\text{-0.81} &amp; \\text{+0.52} &amp; \\text{-1.33} &amp; \\text{-0.29} \\\\\n\\text{-0.74} &amp; \\text{+0.61} &amp; \\text{-1.35} &amp; \\text{-0.12} \\\\\n\\text{-0.69} &amp; \\text{+0.67} &amp; \\text{-1.37} &amp; \\text{-0.02} \\\\\n\\text{-0.64} &amp; \\text{+0.74} &amp; \\text{-1.38} &amp; \\text{+0.1} \\\\\n\\end{array}$$\n\nso basically, the  gets wrong form here. i know i am doing something wrong but i'm not sure what, i don't know how to limit the angles from 0 to pi. is there a better way to find the alpha angles?\n", "tags": "mobile-robot kinematics matlab geometry", "id": "9386", "title": "angle to a circle tangent line"}, {"body": "if i had a single stepper motor how could i use it to create a robotic clamp that could simply grab hold of something like a plank of wood and release it?\n\nare there any standard parts that i could use for this? i'm having trouble finding out what the names of the parts would be.\n", "tags": "robotic-arm", "id": "9388", "title": "building a robotic clamp"}, {"body": "how accurate must my odometer reading be for slam ?\n\ni am writing this extra section because it says my question body does not meet the quality standard. \n", "tags": "slam", "id": "9391", "title": "slam odometer requirement"}, {"body": "i need to develop something in order to update some coordinates in a kuka kr c4 robot predefined program.\n\nafter some research i've found some ways to do it, but all of them non free.\n\ni had several options, like developing a hmi in the console with 3 buttons, to touch up the 3 coordinates that i have to update for example.\n\nsending a xml file would work too but i need a rsi connection, and i can't do it without proper software (i guess).\n\ndo you know about something like this? or a c++ library that allows me to have access the / files or to create a new one with the same \"body\" but with different coordinates?\n\nsumming up, imagine that i have a conveyor that carries boxes and i need to develop a pick and place program. so far so good. but every 100 boxes, the size changes (and i can't predict it). so the operator goes and updates the coordinates, but i want to make sure that he won't change anything else in the program. any ideas?\n", "tags": "robotic-arm industrial-robot kuka", "id": "9395", "title": "kuka robot - update coordinates"}, {"body": "if i need to fly a drone in strong winds, how can i stabilize it? should i use accelerometers and gyroscopes to keep it steady? or should i just use some flight technique under such circumstances?\n", "tags": "quadcopter navigation accelerometer", "id": "9397", "title": "if i must fly my drone in bad weather, how can i maintain control of it in strong winds?"}, {"body": "i am designing a multi modal stent testing machine which will bend, twist, and compress stents (very thin, light, and fragile cylindrical meshes for in arteries) in a tube. the machine will operate at maximum 3.6 hz for months at a time (> 40 million cycles). as the machine will be in a lab with people, the noise should be minimal. i am choosing actuators for my design but was overwhelmed by the range of products available.\n\nfor rotating the stents around their axis, i will need a rotary actuator with the following specs:\n\n\ntorque: negligible max angle: 20 deg\nangular velocity needed: max 70 deg/s\nhollow shafts are a plus\n\n\nfor compressing the stents, i will need a linear actuator with the following specs:\n\n\nforce: low (&lt;1n)\nmax stroke: 20mm but if possible 70mm for allowing different stent lengths \nstroke velocity needed: max 120mm/s\n\n\nprice of these motors is not the driving factor.\n\ni looked into stepper motors, servo motors, and piezoelectric motors. there seems to be s huge selection that fits my requirements. if all motor types have a reliability that suits my needs, which characteristics/advantages/disadvantages should i consider that determine the selection of suitable actuators? i do know what the difference is between the motor types, but there is a lot of overlap. concrete suggestions are welcome.\n", "tags": "actuator reliability", "id": "9400", "title": "choosing motor type for high reliability for many cycles"}, {"body": "i have a question that is puzzling me. i have a simple rectangle enclosure and i have extracted the lidar and odometer data from a test run. if i put my starting position at [0,0,90] it gives bad map. however if i shift it away from [0,0] to something like [50,50,90] the map seems fine. how can this be ? \n", "tags": "slam", "id": "9403", "title": "slam starting point / quardrant issue"}, {"body": "i am trying to get my robot to drive straight and am having trouble.  i find that when running the motors with no load they run fine.  if i put a load on one motor it accelerates.  the other performs as expected, it tries to maintain speed.  i am running 393 motors with encoders and pid selected.  i am running robot c.\n\nsee the following video: https://youtu.be/u3p0wectwco\n\nprogram is as follows;\n\n\n\nthank you,\n\nmark\n", "tags": "pid robotc vex", "id": "9405", "title": "vex cortex motor speeds up under load"}, {"body": "why would a drone need a magnetometer? what would the drone do with this information? i think it would be to tell direction, but why would it need this if it has an accelerometer and a gyroscope?\n", "tags": "mobile-robot magnetometer", "id": "9409", "title": "why would a drone need a magnetometer? are an accelerometer and a gyroscope not sufficient?"}, {"body": "i want to implement rrt for motion planning of a robotic arm. i searched a lot on the internet to get some sample code of rrt for motion planning, but i didn't get any. can someone please suggest a good source where i can find rrt implemented in c++ for any type of motion planning.\n", "tags": "robotic-arm motion-planning algorithm", "id": "9413", "title": "rrt algorithm in c++"}, {"body": "i need my motor to be powered with 12v, 5a for 1 hour continuously. how can i decide the ah rate of the battery. please suggest some lithium ion battery for the specification\n", "tags": "mobile-robot motor power battery lithium-polymer", "id": "9417", "title": "how to decide the battery power for my robot"}, {"body": "i'm implementing a set of loops to control pitch-and-roll angular positions.\n\nin an inner-loop, motor speeds are adjusted to achieve desired angular rates of rotation (the \"inner-loop setpoints\").\n\nan outer-loop decides these desired angular rates (the \"inner-loop setpoints\") based on the aircraft's angular positions.\n\n\n\nouter-loop\n\n\nfrequency = ~400hz\nouter pv = input angular position (in degrees)\nouter sp = desired angular position - input angular position (in degrees)\n\n\n\n\ninner-loop\n\n\nfrequency = ~760hz\ninner pv = input angular rotation (in degrees-per-second)\ninner sp = constant1 * outer mv (in degrees-per-second)\npwm = inner mv / constant2 (as percentile)\n\n\n\n\ni understand what i-gain does and why this is important, but i'm not able to see any practical reason for also having i-gain specified in the outer-loop. surely the inner-loop would compensate for any accumulated error, leaving no error to compensate for in the outer-loop, or is my thinking flawed?\n\nany example gain values to elaborate would be greatly appreciated.\n", "tags": "quadcopter pid", "id": "9420", "title": "why do i need i-gain in my outer-loop?"}, {"body": "how does one today program a fleet of drones autonomously fly together in a formation with visual feedback from onboard camera?\n", "tags": "quadcopter uav", "id": "9429", "title": "drone flight formation"}, {"body": "i want to estimate the poses of a vehicle at certain key frames. the only sensor information i can use is from an imu which yields translational acceleration and orientation measurments. i obtain a 7d pose, i.e. 3d position vector + unit quaternion orientation, if i integrate the translational acceleration twice and propagate the orientation measurements.\n\nif i want to add a new edge to the graph i need a constraint for that edge. in general, for pose graphs this constraint represents a relational transformation $z_{ij}$ between the vertex positions $x_i$ and $x_j$ that are connected by the edge. \n\ncomparing my case to the literature the following questions arised:\n\n\nhow do i calculate a prediction $\\hat{z}_{ij}$ which i can compare to a measurement $z_{ij}$ when computing the edge error? initially, i understood that graph slam models the vertex poses as gaussian distributed variables and thus a prediction is simply calculated by $\\hat{z}_{ij}=x_i^{-1} x_j$. \nhow do i calculate the information (preferred) or covariance matrix? \nhow and when do i update the information matrices? during optimization? or only at edge creation? at loop closure?\ni read about the chi-square distribution and how it relates to the mahalanobis distance. but how is it involved in the above steps? \nstudying current implementations (e.g. mrpt-graph-slam or g2o) i didn't really discover how predictions (or any probability density function) is involved. in contrast, i was even more confused when reading the mrpt-graph-slam example where one can choose between raw poses and poses which are treated as means of a probability distribution.\n\n", "tags": "slam imu data-association", "id": "9434", "title": "pose-graph-slam: how to create edges if only imu-odometry is given?"}, {"body": "can you please help me out? i need my car prototype to follow the object ahead of it. i'm using two ultrasonic sensors hc-sr04 to sense the distance of the obstacle. i want these sensors to determine the direction of the servo motor mg995 using python. how do i calibrate this servo to get an appropriate output? \n\nif anybody could help me out with a code in python on it, i'd be grateful.\n", "tags": "software", "id": "9435", "title": "object following robot"}, {"body": "i implemented a simulation for a robotic arm that has to grab things. this arm has a 6dof structure and a simple gripper on the top. i made a simple ccd ik algorithm to control the arm. i can use it in two ways:\n\n\ncompute the position of the last joint of the arm before the hand\npart (which means 1 end-effector). then use an analytical method to\nplace the hand in a good orientation.\ncompute directly the arm, and the hand position by giving the ccd ik algorithm 2 end-effectors that are the 2 finger of the hand.\n\n\nwhat is the most used method for a grabbing arm robot ? i'm not willing to find a solution, just to know what people usually do. \n", "tags": "robotic-arm inverse-kinematics", "id": "9436", "title": "what is the common process to place a robotic arm gripper"}, {"body": "i use gazebo to simulate a robot arm. to control its joints, i use pid controllers. as you might know, pid are sometimes pretty hard to tune and this is the case for a robotic arm. to avoid any tuning, and because i don't need the pid values to be realistic, i set to zero the derivative and integral parameters, increase a lot the proportional gain and add a lot of damping in my joints. by doing this, i can get a well working arm but only if i disable the gravity. \n\nmy question is the following. do you have an idea how i could simulate a very strong actuator with not necessarily realistic parameters?\n\nedit 1: setting the integral and derivative gain is stupid. the integral gain helps in correcting the effect of the gravity. the derivative gain counters the loss of stability and speed due to the integral gain. \n\nthis question somehow leads to another. do you know what tuning do the robotic arm manufacturer (big arms for car industry for example). i guess that this arm use actuators with a very strong torque and a low maximum speed which reduces the need of tuning.  \n\nedit 2: more info on my setup. i use gazebo 6, with ode. the robot description is in sdf. i control the robot with a model plugin. as a pid controler i use the pid class from the common library of gazebo and get directly the jointcontroler associated to the model. \n\nlet say that i would like actuators very robust without any tuning needed. this way i could have a simulation with dynamics (by opposition to the setposition method). do you think it is possible ?    \n", "tags": "pid power servomotor joint gazebo", "id": "9437", "title": "can we simulate a actuator with a very strong torque with a pid controller"}, {"body": "there is an app called serial, available in the app store. \ni've downloaded it on my mac and am experimenting with it, any ideas on how to send create2 oi commands using \"serial\"?\n\nso far it seems a handy app, i've bypassed all the need for other drivers. anyone else use serial/something of the like? \n\n*when the serial terminal is open and the number 9 is pressed on my mac it seems to activate cleaning mode. thats all the communication i'm getting after hours of playing around in python and mac terminal.  \n", "tags": "mobile-robot irobot-create serial", "id": "9441", "title": "create2 commands with an app called serial"}, {"body": "what is the equivalent code of \"env.checkcollision(robot)\" in c++? even though it is said that conversion of commands from python to c++ is easy and intuitive, where can i find a proper documentation for this conversion?\n", "tags": "motion-planning algorithm", "id": "9443", "title": "openrave chechcollison command in c++"}, {"body": "i've been looking for large robotic arms (with two fingers) and the arm so they are able to pick up and drop things in a space around the arm (and even spin around the 'wrist').\n\ni'm not sure what the terminology is for such an arm. i've seen this, owi-535 robotic arm edge, and it looks close. is there something larger that can be hooked up to a raspberry pi instead of the remote controller?\n\nis there a particular term for this in a generic context? or is there a way to build such an arm using off the shelf parts?\n", "tags": "robotic-arm raspberry-pi", "id": "9445", "title": "name of large robotic arms (two finger) with wrist, arm, hands and spinning shoulder axis"}, {"body": "i am part of my college team which is planning to enter a mars rover challenge. in the point of view of a programmer, where should i start? i know c is the main language nasa used for their rover and i have a basic understanding of it. plus, how much should i look into the rtos part for making a rover?\n\nany books/links to this topic would be greatly appreciated. \n", "tags": "programming-languages c", "id": "9447", "title": "programming a rover"}, {"body": "i refer to these types of brackets as servo brackets, or robot brackets:\n\n\n\ni know that the two specific brackets, shown above, are known as a short-u (some vendors refer to them as \"c\", en lieu of \"u\") and a multi-function bracket, respectively, and that there are other types available, namely:\n\n\nlong u bracket\noblique u bracket\ni bracket\nl bracket\netc.\n\n\nhowever, i am sure that there is a correct name for these types of bracket (or this range of bracket, if you will), rather than just servo brackets - either a generic name or a brand name. i have seen the term once before, on a random web page, but the name escapes me. they are either named after their creator, or, if i recall correctly, the institution where they were developed.\n\ndoes anyone have a definitive answer, preferably with a citation or web reference, or a little historical background?\n", "tags": "mechanism", "id": "9454", "title": "what is the correct name for \"servo brackets\"?"}, {"body": "i am currently implementing an autonomous quadcopter which i recently got flying and which was stable, but is unable to correct itself in the presence of significant external disturbances. i assume this is because of insufficiently tuned pid gains which have to be further tweaked inflight.\n\ncurrent progress:\n\n\ni ruled out a barometer since the scope of my research is only indoor flight and the barometer has a deviation of +-5 meters according to my colleague.\ni am currently using an ultrasonic sensor (hc-sr04) for the altitude estimation which has a resolution of 0.3cm.  however i found that the ultrasonic sensor's refresh rate of 20hz is too slow to get a fast enough response for altitude correction.\ni tried to use the accelerations on the z axis from the accelerometer to get height data by integrating the acceleration to get velocity to be used for the rate pid in a cascaded pid controller scheme. the current implementation for the altitude pid controller is a single loop pid controller using a p controller with the position input from the ultrasonic sensor.\ni had taken into account the negative acceleration measurements due to gravity but no matter how much i compute the offset, there is still the existence of a negative acceleration (eg. -0.0034). i computed the gravitational offset by setting the quadcopter to be still on a flat surface then collecting 20,000 samples from the accelerometer z axis to be averaged to get the \"offset\" which is stored as a constant variable. this variable is then subtracted from the accelerometer z-axis output to remove the offset and get it to \"zero\" if it is not accelerating. as said in the question, there is still the existence of a negative acceleration (eg. -0.0034). my quad then proceeds to just constantly climb in altitude. with only the ultrasonic sensor p controller, my quad oscillates by 50 cm.\n\n\n\n  how can this consistent negative acceleration reading be effectively dealt with?\n\n\npossible solution:\ni am planning to do a cascading pid contoller for the altitude hold with the innerloop (pid controller) using the accelerometer and the outer loop (p controller) using the sonar sensor. my adviser said that even a single loop p controller is enough to make the quadcopter hold its altitude even with a slow sensor. is this enough? i noticed that with only the p gain, the quadcopter would overshoot its altitude.\n\n\n\n\nleaky integrator: i found this article explaining how he dealt with the negative accelerations using a leaky integrator however i have a bit of trouble understanding why would it work since i think the negative error would just turn to a positive error not solving the problem. i'm not quite sure. http://diydrones.com/forum/topics/multi-rotors-the-altitude-yoyo-effect-and-how-to-deal-with-it\nsingle loop pd controller with the ultrasonic sensor only:\nis this feasible using feedback from a slow sensor?\n\n\nsources:\n\n\nlsm303dlhc datasheet: http://www.st.com/web/en/resource/technical/document/datasheet/dm00027543.pdf\nleaky integrator: http://diydrones.com/forum/topics/multi-rotors-the-altitude-yoyo-effect-and-how-to-deal-with-it\nardupilot pid loop: http://copter.ardupilot.com/wp-content/uploads/sites/2/2012/12/alt-hold-pid-version-3.0.1.jpg\n\n", "tags": "quadcopter control pid raspberry-pi sensor-fusion", "id": "9456", "title": "how can we use the accelerometer for altitude estimation?"}, {"body": "i am a computer science major and i only have basic ideas on robotics. i am planning to build a stationary cubical ai. \n\nthe main purpose of this bot will be that, it will have a sensor to check if the door has been opened and immediately asks a question \"who has opened the door?\" i also want it to recognize the correct words to interact the word, i am not talking about voice recognition but word recognition so that who ever speaks the correct words(words in bot's memory) can interact with it. depending on who opens the door(prolly my family) i want it to speak out different things. i want it to respond to simple questions like, \"what is the date and time?\" , \" a random qoute or a fact or a joke\". \n\nis this too hard to achieve? could anyone give me a basic idea on how to approach this project? \n", "tags": "sensors communication first-robotics speech-processing", "id": "9459", "title": "building a stationary robot which can talk"}, {"body": "i am building a quadcopter for my school project. i am trying to program my own flight controller using pid algorithm.\n\ni'll try to make my question simple using as an example below only two motors\n\n\n\nlet's say i am trying to stabilize my two motor system using gyro from the diagram below to one above\n\n\n\nusing the formula output = (gyro - 0) * pgain\n\ndo i need to increase the output only on the motor 2 or would i have to do both:\nincrease the output on the 2nd motor while decreasing the output on the first motor? thank you\n", "tags": "quadcopter pid ardupilot logic-control", "id": "9463", "title": "quadrocopter pid"}, {"body": "is there any theoretical principle, or postulate, that states that the controlling system has to be more complex than the system being controlled, in any formal sense of the notion \"complex\"?\n", "tags": "control theory", "id": "9472", "title": "does a controlling system need to be more complex than the system being controlled?"}, {"body": "i am thinking about working on alternative drone controllers. i am looking into making it more easy to use and a natural feel (debating between sensor bracelets, rings, etc.).\n\nthe main issue i have is, i've been looking over all the standard rc transmitters that are used to control rc aircraft, but i am not sure what technology is inside of them, what kind of ics they use for the actual rc signals. \n\ni want more information on how to make an rc transmitter myself, mainly the protocol that's used to send messages, and what circuitry is needed to actually transmit that, what kind of components do i need and how should i implement the software?\n\ni was aiming at doing this as a side project (hobby), but now i have the chance to use it as a uni project as well, so i'd like to give it a shot now, but i lack the proper information before getting started. \n\ni'd rather not take apart my current rc controller and use an oscilloscope to decode the protocol. \n\nany answers (short or long) and reading material is appreciated.\n\nother questions, can the protocol be implemented in software on an embedded system (raspberry pi, arduino, intel galileo, etc.)?\ni am asking this because the frequency for these are 2.4 ghz.\n\nthis is part of a bigger project, drone related currently, and i could use alternative methods of sending the information, through other wireless means, as the first prototype, suggestions are welcomed.\n\nneed: aircraft rc transmitter protocol info, rc transmitter components &amp; schematics, anything else that might help with the transmission side\n", "tags": "microcontroller radio-control", "id": "9476", "title": "remaking an rc transmitter for controlling aircraft"}, {"body": "i'm trying to attach a small piece of sheet steel (30mm x 50mm x 1mm) to a small piece of nylon (50mm x 50mm x 4mm). does anyone know how they could be fastened using small screws (\n\nany thought appreciated.\n", "tags": "mechanism", "id": "9482", "title": "fastening sheet steel on nylon"}, {"body": "as we all know fixed wing vehicles are designed to have inherent instability which is what enables all fixed wing vehicles to fly.\n\nhowever does this apply to all cases?\n\n\n  do inherently unstable systems desire to be stable for all cases when a closed loop control is implemented on them?\n\n", "tags": "control pid microcontroller uav stability", "id": "9488", "title": "fixed wing uav: do inherently unstable systems desire to be stable for all cases when a closed loop control is implemented on them?"}, {"body": "is there any open interface access to the new braava jet just to drive it around?\n", "tags": "irobot-create", "id": "9491", "title": "can you interface to a braava jet?"}, {"body": "i have read that certain irobot products support or can be hacked to support something close to the open interace. there is even a book about hacking roomba. what robots have this capability?\n", "tags": "irobot-create", "id": "9495", "title": "what irobot products support the open interface besides the irobot create?"}, {"body": "i installed multiple versions of  (2.0.7, 2.0.17, 2.0.18) on windows 7, ubuntu 14.04, and osx 10.11. i could connect to my  but could not install firmware. here's the error i would get:\n\n\n", "tags": "ardupilot", "id": "9500", "title": "apm mission planner 2.0.18 install firmware failure mac os x 10.11"}, {"body": "from a gyroscope i'm getting angular velocities [droll, dpitch and dyaw] as rad/s, sampled at intervals dt = 10ms.\n\nhow do i calculate the short term global orientation (drift ignored) of the gyroscope?\n\npseudo code would be helpful.\n", "tags": "gyroscope", "id": "9502", "title": "use data from gyroscope to calculate orientation"}, {"body": "i was reading an article on euler-lagrange systems. it is stated there that since m(q) and c(q,q') depend on q, it is not autonomous. as a result, we cannot use lasalle's theorem. i have uploaded that page of the article and highlighted the sentence. (ren.pdf)\n\nthen, i read spong's book on robotics, and he had used lasalle's theorem. i am confused. (spong.pdf)\n\ni did some research, and found out that non-autonomous means it should not explicitly depend on the independent variable. isn't independent variable time in these systems? so, shouldn't they be considered autonomous?\n", "tags": "control dynamics robotc", "id": "9505", "title": "euler-lagrange systems, autonomous or nonautonomous?"}, {"body": "let's say i have an industrial sized 6dof robotic arm. i want to control each one of the six joints despite the non-linearity produced by the chain structure, the gravity and the weight of the loads it could lift. \n\ni don't focus here on the speed nor the power limitations, i just want the arm to respond well. moreover, i would like to avoid the use of any prior knowledge such as inertial computation. then i had these considerations, considering that i can play with both the actuator design, and the loop feedback control system:\n\n\nlimit the maximum speed of each actuator to smooth their error variation.\nincrease the damping of the actuators to avoid high frequency instability.     \nfind a good control system, such as a pid, to make sure the targets are reached without oscillations. \n\n\ndo you have any other considerations in mind? do you know what process(es) industrial designers follow?\n\nedit: as it is said in the comments, my question concern the design of an adaptive controller for a robot arm, which is, how to design a joint control system (actuator + loop control) that don't need inertia and masses to be computed (the controller could adapt to its own structure, or to the loads it lifts). \ni'll be very much interested if you know some paper about adaptive control in the field of robotic arms.    \n", "tags": "control pid robotic-arm design actuator", "id": "9508", "title": "considerations to design actuators, and loop feedback systems, for a robotic arm"}, {"body": "my project requires a dc motor for mobility, very similar to an rc car. if precision isn't critical, can i use a solid state relay instead of a motor driver? if the vehicle moves an extra inch on the ground, i don't really care.\n", "tags": "motor driver", "id": "9512", "title": "dc motor control"}, {"body": "there are tons of cameras in devices around us these days. there are used photo cameras, smartphones, tablets at my home gathering dust.\n\ni wonder, what the easiest way could be to get a camera module from some device, connect it to my robot project with a soldering iron and make it work from the software point of view. i am planning to use something like arduino, an stm32 platform, or probably intel edison.\n\nmay be some camera modules are easier to solder and program for a custom  project? or shouldn't i look this way and better find a camera module that is specially designed for custom projects?\n", "tags": "cameras", "id": "9518", "title": "choose and connect a camera to a robot"}, {"body": "what is the difference between g-value and rhs-value of lifelong planning a* algorithm? \n\naccording to this link, d* lite, g(s) directly correspond to the\ng-values of an a* search, i.e. g(s) = g(s') + c(s',s), and rhs(s) is given as \n\n$$\nrhs(s) = \\begin{cases}0 &amp; s = s_{start}  \\\\ \\min_{s'\\in pred(s)}(g(s') + c(s', s)) &amp; \\text{otherwise} \\end{cases}\n$$\n\nwhere, pred(s) denotes the set of predecessors of node 's'. \n\nthus, unless node 's' has more than one predecessor, its g-value and rhs-value will remain same. \n\nso, my question is, in which case will the rhs-value and g-value of a node be different?\n", "tags": "mobile-robot robotic-arm wheeled-robot motion-planning algorithm", "id": "9521", "title": "difference between g-value and rhs-value in lifelong planning a*"}, {"body": "i am new here and i am new to neural network also. :p\ni have gone through the concepts of neural networks but i want to implement it in my project including microcontroller msp430g2553 on launchpad series.\ni am using some sensors and i want to use some neural network code to manipulate the data from sensors to get some threshold.\ni went through this post and tried to implement the codes from the link given but it is giving some error on less ram, i guess it is due to my mcu.  \n\nso, i wanted some help regarding the neural network code or library for energia which i should use.\nthanks in advance.\n", "tags": "microcontroller electronics machine-learning embedded-systems", "id": "9523", "title": "neural nework code or library for msp430g2553 microcontroller"}, {"body": "in propellers as the airspeed increases thrust decreases. is the air speed component taken as a vector quantity perpendicular to the propeller? if thats true the its quiet easy to visualize in case of airplanes but for quadcopters will it be  \"copter_airspeed * sin(copter tilt)\"? \n", "tags": "quadcopter", "id": "9532", "title": "quadcopter propeller physics"}, {"body": "i developed an anthropomorphic arm (structure in aluminium) with 6 dof (3 plus spherical wrist) for direct kinematic. \n\ni chose magnetic rotary encoders to measure angles but i am not satisfied, due to them causing noise on angle measurements. \n\nwhat do you advise me? \n\n\nto add another sensor and perform a sensor fusion? \nto replace magnetic encoders with optical ones? \nor... what else?\n\n", "tags": "arm manipulator", "id": "9535", "title": "anthropomorphic arm"}, {"body": "following is the equation of writhe matrix from the article topology based representation(page no. 6).\n\n\nwhat is the meaning of 'sign' in the second part of this equation? i am not sure if this is some typo in that article as the other article of hierarchical motion planning(page no. 3), compleletely neglects the term 'sign[...]'\n\n", "tags": "mobile-robot control robotic-arm wheeled-robot motion-planning", "id": "9536", "title": "meaning of 'sign' in writhe matrix"}, {"body": "in the article of topological based representation(page no. 12), the equation of the linear gaussian system dynamics is given as \n\n\n\nin above equation what is the meaning of 'curly n'? \n", "tags": "mobile-robot control robotic-arm wheeled-robot motion-planning", "id": "9538", "title": "meaning of symbol, 'curly n' in the equation of linear gaussian system dynamics"}, {"body": "the paper topology-based representations for motion planning\nand generalisation in dynamic environments with\ninteractions by ivan\net.al., says on page 10 that the approximate inference control (aico) framework translates the robot dynamics to the graphical model by the following equation:\n\n\n\nwhat does p(x0:t,u0:t) mean? i feel that p means 'prior of' some uncertain quantity, but i'm not sure about this. \n", "tags": "mobile-robot control robotic-arm motion-planning", "id": "9542", "title": "meaning of the equation of graphical model"}, {"body": "i just got a new irobot create 2. i used to use an element direct bam (bluetooth adapter module) for irobot create previously.\n\nhow can i communicate with a create 2 using bluetooth? what accessories do i need?\n", "tags": "irobot-create", "id": "9544", "title": "enable bluetooth communication with irobot create 2"}, {"body": "i have some questions regarding an ips autonomous robot system,\n\nconfiguration: \n\n\nmounting a camera to the ceiling of a room\nassume the room is a cube of 5mx5mx5m (lxwxh)\nassume the camera is microsoft lifecam studio (cmos sensor technology, sensor resolution: 1920 x 1080, 75\u00b0 diagonal field of view, auto focus from 0.1m to \u2265 10m, up to 30 frames per second, frequency response: 100 hz \u2013 18 khz)\na rover\n\n\nobjectives:\n\n\nby putting the rover in an unknown location (x,y) in the room, the system should localize the rover's position\nafter the rover's coordinates will be known, navigation will be the next step\nwe want the rover to navigate from the known coordinates (x1,y1) (let's say point a) to another point b on the map (x2,y2)\ncontrol signals will be sent to the rover's servos to complete the navigation task\n\n\nmethodology:\n\n\ncamera will capture the environment in real time\nenvironment will be represented as cells (occupancy grid mapping)\nassume each cell represents 5 cm in the environment\nrover will be localized by the system point a\ndetermine the navigation to point b\ndetermine the path of the rover in the grid map (ex: go x cells horizontal then y cells vertical)\ncontrol signal will be sent to rover's servos\n\n\nquestions:\n\n\ncan i use this camera for this task or i need another type of cameras ?\nwhat are the factors affecting the system accuracy ?\n(ex: sensor resolution - fov - fps - frequency response - distance of the camera in the ceiling)\nwhat's is the most important factor to consider to increase the accuracy ?\ni would appreciate any opinions regarding the project\n\n\nking regards,\nthank you\n", "tags": "mobile-robot localization slam computer-vision mapping", "id": "9545", "title": "autonomous indoor positioning system robot based on cv approach"}, {"body": "i'm wondering if there is a way to figure out the actual controllers used in the commercial drones such as ar drone and phantom. according to ar drone sdk, users are not allowed to access the actual hardware of the platform yet they are only capable of sending and receiving commands from/to the drone. \n\n\n\nedit:\ni'm hoping to to check the actual controller utilized in the software. when i fly ar drone, it seems the platform can't stabilize itself when i perform aggressive maneuvers, therefore, i can guess that they use linearized model which is applicable for using simple controllers such as pd or pid\n", "tags": "quadcopter control", "id": "9547", "title": "reverse engineering commercial drone control algorithms"}, {"body": "the article of topology-based representation (page no. 13, line 5) says that, topology-based representation is invariant to certain changes in the environment. that means the trajectory generated in topology-based space will remain valid even if there are certain changes in the environment. but how is this possible? is there any simple example to understand this concept?\n", "tags": "mobile-robot control robotic-arm motion-planning", "id": "9552", "title": "how is topology-based representation invariant to certain change in environment"}, {"body": "we are making a project in which we want to count the no. of people entering and leaving a room with one single entrance. we are using ir sensors and detectors for this ,along with an aurdino. we have a problem in this system,  i.e when two or more persons are entering or leaving the room at a time we are getting a wrong count. thanks in advance for your valuable time and solution.....if there is any other better way,please state that.\n", "tags": "arduino sensors microcontroller", "id": "9553", "title": "counting number of people entering a room"}, {"body": "i'm trying to implement a pid control on my quadcopter using the tiva c series microcontroller but i have trouble making the pid stabilize the system. \n\nwhile i was testing the pid, i noticed slow or weak response from pid controller (the quad shows no response at small angles). in other words, it seems that the quad's angle range has to be relatively large (above 15 degrees) for it to show a any response. even then, the response always over shoots no matter what i, d gains i choose for my system. at low p, i can prevent overshoot but then it becomes too weak.   \n\ni am not sure if the pid algorithm is the problem or if its some kinda bad hardware configuration (low imu sample rate or maybe bad pwm configurations), but i have strong doubts about my pid code as i noticed changing some of the gains did not improve the system response.  \n\ni will appreciate if someone can point out whether i'm doing anything wrong in the pid snippet for the pitch component i posted. i also have a roll pid but it is similar to the code i posted so i will leave that one out.\n\n\n\n\u00a0\n\n\n\nalso, can someone please tell me how this motor mixing works?: \n\n\n\nvs what i did in the function:\n\n\n", "tags": "quadcopter control pid imu pwm", "id": "9554", "title": "quadcopter pid algorithm"}, {"body": "i want to issue two slightly different drive commands what is the smallest loop rate that the robot-create accepts new commands?\n\ni know from reading the documentation that it appears the sensors are read every 15ms. \n\nnot sure what the command rate is?\n", "tags": "irobot-create", "id": "9555", "title": "what is the millisecond rate that robot-create can respond to two different drive commands?"}, {"body": "i want to start designing an arduino project and have telemetry readings that indicate tilt or angle of placement. \n\nwould an accelerometer be the best for determining tilt? are there good tutorials? \n", "tags": "mobile-robot arduino", "id": "9565", "title": "best sensor to determine \"up\" versus \"down\""}, {"body": "i'm trying to make decisions for motors on a robot build. i keep running across cim motors. what is a cim motor? where does the designation cim come from? what does cim mean?\n", "tags": "motor", "id": "9568", "title": "what is a cim motor?"}, {"body": "i am going through a paper, kinematic modelling and simulation of a 2-r robot using solidworks and verification by matlab/simulink, which is about a 2-link revolute joint robotic arm. according to the paper, the trajectory analysis of the robot was done via simulations in matlab/simulink. \n\nit shows the following picture, trajectory generation of 2\u2010r robot with matlab/simulink:\n\n\n\nand then, simulink - simulation block to calculate the trajectory:\n\n\n\ni think this is done in simmechanics, but i am not sure. experienced users, can you please tell me what am i looking at and how can i reproduce this?\n", "tags": "robotic-arm matlab simulation", "id": "9569", "title": "robotic arm analysis in matlab/simulink"}, {"body": "i'm trying to simulate a humanoid robot using gazebo with plugins. since our actual model uses dynamixel motors, i'd like to know how exactly they work to make the simulation as realistic as possible.\n\ngazebo offers two options to control joints. one is a pid controller, provided by the  class. the other way is to directly set a torque to the joint. (the pid method too is ultimately implemented using torques).\n\ncurrently, i'm trying the pid-based implementation. i've used a p-only controller with damping on all joints (i've had to guess both values). however, there is a large amount of noise, and the difference between actual and desired position is at times as much as 10-12 degrees (especially when the foot of the robot hits the ground).\n\ndoes the actual motor use a pid controller as well? i can't seem to find the details here, dynamixel ex-106 user's guide, but this link, dynamixel ex-106+ robot actuator mentions \"compliance/pid : yes\".\n\nif the motor does use a pid controller, then what are the parameters? and how does it allow us to set moving speed then?\n\nif the motor doesn't use a pid controller, then what is the pattern of torque provided? in the manual (first link), i found this\n\n\n  from the current position 200 to 491 ( 512-16-5=491 ), movement  is \n  made  with appropriate  torque  to  reach  the  set  speed; from  491 \n  to  507  ( 512-5=507 ), torque is continuously reduced to the punch \n  value; from  507  through 517 (  512+5=517  ), no torque is generated.\n\n\nthis is rather vague though, and no further details are provided.\n\nalso, i'm aware that extremely high damping and extremely high p-values might do the trick. but i want to simulate what actually happens on the motors, and that is probably not the way to go. \n\ni'd appreciate it if anyone has any idea of what dynamixel servos do, or examples of simulated dynamixel motors anywhere else.\n", "tags": "pid simulation gazebo humanoid dynamixel", "id": "9571", "title": "simulating dynamixel motors in gazebo"}, {"body": "i'm trying to find the transfer function of a quadrotor with two controller loops, following next structure:\n\n\ni know how to calculate the attitude stability controller, which relate rotor speed and desired angles. however, i have no clear at all how to implement the translational controller transfer function, whose output is the desired angle that the rotors must achieve considering the position i want to translate.\n\nconsidering that two controllers are pd, how can you calculate the translational controller transfer function and include it in the system? time domain equations in the outer loop are next, where u terms relate to the thrust axis components. thanks\n\n\n", "tags": "quadcopter control", "id": "9572", "title": "transfer function of a quadrotor position controller"}, {"body": "there is a message system which does not appear to be document in the oi spec. this appears to be a a canonical terminal type serial interface in which messages come back such as firmware version and stuff. i am not sure how to determine what the end of this type of message is? it is a fixed number of end lines? or bytes. one message seems to indicate str730 which would be a 730 byte string.\n\nthe open interface spec seems to indicate a non canonical interface spec in which you read a fixed number of bytes with no processing of end lines. is this correct?\n", "tags": "irobot-create roomba", "id": "9578", "title": "create2 serial. canonical versus number of bytes interface"}, {"body": "i have been looking for a cheap ultrasonic sensor that is not blind under +/-30 cm but the only sensors i could find use the following shape, which is not suitable for my project (because of the robot design that only has 1 hole, and not 2..) : \n\nis there any chance to find a sensor with that other shape with a range starting around 5cm ?\n\n\n\nactually i am wondering if that 2nd shape makes this constraint mandatory or if i just did not found the appropriate product.\n", "tags": "ultrasonic-sensors", "id": "9580", "title": "ultrasonic sensor range and shape"}, {"body": "i want to convert an electric atv (quad) for kids (like the highper atv-6e) to radio control for a robotics project. these small atvs are about a meter long and weigh about 40 kg. i need to choose servo motors for steering and braking. what grade servos do i need and how much torque do they need to have? can i use the strongest rc servo i may find (like this 115kg/cm one or maybe even more, with metal gears of course) or do i need an \"industrial grade\" servo?\n\ni plan to use one servo for steering and one for braking. for braking the atv has mechanical disc brakes - two discs in the front and one common disc in the rear (there are two brake levers - front/rear). i plan to use only one servo and use it either for front or for rear. the plan is to mount the brake wire to the servo which would \"simulate\" the lever movement.\n\ni guess i could also make a \"weak\" servo stronger by adding a proper gear, but i am not really into mechanical engineering much and would prefer an off-the-shelf component.\n\n\n", "tags": "servomotor rcservo steering brake", "id": "9583", "title": "servo motors for large scale rc car"}, {"body": "is it possible to downgrade from ros jade to indigo?\n\nfor those who are not yet familiar with robot operating system (ros), here: ros\n", "tags": "ros", "id": "9590", "title": "downgrade ros from jade to indigo"}, {"body": "in the d*lite algorithm, described in line 21 of figure 3, on page 4, in d* lite, the  starts with defining $s_{last}=s_{start}$. but value of $s_{last}$ is never updated in the entire algorithm. \n\nso what is the purpose of defining this term and what does it mean?\n\n\n", "tags": "mobile-robot control robotic-arm motion-planning algorithm", "id": "9592", "title": "meaning of s_last in d star lite algorithm"}, {"body": "i would like to mechanically measure the distance a kids electric atv traveled. the atv will not be used with kids but as a mobile robot instead. it has a common rear axle for both rear wheels which i think could be a good place to put an odometer on (since the chance both wheels will slip should be minimal). regarding suspension it has a single shock for rear axle.\n\nmy plan is to put a bigger gear on the axle itself and then add a smaller gear to it on which some kind of sensor would measure number of its rotations. one rotation of the axle may be something like 20 rotations of the small gear. what kind of sensor can i use for sensing rotation?\n\nanother way of making an odometer may be some kind of optical solution (disc with holes and an optical sensor) but this seems to be rather complicated and also the the direction of travel could not be easily estimated (unless the motor is running in some direction).\n\ni just found a term called wheel speed sensor which looks interesting and seems to employ primarily non-contact sensing (which is definitely better than mechanical gears). rather then optical solution i like the hall effect sensor solution which may be simple and mechanically robust. but still, my question is open on how to implement this...\n\ni would like to use the odometer for both speed estimation and distance estimation. i need to read the sensor from c/c++ on a linux box.\n\nedit: the thing i am looking for is probably correctly called a rotary encoder or a wheel encoder.\n\nthe atv may look like one of these:\n\n\n\n", "tags": "mobile-robot sensors odometry encoding rotation", "id": "9593", "title": "mechanical odometer with digital output"}, {"body": "i am having an issue with some hand-eye calibration. \n\nso i am using a simple robot which at its tool point has an stereo camera mounted on it. \n\ni want to perform some visual serving/tracking based stereo images extracted from the camera in the \"hand\". the camera provides me x,y,z coordinates of the object i want to track. \n\ni can at all time extract an homogenous transformation matrix from base to tool (not cam) as . \n\nfirstly... i guess i would need perform some form of robot to (vice versa) camera calibration, my idea was that would consist of something like this \n\n\n\nwhere the t_tool_cam would entail the calibration... since the camera is at the tool point, would that entail the t_tool_cam should entail information on how much the camera is displaced from the tool point, and how it is rotated according to the tool point? or is not like that?\n\nsecondly... how do i based purely x,y,z coordinate make an homogeneous transformation matrix, which includes an rotation matrix ?\n\nthirdly.. having a desired transformation matrix which in theory this     \n\n\n\nwould provide me, would an inverse kinematics solution provide me with one or multiple solution?... in theory should this only provide me one, or what?\n", "tags": "robotic-arm inverse-kinematics rotation", "id": "9597", "title": "hand-eye calibration?"}, {"body": "find the origin and coordinate directions of a frame resulting from a rotation of $90^{\\circ}$ about the z axis, followed by a displacement of $\\begin{pmatrix}1\\\\7\\\\3\\end{pmatrix}$. hence find the position, in the original frame, of the vector $\\begin{pmatrix}3\\\\8\\\\1\\end{pmatrix}$, defined in the resulting frame.\n", "tags": "kinematics inverse-kinematics", "id": "9600", "title": "how can i determine the pose of the origin after some transformations?"}, {"body": "in optimized d*lite algorithm as shown in the figure below (page 5, of the paper d*lite), when the procedure computeshortestpath() is called for the first time in line 31, u(list of inconsistent vertices) contains only goal vertex ($s_{goal}$). thus in the procedure computeshotestpath()(line 10-28), $u = s_{goal}$. and as, $k_{old}=k_{new}$ (because $k_m=0$), condition $k_{old}\\leq k_{new}$ is satisfied and $u = s_{goal}$ is again inserted in u with same value of $k_{old}=k_{new}$. thus, it seems that line(11-15) will run forever, and the algorithm will not be able to find the shortest path from goal to start.\n\ni know that this algorithm has been widely used and i am failing to understand it. but where am i going wrong? \n\n\n", "tags": "mobile-robot control robotic-arm motion-planning algorithm", "id": "9602", "title": "computeshortestpath() in dstar lite algorithm"}, {"body": "i'm working on an autonomous quad copter, i have two gps co-ordinates (source and destination co-ordinates). i need to move my quad from the source to the destination, for this i need to calculate the heading and set the yaw value of my quad. how can i calculate the heading and make sure the quad is headed in the right direction as the target co-ordinates?\n\nif i use magnetometer the declination angle will vary from place to place and so i will have to keep changing the declination angle. if i'm calculating based on just the gps co-ordinates, it's not accurate. \n\nwhat is the best way to do this?  how do i calculate the above? \n", "tags": "quadcopter gps magnetometer", "id": "9604", "title": "quadcopter heading calculation"}, {"body": "i have been stuck on this for weeks, i really hope that someone can help me with this,thank you in advance.\ni am trying to write an imu attitude estimation algorithm using quaternion kalman filter. so based on this research paper: https://hal.archives-ouvertes.fr/hal-00968663/document, i have developed the following pseudo code algorithm:\n\npredict stage:\n\nqk+1/k = ak * qk;  where ak contains the gyro measurement. \n\n\npk+1/k = ak * pk *ak.transpose() + q; where q is assumed to be zero.\n\nafter prediction, we can use this formula to get the supposed gravity measurement of accelerometer yg in body frame :\n\nyg = r * g;  // r is the rotation matrix generated from quaternion qk+1/k and g = (0,0,0,9.81).\n\nthis equation then translates to the following equation which allows me to get measurement model matrix h.\n\nh * qk+1/k = 0; //where h stores value related to (yg-g).\n\nupdate stage:\n\nk = p * h * (h * p * h.transpose()+r)^(-1); //r should be adaptively adjusted but right now initialized as identity matrix\n\nqk+1/k+1 = (i-kh)qk+1/k;\n\nqk+1/k+1 = (qk+1/k+1)/|qk+1/k+1|; //normalize quaternion\n\npk+1/k+1 = (i - kh)pk+1/k;\n\nthe following is the main part of my code. the complete c++ code is at here https://github.com/lyf44/fcu if you want to test.\n\n\n\nthe problem that i face is that my code does not work.the prediction stage works fine but the updated quaternion state is only correct for the first few iterations and it starts to drift away from the correct value. i have checked my code against the research paper multiple times and ensured that it is in accordance with the algorithm proposed by the research paper.\n\nin my test, i am rotating around x axis by 60 degree per iterations. the number below the started is the angle of rotation. state and updated state is the predicted and updated quaternion respectivly while true value, meas, result are acceleration due to gravity in body frame.as the test result indicates, everything is way off after rotating 360 degrees.\nthe following is my test result:  \n\n\n\ncan someone help me confirm that my understanding about the theory of this quaternion kalman filter and my pseudo code is correct? also, if anyone has implemented attitude estimation using maybe a different version of quaternion kalman filter, i would greatly appreciate if you can provide a pseudo code and a little explanation.\nthank you guys very much!\n", "tags": "quadcopter kalman-filter", "id": "9608", "title": "quaternion kalman filter algorithm"}, {"body": "assume that i have a rigid body for which i know that it can rotate with respect to a global reference frame (which is considered fixed and already given) for only a few degrees of angle, so i can describe its rotation by using the small angle approximation. for this system, i would like to know if there is a rotation representation that offers more accuracy when compared with other representation methods.\n\nthe main representation methods that i considered are the euler angles and the pitch-yaw-roll transformation. to my perception, i think that pitch-yaw-roll representation is expected to be more accurate, since all the angles are expressed with respect to the initial coordinate frame. on the other hand, euler angles are defined on different frames, so i am not sure if the resulting angles will be really small.\n\nto sum up, i know that the body can rotate for only a few degrees and i would like to know which coordinate representation is much probable to deliver the smallest angles, such that the small angle approximation is more valid.\n\nit could also be the case that there is not a general answer (so it depends on the specific configuration) but still i haven't found anything about this topic on the related literature!\n\nexample (no small angle approx used): assume i have a coordinate frame which describes a point in space by the following vector\n\n$p2=\\begin{bmatrix} 4 \\\\ 1 \\\\ 0.05 \\end{bmatrix}$.\n\ngiven another coordinate frame which is rotated with respect to the previous one, the description of the same point is given by     \n\n$p1=\\begin{bmatrix} 3.8933 \\\\\n    1.3566 \\\\\n   -0.0630 \\end{bmatrix}$.\n\nusing euler angles, i can find that the rotation matrix $r_{euler}$ is characterized by the angles $0.1,0.2,0.1$ rads, which correspond to the angle of rotation around z axis, the rotation around the resulting y axis and the rotation around the resulting z axis, respectively (these are basic stuff, it is explained in many books.). so i have that $p1=r_{euler} p2$.\n\nnow i want to find the corresponding rotation matrix if i use the pitch-yaw-roll representation. here i have to solve an optimization problem and the solution that i get (maximum error between p1 and the estimated p1 is $3 \\times 10^{-8}$) delivers me the following angles\n\n$\\begin{bmatrix}  -0.0103   \\\\ 0.0257  \\\\  0.0902\\end{bmatrix}$,\n\nwhich correspond to the rotation around the x,y and z axis of the initial coordinate frame. \n", "tags": "inverse-kinematics geometry rotation", "id": "9609", "title": "most accurate rotation representation for small angles"}, {"body": "i am working on inverse kinematics for a 5dof arm.  the tool is symmetric about its z-axis so we don't card about those rotations in the solution but we do care about the direction of the z-axis.  in other words instead of the goal states orientation being $r = (n_d~ s_d ~a_d)$ we only care about $a_d$. how would i calculate the orientation error (or adjust normal inverse kinematics) to account for this.  setting $n_d$ and $s_d$ to zero or any value forces a particular orientation which may not be reachable.  for full 6dof situations i have previously used the following equation for orientation error. \n\n$$e_o = \\frac{1}{2} (n_e(q) \\times n_d + s_e(q) \\times s_d + a_e(q) \\times a_d)$$\n\n\n\nis it sufficient to remove $n$ and $s$ from this equation giving\n\n$$e_o = \\frac{1}{2} (a_e(q) \\times a_d)$$\n\nif not how else could i handle this situation?  \n", "tags": "robotic-arm inverse-kinematics", "id": "9615", "title": "orientation error with free rotations"}, {"body": "i have assembled a 4wd car using kits i bought on ebay.\n\ni have 4 motors similar to this one: .\n\nthe description says:\n\n\n  \n  operating voltage: 3v~12vdc \n  (recommended operating voltage of about 6 to 8v)\n  maximum torque: 800gf cm min (3v)\n  no-load speed: 1:48 (3v time)\n  the load current: 70ma (250ma max) (3v)\n  this motor with emc, anti-interference ability. \n  the microcontroller without interference.\n  size: 7x2.2x1.8cm(approx)\n  \n\n\ni am not too fond of the max speed i can reach, but i would be able to provide more power, because i have a 12v 2a battery onboard.\n\nso far i have used 6v, because that seemed to be the safer voltage choice.\n\nhas anybody tried successfully higher voltages, without wearing down the motor in few hours (i've read this can happen)?\n\nalternatively, can someone recommend replacement motors that would tolerate reliably a higher power envelope?\n\ni would like to preserve the gearbox and replace only the motor, if possible.\n\ni think i could fit a motor 2-4 mm longer (replacing the transparent strap which bonds it to the gearbox), if that makes any difference.\n\nbtw, i'm making the assumption:\n\nhigher_voltage => higher_torque => higher_speed\n\nbut i'm not sure it's overall correct.\n\ni expect that it would at least produce higher acceleration during the transients.\n", "tags": "brushless-motor", "id": "9616", "title": "motor upgrade to higher torque?"}, {"body": "i am trying to implement a path planner to generate a path that moves the robot from q_start to q_goal.   \n\nq_goal is extracted from a stereo camera mounted on the tool, from\nwhich i extract x,y,z coordinates of the desired position, the rotation can be arbitrary. \n\nthe robot i am using is an industrial ur5 robot arm, the software i use is capable of performing jacobian based inverse kinematics given a transformation matrix with rotation and translation. \n\nmy inverse kinematics provide me with only one solution, which is ok, but doesn't provide me flexibility for path planning...\n\nhow do i using inverse kinematics determine all possible q-configurations that fulfills my criteria of having the desired x,y,z coordinates? \n", "tags": "robotic-arm inverse-kinematics", "id": "9619", "title": "extracting as many possible end configurations as possible"}, {"body": "i'am trying to implement a path following algorithm based on mpc (model predictive control), found in this paper : path following mobile robot in the presence of velocity constraints  \n\nprinciple: using the robot model and the path, the algorithm predict the behavior of the robot over n future steps to compute a sequence of commands $(v,\\omega)$ to allow the robot to follow the path without overshooting the trajectory, allowing to slow down before a sharp turn, etc.\n$v:$ linear velocity\n$\\omega:$ angular velocity\n\nthe robot: i have a non-holonomic robot like this one (image extracted from the paper above) :\n\n\nhere is my problem: before implementing on the mobile robot, i'am trying to compute the needed matrices (using matlab) to test the efficiency of this algorithm. at the end of the matrices computation some of them have dimension mismatch\n\nwhat i did:\nfor those interested, this calculation is from \u00a74 (4.1, 4.2, 4.3, 4.4) p6-7 of the paper.  \n\n\n  4.1 model\n  \n  $z_{k+1} =  az_k + b_\\phi\\phi_k + b_rr_k$ (18) \n  with:\n  $a = \\begin{bmatrix} 1 &amp; tv \\\\ 0 &amp; 1 \\end{bmatrix}$\n  $b_\\phi = \\begin{bmatrix} {t^2\\over2}v^2\\\\ tv  \\end{bmatrix}$\n  $b_r = \\begin{bmatrix} 0 &amp; -tv \\\\ 0 &amp; 0 \\end{bmatrix}$\n  $t$: sampling period\n  $v$: linear velocity\n  $k$: sampling index (i.e. $t= kt$)\n  $z_k:$ the state vector $z_k = (d_k, \\theta_k)^t$ position and angle difference to the reference path\n  $r_k:$ the reference vector $r_k = (0, \\psi_k)^t$ with $\\psi_k$ is the reference angle of the path at step k \n  \n  4.2 criterion\n  \n  the predictive receding horizon controller is based on a minimization of the criterion\n  $j= \\sigma^n_{n=0} (\\hat{z}_{k+n} - r_{k+n})^t q(\\hat{z}_{k+n} - r_{k+n}) + \\lambda\\phi^2_{k+n}$, (20)\n  subject to the inequality constraint\n  $ p\\begin{bmatrix} v_n \\\\ v_n\\phi_n \\end{bmatrix} \\leq q,$\n  $n=0,..., n,$\n  where $\\hat{z}$ is the predicted output, $q$ is a weight matric, $\\lambda$ is a scalar weight, and $n$ is prediction horizon.\n  \n  4.3 predictor\n  \n  an n-step predictor $\\hat{z}_{k+n|k}$ is easily found from iterating (18). stacking the predictions $\\hat{z}_{k+n|k},n = n,...,n$ in the vector $\\hat{z}$ yields\n  $\\hat{z} = \\begin{bmatrix} \\hat{z}_{k|k} \\\\ \\vdots \\\\ \\hat{z}_{k+n|k}\\end{bmatrix} = fz_k + g_\\phi\\phi_k + g_rr_k$  (22)\n  with\n  $\\phi_k = \\begin{bmatrix} \\phi_k, \\ldots, \\phi_{k+n}\\end{bmatrix}^t$,\n  $r_k = \\begin{bmatrix} r_k, \\ldots, r_{k+n}\\end{bmatrix}^t$,\n  and\n  $f = \\begin{bmatrix}i &amp; a &amp; \\ldots &amp; a^n \\end{bmatrix}^t$\n  $g_i = \\begin{bmatrix} 0 &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ b_i &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ ab_i &amp; b_i &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ \\vdots &amp; \\ddots &amp; \\ddots &amp; 0 &amp; 0 \\\\ a^{n-1}b_i &amp; \\ldots &amp; ab_i &amp; b_i &amp; 0 \\end{bmatrix}$  \n\n\nwhere index $i$ should be substituted with either $\\phi$ or $r$\n\n\n  4.4 controller\n  \n  using the n-step predictor (22) simplifies the criterion (20) to\n  $j_k = (\\hat{z}_k - r_k)^t i_q (\\hat{z}_k - r_k) + \\lambda\\phi^t_k\\phi_k$, (23)\n  where $i_q$ is a diagonal matrix of appropriate dimension with instances of q in the diagonal. the unconstrained controller is found by minimizing (23) with respect to $\\phi$:\n  $\\phi_k = -l_zz_k - l_rr_k$, (24)\n  with\n  $l_z = (lambda + g^t_wi_qg_w)^{-1}g^t_wi_qf$\n  $l_r = (lambda + g^t_wi_qg_w)^{-1}g^t_wi_q(gr - i)$\n\n\ni'am trying to compute $\\phi_k = -l_zz_k - l_rr_k$ but the dimension of $l_r$ and $r_k$ does not match for matrix multiplication.\n\nparameters are :   \n\n\n$t=0.1s$\n$n=10$\n$\\lambda=0.0001$\n$q=\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; \\delta \\end{bmatrix}$ with $\\delta=0.02$\n\n\ni get :\n$r_k$ a (11x2) matrix (n+1 elements of size 2x1, transposed)\n$g_w$ a (22x11) matrix\n$g^t_w$ a (11x22) matrix\n$i_q$ a (22x22) matrix\n$f$ a (22x2) matrix\n$g_r$ a (22x22) matrix    \n\nso lz computation gives (according to the matrix sizes)\n$l_z=(scalar + (11x22)(22x22)(22x11))^{-1} (11x22)(22x22)(22x22)$\na (11x2) matrix.\nas $z_k$ is (2x1) matrix, doing $l_zz_k$ from (24) is fine.  \n\nand lr computation gives (according to the matrix sizes)\n$l_r=(scalar + (11x22)(22x22)(22x11))^{-1} (11x22)(22x22)((22x22) - (22x22))$\na (11x22) matrix.\nas $r_k$ is (11x2) matrix, doing $l_rr_k$ from (24) is not possible.\ni have a (11x22) matrix multiplicated by a (11x2) matrix.\n\ni'm sure i'm missing something big here but unable to see what exactly.\nany help appreciated.\n\nthanks\n", "tags": "mobile-robot navigation", "id": "9620", "title": "mobile robot path following using model predictive control (mpc)"}, {"body": "what are the main differences between electric motor and internal combustion engine for an atv-sized mobile robot platform in terms of functionality, implementation difficulty (\"rc\" conversion, \"electronic\" operation), durability and maintenance when used as an autonomous platform? a full sized atv/utv like polaris ranger (ev) is in question.\n\nare the advantages/disadvantages basically the same as the differences between electric and nitro rc cars or does the bigger scale adds something important to the game? i can think of the main differences like bigger range and faster \"refueling\" with ic and less maintenance with electric but i am interested in a detailed comparison.\n\nthe transmission for the ic engine is considered to be automatic.\n\nedit: the fuel injection for ic is considered to be electronic (efi) but i do not know whether that also means the \"electronic\" throttle (no mechanical wire as with carburetor?). whatever the throttle may be i see the lag between its \"actuation\" and the engine running into higher rpm and giving more power/speed as the main disadvantage for ic control - however, it may probably be quite easy dealt with in software (by adding some timeout when checking desired rpm).\n", "tags": "mobile-robot electronics engine electric propulsion", "id": "9621", "title": "electric vs. internal combustion engine for propulsion"}, {"body": "i am a  uncertain about how to compute the right homogeneous transformation matrix to compute an inverse kinematic q-configuration. \n\nlooking at robot like this \n\nwhere at the end of this robot i have a camera mounted on to it. \n\nthe purpose of my application is to make the robot follow an object, so basically tracking it.  the camera provide me with an x,y,z coordinate, which the position i want place my robot arm. \n\nfirst question - how do i set up the desired homogenous transformation matrix?\n\nthe way i see it, i have 2 transformation matrices being  and  which become \n\nmy question is that how do i compute my desired transformation matrix. \ni think i know how i should setup the transformation matrix for the camera which would be like this\n\nt_world_tool = \n\n(second question is regarding the rotation matrix, how do prescribe such that rotation in arbitrary as long the endpoint has the desired position in the world frame?)\n\nbut what should t_tool_base entail? should it entail the transformation of its current state or the desired transformation, and if so how do i extract the desired t_tool_base transformation?...\n", "tags": "robotic-arm kinematics inverse-kinematics", "id": "9622", "title": "inverse kinematics for 6 jointed robots"}, {"body": "i have some data obtained from an experiment in terms of movements and observations with odometry and sensor data. my task is to find the probability mass on each of the grid cells after each set of motion and observation. i'm a bit lost in figuring out how to compute probability mass for each of the grid cell.\nmy odometry information is in terms of rotation, translation and rotation and my sensor information is in terms of range and bearing angle. \nhow do i calculate the probability of robot present in each of the grid cell?\n\ni have the formula for belief after motion as summation(p(x|u, x')xbel(x'))\nhow do i compute the motion model with noise?\n", "tags": "localization filter", "id": "9625", "title": "bayesian filter for 2-d grid localizaton"}, {"body": "good day,\n\ni would like to ask why is it that when i add the yaw control to my pid controller for each motor. the quadcopter refuses to take off or maintain its altitude. i am curently using a cascaded pid controller for attitude hold using an accelerometer, a magnetometer and a gyroscope, and a 40hz ultrasonic sensor for altitude hold. since the scope is indoor i have done away with the barometer due to its +-12m error. \n\nresulting response\n\nwithout yaw control, the plot below shows the response of the quadrotor.\n\n\nwith yaw control, the plot below shows the response of the quadrotor.\n\n\ndebugging\n\ni found out that each of the outputs from each pid's give a too high of a value such that when summed together goes way over the pwm limit of 205 or full throttle.\n\n\nwithout yawpid contribution\nthe limiter kicks in without damaging the desired response of the system thus is still able to fly albeit with oscillatory motion along the z axis or height\n\n\n\n\nwith yawpid contribution\nthe added yaw components increases the sum of the pid's way above the limit thus the limiter compesates the excess too much resulting in an over all lower pwm output for all motors thus the quad never leaves the ground.\n\n\n\n\n//motor front left (1)\nfloat motorpwm1 =  pitchpid + rollpid + yawpid + basethrottle + basecompensation;\n\n//motor front right (2)\nfloat motorpwm2 =  pitchpid - rollpid - yawpid + basethrottle + basecompensation; \n\n//motor back left (3)\nfloat motorpwm3 = -pitchpid + rollpid - yawpid + basethrottle + basecompensation; \n\n//motor back right (4)\nfloat motorpwm4 = -pitchpid - rollpid + yawpid + basethrottle + basecompensation;\n\n\nbackground\n\nthe pid parameters for the pitch, yaw and roll were tuned individually meaning, the base throttle was set to a minimum value required for the quadcopter to be able to lift itself.\n\nthe pid parameters for the altitude sensor is tuned with the other controllers active (pitch and roll).\n\npossible problem\n\n\nlimiter algorithm\n\n\na possible problem is that the algorithm i used to limit the maximum and the minimum throttle value may have caused the problem. the following code is used to maintain the ratio of the motor values instead of limiting them. the code is used as a two stage limiter. in the 1st stage, if one motorpwm is less than the set basethrottle, the algorithm increases each motor pwm value until none of them are below that. in the 2nd stage, if one motorpwm is more than the set maxthrottle, the algorithm decreases each motor pwm value until none of them are above that. \n\n\n\nthis was obtained from pixhawk. however the difference is that they employ only upper bound compensation limiting, while mine also performs lower bound compensation limiting which may cause more saturation once it reaches the second stage.\n\n\nfrom:https://pixhawk.org/dev/mixing\n\n\ngains are set too high.\n\n\nit is also possible that i've set my p gains too high thus exceeding the max rpm limit of the motors causing the limiter algorithm to overcompensate.\n\ncurrent pid settings:\n\nthe minimum motor value for the quad to lift itself is 160 while the maximum limit is 200 from the pwm time high of 2000ms \n\n\npitch (cascaded p-pid controller)\nrate p = 0.07\nrate i = 0.03\nrate d = 0.0001\nstabilize p = 2\nroll (cascaded p-pid controller)\nrate p = 0.09\nrate i = 0.03\nrate d = 0.0001\nstabilize p = 2\nyaw (cascaded p-pid controller)\nrate p = 0.09\nrate i = 0.03\nrate d = 0.0001\nstabilize p = 2\nhover (single loop pd controller)\np = 0.7\nd = 35\n\n\npossible solution\n\ni think i have set the pid parameters particularly the p or d gain too high that the computed sum of the outputs of the controller is beyond the limit. maybe retuning them would help.\n\n\n  i would just like to ask if anyone has encountered this problem or if you have any suggestions. thank you :) \n\n\nedit\n\ni have added the plots of the response when the control loop is fast (500hz) and slow (300hz)\n\n\n  500hz: does not fly\n  \n  \n  300hz: flies\n  \n\n", "tags": "quadcopter control pid raspberry-pi stability", "id": "9629", "title": "quadcopter refuses to fly when the yaw pid component is added"}, {"body": "i have trouble estimating the heading when close to the \"pivot\" point of the compass, and could use some input on how to solve it. i have set up my angles to be from 0-360 degrees during the testing but will be using radians (-pi, pi) from now on.\n\nthe setup is a differential robot with imu, wheel encoders and a magnetic compass. \n\na complementary filter is used for fusing gyroz and odo measurements to estimate the heading, and then correct it with a kalman filter using the magnetic compass.\n\nmy problem occurs when the robot heading is close to -pi/pi .\n\nthe estimated heading is useless even though the robot is not even moving.\n\ni am thinking this must be a very common problem and probably has a better solution than what i came up with, which was either re-initializing the integrator when crossing zero, adding 180 degrees each time the error is larger, or just ignoring the compass if the error is too large...\n\nit's my first kalman filter so i may have made a poor implementation if this is not a common issue...\n\nedit: trudesagen's solution solved my problem.\n", "tags": "mobile-robot kalman-filter compass", "id": "9636", "title": "regarding kalman filter and estimating heading with magnetic compass"}, {"body": "i'm using processing to send strings to arduino, using functions like \n\n\n\non the processing side and in the arduino side i'm using calls like \n\n\n\nyet i can't get the servo to stop at all. how do i make it shut off?\n\nif it was a regular servo i wouldn't even ask because that's easy but i write 0 or 90 or low and nothing, it just keeps spinning in one direction but when it meets one of the conditions in my statements it switches polarity/direction and that's good - i want that but i made this function to make it stop and it is not doing so, does anyone have any ideas ?\n\ni am using a parallax continuous rotation servo.\n", "tags": "arduino", "id": "9637", "title": "how do we write a stop to a continuous servo?"}, {"body": "when running on a hard surface, the create will shake sometimes during turns or acceleration.\n\nhas anyone ever removed the springs or pinned the wheels in place so they can't move up and down?\n", "tags": "irobot-create roomba", "id": "9640", "title": "steadier wheels - pin them or lock springs"}, {"body": "i was wondering if a 1d point mass (a mass which can only move on a line, accelerated by an external time-varying force, see wikipedia - double integrator) is a holonomic or a nonholonomic system? why?\n\ni think that it is nonholonomic since it cannot move in any direction in its configuration space (which is 1d, just the $x$ axis). e.g. if the point mass is moving at $$x=10$$ with a velocity of 100 m/s in positive $x$-direction it cannot immediately go to $$x=9.9$$ due to its inertia. however, i have the feeling that my thoughts are wrong...\n\nthe background is the following:\n\ni am trying to understand what holonomic and nonholonomic systems are. what i found so far:\n\nmathematically:\n\n\nholonomic system are systems for which all constraints are integrable into positional constraints.\nnonholonomic systems are systems which have constraints that are nonintegrable into positional constraints. \n\n\nintuitively:\n\n\nholonomic system where a robot can move in any direction in the configuration space. \nnonholonomic systems are systems where the velocities (magnitude and or direction) and other derivatives of the position are constraint. \n\n", "tags": "dynamics movement", "id": "9642", "title": "what's the difference between a holonomic and a nonholonomic system?"}, {"body": "i just started learning robotics at school and i have some problems to solve forward kinematics with dh parameters. i don't really understand how i can get them from the image. i would appreciate if somebody could help me with it.\n\n. \n", "tags": "forward-kinematics dh-parameters", "id": "9646", "title": "forward kinematics with dh parameters"}, {"body": "i will have this configuration: \n\n\na2212 brushless motor 1000kv - 4 each\necs - 30a electronic speed control (esc) - 4 each \npropeller - 1045 propeller cw &amp; ccw pair 10 inch * 4.5 pitch\narduino mega - 2560 board\nraspberry pi 3 \nopen pilot cc3d flight controller \n\n\ni want to know what rating li-po battery should i get for this configuration.\nthe reason behind my asking here is because a simple google search is not able to satisfy me with an explanation...\n\nalso, my weight will be 1.5 kg for the quadcopter, so i need a stable current discharge.\n\nthis is my first quadcopter, i am a computer science guy, so i have little knowledge of electronics, i'm learning, but need help...\n", "tags": "quadcopter arduino raspberry-pi battery", "id": "9647", "title": "what rating li-po battery should i get for this configuration?"}, {"body": "i'm still new to rpi and i am currently trying to do a smart home model. \n\ni planned to use rpi only to control 5 servos (which will be controlling the open/close of the doors by setting the angle) and 5 leds. \n\nwill i need to use an external circuit to supply the power for the servos or is it fine to just connect them to the rpi?\n", "tags": "raspberry-pi rcservo", "id": "9652", "title": "smart home model with raspberry pi"}, {"body": "i'm doing a mobile robot project with robotic arms, i wanted to buy a chassis for my robot that can carry enough weight, but many websites don't give definitive answers about maximum payload.\n\nis there is a way to figure this out just by knowing details about the motors?\n", "tags": "mobile-robot", "id": "9654", "title": "how to know the payload of the chassis from its motors?"}, {"body": "i'm stuck on equation 4.30 of page 176 in\nhttp://www.cds.caltech.edu/~murray/books/mls/pdf/mls94-complete.pdf\n\nthis equation:\n$\\frac {\\partial m_{ij}} {\\partial \\theta_k} = \\sum_{l=\\max(i,j)}^n \\bigl( [a_{ki} \\xi_i, \\xi_k]^t a_{lk}^t {\\cal m}_l' a_{lj} \\xi_j + \\xi_i^t a_{li}^t {\\cal m}_l' a_{lk} [a_{kj} \\xi_j, \\xi_k] \\bigr)$  \n\nseems impossible to process because it requires adding a 2x1 to a 1x2 matrix.\ngoing by rowsxcolumns notation. matrices m and a are 6x6 and $\\xi$ is a 6x1, so how does this addition statement fit the rules of matrix addition?  this must be my mistake, i just don't see how.\n", "tags": "dynamics matlab", "id": "9657", "title": "dynamic model of a manipulator"}, {"body": "i want to know if there is any equation that calculates the maximum force of a robot joint. the force that we should not exceed.\n\nfor example in human leg, if we apply a big external force to the knee, it will break. now how can i find the necessary force that will just make the leg move without breaking the knee.\n\ni have a programme that generates robot morphologies randomly with different sizes, so i have to know the force to not exceed for each joint. i think this depend on weight, mass, inertia of each robot part.\n\ni can not do this by trial and error because i have hundreds different morphologies.\n\nthis video shows the behaviour of robot if i apply a big force. it is in gazebo robotic simulator.\n\nthanks in advance!\n", "tags": "simulation joint force", "id": "9659", "title": "how to find maximum force of a robot joint"}, {"body": "people have recommended me implement an analytic version of inverse jacobian solver, such that i won't be forced only the least square solution, but would have an local area of solution near to the one i desire. \n\ni can't seem to implement it correctly, i mean how much does it differ from the least square inverse kinematics which i have implemented here?\n\n\n\ni want a vector of solution - how do i get that?\n\nthe q is related to this how do i construct i a transformation matrix given only x,y,z of tool position?\n\nthe robot being used is a ur5  - https://smartech.gatech.edu/bitstream/handle/1853/50782/ur_kin_tech_report_1.pdf\n", "tags": "robotic-arm inverse-kinematics industrial-robot c++", "id": "9662", "title": "implementing an analytic version of an inverse kinematic"}, {"body": "i am working with sick lidars and have to mount/unmount them quite often on my robot. the mounting process is very tedious especially when it comes to making sure that the lidars are horizontal. i thought about using ir goggles (like the night vision ones) and some fog machine (like the one in nightclubs) in order to see the surface covered by the lidar's rotating laser ray. as a result i would expect to see something like this but planar. \n\nbefore thinking about trying to get my hands on such hardware i wanted to ask:\n\ndo sick laser have enough intensity to be observed by such goggles?\ndoes anybody tried such an approach?\n", "tags": "laser lidar", "id": "9668", "title": "directly observing lidar laser rays"}, {"body": "i need to find out if there is a way to get at least 60 hz of linear motion with at least 5 mm of stroke that i intend to make linear persistence of vision device(not rotating one)it must be small and light as possible. ( maybe 50 mm long and 10-15 mm diameter or around these) (less than 500 grams) the load will be around 50 grams.  there are voice coils that is very expensive, can i use solenoids for instance or what do you recommend? \n\nthanks \n", "tags": "actuator motion", "id": "9669", "title": "what type of actuator should i use?"}, {"body": "the diagram below shows an old beambot strategy:\n\n   \n\nis there code or an example using this method? i would rather avoid opencv, ultrasonic, gps etc. i just want the roomba wheels to react as i go straight, turn left or right. finally, i could add a front wheel on a servo and try having the roomba turn with me. \n\nalso has anybody added big, all terrain wheels to a roomba to replace the originals?\n", "tags": "control irobot-create roomba", "id": "9673", "title": "is there a \"follow me\" roomba/create that works like a beambot?"}, {"body": "i am going to build an autonomous robot with kalman-filter for localization integrated from lidar, encoder, imu and gps. i will also add obstacle avoidance while moving to the required position.\n\nis the atmega32 8-bit suitable for that (or arduino mega) or do i have to use avr32, arm, or pic32 and which is better?\n", "tags": "mobile-robot arduino localization microcontroller", "id": "9675", "title": "suitable uc for atonomous robot"}, {"body": "the issue is regarding simulation of kuka robot kr16l6-2 in matlab using robotics toolkit by peter corke. i wish to simulate  the kinematic before passing a command to real robot for motion.\n\ni have attached the dh-parameters. apart from this i have also tried many other combination of orientations but to no useful effect.\n\n\n\nthe problems is that the robot base rotates counter-clockwise by default for positive increases in , while the original robot moves in the opposite direction. similarly for the wrist roll, i.e. joint 4, the direction of simulation is reversed.\n\nin order to confirm that it's not my mistake only, i searched for similar ready made simulation software. although it did not include the same kuka robot, a similar variant (kuka_kr_5_sixx_r650) was available. hence, kuka_kr_5_sixx_r650 had one set of motions for base and wrist in rokisim v1.7 for positive increases in joint angle and reverse motion in roboanalyzerv7  .\n\n\n\nnote: only the rotation of j1 (base) and j4 (wrist roll) are reversed\nand i want to recreate the results of rokisim v1.7 in matlab where rotations are similar to the real world robot spec provided by kuka.\n", "tags": "matlab industrial-robot simulation kuka", "id": "9677", "title": "kuka kr16l-2 robot simulation base and wrist rotation inconsistent with original robot"}, {"body": "i created a package in catkin workspace and put a publisher.py node inside the src directory of package which worked fine. then i added another node subscriber.py node and used catkin_make to build. now when i try to run any of the nodes or find package i am getting above error. am i missing any step ?\n\nthanks.\n", "tags": "ros", "id": "9680", "title": "getting \u201crospack package not found error\u201d in ros"}, {"body": "i was recently looking into purchasing either a dynamixel ax-12a or xl-320. the xl seems to use ollo frames, which only seem to be available in a toy-like set. \n\ni was wondering if there are any other frames available or if i should just get an ax-12?\n", "tags": "servos walking-robot dynamixel", "id": "9682", "title": "what frames are supported by the dynamixel xl-320 ollo?"}, {"body": "i am part of my college robotics team which is preparing for robocon 2017.\n\nwe have used mecanum wheels in last robocon competition, but we have faced huge slip and vibration. i have looked for all kinematic and dynamic formulas and all stuff about mecanum wheels, but still can't get to a conclusion for my problem.\n\nvideo of the problem\n\nthe robot is around 25 kg and the mecanum wheel diameter is about 16 cm with 15 rollers (single type). please help me why it happened like that!?\n\nalso suggest me what to do now - should i design a new mecanum wheel or bring it from market? \n\nif i should design, what parameters should i consider, and please help me how to design in cad software like solidworks? and then, shall i give it to 3d printing?\n\nif i should buy directly from market, where should i buy?\n", "tags": "design wheel", "id": "9683", "title": "choosing the right mecanum wheel"}, {"body": "i tried to disable sleep by pulsing the brc pin low for one second every minute as suggested in the oi, but my create 2 still goes to sleep after 5 minutes.\n\nmy firmware is r3_robot/tags/release-3.2.6:4975 clean\n\nthe create 2 is connected to an arduino, and the brc is driven by one of the arduino pins.  i verified on a dmm that the voltage is indeed toggling.  i am able to both send and receive serial data between the arduino and create2.\n\npseudo-code:\n\n\ninitialize roomba.  connect serial at 115200 baud.  toggle brc: high for 200 ms, low for 200 ms, then high again.  leave it high.\nask roomba to stream sensor data in passive mode.  wait 1 second after brc toggle to give some extra time to wake-up.  then send opcode 7 (reset), wait for reset message to complete by looking for the last few characters, then wait another second for good measure.  next, send opcode 128 (start into passive mode), wait 100 ms to let opcode stick, then ask for stream of data (opcode 148 followed by number of packet ids and the packet ids themselves).\nmain loop: echo data from create2 to the serial-usb output of the arduino so that i can view the create2 data.  the data sent by the create2 look valid (good checksum) and are sent in the expected time interval of ~15 ms.  the main loop also toggles the brc low for 1 second every minute.\n\n\nfor the full gory details, the complete arduino sketch is shown below\n\n\n", "tags": "irobot-create", "id": "9684", "title": "cannot disable sleep in passive mode for irobot create 2"}, {"body": "i think i have an simple problem, but can't my head around how i should resolve it...\n\nmy setup looks like this: \n\n\n\n\nthe grey box on end effector is supposed to be an camera, which measures a dx,dy,dz between the object and the camera. these are used to  position the camera such that dz between the object and the camera is equal to 0.5, and dx = dy = 0. \n\ni know that i using inverse kinematics can determine the q which positions it according the given rotation and position, but what if i only provide it a position only?\n\nhow do extract all q that make dx = dy = 0, and dz = 0.5, while keeping the object in sight at all time?\n\nan example could be if an object was placed just above the base (see second image), it should then find all possible configurations which in this case would consist of the arm rotating around the object, while the camera keeps the object in sight...\n\nupdate\n\ni just realized a possible solution would be to create a sphere with the object in centrum  an radius of dz, and then use this sphere to extract all pairs of rotations and position... but how would one come by with such an solution?\n", "tags": "robotic-arm inverse-kinematics stereo-vision", "id": "9686", "title": "performing inverse kinematics based on a displacement of the end effector?"}, {"body": "how do i compute all transformation matrices which places a robot endeffector at the shell of this sphere, with the end effector pointing toward the object in the center. \n\n\n\ni know at all time how far the object is relative to the endeffector, and radius of the sphere is the desired distance i want between the object and endeffector.  \n\ni want by using inverse kinematics pan around this object in a sphere shaped trajectory. \n\neach transformation matrix should contain different positions on the sphere and the rotation should be oriented such that the arm looks at the object. \n\nthe position should be relative easy to compute, as i already know the distance to to object, and radius of the sphere. \n\nbut the rotation matrix for each position is still a mystery for me.  \n", "tags": "robotic-arm rotation", "id": "9691", "title": "generate transformation matrices for rotating around a object?"}, {"body": "i am currently working on an autonomous quadcopter project using stereo vision for obstacle detection. i am planning to use vfh+ for 2d trajectory planning, meaning movements of the quadcopter are only available on the x and y axes and no movement is permitted along the z axis or altitude.\n\ni currently have no methods implemented on position tracking. accelerometers have been known to generate a lot of errors from integration. i tried to look for optical flow sensors for low computation needed however have found no luck as most of them are out of stock (probably due to company change (agilent/avago to pixart)). \n\nfrom what i understand, when the after the certainty/occupancy grid is constructed and reduced to a polar coordinate system, the robot's heading is aligned to the computed unblocked sector.\n\n\n  is it possible to generate the occupancy grid without position feedback?\n\n\ni am planning to run the quadcopter at a slow constant velocity due to the data throughput of the raspberry pi stereo vision system of 2hz. i plan to use the obtained angle direction from the vfh algorithm and align the quadcopter's heading.\n\nthe motor control scheme to be used for the quadcopter is mostly yaw and pitch based. yaw for heading control and pitch for forward control.\n\ntwo raspberry pi b+'s are used. one for motor control which is currently running the pid control loop at 530hz. another for stereo vision which is currently running at 2hz.\n", "tags": "quadcopter motion-planning stereo-vision vector-field-histogram vfh", "id": "9696", "title": "vector field histogram: is it possible to generate an occupancy grid without position feedback?"}, {"body": "i have an uav modeled in three dimensions with let's say position coordinates $p_{uav} = (x_1,y_1,z_1)$ that is moving in a direction $d = (d_x,d_y,d_z)$ and a moving obstacle modeled as a sphere with known centre coordinates $p_{sph}=(x_2,y_2,z_2)$ and radius $ r_{sph}$. \n\nif i have a plane $p$  in the direction of movement of the uav that intersects the sphere, i want to be able to calculate the angles with respect to the vehicle's movement formed by the tangents to the sphere in the plane $ p$. in the figure, i would like to know how to calculate the angles $\u03b1_1$ and $\u03b1_2$.\n\n\n\nif it helps, what i am looking is an extension in three dimensions for this:\n\n\n\nwhich is a vehicle in two dimensions ;it is obviously an easier problem which requires only the centre of the circle. however i am not really sure how to make it work in 3d, as supposedly the plane can intersect the sphere at any two points, not necessarily the centre. \n\nthanks in advance for your help.\n", "tags": "localization kinematics geometry", "id": "9697", "title": "how to determine the angles between a uav and a sphere"}, {"body": "i am having some issue with implementing a least square solution of the inverse kinematics problem. \n\nthe q configuration i get are rather large, or makes no sense, so i was hoping someone here could help me find my error in my program. \n\n\n\nwhat am i trying to do, i am trying to orbit a robot endeffector  around an object in the center. the trajectory of the endeffector is an sphere where the endeffector should always point in to the object.  the sphere function should compute all transformation matrices which move the robot arm to the different position on the sphere with a given rotation, and the inverse kinematics should compute all the different q-states, given an  which is the actual displacement to the object itself.\n\ni am not quite sure where my error could be at, but i think it might either be at  function where i generate my desired transformation matrix, or in  where i create , i think i might have made an mistake in creating ,  , \n\nthe libraries i've been using is eigen, robwork (basically all rw::) if anyone want to look syntax through. \n\nupdate\n\nbased on @ghanimmukhtar i began checking for singularities for the jacobian.. which seems in general supringsly low. i computed it for a list of random q configurations which resulted into this...\n\n\n\n is a is the distance between tcp and an object i want to keep in sight. the sphere is like a safety zone, but is mainly used to compute the orientation of the tool. \n", "tags": "inverse-kinematics c++", "id": "9700", "title": "implementation of inverse kinematics solution in c++"}, {"body": "i am working on a high speed autonomous robot (about 6-7 m/s), which does obstacle detection as well as senses traffic lights (i have used raspberry pi 3 and arduino uno).  \n\nfor the steering mechanism, i wanted to implement an ackerman's steering. i've read about the principle and have understood its basics. now to actually make the design, i am currently using switchboards, sold here in india, they are surprisingly strong, lightweight, waterproof(they are switchboards) and cheap. now i got 1 big axle and the small axle cut out already, along with the two l-shaped pieces that join the 2 axles together... i'm just now confused as to how to connect the wheels to the axle and how to make them rotate along side it. the site won't let me upload any pics right now, i'll try again asap.\n\ni have the switchboard, an electric drill and will to do anything to make this happen ( ;p ). i don't have access to a 3d printer.\n\nany help would be greatly appreciated...  \n\np.s- and if you have any suggestions of your own, which might be better for my robot, feel free to share them, i'm just looking for a good steering method for my robot.\n", "tags": "arduino raspberry-pi navigation steering", "id": "9701", "title": "designing ackerman's steering principle for an autonomous robot"}, {"body": "i'm using stereolabs zed camera for my computer vision project. i did a small research about several sensors on the market and ultimately we decided to go with the zed camera. \n\nhowever i'm finding that the precision of the camera isn't that great. and the point cloud takes too much storage space. anyone found the same problems? and if so, how did you managed them?\n\nthank you!\n", "tags": "computer-vision stereo-vision", "id": "9702", "title": "how can i improve zed camera precision?"}, {"body": "in modeling dynamics of a robot ,in which servo motor is adjusted inside the link, there is a need to find inertia tensor of the motor itself,right?\n\n\n\nso if it is needed how can i get the inertia tensor of motor since i couldn't find its solid works model having internal components,i mean gears and other stuff(with related specified materials)?\n", "tags": "servomotor dynamics", "id": "9704", "title": "motor inertia tensor?"}, {"body": "i got a new servo a few days back (rc servo, futaba fp-s148). i first tested it out with the sweep sketch on arduino, powering it with the arduino 5v and gnd pins only. it was working, just fine.\n\ntoday i was trying to use it in my robot and i tried powering it with 2 lipo batteries (samsung icr16850 2200mah, from an old laptop battery) connected in series, giving 8.32v. as soon as i connected my servo, it started rotating randomly, i had not connected it to my arduino yet. i quickly took it out.\n\nnext, i used a l7805 to get 5.13v regulated supply out of my batteries that i used earlier. when i connected my batteries to the servo, and the servo to the arduino, uploaded the sketch, the servo started behaving rather strangely, it first did a complete turn and then stopped. only a humming sound came from the servo. strange thing is, whenever i connect one of my multimeter leads to the power cables, the servo immediately turned in the opposite direction only as long as only lead was in contact with either the positive or negative wire.\notherwise, the servo just gives a humming sound.\n\nhave i fried my servo? or is it some other issue?\n\nupdate 1\ni stripped down the servo and checked the motor. it is working fine, seems like this is a gear problem.\n", "tags": "arduino battery rcservo", "id": "9709", "title": "is my servo fried?"}, {"body": "i'm looking for a testbed (simulator or web-based interface that lets me to have control on a robot) for testing different routing and navigation algorithms. is there such a system on the web?\n", "tags": "navigation routing", "id": "9711", "title": "testbed for testing navigation algorithms"}, {"body": "how to develop a robot based system to continuously monitor and check products for defeat which are moving on a conveyer belt using sensors and kick out the defect product from the queue?\n", "tags": "microcontroller", "id": "9716", "title": "quality check robot"}, {"body": "i want the dynamic model for 8 wheeled robot. i expected to find it easily like the 4 wheeled bicycle model, but i couldn't.\n\nhere is my effort only for rotation not for feedback.\n\n\nfrom that i can calculate steering angle, but it was very messy to manage them all.\n\ni need the model for controlling.\n", "tags": "wheeled-robot dynamics motion robotc", "id": "9720", "title": "8 wheeled vehicle model"}, {"body": "i working on dynamic modeling and simulation of a mechanical system (overhead crane), after i obtained the equation of motion, in the form: $$ m(q)\\ddot{q}+c(q,\\dot{q})\\dot{q}+g(q)=q $$\n\nall the matrices are know inertia, $ m(q)$, coriolis-centrifugal matrix $ c(q,\\dot{q})$, and gravity $ g(q)$ as functions of the generalized coordinates $q$, and their derivatives $\\dot{q}$.\n\ni want to solve for $q$, using matlab ode (in m-file), i got the response for some initial conditions and zero input, but, i want to find the response, for the aforementioned control signal (a bang-bang signal of amplitude 1 n and 1 s width), i'm trying to regenerate some results from the literature, and what the authors of that work said, regrading the input signal is the following: \"a bang-bang signal of amplitude 1 n and 1 s width is used as an input force, applied at the cart of the gantry crane. a bang-bang force has a positive (acceleration) and negative (deceleration) period allowing the cart to, initially, accelerate and then decelerate and eventually stop at a target location.\" i didn't grasp what do they mean by bang-bang signal, i know in matlab we could have step input, impulse, ...etc. but bang-bang signal, i'm not familiar with. according to this site and this bang bang is a controller rather.\n\ncould anyone suggest to me how to figure out this issue and implement this input signal? preferably in m-file.\n\nthe code i'm using is given bellows, two parts:\n\n\n\nand:\n\n\n\nfurthermore, the results i'm getting and what i supposed to get are shown:\n\n\n\nmajor difficulties, initial conditions are not clearly stated in the paper, the input force direction, is only in y (which it should be), or it has different direction. i appreciate any help.\n\n\n  the paper i'm trying to recreate is:\n  r. m. t. raja ismail, m. a. ahmad, m. s. ramli, and f. r. m. rashidi, \u201cnonlinear dynamic modelling and analysis of a 3-d overhead gantry crane system with system parameters variation.,\u201d international journal of simulation\u2013systems, science &amp; technology, vol. 11, no. 2, 2010.\n  http://ijssst.info/vol-11/no-2/paper2.pdf\n\n", "tags": "control robotic-arm dynamics matlab input", "id": "9724", "title": "how to apply a bang-bang signal of amplitude 1 n and 1 s width as an input force to reproduce certain results in matlab?"}, {"body": "consider this map\n\n\n\nthe contest arena shown in figure 1 consists of two sub arenas, both the sides are identical to each other and their scientists and safe zone locations are similar.\n\neach sub arena has 3 different colored rooms and a fourth shared room. each robot will be placed at identical start locations, respective to their arena. these locations will be random and anywhere on the map.\n\neach room (other than the shared room) will have two entry and exit gates. both of these gates will be open at all times. the robot can enter and exit from any gate it chooses.\n", "tags": "localization", "id": "9727", "title": "how can my robot find its position in any given map without gps, including when the initial point is not given?"}, {"body": "i have the mbot robot and i want to program it to follow the line. so far it can pass any kind of line that is >90\u00b0.\n\ni want it to be able to pass 90\u00b0-ish angles as well. like this one:\n\n\n\nthe problem is that my mbot robot has only 2 line following sensors (they are 5 mm apart and the line is 2 cm wide) so i can't use just the sensors.\n\n\n\nmost of the times it just goes to the line and when it's supposed to turn it just misses the line (goes on the white) and goes back to get back on track. once it's back on the black line it once again tries to go forward but goes on the white instead of taking a turn. this happens endlessly.\n\nsometimes it passes the angle by going back and forth and accidentally turning, but that's not even a workaround, let alone a solution.\n\nhere's a test course of the first round of the competition.\n\n\nmy robot can pass this without a problem, but it gets stuck on this (poorly edited, sorry) course:\n\n\n\nit can't pass the 20 block if the robot enters it from a 15 or 20 block (so basically it gets stuck if it's coming from an angle and hits a 90 degree turn).\n\nthe sensor value could be read as either 0, 1, 2 or 3 depending on what the robot currently sees:\n\n\n\n0 - on the line \n1 - on the right of the line \n2 - on the left of the line \n3 - not on the line \n\npseudo code of my current program:\n\n\n\nso how would i go about taking such sharp turns?\n", "tags": "arduino motor line-following", "id": "9728", "title": "how to check for a sharp angle with a line follower?"}, {"body": "could anyone expain me shortly what is a diffrence between degrees of freedom (dof) and degrees of motion (dom)? i know that dof is the number of independent movements a manipulation arm can make and robot system can have max 6 independent dof and unlimited number of dom but i do not distinguish them from each other.\n", "tags": "manipulator theory", "id": "9729", "title": "diffrence between degrees of freedom (dof) and degrees of motion (dom)"}, {"body": "i need to get my drone flying still enough that i can rest a glass of water on it.\n\ni've tried a few kk boards and apm 2.6 (3.1 software). i've balanced props, set pid settings, auto-trim / autotune and the drone still tends to inconsistently drift a little one way or another.\n\nwhat is a plausible way to completely isolate drift?\n", "tags": "quadcopter", "id": "9738", "title": "how can a quadcopter be made to hover perfectly still?"}, {"body": "i have succeeded in making my first quadcopter from scratch with a readymade frame. i designed the flight controller myself with help from ymfc-3d youtube series of videos. https://www.youtube.com/watch?v=2phdo8m6t7c\n\nbut in the process, i discovered that using the euler angles or the 'ypr' values from the mpu6050 as the feeback to the pid loop makes it super difficult to tune the quadcopter and even then it doesn't fly great. \n\nwhereas although not intuitive to me, using the gyroscope values with a complementary filter instantly made the quad respond much better and the tuning also was not too difficult.\n\nlet me clearly define the response in both cases.\n\nusing ypr values:-\n+always keeps overshooting or 'underreaching'\n+very small range of values that can let the quad fly stable\n+drastic reactions to extreme values of p (kp)values\n\nusing gyro values:-\n+reaction is much more stable\n+tuning the pid was also simple\n+ even under high values of p(kp) the quad might crash due to oscillations but not flip or react extremely\n\nbelow is a portion of the pid loop:\n\n\n", "tags": "quadcopter pid gyroscope", "id": "9739", "title": "quadcopter flight controller:why does using gyroscope data give better results?"}, {"body": "i'm an electronics student taking a module in robotics. \n\nfrom the example,\n\n\n\ni understand line 1 as the jacobian is found from the time derivative of the kinematics equation and such relates joint angles to velocity.\n\ni do not understand why the transpose has been taken on line 3 and how line 4 is produced.\n", "tags": "kinematics jacobian", "id": "9741", "title": "principle of virtual force - general help in understanding / explanation"}, {"body": "i want to measure the acceleration (forward and lateral separately) using an android smartphone device in order to be able to analyse the driving behavior. \n\nmy approach would be as follows:\n\n1. aligning coordinate systems\n\ncalibration (no motion / first motion):\nwhile the car is stationary, i would calculate the magnitude of gravity using  and rotate it straight to the z-axis (pointing downwards assuming a flat surface). that way, the pitch and roll angles should be near zero and equal to the angles of the car relativ to the world.\n\nafter this, i would start moving straight forward with the car to get a first motion indication using  and rotate this magnitude straight to the x-axis (pointing forward). this way, the yaw angle should be equal to the vehicle's heading relativ to the world.\n\nupdate orientation (while driving):\nto be able to keep the coordinate systems aligned while driving i am going to use  to maintain the roll and pitch of the system via\n\n\n\n\n\nwhere a_x,y,z is the acceleration of gravity. \n\nusually, the yaw angle would be maintained via  or . however, the reason behind not using them is because i am going to use the application also in electrical vehicles. the high amounts of volts and ampere produced by the engine would presumably make the accuracy of those sensor values suffer. hence, the best alternative that i know (although not optimal) is using the gps course to maintain the yaw angle.\n\n2. getting measurements\n\nby applying all aforementioned rotations it should be possible to maintain an alignment between the smartphone's and vehicle's coordinate systems and, hence, giving me the pure forward and lateral acceleration values on the x-axis and y-axis.\n\nquestions:\n\n\nis this approach applicable or did i miss something crucial?\nis there an easier/alternative approach to this?\n\n", "tags": "sensors accelerometer gps", "id": "9751", "title": "measuring vehicle's forward and lateral acceleration using a smartphone"}, {"body": "i want to install ros on my xubuntu 16.04, xenial xerus. i have followed the ros's site instruction: http://wiki.ros.org/jade/installation/ubuntu, and did the following: first, setup my sources.list:\n\n\n\nsecond, set up keys:\n\n\n\nthen, make sure my package is up-to-date: \n\n\n\nlast, try to install ros jade:\n\n\n\nand get this error:\n\n\n\nwhere did i go wrong, and how can i get ros (any version is ok) running on my xubuntu 16.04?\n", "tags": "ros", "id": "9752", "title": "e: unable to locate package ros-jade-desktop-full"}, {"body": "i'm about to start a project, where i'm sniffing data between remote controls and flight controllers on rc copters and doing stuff with that information.  do all (or most) flight controllers use the same protocol to communicate with the remote controls, or does it vary based on which one you buy?  i would be testing on drones (dji phantom and the like).  \n\nso, my real question is:\n\nif i want to write something to read the data, will i need to buy a different flight controller for each protocol used, or do they all use the same protocol, and i can just buy one flight controller, and the info i can get out will be the same for all types of flight controllers?\n\nalso, are the protocols only spoken by the ground remote control and the flight controller?  does the receiver care what protocol is being used, or is it just a middle man?\n", "tags": "quadcopter radio-control research", "id": "9754", "title": "are all flight controllers and remote controls using the same protocol?"}, {"body": "good day,\n\ni have been reading papers about position integration from accelerometer readings.\n\ni have consulted this paper from freescale on how that is achievable and this article regarding leaky integrators to help in preventing accumulation of errors from integration.\n\ni was testing this algorithm by moving the imu by approximately 0.1 meter. the algorithm does get it right at the instant it arrives at approx 0.1 meter however when left still at that position, the integrated position goes to zero.\n\nit turns out the velocity readings become negative at a certain period after reaching 0.1 meters.\n\n\n  does anyone have any suggestions in dealing with this error?\n\n\nplots (red is the position, blue is the velocity.)\n\nthe imu(accelerometer) was moved alternating positions 0 meters and 0.1 meters with a stop of approximately 3-5 seconds in between before moving to the next position\n\n\nactual data\n\n\ndesired data output (green - desired position integration)\n\n\n\ncode:\n\n\n", "tags": "quadcopter sensors localization integration dead-reckoning", "id": "9755", "title": "dead reckoning: obtaining position estimation from accelerometer acceleration integration"}, {"body": "i get position information and a corresponding timestamp from a motion tracking system (for a rigid body) at 120 hz. the position is in sub-millimeter precision, but i'm not too sure about the time stamp, i can get it as floating point number in seconds from the motion tracking software. to get the velocity, i use the difference between two samples divided by the $\\delta t$ of the two samples:\n\n$\\dot{\\mathbf{x}} = \\dfrac{\\mathbf{x}[k] - \\mathbf{x}[k-1]}{t[k]-t[k-1]}$.\n\nthe result looks fine, but a bit noisy at times. a realized that i get much smoother results when i choose the differentiation step $h$ larger, e.g. $h=10$:\n\n$\\dot{\\mathbf{x}} = \\dfrac{\\mathbf{x}[k] - \\mathbf{x}[k-h]}{t[k]-t[k-h]}$.\n\non the other hand, peaks in the velocity signal begin to fade if i choose $h$ too large. unfortunately, i didn't figure out why i get a smoother signal with a bigger step $h$. does someone have a hint? is there a general rule which differentiation step size is optimal with respect to smoothness vs. \"accuracy\"?\n\nthis is a sample plot of one velocity component (blue: step size 1, red: step size 10):\n\n\n", "tags": "motion pose", "id": "9756", "title": "step size in numerical differentiation"}, {"body": "i am working in reproducing a robotics paper, first simulating it in matlab in order to implement it to a real robot afterwards. the robot's model is:\n\n$$\\dot{x}=v(t)cos\\theta $$\n$$\\dot{y}=v(t)sin\\theta$$\n$$\\dot{\\theta}=u$$\n\nthe idea is to apply an algorithm to avoid obstacles and reach a determines target. this algorithm uses a cone vision to measure the obstacle's properties. the information required to apply this system is:\n\n1) the minimum distance  $ d(t) $ between the robot and the obstacle (this obstacle is modelled as a circle of know radius $ r $).\n\n2) the obstacle's speed  $ v_{obs}(t) $\n\n3)the angles $ \\alpha_{1}(t)$ and $ \\alpha_{2}(t)$  that form the robot's cone vision, and\n\n4) the heading $ h(t) $  from the robot to the target\n\nfirst a safe distance  $ d_{safe}$  between the robot and the obstacle is defined. the robot has to reach the target without being closer than        $ d_{safe}$ to the obstacle.\n\nan extended angle $ \\alpha_{0} \\ge arccos\\left(\\frac{r}{r+d_{safe}} \\right) $  is defined, where $ 0 \\le \\alpha_{0} \\le \\pi $ \n\nthen the following auxiliary angles are calculated:\n\n$ \\beta_{1}(t)=\\alpha_{1}(t)-\\alpha_{0}(t)$ \n\n$ \\beta_{2}=\\alpha_{2}(t)+\\alpha_{0}(t)$ \n\nthen the following vectors are defined:\n\n$ l_{1}=(v_{max}-v)[cos(\\beta_{1}(t)),sin(\\beta_{1}(t))]$ \n\n$ l_{2}=(v_{max}-v)[cos(\\beta_{2}(t)),sin(\\beta_{1}(2))]$ \n\nhere $ v_{max}$  is the maximum robot's speed and  $ v $ a constant that fulfills  $ \\|v_{obs}(t)\\| \\le v \\le v_{max} $ \n\nthis vectors represent the boundaries of the cone vision of the vehicle\n\ngiven the vectors $ l_{1} $  and $ l_{2}$ , the angle $ \\alpha(l_1,l_2)$  is the angle between $ l_{1}$  and $ l_{2} $  measured in counterclockwise direction, with $  \\alpha \\in (-\\pi,\\pi) $ . then the function $f$ is \n\nthe evasion maneuver starts at time $t_0$. for that the robot find the index h:\n\n$h = min|\\alpha(v_{obs}(t_0)+l_j(t_0),v_r(t_0))|$\n\nwhere $j={1,2}$ and $v_r(t)$ is the robot's velocity vector \n\nthen, from the two vectors  $v_{obs}(t_0)+l_j(t_0)$ we choose that one that forms the smallest angle with the robot's velocity vector. once h is determinded, the control law is applied:\n\n$u(t)=-u_{max}f(v_{obs}(t)+l_h(t),v_r(t))$\n\n$v(t)=\\|v_{obs}(t)+l_h(t)\\| \\quad \\quad (1)$ \n\nthis is a sliding mode type control law, that steers the robot's velocity  $v_r(t)$ towards a switching surface equal to the vector $v_{obs}(t)+l_h(t)$. ideally the robot avoids the obstacle by surrounding it a \n\nwhile the robot is not avoiding an obstacle it follows a control law:\n\n$u(t)=0$\n\n$v(t)=v_{max} \\quad \\quad  (2) $    \n\nhence the rules to switch between the two laws are:\n\nr10 switching from (2) to (1) occurs whenthe distance to the obstacle is equal to a constant c, which means when $d(t_0)=c$ and this distance is becoming smaller in time  i.e. $\\dot{d(t)}&lt;0$\n\nr11 switching from (1) to (2) occurs when $d(t_*)&lt;1.1a_*$ and the vehicle is pointing towards the obstacle, i.e. $\\theta(t_*)=h(t_*)$\n\nwhere $a_*=\\frac{r}{cos\\alpha_0}-r $\n\nideally the result should be similar to this\n\nbut i'm getting this instead\n\nwhile i understand the theory there's obviously a flaw in my implementation that i haven't been able to solve. in my opinion the robot manages to avoid the obstacle but at certain point (in the red circle), the robot turns to the wrong side, making impossible the condition $h(t) = \\theta(t) $ to be achieved.\n\ni feel that i am not measuring properly the angle alpha between the $v_{obs}(t)+l_h(t)$ and $v_{r}(t)$ , because while debugging i can see that at certain point it stops switching between negative and positive values and become only positive, leading the robot's to the wrong side. it also seems to be related with my problem here: angle to a circle tangent line\n\n\n", "tags": "mobile-robot kinematics matlab geometry", "id": "9770", "title": "mobile robot algorithm implementation error"}, {"body": "i at moment trying to compute the q configuration that moves my robot from it current state described by this transformation matrix.\nwith rotation\n\n\n\nand position as this:\n\n\n\nto this rotatation \n\n\n\nand this position\n\n\n\ndue to the drastic change in the z direction, i thought i could split the path between start and end into small chunks by creating data inbetween by interpolating, and compute the inverse kinematics for each of these small position.  problem is that the output i am getting is pretty large.. which making me suspect that some of the output might be wrong. the simulation i am using constrains the rotation to 360 degrees.. i think something goes wrong.. \n\nthe only reason i could think would do this, would be if the jacobian i am was using had singularities... which why i assumed that i was running into singualarity issue.. \n\n\n\n is just a function for my simulation, the numbers are the actual q values starting from 0 - 5. (i am using a 6 jointed robot (ur5))\n\nupdate\n\ni am using a sphere to compute my desired transformation matrix.. the idea is that i want my arm be on this sphere, point inward to the center. \n\n\n\n\n\nthis function provides me with the point on the sphere, which is use to create my rotation matrix using .\n\n\n\nthe transform basically computes the rotation matrix. the math is based on the on this post by @ben, which is an answer to a similar problem i am having..      \n\nupdate\n\nerror with the rotation matrix was due to the polar coordinate being 0 => sin(0) = 0. \n\ni made this plot displaying the determinant of the jacobian, while i compute the inverse kinematics for the large displacement. for each inverse kinematics iteration, i set the robot to the new q_i and use that as current and continue computing until i reach the end configuration. \n\n\n\nit seems that alot of them goes toward a singularity or in general a pretty low number..\n\nupdate\n\nagain i think the singularities might be the culprit here.. \n\n\n\neverytime i compute a new q i set the robot in that state, and perform inverse kinematics from that state.. q is the joint angles for the 6 joints. \n\nupdate\n\ninterpolation is done by lineary dividing the path from start to end into a specified amount of of data points.  \n\n\n\nthis plot shows  each tranformation matrices generated from the interpolation and with their the position part plotted. the red dots is the path (every 1000th position). the blue ball is the object in want to track, and green dots represents the sphere.. as i am only doing this for the first point on the sphere, it only hits one point on the sphere, which is the top point, which the plot also shows. \n\nrotation doesn't show that much change, which also makes sense based difference between the current and desired rotations. \n\nupdate\n\nmy invkin implementation for largedisplacements:\n\n\n\ni am pretty sure that my interpolation works, as the plot shows.  my inverse kinematics on the other hand not so sure..\n\nupdate\n\n@chuck mentions in his answer that it would be a good idea to check the core functionality, which might shed some light on what could be going wrong. \n\ni tried it with an inv.kin function i know would work, which didn't return any result, which make me doubt whether my transformation function i create is accurate?\n\nthe robot simulation is the one shown above..  the   function shown above, is the function which i use to compute my desired, and provide my inverse kinematics..  is something incorrectly setup?\n\n\nupdate\n\n@chuck came up with an different approach to my problem, which only has 3 dof, being the position.  i choose change track, and peform a simple inverse kinematics given a distance dx,dy,dz.. which for some reason isn't working quite good for me? even for small differences... \n\nhere is my code:\n\n\n\nwhich outputs this\n\n\n\n is the state which moves the robot to the desires state.. \neither is something wrong with my implementation, or it is a singularity..\n\nespecially because i am not moving it that much (0.00001)!!!\n\nupdates\n\ni think i have solved the mystery.. it must be the sphere function which creates points that outside the reach of the robot.!! \n", "tags": "robotic-arm inverse-kinematics", "id": "9772", "title": "is this a singularity or incorrect implementation of inverse kinematics?"}, {"body": "i am at the moment trying to implement an inverse kinematics function which function is to take a desired transformation matrix, and the current transformation matrix, and compute the q states that is needed to move my robot arm from current state to end state. \n\ni have already written the code, but since my simulation isn't showing the right path, or what i would expect it to be, this makes me unsure as to whether my implementation is correct.  could someone comment on my implementation and maybe spot an error?\n\n\n\nexample of output: \n\n\n", "tags": "robotic-arm inverse-kinematics c++", "id": "9776", "title": "how do i compute the inverse kinematics given a desired transformation matrix?"}, {"body": "i need to caculate the pose of a camera using an image of an artificial landmkark. for this porpouse i am trying to use the perspective n point approach so i can calculate it using the intrinsic camera matrix, the world coordinates of the landmark (i am using 4 points) and its projection in the image.\n\nthere are some algorithms to solve this (pnp, epnp, rpnp, etc) and i am trying to use the rpnp. i have found an implementation of this here:\nhttp://xuchi.weebly.com/rpnp.html\n\ni used this code but i am having some problems because i can't obtain the correct pose.\n\ni am using the p.corke's robotics toolbox for matlab to create a centracamera with a known pose and calculating the projection of the landmark in this camera, but the rotation and translation that the rpnp returns me is not the same as i defined before.\n\nanyone has used this rpnp algorithm to solve that kind of problems?\n", "tags": "computer-vision cameras 3d-reconstruction", "id": "9783", "title": "perspective n point - rpnp algorithm"}, {"body": "\n  question: a pid controller has three parameters kp, ki and kd which could affect the output performance. a differential driving robot is controlled by a pid controller. the heading information is sensed by a compass sensor. the moving forward speed is kept constant. the pid controller is able to control the heading information to follow a given direction. explain the outcome on the differential driving robot performance when the three parameters are increased individually. \n\n\nthis is a question that has come up in a past paper but most likely won't show up this year but it still worries me. it's the only question that has me thinking for quite some time. \ni'd love an answer in simple terms. most stuff i've read on the internet don't make much sense to me as it goes heavy into the detail and off topic for my case.  \n\nmy take on this:\n\ni know that the proportional term, kp, is entirely based on the error and that, let's say, double the error would mean doubling kp (applying proportional force). this therefore implies that increasing kp is a result of the robot heading in the wrong direction so kp is increased to ensure the robot goes on the right direction or at least tries to reduce the error as time passes so an increase in kp would affect the robot in such a way to adjust the heading of the robot so it stays on the right path.\n\nthe derivative term, kd, is based on the rate of change of the error so an increase in kd implies that the rate of change of error has increased over time so double the error would result in double the force. an increase by double the change in the robot's heading would take place if the robot's heading is doubled in error from the previous feedback result. kd causes the robot to react faster as the error increases. \n\nan increase in the integral term, ki, means that the error is increased over time. the integral accounts for the sum of error over time. even a small increase in the error would increase the integral so the robot would have to head in the right direction for an equal amount of time for the integral to balance to zero. \n\ni would appreciate a much better answer and it would be great to be confident for a similar upcoming question in the finals. \n", "tags": "pid wheeled-robot differential-drive", "id": "9786", "title": "how do the pid parameters (kp, ki, and kd) affect the heading of a differential driving robot when they are increased individually?"}, {"body": "what is the wheel base distance that should be used for the create2 to calculate angle? i have seen 230.8mm in code samples but the manual seems to indicate 235.0 mm.\n", "tags": "irobot-create roomba", "id": "9797", "title": "what is the wheel base distance of the create2?"}, {"body": "i am working on a quadcopter project based on arduino board, my system is powered by a 4s lipo battery (14.8v) but the motors behave differently as the battery voltage drops, when discharging. \n\nis there any way that i can make the motors behave the same until a minimum value of, say, 5 volts?\n\nmy current system works fine at the range from 14.8 to 10 volts, but below that i can't even hover.\n", "tags": "quadcopter", "id": "9798", "title": "how to compensate the brushless dc motor for voltage drop?"}, {"body": "lets say that i needed to send sensor readings in increments of 100 bytes from a micro controller to a laptop with sub 2 ms latencies in real time (the data needs to be processed and acted upon immediately (to control a robot)). what interfaces would one use? \n\nftdi usb-serial converters aren't an option because they introduce 5-10 ms latencies both ways. pci cards are an option though.\n", "tags": "low-latency laptop", "id": "9801", "title": "low latency control from a laptop"}, {"body": "is there something like an all-in-one satellite based localization solution that would contain both hardware and software to do gnss localization for robotics? i mean a package that would also contain an imu, would fuse it with gps and filter the result accordingly and then provide a software api to query for location/speed etc.\n\ni am interested rather in some affordable solution but is there some professional hardware too?\n\ni am trying to implement this for my mobile robot and i realize that a smartphone-grade gps (samsung j5) gives me better preliminary results than an u-blox eval board (this neo-m8t with integrated antenna and ground plane) - i wonder why, i guess android may fuse the imu and have better readings even with worse antenna?\n", "tags": "localization software gps gnss hardware", "id": "9802", "title": "all-in-one gnss localization solution (hardware+software)"}, {"body": "i'm working on a robot that is controlled by an xbox controller connected to a windows computer and commands are sent to a pcduino through a tcp connection. i have it working by sending a string of 1's and 0's to tell the pcduino which motors to turn on. i'm trying to optimize it by just sending an int and using bit masks to make the decisions on the pcduino but i can't get the pcduino to receive the int correctly. i tested the windows function sending the command with sokit and its sending the correct values but the pcduino is receiving the same number even when the commands are changing.\n\nthis is what its doing:\n\nwindows          -> pcduino\n\ncommand = 1      -> sendbuff = 73932\n\ncmdstring = 1    -> n = 1\n\n\n\ncommand = 1025   -> sendbuff = 73932\n\ncmdstring = 1025 -> n = 4\n\n\n\nmy windows functions are:\n\n\n\n\n\n\n\n\n\npcduino loop function\n\n\n\n\n\ni have a printf after the read call and that's where i am getting the 73932 number. i think i have everything you guys need but if there's anything else i need to add let me know. i'm stumped...i don't know if its just a casting problem or what.\n\nupdate 1\n\nwhat i have before everything the setup and loop functions on the pcduino run is:\n\n\n", "tags": "communication c++ c", "id": "9803", "title": "tcp communication with pcduino"}, {"body": "i've built a quadcopter using the dji wookong-m. as of a couple of weeks ago i have been able to get everything to work except for one small thing. when i throttle up the drone tends to flip to the side. i have tested all the motors over and over again and i know that they are spinning the right direction and that i have the right props on the right motors. i tested on both grass and concrete but both times it flipped. it starts to flip once the throttle is past 50%. i don't know if it is catching or if something is off balance although i don't think this is the problem since the quadcopter tips different directions almost every time. if any one could tell me what is wrong i would appreciate it a lot since my project is due in 2 1/2 weeks.\n\nthanks in advance\n", "tags": "quadcopter", "id": "9819", "title": "dji wookong-m - to unstable to take off"}, {"body": "how can i see attributes and methods of ros objects ? can i use inspect module of python ?\n\nlike in python i can use dir(), type() commands.\n", "tags": "ros python", "id": "9822", "title": "intropection ros objects using python client library"}, {"body": "a reviewer of the last paper i sent replied me that the it is very dangerous to update a pid with next kind of formula (paper is about quadrotor control):\n\n$$\nk_p (t + 1) = k_p (t)+e(t) (\u03bc_1 (pe(t)) + \u03bc_4 (pe(t)))\n$$\n\n$pe(t)$ is the % relationship between the desired angles and the real angles, and $e(t)$ is the difference between those angles. $\u03bc_1$ and $\u03bc_4$ are the membership functions of a fuzzy function. i think that the reviewer is talking about the time increment update rather than the fuzzy usage and specific formula.\n\nhow can stability of this formula be tested, please?\n\nedit: \n\nmembership functions are represented in following graph:\n\n\n$e(t)$ is not the absolute difference between angles, just the difference. it can be negative\n", "tags": "quadcopter control pid", "id": "9825", "title": "stability of pid values update function for quadrotor"}, {"body": "in tad mcgeer's work, passive dynamic walking, in 1990, he mentions the rimless wheel model, which is used to approximate the bipedal locomotion. i can't understand why the angular momentum is written as follows.\n\n$h^-=(\\cos 2\\alpha_0+r^2_{gyr})ml^2\\omega^-$\n\ni have the following questions:\n\n\nisn't the angular momentum be $i*\\omega$, $m^2l\\omega$ as the paper's notation?\nif $\\alpha_0$ is $\\frac{\\pi}{2}$ and $r_{gyr}$ approaches to 0, shouldn't the angular momentum before impact, $h^-$, be negative? then how the conservation goes?\n\n", "tags": "wheel walking-robot", "id": "9826", "title": "angular momentum of rimless wheel in passive dynamic walking"}, {"body": "i recently bought a rc car kit and after 10 minutes it stopped going. \n\nwhen i throttle, i can see the motor trying to spin but it will just grind and get hot quite fast.\n\nthe motor does move if i disconnect it from the big gear, but not as fast as it did when new and it will still get very hot. also, i can stop it with my fingers with a very slight touch.\n\ni don't know anything about motors or escs, so i'm not sure if my problem is the motor or the esc.  did i burn it out?\n", "tags": "brushless-motor esc radio-control", "id": "9828", "title": "brushless motor from rc car won't spin with even small resistance"}, {"body": "when looking at the robotic hands made by researchers that are said to be rather close to a real human hand, they can easily cost tens of thousands of dollars.  \n\nwhat makes them so much expensive? sure there are lots of joints where parts must move, but it's still hard to figure out how it can cost so much even with highly precise servomotors.  \n\nwhat is so much expensive when trying to build a humanoid hand? how can we make it less expensive? what do these expensive hands can do, that a diy cheap hand project can't?  \n\nthank you.\n", "tags": "humanoid", "id": "9832", "title": "what are the biggest challenges to build an highly performant robotic hand?"}, {"body": "i'm trying to build a test-automation robotic arm which can repeatedly present an id-tag (such as rfid or nfc card or fob) to a card reader.\n\ni suspect our reader fails either (a) after hundreds of presentations or due to fast presentations or (b) at a specific moment in the reader duty cycle.\n\nthe tag needs to move in a well-controlled manner:\n\n\nquickly present the card, \npause (mark)\nquickly remove the card,\npause (space)\nrepeat at 1.\n\n\ni'm calling the present/remove sequence the mark-space ratio for simplicity.\n\nthe tests i want to perform involve varying (a) the frequency and (b) the mark-space ratio, to (a) stress-test and  (b) boundary-test the re-presentation guard times built into the reader to debounce presentations.\n\nthe guard times are around 400ms, response around 100ms, so i need something that can move in and out of a 5-10cm range quickly and repeat within those sorts of timescales. \n\nthe distance the card needs to move depends on the reader model, as they have different field ranges. i want to get through the edge of the field quickly to avoid any inconsistencies in testing.\n\ni'm able to do any programming (professional) and simple electromechanical design and build (ex-professional, now hobbyist). i only need to build one, it doesn't have to be particularly robust, but it does need to be fairly accurate with regard to the timings to do the second test.\n\nwhat i've done so far:\n\ni've built one version already using a raspberry pi, gpio, a stepper motor with an aluminium arm screwed to a wheel.  it works, but it's a bit jerky and too slow, even with a 30cm arm to amplify the motion. it will probably do for the repeat test, but it's not time-accurate enough to do the timing tests.\n\nmy other design ideas were: \n\n\nservo (are these also slow?)\nsolenoid (fast, but too limited range? and might cause em?)\nmotor (too uncontrollable, and will require too much mechanical work for me)\nrotating drum (fast, stable, but cannot control mark-space ratio)\n\n\ni'm not a electro-mechanical design expert, so i'm wondering if i'm missing an electrical device or mechanical design which can do this more easily.\n", "tags": "robotic-arm raspberry-pi stepper-motor industrial-robot automation", "id": "9839", "title": "which mechanical device could repeatedly present an id tag to a card-reader"}, {"body": "i am presently doing a robotics project. i am using usarsim (urban search and rescue simulation) to spawn a robot. i am trying to create different behaviors, like: \n\n\ngoal following behavior; \nobstacle avoidance behavior, and; \nwall following behavior for my robot. \n\n\ni first generate the robots in usarsim. then i specify a goal location to the robot and provide it with a speed. the robot then moves to the goal location at the specified speed. usarsim provides me the (x, y, z) coordinates of the vehicle at every time stamp. based on the the coordinates received, i am trying to calculate the instantaneous speed of the robot at every time stamp. the instantaneous speed graph is fluctuating a lot. \n\nin a specific case, i am providing the robot with 0.2 m/s. the velocity profile is shown below. i am unable to understand the reason behind it.\n\n\n\nhere are some observations that i have made. \n\n\nas i increase the speed of the robot, the variations are decreasing.\nsuppose, i provide a straight trajectory to the robot, it doesn't follow the straight trajectory. does it explain why my velocity profile is fluctuating a lot ? \n\n\n\n\nplease let me know if any one can provide me a possible explanation for the variance in my velocity profile.\n", "tags": "mobile-robot", "id": "9840", "title": "the velocity profile of my robot is fluctuating"}, {"body": "i need to compute the voronoi diagram for a map with some obstacles but i can't find any pseudo-code or example in matlab.\n\nthe \"voronoi\" function in matlab works with points, but in this case the obstacles are polygons (convex and non-convex). you can see the map in the attached image.\n\n\n\nbecause the obstacles are polygons i found that the voronoi algorithm needed is the gvd (generalized voronoi diagram).\n\ncan anyone help with code or examples on internet explaining how to compute this gvd?\n", "tags": "mobile-robot motion-planning geometry", "id": "9842", "title": "generalized voronoi diagram"}, {"body": "i at moment trying to convince myself that what i need is a simple path planning algorithm, instead of linearly interpolating between a current and a desired state. \n\ni am at moment working with an robot arm (ur) with an camera mounted on to its tcp.  the application i am trying to create is a simple ball tracking application which tracks the movement of the ball, while always keeping the object within sight. \n\nwhich meant that i needed some form of path planning algorithm which plans the path between my current state and the desired state. the path should should be such that the ball is always kept in focus while the arm moves to the desired state.\n\nbut then i began question myself whether it was a bit overkill, and whether a simple straight line interpolation wouldn't suffice?.. i am actual not sure what form of benefit i would have by choosing a pathplanner, than a simple interpolation..\n\ninterpolation would also generate the path i desire, so why choose a pathplanner at all?\n\nwould someone elaborate?\nit should be noted that obstacle avoidance is also a part of the task, which could cause trouble for a straight line interpolating. \n", "tags": "robotic-arm", "id": "9845", "title": "path planning vs. linear interpolation?"}, {"body": "i am at moment trying to implement a visual servoing application. \nthe robot i am using is a ur5, and tcp has a stereo camera mounted on to it. the idea is to move the end effector according to the object being tracked. \n\nthe path-planning algorithm for this system should comply with some rules. \n\n\nthe path which it creates should be collision free, and always keep the object being tracked at sight at all time. \n\n\nhaving a path that keeps the object in sight has been bit of problem.  sometime will the end effector rotate around itself, messing up  measurements taken and thus the tracking itself. \n\n\nit should be able to maneuver away from static obstacles. \n\n\na possible solution?\n\ni thought of a possible solution. since my current state and desired state is defined by two different sphere, a possible solution would be to create a straight line between each center of each sphere, and between the current position and desired position, such that a straight path in between could be computed easily.  which always keeps itself oriented toward the object. problems is that i am not sure how i should handle collision here.\n\n\n\nupdate\n  or use it as a heuristic for a heuristic based path planning? \n", "tags": "robotic-arm motion-planning algorithm", "id": "9848", "title": "path planning for visual servoing"}, {"body": "good day i would just like to ask if a fixed wing aircraft such as a glider(without thrust capability therefore needs external forces such as air flow to move constraining its movement) can be considered a non-holonomic system considering the fact that it cannot move freely compared to a quadcopter that is holonomic.\n\ni found this information from: what&#39;s the difference between a holonomic and a nonholonomic system?\n\nmathematically:\n\nholonomic system are systems for which all constraints are integrable into positional constraints.\n\nnonholonomic systems are systems which have constraints that are nonintegrable into positional constraints.\n\nintuitively:\n\nholonomic system where a robot can move in any direction in the configuration space.\n\nnonholonomic systems are systems where the velocities (magnitude and or direction) and other derivatives of the position are constraint.\n", "tags": "quadcopter control uav glider", "id": "9850", "title": "holonomic and non-holonomic uav's: gliders vs quadcopters"}, {"body": "i would like to build a visual slam robot (just for self-learning purpose) but i get frustrated how i know which processor and camera should be used for visual slam. \n\nfirst, for the processor, i have seen three articles, which shows different systems are used for implementing their slam algorithm:\n\n\nimplementing slam algorithm (however it uses ultrasonic sensor rather than visual sensor) in raspberry pi (processing power is only 700 mhz) in implementing odometry and slam algorithms on a raspberry pi to drive a rover  \ni have also seen that boston dynamics use pentium cpu, pc104 stack and qnx os for their big dog project, bigdog overview\nnovember 22, 2008\nthen, i also found a project uses a modern xilinx zynq-7020 system-on-chip (a device that combines fpga resources with a dual arm cortex-a9 on\na single chip), for a synchronized visual-inertial sensor system, in a synchronized visual-inertial sensor system with fpga pre-processing for accurate real-time slam\n\n\nbut after reading those, i have no clue how they end up with those decisions to use those kinds of processors, stacks or even oses for their project. is there a mathematical way, or a general practice, to evaluate the minimum requirement of the system (as cheap and as power efficient as possible) for an algorithm to run? \n\nif not, how could i know what processor or system i have to prepare for a visual slam robot? if there is no simple answer, it is also cool if you can recommend something i could read to have a good start.\n\nsecondly, i also cannot find clear information which camera i should use for a visual slam robot. i also have no idea how they evaluate the minimum requirement of the camera. i found a lot of papers saying they use rgb-d camera but when i google to find one, there are very few commercially available. the one i found is xtion pro live from asus global (for $170 which is quite affordable for me), but they are out of stock. are there any practice i can choose a suitable camera system for visual slam too? \n\nsorry if my question is too long. i feel that choosing the system and camera looks like a thing that requires a lot of experience and background knowledge. so rather than direct suggestions, it is cool if you have some ideas/recommended resources for me to learn the general ways people make such decisions in general or in similar projects, if any.\n", "tags": "slam", "id": "9851", "title": "how do i evaluate the minimum requirements of the processor and camera for a visual slam robot?"}, {"body": "good day i am currently trying to implement a vector field histogram algorithm. may i ask if anyone knows what the certainty value pertains to? from my understanding it is like a point system for a cell to increment its certainty every sensor read. is this right? thank you.\n", "tags": "quadcopter motion-planning vfh", "id": "9852", "title": "vector field histogram: (vfh) certainty value"}, {"body": "i have a 6dof robotic arm which i am using to throw a ball. each joint can achieve a maximum velocity of 30 rpm (180 deg/s). i have been trying to generate joint angles manually and feeding them to see how far i can throw the ball until now. this has shown me that it's like less than 2 meters. \n\nbut i feel that i may not be combining the motions of the various motors in order get better throwing distance. i wanted to know if there is a simple way of theoretically determining the maximum distance i can throw. i read a few papers that appear very complicated, i do not need a very accurate value, just an estimate so that i decide whether i should move to a different arm. \n", "tags": "robotic-arm", "id": "9853", "title": "maximum distance for robotic arm throwing"}, {"body": "is it possible to perform cosine interpolation between two transformation matrices?\n\nit make sense for the translation part, but how about the rotational part?\n", "tags": "robotic-arm stereo-vision", "id": "9856", "title": "cosine interpolation between two transformation matrices?"}, {"body": "i need to calculate the configuration space obstacle to planning a path with a mobile robot. the idea is to divide the obstacles and the robot in triangles and test whether is there any triangle from the robot colliding with any triangle from the obstacles.\n\nthe approach to solve this is to test this between two triangles each time so i need to look if any of the 6 edges (3 for each triangle) divide the triangles so 3 vertex from one of them lie in one side and the other 3 vertex lie on the other side of the line.\n\ni wrote some code to calculate the line equation (y = m*x + b) and i think it is correct, but i am having problems when the line is vertical (this means that m = -inf) because matlab gives me a nan when i calculate the equation for it. i am not sure how to handle this.\n\nhere you can see a snippet from the code where i test the 3 edges from the \nrobot triangle:\n\n\n\nanyone could help with this issue?\n", "tags": "mobile-robot motion-planning matlab", "id": "9863", "title": "configuration space obstacle - calculating collision"}, {"body": "i am new to quadcopters. just recently started with \n\n\n4, 1000kv motors\n2 esc with 490hz / 2 esc with 50 hz (got it from ebay, just found from software)\ncc3d openpilot (now librepilot)\nflysky fs-t6 (6 channel)\n\n\nproblem:\n\n\nwhen i configure and calibrate with librepilot software, motors run fine with radio, slow on min and fast on max. but as soon as i remove the usb cable and runs it directly from radio, suddenly on min they go to max speed.\ni have calibrated manually also with esc cable in receiver with radio, it works perfectly during process and when cable move to cc3d and runs again, shows same behaviour. \ni have calibrated the motors directly, using only the librepilot configuration software and it works fine while connected through usb cable. \n\n", "tags": "quadcopter calibration", "id": "9870", "title": "quadcopter starts at max speed"}, {"body": "i am not sure how i should explain this, i am looking for a way to plot the trajectory an robot arm.  an object is seen from the toolframe frame, but how do i plot the position of each joint, such that the frame they uses are the same.\n\none way would be to use the world frame as reference, but how would i plot the position of the object related to the world frame?\n", "tags": "kinematics matlab visualization", "id": "9871", "title": "convert toolframe coordinate to world frame coordinates?"}, {"body": "i have 4 esc with male xt60 connector and the battery has 3 male jst connectors.\n\ni want to connect all 4 esc to this one battery. i will have the following connectors: \n\n\n  xt60 male to xt60 female jst female\n\n\nbut this can connect only one esc to the battery. how can i connect all 4 esc to the battery.\n\ni know last option is soldering, which i want to avoid as i am cse guy.\n", "tags": "quadcopter battery esc connector", "id": "9876", "title": "connect 3s li-po battery to 4 esc"}, {"body": "i am studying the popular madgwick algorithm for imu, and stumbled with two issues:\n\n\nin magnetic distortion compensation, he used the following:\n\n\n\n\nwhy is this assigned to  but not ? by formula (46) in his article, it is . is my understanding correct?\n\n\nthe raw gyro information is used as:\n\n\n\n\nshall the bias drift compensation done in this step? assuming we track the bias with his formula (48), the bias value shall be applied here as:\n\n\n\nis my understanding correct?\n", "tags": "sensors imu calibration", "id": "9882", "title": "madgwick sensor function algorithm: two issues"}, {"body": "i am having trouble using the emss interface to connect to the irobot create 2?\ncan i use that framework for create 2 or is that strictly made for create 1?\nsorry in advance i'm very new to the robotics field. \n\nhttp://emssframework.sourceforge.net/emssframework_main_page\n", "tags": "irobot-create software roomba", "id": "9890", "title": "irobot create 2: has anyone used the emss irobot create framework to control the create 2?"}, {"body": "i am trying to programme my irobot create using a serial to usb cable. i have connected the serial end in the cargo bay connector port of the irobot. i am using a software called realterm (http://realterm.sourceforge.net/index.html#display_formats) to send commands to the irobot. i have set the correct baud rate and other parameters. i downloaded the required driver from http://www.ftdichip.com/drivers/vcp.htm .\ninspite of all this, my irobot is just not responding to the commands.\n", "tags": "irobot-create", "id": "9891", "title": "how to programme an irobot create using a serial to usb cable?"}, {"body": "i tried to use microsoft robotics dev studio (sample 4) to write a code that was able for robot to go with a square path by just one clicked. however, there is one problem.\nwhen i try to put drivedistancerequest and rotatedegreesrequest in a loop. it would only execute the last request. the problem is that arbiter.choice within the drivedistance is activated immediately as soon as the drive operation starts. did anyone have this kind of problem before? if so, how do i solve it? if no, how am i able to fix this problem? thanks your so much.\n\n//-----------------------------------------------------------------------\n\n//  this file is part of microsoft robotics developer studio code samples.\n//\n\n//  copyright (c) microsoft corporation.  all rights reserved.\n//\n\n//  $file: roboticstutorial4.cs $ $revision: 22 $\n\n//-----------------------------------------------------------------------\n\nusing microsoft.ccr.core;\n\nusing microsoft.ccr.adapters.winforms;\n\nusing microsoft.dss.core;\n\nusing microsoft.dss.core.attributes;\n\nusing microsoft.dss.servicemodel.dssp;\n\nusing microsoft.dss.servicemodel.dsspservicebase;\n\nusing system;\n\nusing system.collections.generic;\n\nusing system.security.permissions;\n\nusing xml = system.xml;\n\nusing drive = microsoft.robotics.services.drive.proxy;\n\nusing w3c.soap;\n\nusing microsoft.robotics.services.roboticstutorial4.properties;\n\nusing microsoft.robotics.services.drive.proxy;\n\nusing system.componentmodel;\n\nnamespace microsoft.robotics.services.roboticstutorial4\n{\n\n\n\n}\n", "tags": "mobile-robot control irobot-create mrds", "id": "9892", "title": "issues with running multiple instructions in sequence"}, {"body": "i am learning about robot kinematics and the jacobian matrix, and i'm trying to understand how to compute the jacobian matrix given a kinematic chain, such as a robot arm. i understand the theory behind the jacobian matrix, but i'm not sure actually how it would be calculated in practice.\n\nso, let's say that i have a 7 dof robot arm, with 7 joints and 6 links between the joints. i know how to compute the transformation matrix between each joint, and by applying forward kinematics, i know the pose of the end effector for any configuration of joint angles. to calculate this, i have written some code which stores each transformation matrix, and then multiplies them in series to create the transformation matrix between the first joint and the end effector.\n\nhowever, how do i now go about computing the jacobian matrix? my solution so far, is to write down each transformation matrix by hand, then multiply them all by hand, to yield the overall transformation matrix, with respect to the joint angles. i could then differentiate this to create the jacobian matrix. the problem with this though, is that the maths becomes very, very complicated as i move along the kinematic chain. by the end, there are so many terms as a result of the multiple matrix multiplications, that it just becomes so tedious doing this by hand.\n\nis there a better way to do this? in the case of calculating the forward kinematics, i didn't have to do it by hand, i just wrote some code to multiply the individual matrices. but when i want the jacobian matrix, it seems like i need to compute the derivative of the overall transformation matrix after it has been computed, and so i need to do this by hand. what's the standard solution to this? is it something to do with the chain rule for differentiation...? i'm not sure exactly how this applies here though...\n\nthank you!\n", "tags": "robotic-arm kinematics inverse-kinematics jacobian manipulator", "id": "9893", "title": "computing the jacobian matrix -- chain rule?"}, {"body": "problem\n\ncurrently working on reverse engineering this application zepp.com/baseball. this is a wearable device that can track a users \n\n\nspeed\npositional tracking\nwhen the object makes contact with another one \n3-d rendering\n\n\ncurrently using an accelerometer and gyroscope to get the yaw, pitch, and roll(orientation) of the device, but do not know how to use that information to calculate speed, or if the device has collided with another object?\n", "tags": "imu accelerometer precise-positioning", "id": "9894", "title": "using accelerometer, gyroscope and any sensor to track speed, position,"}, {"body": "assuming a drone is in two dimension, it has to predict its future position by calculating its future displacement:\n\n\n\nfor a real quad-rotor, why should we not only estimate the displacement of a robot in three dimensions but also the change of orientation of the robot, its linear velocity and its angular velocity?\n", "tags": "quadcopter motion-planning uav", "id": "9895", "title": "estimating the displacement of a drone in three dimensions"}, {"body": "good day\n\ni am currently implementing the vfh algorithm. \n\n\n  is it possible to configure the algorithm such that a reactionary motion is generated at the presence of an obstacle? \n\n\ni have been able to generate the obstacle map, primary polar histogram and the binary polar histogram.\n\n\n  how does one prioritize a sector to pass through?\n\n\ni have seen an implementation in labview where in it is possible to implement a simple vector field histogram path planning without any goal points here\n", "tags": "mobile-robot motion-planning mapping c++ vfh", "id": "9899", "title": "vfh+ (vector field histogram+) : is it possible to choose a candidate sector without a set goal point?"}, {"body": "i am trying to implement my own inverse kinematics solver for a robot arm. my solution is a standard iterative one, where at each step, i compute the jacobian and the pseudo-inverse jacobian, then compute the euclidean distance between the end effector and the target, and from these i then compute the next joint angles by following the gradient with respect to the end effector distance.\n\nthis achieves a reasonable, smooth path towards the solution. however, during my reading, i have learned that typically, there are in fact multiple solutions, particularly when there are many degrees of freedom. but the gradient descent solution i have implemented only reaches one solution.\n\nso my questions are as follows:\n\n\nhow can i compute all the solutions? can i write down the full forward kinematics equation, set it equal to the desired end effector position, and then solve the linear equations? or is there a better way?\nis there anything of interest about the particular solution that is achieved by using my gradient descent method? for example, is it guaranteed to be the solution that can be reached the fastest by the robot?\nare there cases when the gradient descent method will fail? for example, is it possible that it could fall into a local minimum? or is the function convex, and hence has a single global minimum?\n\n", "tags": "robotic-arm kinematics inverse-kinematics jacobian", "id": "9904", "title": "solving inverse kinematics with gradient descent"}, {"body": "i'm reading a paper:\n\n\n  choi c, trevor a j b, christensen h i. rgb-d edge detection and\n  edge-based registration[c]//intelligent robots and systems (iros),\n  2013 ieee/rsj international conference on. ieee, 2013: 1568-1575.\n\n\nwhich refers: \n\n\n  visual features such as corners, keypoints, edges, and color are\n  widely used in computer vision and robotic perception for applications\n  such as object recognition and pose estimation, visual odometry, and slam\n\n\ni previously assume pose estimation to be roughly equal to visual odometry, yet the text above seems to deny.\n\nso what's their difference? i didn't find much info from google. imho, it seems pose estimation is estimating the pose of moving object with the camera static, while visual odometry is estimating the pose of camera in a static(mostly) scene, is that precise enough?\n", "tags": "slam odometry pose", "id": "9916", "title": "what's the difference between the term \"pose estimation\" and \"visual odometry\"?"}, {"body": "i am trying to run this motor. \n\nusing the batteries stated in the title. the motor requires 12 v and i am supplying 11.98v to the motor, through a motor driver. after a while, the motor keeps slowing down and the battery voltage drops down to 5-6 v, but after i remove the battery from the motor driver it again shows 11.9v.\n\nis this battery capable enough to run my motors, or do i need a new one?\n", "tags": "motor battery", "id": "9918", "title": "tring to run 12 v dc geared motor using samsung li ion icr16850 batteries"}, {"body": "i am using mavlink protocol (in c++) to communicate with the ardupilotmega, i am able to read messages such as attitude for example.\ni am currently getting only 2hz (message rate) and i would like to increase it. i found out that i should use message_interval in order to change it, and that i probably need to use the command  to set it.\n\nso my question is, how do i send that command using mavlink in c++?\n\ni tried doing this with the code below but it did not work, i guess that i should use the command that i mentioned above but i don't know how.\n\n\n\nupdate: i also tried this code below, maybe i am not giving it the right system id or component id.\n\n\n\nmaybe i am missing something about the difference between ,  and , . i tried few values for each but nothing worked. is there any ack that will be able to tell me if it even got the command? \n", "tags": "quadcopter c++ ardupilot mavlink", "id": "9923", "title": "change message interval ardupilot"}, {"body": "good day\n\nnote: i have found out that my code works. i have placed a minor explanation to be further expounded.\n\ni have been having trouble obtaining the right directional output from my implementation. i noticed that every time i put an obstacle at the right, it gives left, it gives the right steering direction, the problem is with the presence of a left obstacle where it still tends to steer itself towards that obstacle. i have checked the occupancy map generated using matlab and was found to be correct. i couldn't pinpoint what is exactly wrong with my code for i have been debugging this for almost a week now and was hoping if someone can see the error i cannot.\n\nhere is my code implementation:\n\n\n", "tags": "mobile-robot motion-planning vfh path-planning", "id": "9925", "title": "vfh (vector field histogram+): obtaining the primary polar histogram"}, {"body": "this question is strongly related to my other question over here.\n\ni am estimating 6-dof poses $x_{i}$ of a trajectory using a graph-based slam approach. the estimation is based on 6-dof transformation measurements $z_{ij}$ with uncertainty $\\sigma_{ij}$ which connect the poses. \n\nto avoid singularities i represent both poses and transforms with a 7x1 vector consisting of a 3d-vector and a unit-quaternion:\n\n$$x_{i} = \\left( \\begin{matrix} t \\\\ q \\end{matrix} \\right)$$\n\nthe optimization yields 6x1 manifold increment vectors \n\n$$ \\delta \\tilde{x}_i = \\left( \\begin{matrix} t \\\\ log(q) \\end{matrix} \\right)$$\n\nwhich are applied to the pose estimates after each optimization iteration:\n\n$$ x_i \\leftarrow x_i \\boxplus \\delta \\tilde{x}_i$$\n\nthe uncertainty gets involved during the hessian update in the optimization step:\n\n$$ \\tilde{h}_{[ii]} += \\tilde{a}_{ij}^t \\sigma_{ij}^{-1} \\tilde{a}_{ij} $$\n\nwhere \n\n$$ \\tilde{a}_{ij} \\leftarrow a_{ij} m_{i} = \\frac{\\partial e_{ij}(x)}{\\partial x_i} \\frac{\\partial x_i \\boxplus \\delta \\tilde{x}_i}{\\partial \\delta x_i} |_{\\delta \\tilde{x}_i = 0}$$\n\nand\n\n$$ e_{ij} = log \\left( (x_{j} \\ominus x_{i}) \\ominus z_{ij} \\right) $$\n\nis the error function between a measurement $z_{ij}$ and its estimate $\\hat{z}_{ij} = x_j \\ominus x_i$. since $\\tilde{a}_{ij}$ is a 6x6 matrix and we're optimizing for 6-dof $\\sigma_{ij}$ is also a 6x6 matrix.\n\n\n\nbased on imu measurements of acceleration $a$ and rotational velocity $\\omega$ one can build up a 6x6 sensor noise matrix\n\n$$ \\sigma_{sensor} = \\left( \\begin{matrix} \\sigma_{a}^2 &amp; 0 \\\\ 0 &amp; \\sigma_{\\omega}^2 \\end{matrix} \\right) $$\n\nfurther we have a process model which integrates acceleration twice and rotational velocity once to obtain a pose measurement.\n\nto properly model the uncertainty both sensor noise and integration noise have to be considered (anything else?). thus, i want to calculate the uncertainty as\n\n$$ \\sigma_{ij}^{t} = j_{iterate} \\sigma_{ij}^{t-1} j_{iterate}^t + j_{process} \\sigma_{sensor} j_{process}^t$$\n\nwhere $j_{iterate} = \\frac{\\partial x_{i}^{t}}{\\partial x_{i}^{t-1}}$ and $j_{process} = \\frac{\\partial x_{i}^{t}}{\\partial \\xi_{i}^{t}}$ and current measurement $\\xi{i}^{t} = [a,\\omega]$.\n\naccording to this formula $\\sigma_{ij}$ is a 7x7 matrix, but i need a 6x6 matrix instead. i think i have to include a manifold projection somewhere, but how?\n\n\n\nfor further details take a look at the following publication, especially at their algorithm 2:\n\ng. grisetti, r. k\u00fcmmerle, c. stachniss, and w. burgard, \u201ca tutorial on graph-based slam,\u201d ieee intelligent transportation systems maga- zine, vol. 2, no. 4, pp. 31\u201343, 2010.\n\n\n\nfor a similar calculation of the uncertainty take a look at the end of section iii a. in:\n\ncorominas murtra, andreu, and josep m. mirats tur. \"imu and cable encoder data fusion for in-pipe mobile robot localization.\" technologies for practical robot applications (tepra), 2013 ieee international conference on. ieee, 2013.\n\n\n\n.. or section iii a. and iv a. in:\n\nila, viorela, josep m. porta, and juan andrade-cetto. \"information-based compact pose slam.\" robotics, ieee transactions on 26.1 (2010): 78-93.\n", "tags": "slam ekf jacobian", "id": "9926", "title": "calculate the uncertainty of a 6-dof pose for graph-based slam"}, {"body": "i am implementing a graph-based slam system which works fine, i.e. it converges, if i assume a constant covariance matrix $\\sigma_{ij}$ for all my constraints. however, if i model my $\\sigma_{ij}$ more realistically, i.e. with increasing entries for progress in time (see my other question) it fails after one successful iteration, i.e. after solving the system once but before applying the pose increments $\\delta \\tilde{x}_{ij}$ to the poses $x_{ij}$ for the first time. i use the cholesky based sparse solver implementation of eigen which fails with a numeric error.\n\nof course, the failure might occur due to a bug in my implementation. but maybe there's also a problem about representing the math with computers, e.g. some overflow in double representation or something similar. could anybody comment on this assumption, please?\n\n\n\nmy implementation makes use of manifolds and is based on algorithm 2 in:\n\ng. grisetti, r. k\u00fcmmerle, c. stachniss, and w. burgard, \u201ca tutorial on graph-based slam,\u201d ieee intelligent transportation systems maga- zine, vol. 2, no. 4, pp. 31\u201343, 2010.\n", "tags": "slam", "id": "9927", "title": "graph-based slam optimization fails with numeric error"}, {"body": "i'm working on a robotics project where i have 3 services running. i have my sensor daq, my logic isr (motor controller at 20khz) and my ethercat slave controller.\n\ndaq and ethercat run in the idle and the logic runs during an interrupt. the logic does some calculations and controls the motor. the ethercat service (kinda like canbus) runs together with my daq in the idle loop. i can not run the daq in the interrupt because that leaves me with less than 100ns for the ethercat service to run.\n\ni'm not sure whether this is the right way to do this especially considering all the scary things i've read regarding data corruption when using interrupts.\n\ndoes anyone have some nice ideas on how to handle these services?\n\ni'm running all my code on a zynq 7020 (on the arm cortex) and it's written in c++.\n\nhere is an example of my code:\n\n\n", "tags": "c++ interrupts", "id": "9932", "title": "how to split tasks between interrupts and the main loop on a bare metal controller?"}, {"body": "please guide me \nhow to find friction or viscous force b (nmsec) in dc motor for a particlar speed.\nthe motor is connected with a gear and the ration is 26:1\ni want to find for 200 rpm and the motor no load speed is 4900rpm\nplease guide me\n", "tags": "quadcopter wheeled-robot brushless-motor stepper-motor", "id": "9935", "title": "how to find friction or viscous force b (nmsec) in dc motor"}, {"body": "i need to solve a path planning problem using a cell decomposition method, more precisely the quadtree decomposition. i need to do it in matlab so i would like to know whether there is any exmaple or code i could to make some tests.\n\ni read about the  matlab function but i couldn't do anything useful. in addition i would need to decompose the cells until a path is found and this functionality is not in the previous function.\n\nanyone knows how could i program this quadtree decomposition?\n", "tags": "mobile-robot matlab path-planning", "id": "9936", "title": "path planning - quadtree decomposition (cell decomposition)"}, {"body": "since the encoder is square wave not quadrature, do you have to stop first before changing directions for proper measurements?\n\nin other words, if you are commanding along in one direction at some low speed like 50mm/s or less and want to change direction to -50mm/s, would you first need command it to zero and wait for the encoder to read 0 speed, and then command the reverse direction, in order to get as accurate as possible encoder readings?\n", "tags": "irobot-create roomba", "id": "9937", "title": "do you have to stop first when switching direction for proper encoding readings?"}, {"body": "i am trying to run 2 12v geared dc motors which has no-load current = 800 ma(max), load current = upto 9.5 a(max). runtime to be atleast 3-4 hours.\nthe motor takes about 10-12 v for operation.\ni need a proper battery pack for these, but how can i determine the specs i should go for?\n", "tags": "motor battery", "id": "9941", "title": "suggestion for correct battery pack"}, {"body": "good day,\n\nintroduction\n\ni am currently working on an autonomous quadcopter project. i have currently implemented a cascaded pid controller consisting of two loops. the inner rate loop takes the angular velocity from the gyroscope as measurements. the outer stabilize/angle loop takes in angle measurements from the complementary filter (gyroscope + accelerometer angles). \n\nquestion:\n\ni would like to ask if it is effective to cascade a lateral velocity (x and y - axis) pid controller to the the angle controller (roll and pitch) to control drift along the x-y plane. for the outermost pid controller, the setpoint is 0 m/s with the measured velocities obtained from integrating linear accelerations from the accelerometer. this then controls the pid controller responsible for the pitch (if y velocity pid) and roll (if x velocity pid).\n", "tags": "quadcopter control pid raspberry-pi stability", "id": "9946", "title": "quadcopter: x-y velocity pid controller"}, {"body": "for my final year in computer science university i will be doing a dissertation that includes controlling a drone through computer and communication with an onboard camera for computer vision.\n\nthe first step is obtaining a drone that suits my needs, and i have no clue how to go about it. basically what is needed is a drone that will be able to communicate with a computer both for its movement and to \"stream\" the video to the computer for analysis. \n\nso, would i go for a store bought drone, a rasperry pi or some other microcontroller based one etc. what do i need to take into consideration etc?\n\np.s. the project is going to be based indoors, so i don't need crazy range, or very powerf\n", "tags": "quadcopter control microcontroller computer-vision", "id": "9947", "title": "starting out, dissertation project using computer controlled drone"}, {"body": "the other day, somebody was telling me about a robot in their lab, and they mentioned that it has \"series elastic\" actuators. but after doing a bit of googling, i'm still not sure as to what this means, and have been unable to find a simple explanation. it seems that it is something to do with the link between the actuator and the load having a spring-like quality to it, but this is rather vague...\n\nin any case, the what i am really interested in is the advantages and disadvantages of series elastic actuators. specifically, i have read that one of the advantages is that it allows for \"more accurate and stable force control\". however, this appears counter-intuitive to me. i would have thought that if the link between the actuator and the load was more \"springy\", then this would lower the ability to have accurate control over the force send to the load, because more of this force would be stored and dissipated in the spring, with less directly transferred to the load.\n\nso: why do series elastic actuators have \"more accurate and stable force control\"?\n", "tags": "robotic-arm actuator dynamics torque", "id": "9951", "title": "why do series elastic actuators have more accurate and stable force control?"}, {"body": "suppose i have one robot with two 3d position sensors based on different physical principles and i want to run them through a kalman filter. i construct an observation matrix two represent my two sensors by vertically concatenating two identity matrices.\n\n$h = \\begin{bmatrix} 1&amp;0&amp;0\\\\0&amp;1&amp;0\\\\0&amp;0&amp;1\\\\1&amp;0&amp;0\\\\0&amp;1&amp;0\\\\0&amp;0&amp;1 \\end{bmatrix}$ $\\hspace{20pt}$\n $\\overrightarrow x = \\begin{bmatrix} x\\\\y\\\\z \\end{bmatrix}$\n\nso that \n\n$h \\overrightarrow x = \\begin{bmatrix} x\\\\y\\\\z\\\\x\\\\y\\\\z \\end{bmatrix}$\n\nwhich represents both sensors reading the exact position of the robot. makes sense so far. the problem comes when i compute the innovation covariance\n\n$s_k = r + hp_{k|k-1}h^t$\n\nsince \n\n$h h^t = \\begin{bmatrix}\n 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\\n 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\\n 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\\n 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\\n 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\\n 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\\n \\end{bmatrix}$ \n\nthen, no matter what $p$ is, i'm going to wind up with $x$ innovations from the first sensor being correlated to $z$ innovations from the second, which seems intuitively wrong, if i'm interpreting this right. \n\nproceeding from here, my gain matrix ($k = p_{k|k-1} h^t s_k^{-1}$) winds up doing some pretty odd stuff (swapping rows and the like) so that, when updating a static system ($a = i_3, b = [0]$) with a constant measurement $\\overrightarrow z = [1,0,0]$ i wind up with a predicted state $\\hat x = [0,0,1]$.\n\nif i separate the sensors and update the filter with each measurement separately, then $h h^t = i_3$, and i get sensible results.\n\ni think i am confused about some technical points in one or more of these steps. where am i going wrong? does it not make sense to vertically concatenate the observation matrices?\n\ni suppose that i could just set the off-diagonal 3x3 blocks of $s_k$ to 0, since i know that the sensors are independent, but is there anything in the theory that suggests or incorporates this step?\n", "tags": "kalman-filter", "id": "9953", "title": "kalman filter with redundant sensors"}, {"body": "i am trying to the force control experiment with kuka lwr iv. i have a mini45 ati force sensor. our data acquisition board is pcie-6320 which is not supported using linux and it seems that i should use windows. one of the lab guys has previously tried to use \"fri\" and \"aticombineddaqft\" simultaneously in windows but he was not hopeful. two libraries did not work with each other.\ndid anybody do the same experiment in kuka? or at least encounter the same problem with another sensor and fri? it is noteworthy that our force sensor does not have \"net box\" to use \"udp\". i am a bit in a hurry and i really appreciate your suggestions.\n\nthanks.\n", "tags": "force-sensor kuka", "id": "9954", "title": "using fri and aticombineddaqft simultaneously in visual studio"}, {"body": "i'm looking to find out, how do human-like legs compare to chicken legs and four-leg systems. in terms of cost, performance, speed, strength and accuracy.\n\ni'm interested in things like speed, agility, turning radius, complexity and cost.\n\nfor a design large enough for a person to ride, rider fatigue is also important -- how do they compare in terms of achievable ride smoothness, vibration, and so on?\n\nare there quantitative benefits of 3 dof hip joints, compared to 2 dof?\n\ni realize other factors will come into play as well, such as actuators, joint designs and control systems.\nhowever, my interest at the moment is how basic leg designs compare to one another.\n\nedit: i'm looking for someone who has used these mechanisms first hand.\n", "tags": "kinematics walking-robot", "id": "9955", "title": "i'm looking for hands-on experience with different types of leg and hip designs for walking robots"}, {"body": "i just started learning about slam and i have been trying to simulate a robot moving around a set of landmarks for the past 3 days. the landmarks have known correspondences. \n\nmy problem is, if i add motion noise to the covariance matrix in the prediction step, the robot starts to behave very weirdly. if i don't add motion noise in the prediction step, the robot will move around perfectly. i have been trying to figure out why this is happening for 3 days now but cannot find anything wrong with my code.\n\ni have attached a link to github which has all the files pertaining to my project. in the folder named 'octave' the file 'prediction_step' and 'correction_step' contains code for the prediction and correction steps respectively. the ekf_slam file is the main loop which calls the above two functions.\n\nmy github repository also contains 3 videos which correspond to robot with no motion noise, robot with motion noise and another video which shows how the robot should ideally go about.\n\nplease help me in figuring out what is wrong with my code in 'prediction_step' and 'correction_step'.\n\nlink to my github repository: please click here \n", "tags": "slam ekf", "id": "9956", "title": "need help in implementing ekf based slam"}, {"body": "how do i linearize the following system using taylor series expansion:\n$\\dot x = v cos\\theta \\\\ \\dot y = v sin\\theta \\\\ \\dot \\theta = u$\n\nhere, $\\theta$ is the heading direction of my robot, measured counter clockwise with respect to $x$ axis.\n$v$ is the linear velocity of the robot,\n$u$ is the angular velocity of the robot.\n", "tags": "mobile-robot localization", "id": "9962", "title": "linearize a non linear system"}, {"body": "i have a problem with two robots and two obstacles in a space. each robot can communicate its measurements to the other and can measure angles and distances.\nthe two obstacles in the environment are identical to each other.\n\neach robot can see both obstacles but not each other. therefore have angle theta 1 and 2 combined with distance 1 and 2. can the distance between the two robots be calculated?\n\n\n\nso far i have placed circles with radius of the measured distance over each landmark (triangles in my workings), this provides 4 possible positions for each robot. red and black circles correspond to robot 1 and blue and green to robot 2. using the relative size of the angle measurements i can discount two of these positions per robot.\nthis still leaves me with two possible positions for each robot shown with the filled or hashed circles.\n\n\nis it possible to calculate which side the robot is to the landmarks and the distance between each other?\n\nrobot 1 only has the two measurements of angle and distance and can therefore assign an id to each obstacle, but when information is transmitted to robot 2, robot 2 does not know which obstacle will have been designated an id of 1 or 2.\n", "tags": "localization kinematics", "id": "9963", "title": "distance calculation with two robots and two obstacles"}, {"body": "i am working through the book learning robotics using python, which is for python programmers who want to learn some robotics. chapter 2 shows you how to use librecad to design the plates and poles that form the chassis of the turtlebot-like robot. for instance, the base plate looks like this:\n\n\n\nthen, there is nothing more about it but suddenly in a later chapter there is a picture of the fully manufactured plates assembled into a chassis, and the author acts as if this should just be something we know how to do:\n\n\n\nhow did he do that? we have these cad drawings, and suddenly there are these plates that were manufactured via some magical process the book never discusses, he never gives things like tolerances, the material that they are supposed to be made out of, etc., the kinds of things discussed here: \n\nhttp://www.omwcorp.com/how-to-design-machined-parts.html\n\ni know nothing about this stuff, in terms of how to go from cad design specs to getting an actual part manufactured. what kinds of companies should i use, what is a reasonable price to expect, what is the process? \n\nin general, how do i go from cad design to manufactured item? do i find a local machine shop that specializes in robotics, bring my cad drawings, and work with them to try to build the parts?\n\ni am totally a noob, i hope this isn't a question like this:\n\nhttp://blog.stackoverflow.com/2010/11/qa-is-hard-lets-go-shopping/\n", "tags": "design software manufacturing chassis hardware", "id": "9965", "title": "how to get manufactured part from cad file?"}, {"body": "for the following controller what do $q_{des}$ and $q_{act}$ stand for? also, what is the general principle of this controller? \n\nthanks!\n", "tags": "control microcontroller torque", "id": "9967", "title": "how to further understand the computed torque model controller"}, {"body": "is it possible to enhance (or redirect) the earth's magnetic field in a room or house so that one can write a small program that makes smartphones with hall-effect sensors detect more reliably in which direction they are pointing?\n\ni presume a fridge magnet won't do the job...\n", "tags": "sensors hall-sensor", "id": "9969", "title": "augmenting room magnetic field for smartphone sensors"}, {"body": "i read most of the irobot create 2 open interface (oi). it says send these serial commands to the create 2 to get it to do the described action, but no suggestion of what software to use to send these serial commands through the usb interface.  i did install the ftdi drivers to enable the usb to serial connection.  question: what serial software should i use to communicate with create 2?  is there a tool to verify that the supplied usb to serial cable supplied with create 2 is functioning and if the create 2 is functioning? (i did a reset on create 2 using spot and dock buttons)\n", "tags": "irobot-create serial", "id": "9971", "title": "what software to use to send (oi) commands to create 2. using windows laptop and supplied create 2 cable?"}, {"body": "if i understand the manual, each leg in each of the 7 segment displays is labeled with a letter a-g.  these letters then map to specific bits in a byte - 1 byte for each of the 4 displays.  setting a bit turns on the corresponding leg while not setting it leaves it off.\n\nwith this understanding, i tried to turn on all the a segments by sending\n\n\n\ninstead of the a segment in each display turning on, the displays all showed a 1. further testing shows that if i send the numbers 1-9 for any of the displays, they will display the number sent.  sending the number 10 or greater turns on various combinations of the segments.\n\ni was able to activate individual segments with the following numbers:\n\n\n\nhowever, i haven't been able to determine how the bytes sent affect the individual segments. either i don't understand the manual or digit leds raw does not work as the manual specifies.\n\nupdate 03june2016\n\ni have confirmed this behavior exists in the following firmware versions:\n\n\nr3-robot/tags/release-3.4.1:5858 clean\nr3_robot/tags/release-3.2.6:4975 clean\n\n", "tags": "irobot-create", "id": "9972", "title": "issue using digit leds raw (op code 163) on create2"}, {"body": "i have a 18 v rated driver i'm using to drive two 12 v dc gear motors using my arduino. i bought a new battery pack which is rated 3300 mah 25c, 11.1v making the total current input 82.5 a. my driver is rated for 7 v min and 18 v max, no current rating is given.\n\nmy motors are 12v max current under load is 9.5 a.\n\nso just to be sure, can using this battery destroy my motor driver?\n\nthis is the datasheet.\n", "tags": "motor battery current", "id": "9978", "title": "can too much input current destroy my motor driver?"}, {"body": "i know given the intrinsics  (where  are the horizontal and vertical focal length, and  is the location of principal point of the camera if pinhole camera model assumed) of an kinect depth camera(or other range sensor?), a depth pixel  ( is the pixel coordinate,  is the depth value) can be converted to a 3d point :\n\n\nso that a depth image can be converted to a point cloud, and indeed, a depth image represents a unique point cloud physically.\n\nslam systems e.g. kinectfusion use such point clouds for icp based registration to obtain camera pose at each time and then fuse new point cloud to the previously reconstructed model.\n\nhowever, my mentor told me that depth image cannot be inveribly converted to a point cloud since it's 2d->3d mapping with ambiguity (which i disagree), and he claims that i should use the depth image at time (i-1) and (i) for registration, not the derived point cloud.\n\n\uff08if i have to obey my mentor's order) i've been reading papers and found one using gradient descent to solve camera pose :\n\n\n  prisacariu v a, reid i d. pwp3d: real-time segmentation and tracking\n  of 3d objects[j]. international journal of computer vision, 2012,\n  98(3): 335-354.\n\n\nwhich uses rgb images and a known model for pose estimation. however, i've never found a paper (e.g., kinectfusion and other later rgb-d slam algorithms) deals with depth data just as plane image but not point cloud for registration. so could someone give me some hint (papers or opensource code) about: \n\nhow to do depth image registration without converting them to point clouds?\n", "tags": "localization slam kinect", "id": "9985", "title": "is there an algorithm using the kinect depth image (not the point cloud) for registration?"}, {"body": "i need to select motors for the wheeled robot i'm planning to build. my requirements are:\n\n\nrobot should be able to carry 35 kg\nplatform size around 20 cm x 35-40 cm\nspeed = 6 mph or 3m/s\nplanning to use 4 driven wheels, but can increase if not enough.\n\n\nother parameters like diameter of the wheels, acceleration are not fixed. please help me to calculate required torque and rpm of the motors. it would be great if you can provide equations. or post any lessons related to this problem.\n", "tags": "motor design wheeled-robot torque", "id": "9986", "title": "need help with motors for high load"}, {"body": "i am using a library precompiled on x86 on my pc (x86_64). does there exist any toolchain to compile the x86 library and in the end generate an executable for armv7l ubuntu?\n", "tags": "arm linux", "id": "9987", "title": "how to compile arm compatible binary from an x86 precompiled library on a pc host to be run on an arm target?"}, {"body": "i am trying derive velocity from accelerometer (mpu9250 in sensor-tag board). i saw lot of blogs which talk about noise and exact estimation problems. i started seeing velocity derivation (integration of accelerometer data over time) yielding me towards ramp because of noise presence in mpu9250.\n\nis the velocity can be estimated only by accelerometer or we need assistance of another sensor such as gps or gyroscope, etc..\n\nplease let me know as i see my velocity calculations never converge at all.\nalso i have limitation in compute power, so kalman filter kind of estimation techniques is difficult to implement. can you please suggest whether i am in right direction or not.\n", "tags": "sensors accelerometer", "id": "9988", "title": "instantaneous velocity calculation from accelerometer?"}, {"body": "i am working on an arduino based robot which engages a braking mechanism detecting anything in front. i was using an ultrasonic sensor, to detect obstacles which worked well while the robot was on my table (i.e under construction). but when i ran it on the ground, it doesn't stops and crashes.\n\nthe robot is programmed as if anything is detected 50 cm ahead if the robot, the braking mechanism stops the wheels. but when testing, the robot just wouldn't stop.\n\nmy robot is running at an average 7.5m/s . thinking that doppler's effect might have rendered my sensor useless, i tried a little ir sensor i had lying around (range 25 cm approx), but that didn't work as well.\n\nwhat am i doing wrong here?\n", "tags": "arduino motor ultrasonic-sensors", "id": "9994", "title": "why wouldn't my robot stop?"}, {"body": "i've implemented smc (sliding mode controller) on wmr in both x-y and x-z plane. \nnow i want to combine both of these to control wmr in 3d. for this purpose i'm trying to use resultant vector of simulation in xy plane and track that resultant vector in xz plane as value of x in previously designed code.  tracking control of resultant vector is shown in figure 1 while vector sum decomposed in rectangular coordinates after simulation is shown in figure 2. \n\nam i going wrong?  \n\nwhat other tecniques can i apply to do 3d control of vehicle using sliding mode controller.\n\ncan i reduce the time delay offset? i've implemented right equations for smc tracking controller equations but simulation does not gives exact results.these equations work well for control of vehicle in two dimensions (x-z plane).\n\n\n\n\n", "tags": "wheeled-robot matlab simulation", "id": "10002", "title": "control of wmr (wheeled mobile robot) in 3d"}, {"body": "i have the mbot robot and i'm trying to get it to go to the other side of a cylindral obstacle. \n\nsomething like this:\n\n\n\nwhat i know:\n\n\nradius of the cylinder - r\nrobot's distance from the cylinder\nwheel thickness - 1.5 cm\ndistance between the middle of each wheel - 11.5 cm\n\n\nhow would i achieve the above path?\n\nthe only thing i saw was this so question that says: \n\n\n  the distance between the left and right wheel of the robot is 6\n  inches.\n  \n  so the left wheel should travel at a distance of 2(pi)(radius+6)\n  \n  and the right wheel should travel at a distance of 2(pi) (radius-6)\n\n\nthe problem with my robot is that you can't tell it to go 20cm to the right, nor can you tell it to turn 90 degrees to the right.\n\nall you can do is set each motor's speed 0-255, so there's not way to put it in the formula disatance = time x speed.\n\ni assume i have to set each motor's speed to a different value so they would go in a circle of radius x and then just exit at the half of the circle (like shown in the picture)\n", "tags": "arduino wheeled-robot", "id": "10003", "title": "how to go around in a circle?"}, {"body": "i want to write my own inverse kinematics solver, and i have been recommended to use google's ceres solver to help. now, according to the documentation, ceres solver is usually used for non-linear least squares problems (http://ceres-solver.org/nnls_tutorial.html). this minimises the sum of squared differences between the measured and target values, over all data. what i am confused about, is how this relates to inverse kinematics.\n\nin inverse kinematics, with the example of a robot arm, the goal is to determine what joint angles the robot should be positioned in, in order to reach a target end effector pose. there exists a single equation which determines the end effector pose, given the set of joint angles, and we want to determine the parameters in that equation (the joint angles).\n\nbut how does this relate to the least-squares problem, where there are multiple measurements? is the problem i am trying to solve essentially the same, except that the number of measurements is one? and in that case, is using ceres solver's non-linear least squares solver really necessary?\n\nthanks!\n", "tags": "robotic-arm inverse-kinematics", "id": "10004", "title": "solving inverse kinematics with non-linear least squares"}, {"body": "i am a beginner to ros and i wanted to know if i could build a simple robot to learn ros. \ni currently have the following components available:\n\n\narduino uno\nsimple two wheeled robot chassis\nsome motors\nl293d motor driver\nsome ultrasonic sensors\nsome infrared sensors\n\n", "tags": "arduino slam ros beginner ultrasonic-sensors", "id": "10006", "title": "build a simple robot to learn ros"}, {"body": "i have a differential equation that connects the \"velocity\" of a point in the fov of a camera with the velocities of a robot's joints, that is $$\\dot s=j(s) \\dot q$$ where s is a vector with the $x$,$y$ coordinates of the point in the fov, $j$ is the interaction matrix and $q$ is the vector of the joint positions. \n\nif i have a certain point whose velocity i am tracking and this point remains in the fov, then $\\dot s$ is well defined. but if i change this point online, that is at the time instant $t$ i have point $s_t$ and at the time instant $t+dt$ i have the point $s_{t+dt}$, then $\\dot s$ is not defined.\n\ncan i create a filter to produce a continuous variation of $\\dot s$? if not, what can i do?\n\nmore specifically, i want to perform occlusion avoidance. in order to do this i want to compute the minimum distance of each feature point of my target object from the possibly occluding object. but, obviously, this distance can be discontinuous due to the fact that another possibly occluding object can appear in the fov nearer to my target than the previously measured.  \n", "tags": "cameras visual-servoing filter", "id": "10007", "title": "how to produce a continuous variation of a discontinuous function?"}, {"body": "with the problem of stabilising an inverted pendulum on a cart, it's clear that the cart needs to move toward the side the pendulum leans. but for a given angle $\\theta$, how much should the cart move, and how fast? is there a theory determining the distance and speed of the cart or is it just trial and error? i've seen quite a few videos of inverted pendulum, but it's not clear how the distance and speed are determined.\n", "tags": "mobile-robot sensors accelerometer gyroscope", "id": "10011", "title": "stabilising an inverted pendulum"}, {"body": "apologies if this isn't really the right place to be asking, but i was wondering whether third party design firms are ever contracted to design industrial and or consumer robots? \n\nif not is it something that is usually done in house, and who within an org would usually take care of this process?\n\nthanks.\n", "tags": "design", "id": "10014", "title": "design in the robotics world"}, {"body": "i would like to use an extended kalman filter for the localization of a wheeled robot. with this filter i would like to do sensor fusion between 2 encoder sensors and an imu with gyro and accelero sensors. \nthe only method i can find for the odometry data to blend in the filter, is to add this as input [uk] to the system xk = f(xk-1,uk).\ni would like to add the odometry data as normal measurement data (so in the zk vector). but then i need measurement equations g(xt). \n\nthis was what i had:\nnkleft = ((t*n0)/(2*pi*r))sqrt(xv\u00b2+yv\u00b2)+((tn0*b)/(2*pi*r))tauv + n\nnkright = ((tn0)/(2*pi*r))sqrt(xv\u00b2+yv\u00b2)-((tn0*b)/(2*pi*r))*tauv + n\n\nwith: nkleft/nkright = number of odometry pulses during sample period\n      t = sample time\n      n0 = total pulses in one wheelspeed sensor\n      r = wheel radius\n      xv = speed in x-direction\n      yv = speed in y-direction\n      tauv = angular velocity\n      n = sensor noise\n\nbut when i test my implementation, i don't get the results i expect, so i think there is something wrong in these equations. \n\ndoes anyone know an example where the odometry is inserted as a sensor measurement, so not in the system equation? \n", "tags": "kalman-filter odometry", "id": "10018", "title": "what is the sensor equation for odometry in a kalman filter?"}, {"body": "i've recently succeeded in building my first collision-avoidance arduino robot with 2 dc motors, and it works pretty well. however, it doesn't move in a straight line yet, when it should. i'm now studying which method should i implement to make the robot go straight. i've heard about using imu or encoders with feedback control. i pretty much understand how to use the encoders, but i'm not sure about the gyro. should i use just one of those or a combination of them?\n", "tags": "mobile-robot arduino control gyroscope", "id": "10019", "title": "should i use gyro or encoders for robot moving in straight line?"}, {"body": "i am looking for a physics simulator which can accurately model a robot hand picking up an object. the main requirement is for accuracy / realism, rather than speed. it needs to be able to model soft bodies, such as the rubber \"skin\" on robotic finger tips. it also needs to be a dynamics engine, such that the object is actually moved around by the hand, modelling effects such as slippage.\n\nfrom the research i have already done, there are two good candidates. first, graspit! (http://graspit-simulator.github.io/). this is open-source, and specifically designed for grasping, rather than physics simulation in general. second, mujoco (http://www.mujoco.org/). this is a more general simulator, is a commercial product, and has been adopted by some big names such as deepmind.\n\ni have tried using the bullet physics engine for robot grasping simulation, but soon realised that this was not going to be strong enough, because bullet is really designed for games, and hence sacrifices realism for speed. however, i'm much more interested in something which is as realistic as possible, even if the computation is slow.\n\ndoes anyone have any suggestions as to how i can proceed? anybody with any experience with graspit! or mujoco?\n\nthanks!\n", "tags": "robotic-arm simulator simulation", "id": "10023", "title": "what is the most realistic grasping simulator?"}, {"body": "i'm developing a project which involves a raspberry pi 3 remote control rover and i need to know the exact location of the raspberry pi rover in a set field.\n\nlet's say i have four logs, one in each corner of the square field (the goal right now is to extend this to any shape field, any number of corners), every of them equipped with (some kind of wave technology) that allows me to triangulate the position (based on signal intensity) of the raspberry pi rover.\n\nthe distance between logs should not be bigger than 30m (~100feet) and there is no line of sight guaranted.\n\nthe question is: which kind of technology should i use, infrared, wifi, bluetooth, radio, ultrasound, etc? or, is there any better approach to this problem?\n", "tags": "wireless", "id": "10026", "title": "raspberry pi 3 location in a set field, no gps"}, {"body": "i am using a physics simulator to simulate a robot arm. for a revolute joint in the arm, there are two parameters which need to be specified: damping, and friction. if a torque is applied to the joint, both the damping and the friction seem to reduce the resultant force on the torque. but what is the difference between the damping and the friction?\n", "tags": "robotic-arm dynamics", "id": "10029", "title": "damping vs friction"}, {"body": "i am building my first drone,\n\n\n  objective: - need to control drone by wifi on phone or laptop using\n  ground station software of openpilot\n\n\ni have a arduino 2560 , cc3d openpilot flight controller , raspberry pi with wifi bluetooth in built...\n\nnow i am not able to understand , how to go forward , should i connect arduino with openpilot cc3d flight controller , or raspberry pi directory with cc3d flight controller ....\n\ndo i really need arduino 2560 now ?\n\nalso how to connect r pi with cc3d flight controller , and how to mock pwm signals ?\n", "tags": "quadcopter arduino raspberry-pi", "id": "10033", "title": "first build - quadcopter , need help deciding hardware and connections"}, {"body": "i am trying to implement an s-curve motion profile to reduce the effects of the jerk on a mobile robot.  i had succeeded in calculating the equations of the trajectory in case of a point-to-point trajectory. \nmy problem is in the case of multipoint trajectory. first i introduce to my robot a start position and stop position with initial speed, max speed, max acceleration and max jerk. then, while he is running, i introduce a new stop position and i re-calculate the equations of the trajectory. when i generated the trajectory, i found that the acceleration profile becomes null suddenly.\nwhat should i do to fixe this problem? \n\n\n", "tags": "mobile-robot", "id": "10034", "title": "s-curve motion profile: discontinuous acceleration profile in a multipoint trajectory"}, {"body": "i have a 'baron' robot frame with 4 static wheels, all driven by a motor. at the moment i'm thinking of handling it like a 2 wheel differential drive. left and right wheels would receive the same signal. actually you can interpret it as a tank on caterpillars, exept there is no link between the two tires. \ndoes anyone have a different idea about this? \n\nps: the purpose of the robot will be to know it's exact location. i will use a kalman filter (ekf) to do sensor fusion of the odometry and an imu with accelero, gyro and magnetometer. so in the kalman filter i add the odometry model of a differential drive robot.\n", "tags": "differential-drive", "id": "10035", "title": "handling of a 4wd robot frame as a 2 wheel differential drive"}, {"body": "i want to rotate the whole value of a 3d vector into one axis using quaternion rotations.\n\nthe reason behind is that i want to align the x and y axis of my smartphone with the x and y axis of my vehicle in order to detect lateral and longitudinal acceleration separated on these two axis. therefore i want to detect the first straight acceleration of the car and rotate the whole acceleration value into the heading axis (x-axis) of the phone assuming a straight forward motion.\n\nhow do i achieve this?\n\n\n\n", "tags": "sensors accelerometer rotation", "id": "10040", "title": "rotate 3d vector value into a single axis using a rotation quaternion"}, {"body": "an inertial measurement unit (imu) is an important sensor used in aerial robotics. a typical imu will contain an accelerometer and a rate gyroscope. which of the following information does a robot get from an imu? \n\n\nposition\norientation\nlinear velocity\nangular velocity\nlinear acceleration\nangular acceleration\n\n\ni don't think it gets its orientation information from imu. the last time i took the test, i said that all but the first two are true. i failed.\n", "tags": "imu", "id": "10045", "title": "what information an imu gives to a drone?"}, {"body": "all of the examples of keeping a double/triple inverted pendulum balanced using a pid controller i've seen seem to be on a cart. like this one https://www.youtube.com/watch?v=cyn-crnrb3e\n\nhow come the pid controller always controls a cart rather than a servo that holds the first pendulum? the second/third pendulum could be connected loosely on the first pendulum and the pid controller controls the first pendulum. is it because servos tend to be too slow or are there other reasons?\n", "tags": "motor mechanism servos", "id": "10048", "title": "double/triple inverted pendulum always on a cart?"}, {"body": "i'm trying to reimplement the gmapping algorithm (which is based on the paper by grisetti et al. 2007) for my own purposes and therefore would like to understand in detail both the algorithm and the default parameter values people use.\n\nto my understanding, gmapping uses a proposal distribution for each particle whose moments are determined by sampling around a scan-matching estimate. my questions are:\n\n\nhow many samples $k$ does the standard gmapping implementation use to estimate the mean and covariance of the proposal distribution? \nthe samples $\\{x_k\\}_{k=1}^k$ are drawn from $x_k \\sim \\{x_j|x_j - \\hat{x}^{(i)}| &lt; \\delta\\}$, where $\\hat{x}^{(i)}$ is the scan-matching estimate of particle $i$. how is $\\delta$ determined? how much does this parameter matter?\n\n\ni couldn't find the values they used neither in their paper nor in their implementation of gmapping at openslam.org. any pointers regarding the practical significance of these parameters is highly appreciated!\n", "tags": "slam particle-filter", "id": "10049", "title": "question about sampling of proposal distribution in gmapping algorithm"}, {"body": "i took a course to have a better understanding of drones and their design. at the end of the course there was a test question that i got wrong and i would like to understand why. \n\ni was supposed to select the choices that best describe slam.\n\nand the possible answers were:\n\n\nestimates the location of features in the environment? \ncontrols the robot's flight through the environment?\ncauses the robot to avoid obstacles in the environment?\nnavigate in a cluttered environment?\nestimates the position and orientation of the robot with respect to\n    the environment?\n\n\nat first i knew that at least 3 and 4 were right because i watched a drone doing these things. i also thought that the last answer was linked to these two so i said yes to it too. finally, i thought that the only thing that was still controlled by the user would be the flight...\n\nyet i failed again... therefore what does simultaneous localization and mapping (slam) software do?\n", "tags": "slam", "id": "10050", "title": "what does simultaneous localization and mapping (slam) software do?"}, {"body": "please can somebody explain to me the difference between position control, velocity control, and torque control? specifically, i am thinking in terms of a robot arm. i understand that position control tries to control the position of the actuators, such that the error signal is the difference between the current position and the desired position. velocity control is then trying to control the velocity of each actuator, and torque control is trying to control the torque of each actuator.\n\nhowever, i don't understand why these are not all the same thing. if you want to send a robot arm to a certain position, then you could use position control. but in order to move an actuator to a certain position, you need to give it a velocity. and in order to give it a velocity, you need to give it a torque. therefore, whether the error is in position, velocity, or torque, it always seems to come back to just the error in torque. what am i missing?\n", "tags": "control kinematics dynamics roboti-arm", "id": "10052", "title": "position control vs velocity control vs torque control"}, {"body": "i have built an occupancy map using the gmapping package in ros for a (real) hallway. now i want to use this map in gazebo so that i can simulate my robot in this environment. it seems that gazebo can only support the map built from its own models, not from an external occupancy map. is there any way that i can use an occupancy map in gazebo or some other simulators? it is easier to obtain an occupancy map from real world than building a physical world in a simulator...\n", "tags": "mapping gazebo occupancygrid", "id": "10053", "title": "load pre-built occupancy map in gazebo"}, {"body": "this is my first post here, so hello all. i really hope i can learn a lot from you guys.\n\ni am trying to build a robotic arm to carry an object and put it inside of different boxes that are placed in different fixed locations.\n\ni found a few robotic arms that can do it, but i am still trying to find the right motor for the job. i read a lot on-line about the different motors, but i am not sure which on to pick. since the boxes are located in fixed places, the motors have to move in a precise way, so, according to my research, servo motors are the ones i should use.  \n\nsince it is a low budget project (i am college student), i wasn't sure which motor to choose (there are a lot of servo motors out there). i found several servo motors on-line, for example , analog feedback servo, and i was wondering what is the best servo motor i can buy for a really low cost project? i think i can spend about 10-20$ per motor (i need 5 motors).\n\ni already have an rpi and i know that pin 18 is the pwm pin that controls the motor's precision movement, but before i purchase a pwm controller and additional motors i need to run some testing to find how precise the motor is.\n\nby the way, how can i calculate the amount of weight the motor can handle?\n\nany ideas and information will be greatly appreciated.\n\nthank you\n", "tags": "robotic-arm raspberry-pi servomotor python", "id": "10056", "title": "choice of a motor for robotic arm"}, {"body": "i thought it were twelve ways:\n\n\nsix for each ways between two propellers\nsix others for each rotation on these ways.\n\n\nbut according to vijay kumar dean of penn engineering , it seems that i was wrong...\n\nthen i read this article  about modeling and robust trajectory tracking control for a novel six-rotor unmanned aerial vehicle and this one about navigation and autonomous control of a hexacopter in indoor environments but was never able to find such an information.\n\ni then guessed that 3 of the rotors could go one direction and three others into another which would add 6 other ways for rotating and therefore 6 others for simply flying but that is only a guess.\n\n\n", "tags": "multi-rotor", "id": "10059", "title": "in how many ways can a six propellers drone fly or rotate?"}, {"body": "good day everyone :)\n\ni am an undergraduate student working on a project involving the use of high torque small-sized dc motors for use in designing a person following trolley bag. where in the problem is to use small sized motors but still maintain the usability and efficiency in carrying loaded luggages.\n\ni have been looking for motors in local stores as well as in rs components as well as element 14. however i am not sure if my choices are the right fit as i am at a loss on what specifications to look for when selecting a particular motor for this application. i have also tried to look for motors used in current products that can be used in my application such as todays electric skate boards but unfortunately have no luck on finding their suppliers.\n\n\n  basically the question i would like to ask  is what specifications or calculations can i perform to select the proper motors given size constraints and weight carrying requirments. or does anyone have suggestions on common motors that are already normally used for this application. my target maximum load capacity is 20kg.\n\n\nthank you!\n", "tags": "mobile-robot motor gearing", "id": "10060", "title": "motor choice given size constraint and load requirement"}, {"body": "what is the degrees of freedom (dof) of the rostock delta robot 3d printer (delta mechanism that consists of three prismatic joints)?\n\nhere's the link to the delta mechanism i'm referring to:\nhttps://www.youtube.com/watch?v=ays6jasd_ww.\n\nthanks in advance for your help!\n", "tags": "kinematics inverse-kinematics actuator manipulator 3d-printing", "id": "10063", "title": "rostock delta robot 3d printer degrees of freedom (dof)"}, {"body": "i'm designing a tank tracked robot. i would like to know how do we calculate the minimum height difference between the sprocket wheel axis and the boogie wheel axis( if the maximum height of any obstacle is 16 cm)?\n", "tags": "design mechanism tracks", "id": "10070", "title": "tracked robots dimensioning"}, {"body": "for instance, how would you hook up a electric pump communicate with a motherboard? let's say i buy a electric pump, i hook it up to some sort of metal structure that if the pump is turned on it moves the metal structure, how would i hook up the pump to my motherboard so that i can program it? \n", "tags": "control motor robotic-arm microcontroller machine-learning", "id": "10072", "title": "how to make a robot?"}, {"body": "hi i am having a problem with my vex robot updating system.  right now i am using vex iq firmare update and my mac states that everything is up to date.  however when i look online there is a new update out.  i can not use the radios for the controller because i can't update the brain.\n", "tags": "vex", "id": "10076", "title": "problem with vex updating"}, {"body": "for hobbyists, you go to a store to buy products. the prices for these products are all clearly listed in the store catalog, and you can easily search for parts by lowest price or read customer reviews of the products.\n\nfor industrial engineers building complex machines, how do they buy components? or don't they worry about cost, and leave it to their employer to eat the cost as a part of doing their line of work?\n\nis it possible to \"shop around\" for low-cost engineering components?\n\n&nbsp;\n\nit is unclear to me how someone building a robot on their own as a small one-man startup can make the step from the world of toy robots, to larger and more industrial robotic components.\n\nmost of the non-hobbyist stuff is hidden away and not exposed to the world. while a product catalog might be available, there are no prices listed for anything.\n\nfor larger industrial components, there does not seem to be any realistic way to shop around for the lowest price or best value, since pricing for much of the big stuff is basically unavailable.\n\n&nbsp;\n\nfor me personally, i am interested in trying to build my own powered exoskeleton on a middle class american income, so i can't afford to be paying 1000 bucks for a single electrohydraulic proportioning servo valve, when i'll need probably 50 of them. but shopping around for low cost ones is basically impossible as far as i can determine, because pricing info is generally not available or searchable from the majority of manufacturers.\n", "tags": "industrial-robot", "id": "10077", "title": "how are industrial robotics components purchased?"}, {"body": "i am the moment trying to read and understand this paper task constrained motion planning in robot joint space but seem to have a hard time understanding the math.  \n\nthe paper describes how to perform task constrained motion planning in cases where a frame is constrained to a specific task.\nthe problem the paper tackles is when sampling in joint space, randomized planners typically produce samples that lie outside the constraint manifold. the method  they proposed methods use a specified motion constrain vector to formulate a distance metric in task space and project samples within a tolerance distance of the constrain.   \n\ngiven the this i am seem to a bit confused on some simple terms they define in this paper. \n\nexamples. how is a task space coordinate defined ? what information does it have?\n\nthey compute the $$\\delta x = t_e^t(q_s)$$ which is transformation matrix of the end effector with respect to the task frame. \n\nwhat i don't get is why the end effector? and why the end effector with respect to the task frame?\n\nsecondly.\nlater in the paper they write down an expression that relates the task space to the joint space motion. they do it using the jacobian, but seem to miss explaining (in my opinion) what $e(q_s)$ actually do. \n\n$$j(q_s) = e(q_s)j^t(q_s)$$\n\nwhat is said about it in the paper is that \n\n\n  given the configuration $q_s$, instantaneous velocities have a linear\n  relationship $e(q_s)$\n\n\nwhy the need of instaneous? what is the definition of an instantaneous component? how does it differ from the information given by the jacobian?\n\nbasically i don't understand how and why the mapping is as it is?.. \n", "tags": "robotic-arm motion-planning jacobian", "id": "10087", "title": "task space to joint motion space conversion"}, {"body": "i have open pilot cc3d attached to a receiver which can accept signals from a rc transmitter after configuration through gcs ..\n\n\n  objective is to - send these control signals ( yaw/throttle/roll/pitch\n  ) from r pi or through arduino ( whichever is best to transmit\n  signals) by using a simple radio transmitter , like 433 mhz tx,\n\n\ni want to use a usb based game controller , which will be connected to raspberry pi and then it can transmit signals through radio to receiver...\n\nis this feasible  or need to be scrapped...\n", "tags": "quadcopter arduino raspberry-pi radio-control", "id": "10088", "title": "send quad copter control signals from arduino or raspberry pi to receiver on open pilot cc3d"}, {"body": "i have a laser pointer on a handle grip and i'm trying to keep the laser pointer's yaw direction, which can rotate at around 10deg/s. so i have the laser pointer on a stepper motor and an accelerometer/gyro in the handle, but what's a good way for maintaining its yaw direction? could i simply turn the shaft according to the accelerometer/gyro's yaw readings or is control theory (pid) needed?\n\nthat is, if my stepper makes 4096 steps/rev, one gives 0.0879 deg. if the handle is turned by, say, 0.879 deg, then turn 10 steps in reverse (instantaneously). would this be jerky and pid be needed?\n\nany thought appreciated.\n", "tags": "sensors stability", "id": "10089", "title": "locking the yaw direction of a laser pointer"}, {"body": "i want to make a object tracking quadcopter for a project. while i'm using the arduino mega 2560 as the flight controller, i was thinking of using an additional offboard microcontroller/board for getting data from the onboard camera,which would then send an appropriate command to the onboard arduino.\n\ni was hoping someone could provide clarification on the advantages/disadvantages of doing object tracking with either choice.\n\nthanks!   \n", "tags": "quadcopter arduino raspberry-pi", "id": "10092", "title": "arduino or raspberry pi?"}, {"body": "i didn't found any modules to charge my 11,1 volt lipo akku, only for 3,7 volt with 5 volt power supply. how can i handle that with a micro-usb connector on my robotplatform?\n", "tags": "arduino power lithium-polymer", "id": "10096", "title": "how can i charge a 11,1 volt lipo akku?"}, {"body": "i am trying to build a 3d map using two cameras. i have found the coordinates of all the objects.\n\nwhat is the best way to store this data.i would also like to display this map later. my range will be 5-6 meters(500-600 cms). the accuracy i have managed to achieve is within 1 cm.\n\ni have used opencv and python on a laptop.i would like to shift to a raspberry pi later if the pi can perform well enough.\n\nmy main issue is how can store this 3d map.\n", "tags": "cameras mapping 3d-reconstruction opencv", "id": "10098", "title": "storing a 3d map"}, {"body": "i'm unsure if this is the correct community to ask this question (vs stackexchange electronics or aviation, for example), but i recently purchased a hubsan x4 hd video drone from amazon.\n\nthis is my second hubsan drone so i am already familiar with using the recording feature. however, after every recording, the recordings are the correct length, with the correct audio, but the image is black. i tried formatting the micro sd, using different micro sds, reading up on forums, etc. but nothing seems to do the trick.\n\nis mine defective, or has someone had this issue and has been able to solve it?\n\nthanks\n", "tags": "quadcopter cameras", "id": "10099", "title": "hubsan x4 drone camera recording black"}, {"body": "i am trying to measure the height of water inside a column. the column is 50mm in dia and 304mm long. i have mounted the sensor just above the column.\n\n\n\nto measure the accuracy of the sensor, i filled the column up to a known value (a) and got the average sensor reading (b). a+b should give me the height of the sensor from the base of the column.\n\nrepeating this for different values of a, i got very different values for (a+b). see attached chart.\n\n\n\nmy question is\n\n\nis the sensor expected to have error of this order? \nis my setup of confining the sensor through a column producing such errors. \nany other ideas to get the water column height. please note that during the actual test, the water inside will be oscillating (up and down).i am thinking of making a capacitive sensor using aluminium foil. water will work as the dielectric and the level of water will determine the capacitance.\n\n\np.s. i also did some open tests (not through a column) to get the distance of a fixed object, and it was quite accurate.\n\nany help is appreciated.\n\narduino code\n\n\n", "tags": "arduino ultrasonic-sensors", "id": "10101", "title": "ultrasonic sensor through a column"}, {"body": "i am searching for a way to minimize the size of a stereo vision module and cannot find any ics that will combine and sync two mipi csi-2 (4 lane) data streams without an fpga and too much code.  there was one online (max7366a 3d video combiner/synchronizer with two mipi csi-2 input and one mipi csi-2 output) but the product is not publicly available.   does anyone have knowledge of an arrangement of ics that i could try?.\n", "tags": "computer-vision", "id": "10103", "title": "is there a way to combine and sync two 2k cameras @ 90fps with ics"}, {"body": "i am new to robotics.recently came into contact code.so the teacher let me use the serial port app for android to enter the opcode.but the robot did not have any reaction.\n\n\n\ni use the communications cable with adapter in android phone.\napp use [droidterm: usb serial port]\n\nserial port settings\nbaud: 115200   (19200 also used)\ndata bits: 8\nparity: none\nstop bits: 1\nflow control: none\n\n\n\ni try to enter opcode, but no response.--> enter:128,135,134.....\nbut it did not show any reaction to the phone and robot.\ni hope according to opcode instructions to control robots to make the specified action.\n", "tags": "irobot-create", "id": "10106", "title": "how to use the opcode to start?"}, {"body": "i have an arduino wired to an mpu6050 breakout board. the arduino continuously collects accelerometer and gyroscope data from the mpu6050 and calculates angle and velocity. \n\nsimply plotting the vector components (x,y,z) of this data does not allow one to reason about the motion of the sensor or robot. it's possible, though not easy, to do sanity checks (is the sensor oriented as expected? is gravity working?). but it's very difficult to look at a x,y,z plot of accelerometer log data and imagine what the robot did for instance. \n\ni was wondering if there is some sort of tool or python library to visualise accelerometer and gyro, or imu data? (i'm looking for something like this- https://youtu.be/6ijarke8vku)\n", "tags": "arduino accelerometer gyroscope visualization", "id": "10110", "title": "visualizing raw accelerometer and gyro data"}, {"body": "i am making a project with a 4 wheeled differential robot to make visual slam using a stereo rig. i have some encoders to measure de displacement and the steering angle of the robot and i want to use the odometry motion model in the fastslam algorithm.\n\nto use the odometry motion model you need to calculate the values it needs from the odometry reading (incremental encoders), $u_t=(\\bar{x}_{t-1},\\bar{x_t})$ where $\\bar{x}_{t-1}=(\\bar{x}\\&gt;\\bar{y}\\&gt;\\bar{\\theta})$ and $\\bar{x}_t=(\\bar{x}'\\&gt;\\bar{y}'\\&gt;\\bar{\\theta}')$ are the previous and the current pose extracted from the odometry of the vehicle.\n\nmy question is about how to obtain those values from the encoders. i guess that in this case i would need to obtain the equations from the geometric model for the differential robot:\n\n$d_l=\\frac{2\\cdot\\pi \\cdot r_l}{n_c}\\cdot n_l$\n\n$d_r=\\frac{2\\cdot\\pi \\cdot r_r}{n_c}\\cdot n_r$\n\n$d_t=\\frac{d_l+d_r}{2}$\n\n$\\delta\\theta=\\frac{d_l-d_r}{l}$\n\nwhere $d_l$ is the advance of the left wheel, $d_r$ is the advance of the right wheel, $r_l$ is the lecture from the left encoder, $r_r$ the lecture from the right encoder, $n_c$ the total number of pulses of the encoder type, $d$ is the total distance achieved by the robot and $\\theta$ the angle steered. $l$ is the distance between the wheels.\n\nusing those equations is possible to obtain the pose in every time step:\n\n$\\bar{x}_{t}=\\bar{x}_{t-1}+d\\cos(\\theta_{t-1})$\n\n$\\bar{y}_{t}=\\bar{y}_{t-1}+d\\sin(\\theta_{t-1})$\n\n$\\bar{\\theta}_{t}=\\bar{\\theta}_{t-1}+\\delta\\theta$\n\nso those last are the values i need to inject to the modometry motion model and then add gaussian noise to them.\nam i right? or is there another way of computing the pose from odometry for a differential 4-wheel robot?\n", "tags": "mobile-robot slam odometry movement", "id": "10115", "title": "slam - odometry motion model"}, {"body": "i want to program a set path for the irobot to follow without having to be tethered all the time to my computer. what is the best way to do that? \n", "tags": "programming-languages", "id": "10117", "title": "how to preprogram irobot create 2"}, {"body": "how to decide the torque of the motor and the gearbox ratio for a (say 6 dof) robotic arm, having a 5 kg payload capacity for instance. i am mainly concerned about the inertial mismatch. how do i calculate it? are there any other factors that i should consider? \n", "tags": "motor robotic-arm torque manipulator", "id": "10118", "title": "how to decide the torque of the motor and the gearbox ratio for a robotic arm?"}, {"body": "i have a heated compartment, inside which, there is another object heated up by independent heater. i want to control temperatures of both chamber and the object. \n\ni could achieve this by simple pid (or pi) controllers for both chamber and object, but i would like to try more thoughtful approach :) i have two temperature sensors, and two pwm outputs for heaters. how do i identify a model for an object i want to control?   \n", "tags": "control pid automation", "id": "10119", "title": "how to create a model for temperature control?"}, {"body": "i'm new to robotics and i'm looking to make a 5-6 axis robotic arm out of stepper motors but i honestly dont know how much torque i should have for each part. below i have described in more detail what my current plan is but i'm really not sure as to how much i really should be spending on each of these joints.\n\nmy general plan for this project was to make a arm that when fully extended would only be around 40-50(max) cm long. it would be consisted of light weight aluminum and i am hoping for it to weigh only a couple of pounds when done.\nanyway here is my current list of actuators for each of the joints:\n(bottom = 1, top = 6)  \n\n1st joint, (i cant actually post the link because i don't have enough rep, but this is what it is called on amazon: nema 23 cnc stepper motor 2.8a)  \n\n2nd and 3rd joints  \n\n4th, 5th and 6th joints  \n\nmy real questions is, is this overkill or is it not enough for what i'm really trying to make. i really don't need it to be able to pick up a lot of weight, at most 1 to 2 kilos but i highly doubt i will ever be picking up anything more than that. anyway i just wanted to see if this was sufficient enough for my project... i know this isn't really the best place to ask but i really need some help because i am new to this and i don't want to throw money where i don't need to. thanks in advanced ;)\n", "tags": "robotic-arm stepper-motor actuator", "id": "10125", "title": "what kind of torque is needed for a small 5-6 axis robotic arm?"}, {"body": "i'm trying to design a control system for a robot that tracks moving object. thus i want to robot to match the position and velocity state of the object. i don't want robot to simply to arrive at the position, but i want to arrive at the position with the same velocity of the object. \n\nobject velocity and position data will be provided externally.\n\ni'm not sure if a traditional pid controller (with velocity controls) with just a position based error is enough. wouldn't position only state goal result in tracking that is always lagging behind?\n\nis pid what i want or should i be looking at something else like trajectory controls?\n", "tags": "pid", "id": "10127", "title": "pid with position and velocity goal?"}, {"body": "i am using an apm 2.6 that is connected to an odroid usb via telemetry port (uart to usb). i am trying to get mavlink messages without the need to keep sending a heartbeat message.  \n\nif i switch to usb connection (not telemetry) i get mavlink messages continuously  without sending heartbeat messages to the apm. i want to be able to do the same using the telemetry port.\n\nis there any place in the firmware (arducopter) code that i can change? or maybe just a parameter?\n\ni am using 57600 baud-rate, i tried also 115200. and the usb cable is not connected.\n", "tags": "quadcopter ardupilot mavlink", "id": "10129", "title": "disable mavlink heartbeat using telemetry"}, {"body": "i'm making a target to an outdoor robot competition.\n\nthe target should detect if some of the robot got touched or got an hit   automatically. and the target can get hit 360 degree. \n\ni'm searching for the perfect sensor to detect an hit, without get false positive from a wind.\n\nmy option right now are:\n1- ultrasonic sensor (bad coverage)\n2- tilt sensor  (bad fp rate)\n3- wooden conductive \n\ni would like to know if someone has other ideas (that affordable - less than 30$ dollar per target might be o.k)\n\nedit: the target is static, and just waiting to a robot to touch it.\n\nedit: the specs are:\n\n1- the target dimension is 1 meter height, 0.5 meter width , 0.3 depth.\n\n2- to trigger the target ,the robot should be around 10 centimeter long to any point of the target surface.\n\n3-to trigger the target the robot needs to get close up to 10 centimeter or even press with around 1 newton force. the robot might even throw an object that satisfy the previous condition.\n\n4-detection must be only from intentional touch.\n\n5-wooden conductive is trigger because a human is electrically conductive. this might not be the option when we throw an object.\n\n6- target will be placed outdoor, so the sensor need to be wind-resistance (not extreme wind condition- just around 20-25 km/h)\n\n7- i  prefer a sensor that detect touch (more than proximity)because it might make my solution more cheap and reliable(in factor of amount sensors as i estimate).\n\nthanks.\nguy\n", "tags": "arduino sensors force-sensor", "id": "10130", "title": "detect physical touch/hit"}, {"body": "i'm developing a 5 axis robotic arm with stepper motors and i am getting around to ordering most of my parts. i plan on using 5 easydriver shields to drive all of my motors. i am also planning on using just a basic arduino uno board to go with it. so here are my questions:\nis there any alternative instead of buying a ton of easy drivers and connecting all of them to a single board?\nand if there isn't, then how would the setup look to use more than 3 stepper motors? this is the most useful picture i found, however it only shows 3 and while i know i could plug in a 4th i am unsure whether i could plug in a 5th.\n", "tags": "stepper-motor", "id": "10132", "title": "what is the best way to plug in more than 3 stepper motors into a arduino uno board?"}, {"body": "i own an irobot create2 on which i am planning to implement a control algorithm. after playing with the different drive commands, i noticed that changing the desired velocity values marginally doesn't seem to do anything.\n\neven the drive pwm command that ranges from -255 to 255 seems to have an internal granularity that is bigger than 1.\n\nin this video the create seems to change its driving direction nearly seamlessly, which i am not able to reproduce with the described behavior. \n\ndoes anyone have any suggestions?\n", "tags": "control motor irobot-create", "id": "10136", "title": "irobot create2: granularity of drive control?"}, {"body": "in recent years, we've heard a lot about drones. what new technology is enabling these new devices? why are they making news?\n\nwhen i was a kid, we used to call those things (similar to what we now call drones) remote control or rc. but, apparently it's not just a nomenclature change because we now have new fcc regulations, commercial applications like amazon prime air, news of military applications and i still see rc labeled devices in the stores.\n\nso, what is this, new drone stuff all about? and why now? what new technology has recently emerged that enables these devices?\n", "tags": "untagged", "id": "10148", "title": "what's new about drone technology?"}, {"body": "i was looking at the contact points for the atlas in the drcsim package. each foot has 4 contact points at each vertex of the rectangle. i'd like to know how these points are determined. i've tried looking at the ode code, but c++ isn't my strong suit so i had some difficulty figuring out what was going on. what i understand is that ode compares the geometries one by one however it's not possible to compare all points so it only compare a select few points. what i'm trying to understand is what basis are those particular points selected? why does the atlas have the 4 contacts set up the way they are, and not some additional points on the heel? can i add them myself?\n\nthanks.\n", "tags": "gazebo", "id": "10150", "title": "how does ode determine contact points in gazebo?"}, {"body": "i am trying to get mavlink messages from apm 2.6 via telemetry that is connected to my ordoid u3 usb port. \ni am able to read messages when i send the request_data_stream message, but it sends them only once, i want to be able to get them continuously without needing to send the request again.\n\nany ways to solve this?\n", "tags": "quadcopter communication ardupilot mavlink", "id": "10153", "title": "apm 2.6 response only once after request_data_stream"}, {"body": "i've been making my own quadcopter flight controller using arduino mega. this is the sample code i wrote in order to test the esc timers and motors:\n\n\n\nhowever, my issue here is that the bldc motors i'm using don't work smoothly when connected to the arduino. they erratically stop and even change direction of rotation at the same throttle input. i've tested them by connecting them directly to the transmitter and they work fine there with perfect rotation and speed. can someone please help me out and tell me where i might be going wrong? \n\nedit: i do realize posting the entire arduino code might be overkill, but i've been trying to solve this problem for three days (as of 22nd june,16) and i really do hope someone can point out any improvements/corrections in my code.\n", "tags": "quadcopter arduino brushless-motor esc", "id": "10154", "title": "bldc motors erratic behavior with arduino program"}, {"body": "i am new to the irobot create 2 but i do know a thing or two about the arduino (don't assume too much though). however, in this case, i am beyond stumped over what i am sure is something simple but is somehow not obvious to me. three people have confirmed my wiring from the create 2 to the arduino to be correct and the code i have looks similar to many examples that i have seen on this forum. however, i cannot get my create 2 to do anything. i am not at all sure what is wrong and i am starting to wonder if the robot is even receiving commands let alone doing anything with them. is there anything wrong with this code and can anybody suggest a way to verify that the robot is receiving data (since it does not beep or provide return messages)? thank you.\n\nedit (06/24 01:10 est); updated code (with a few notes).\n\n\n", "tags": "arduino irobot-create", "id": "10159", "title": "icreate 2 with arduino (just getting going)"}, {"body": "i am trying to get the airspeed for arduplane from erle brain 2 ( ardupilot) through its i2c port, and send it to arduino. \n\nwhat i have discovered:\nthere already exists i2c_driver.cpp, and i can use this to send data, by using the functions in arduplane.cpp. however, i am lost on how to implement the sending part, as in how i use the functions? the functions like write accept arguments of address, length, data. how do i know that? and do i send the data in binary?\n\nany help will be really appreciated! \n\nthanks!\n", "tags": "arduino ardupilot i2c", "id": "10167", "title": "getting i2c sensor output from ardupilot to arduino"}, {"body": "it is helpful in robotics to first learn about \"linux kernel development\" or \"device driver development in linux\" before i start learning ros? i know c and java! in brief, i want to know any prerequisites which are essential to understand ros better.\n", "tags": "ros linux", "id": "10169", "title": "what are the prerequisites for learning ros?"}, {"body": "i'm using a basic trig/echo ultrasonic sensor with an arduino uno. i get accurate readings until i cover the sensor at which point i receive very large numbers. why is this?\n\nprogram\n\n\n\nexample output\n\ni moved my hand from 10\" away until i cover the sensor\n\n\n\nedit\n\ni changed the amounts from  to  to get a more precise reading. i held the sensor ~100mm from a granite counter-top and quickly lowered it until the tabletop covered the front of the sensor.\n\n\n", "tags": "arduino ultrasonic-sensors", "id": "10172", "title": "covering up ultrasonic sensor"}, {"body": "i'm working on a project where i'm using a voltage that is higher than what most microcontrollers can handle. i'm looking for a kind of switch that will connect a power source to an electromagnet and all of this controlled by my microcontroller. i also thought about using a potentiometer to control the speed of two high voltage dc motors via my microcontroller so please tell me if this is a good idea aswell. \nthanks for your time\n\nzakary\n", "tags": "arduino microcontroller", "id": "10178", "title": "switch activated by a microcontroller"}, {"body": "i was wondering either there is any special naming for regulators that:\n\n\noutputs unit is the same as inputs, ie. velocity [m/s] as input and velocity as output [m/s].\noutputs unit is different than inputs, ie. position as input [m], velocity as output [/m/s]\n\n\ni would appreciate all help.\n", "tags": "pid", "id": "10187", "title": "proper naming of pid regulators"}, {"body": "so i was thinking about projectiles that don't need a propellant like gunpowder i've seen coils gun but that's a little out my way. i was wondering if i know the force required to propel a object could i program a robot to exert that force to propel the object the same way (in a linear propelled fashion).\n", "tags": "force-sensor", "id": "10190", "title": "can a robot or mechanical part be programmed to exert a specific force"}, {"body": "i have never yet had the create2's incremental encoder rollover but want to write my code to be prepared for this to happen and test it. when the encoder rolls past 32767 (14.5m), does it rollover to -32768 and count there or start at 0 again and count up from there?\n\none other odd thing but not a big deal. when i reset the create2, the first value is 1 not 0. \n", "tags": "irobot-create roomba", "id": "10192", "title": "create2 incremental encoder rollover method"}, {"body": "i am new to robotics and working on autonomous rc car for indoor purpose only.\ni was wondering how can i detect expected collision (i am planning to put dummy cars near by rc car).\n\nplease guide me if there is any other alternative or any one has worked on similar project.\n\nreference:what are some low cost alternatives for lidar?\n\nthanks,\nsandy\n", "tags": "mobile-robot", "id": "10193", "title": "what is liadar alternative for indoor rc car"}, {"body": "for my dissertation project, i will have to use a drone. what it will do is look for an object in a closed space.\n\nwhat i will most deffinately need are:\n\n\na camera\nsensors to avoid collision\npc communication: \n\n\nstream video \nreceive directions (the pc will control the drone, give it directions etc)\n\n\n\nnow i'm wondering what would be the best platform to build on considering these requirements and i have absolutely no idea what's going on in the microcontroller world. so i don't know which of the two has more shields and whatnot that would be suitable for my needs.\n", "tags": "quadcopter arduino raspberry-pi", "id": "10195", "title": "starting out: arduino vs raspberry pi drone"}, {"body": "i am trying to control my f450 dji quadcopter using a pid controller. from my imu, i am getting the quaternions, then i convert them to euler's angles, this is causing me to have the gimbal lock issue. however, is there a way that i directly use the quaternions to generate my control commands without converting them to euler's angle?\n\nthis conversation here discusses a similar issue but without mentioning a clear answer for my problem.\n\nthe three errors so far i am trying to drive to 0 are:\n\n\n\nwhere the master generates the desired rotation and the slave is the imu.\n\nupdate:\n\nhere are some pieces of my code:\n\ngetting the current and the reference quaternions for bot the master and the slave from the rotation_vector:\n\n\n\nthen i want to know the orientation of the current quaternion relative to the reference quaternion for both the master and the slave. \n\n\n\nfinally, i calculate the euler angles:\n\n\n\nand i do the same thing for the slave. \n\nat the beginning, the reference quaternion should be equal to the current quaternion for each of the slave and the master, and thus, the relative roll, pitch and yaw should be all zeros, but they are not!\n", "tags": "quadcopter pid stability", "id": "10196", "title": "using quaternions to feed a quadcopter pid stabilizing controller to avoid gimbal lock"}, {"body": "i am trying to solve some create 2 sensor reading problem that i am having when i came across @nbckly's posts (part 1 and part 2) that i believe are exactly what i am looking for. i copied his code from the original post into my project and updated the code from the second post as best as i could interpret...but something is not going according to plan.\n\nfor example, i am printing the angle to my serial monitor (for now) but i am constantly getting a value of 0 (sometimes 1).\n\ncan @nbckly or anybody please check out this code and tell me what i'm doing wrong? i would appreciate it. thank you very much.\n\n\n\n#\n\nwhat i am asking is why do i only get an angle of rotation of 0 or 1 degrees when the robot is moving in a circle. the angle should be incrementing while the robot is moving. \n\nthe output i am getting on the serial monitor shows a line of what looks like garble which i assume is supposed to be the bytes sent back from the create which is followed by \"angle: 0 (or 1)\" what i was expecting to see was an increasing angle value. (1,2,3...360, and so on).\n", "tags": "arduino sensors irobot-create", "id": "10200", "title": "create 2 reading sensor values"}, {"body": "i've googled a lot but wasn't able to find official definitions of these 3 parts. maybe the explanations of servo and controller are good enough, but i'm still trying to look for a more \"official\" one.\n\nany ideas?\n\nthanks,\n\nsnakeninny\n", "tags": "microcontroller servos", "id": "10202", "title": "the 3 key components of a robot are controller, servo, and reducer. can someone give us an \"official\" explanation of what they do respectively?"}, {"body": "these are the motor mixing formulas i've written for my quadcopter's flight controller (arduino-mega) and i was wondering if its all right to use all three (roll-pitch-yaw) in each of the esc's signals. \n\n\n", "tags": "quadcopter arduino pid esc", "id": "10210", "title": "is this the right way to do motor mixing with pid outputs for a quadcopter?"}, {"body": "i am using a stereo rig to do slam, calibrated using the matlab calibration tool. i need to compute the 2d coordinates of a landmark using the observation model obtained from triangulation (the images are rectified). \n\nthe equations obtained from triangulation are the ones presented in the blue box here. because i am doing slam in 2d the coordinates i need to use are $z_p$ and $x_p$. the parameters needed to compute those values are $f$, $t$ and $disparity (x_l - x_r)$.\n\nafter doing the calibration intrinsics matrices $k_l$ and $k_r$ are obtained and a common intrinsic matrix for the stereo rig is calculated from $k = 1/2*(k_l +k_r)$ so i get the parameters needed in triangulation from  this common matrix.\n\nthe focal length is supplied from the manufacturer, and for my logitech c170 is 2.3mm. the baseline $t$ from the calibration is 78.7803 mm. to compute the disparity i am obtaining surf points and using ransac to discard the outliers so i get x coordinates from both rectified images.\n\nthe problem is that with those values i can't obtain correct values for $z_p$ and $x_p$ and i am not sure why or where i am doing the wrong step. anyone can help with this? are those the correct steps to do triangulation from rectified stereo images?\n\nedit: my stereo rig looks like the figure i attach:\n\nif you compare the coordinates system with the one used in the link before is easy to see that my $x_r$ corresponds to the $z_p$ from the link and the $y_r$ corresponds to $x_p$, so the equations to calculate the distance using triangulation and with the coordinate system of the figure are:\n\n$x_r=\\frac{fb}{x_l-x_r}$\n\n$y_r = \\frac{(x_l-p_x)b}{x_l-x_r}-\\frac{b}{2}$ \n\nbeing $f$ the focal length, $b$ the baseline, $p_x$ the x coordinate of the central point and $x_l-x_r$ the disparity. the $x_r$ $y_r$ coordinate system is situated between the two cameras, so this is the meaning of the $\\frac{b}{2}$ displacement in the equations.\n\ncalibration \n\nto obtain the cameras calibration i am using the stereo camera calibrator toolbox with the chessboard pattern.\n\nafter calibration, i made some tests using matlab functions triangulate and reconstructscene to know whether the parameteres are well calculated. the distances i obtained using this functions (which use the stereoparams object created by the calibrator) works well and i obtain distances very similar to the actual ones. so i suposse the calibration works well.\n\nthe problem, as i explained before, is when i try to calculate the distances using the equations $x_r$ and $y_r$ because i am not sure how to obtain the common matrix $k$ for the stereo rig (the calibrator gives one intrinsic matrix for each camera, so you have two matrices).\n\nthe value of the baseline given from calibration make sense, i made a measurement with a ruler and gives me 78 mm approximately.\n\nthe $f$ value i assume should be in pixels but here again the calibration gives an $f_x$ and $f_y$ value so i am not sure which one should i use.\n\nthose are the intrinsic matrices i obtain:\n\nleft: \n$\\begin{pmatrix} 672.6879&amp;-0.7752&amp;282.2488\\\\0&amp;674.3705&amp;240.1287\\\\0&amp;0&amp;1 \\end{pmatrix}$\n\nright: $\\begin{pmatrix} 681.7049&amp;0.0451&amp;331.2612\\\\0&amp;681.8235&amp;246.1209\\\\0&amp;0&amp;1\\end{pmatrix}$\n\nbeing the parameters of $k$: $\\begin{pmatrix}f_x&amp;s&amp;p_x\\\\0&amp;f_y&amp;p_y\\\\0&amp;0&amp;1\\end{pmatrix}$\n", "tags": "mobile-robot slam computer-vision stereo-vision", "id": "10213", "title": "triangulation from calibrated stereo rig"}, {"body": "i got asked to make some sort of trigger pads for the foot section of an organ working over midi to a electric piano and my friend wants it to be pressure sensitive so we can program in the note velocity when he's not using the organ sound.\n\nhttps://www.youtube.com/watch?v=duariihwjqg\n\nthat is what i try to achive. i want the pads to not just be on/off but also be able to control the velocity of the midi note.\n\nim planning to use a adruino uno with a mux shield ii from mayhew labs to get 36 analog inputs. not exactly sure on the wiring yet but have looked at some guides and videos on google to get a feel for how it can be made. \n\nall these 36 piezo-\"sensors\" is planned to register how hard you push the pedals and then send out a midi signal with a specific note correspondig to the pedal, and velocity to the electric piano so you can control the low notes with your feet.\n\nhttp://www.thomann.de/se/clavia_nord_pedal_key_27.htm\n\njust like that but more pedals and a lot cheaper. \n\nwill the arduino be able to read the analog output of the piezo sensor even though it's going through a multiplexer?\n", "tags": "control electronics", "id": "10215", "title": "piezo sensors and multiplexers"}, {"body": "so, while i was out drinking with a couple of my friends, one of us said something like 'man, wouldn't it be cool if the beer just came to us?' and that got me thinking.\n\nwe all have seen some crazy things people do with quadcopters (or polycopters even), but would it be possible (and not too expensive) to build a quadcopter that could carry, say, a crate of beer? (16-20kg)\n\ni'm a bit of a tinkerer and i've built some minor things with rasp. pi's before but never tried myself at a quadcopter, because they are quite a big piece of work, but being able to fly a crate of beer right in front of me would be pretty awesome.\n\nthat aside, how strong would such a quadcopter have to be? in terms of motors, propellers, battery &amp; frame. i'm a complete noob when it comes to rpm and the like, so i wouldn't even know where to begin. i have, of course, read through most of the available tutorials on the internet, but they don't answer my question of what exactly to look for when i want my quadcopter to be able to carry something specific.\n", "tags": "quadcopter raspberry-pi", "id": "10220", "title": "quadcopter that can carry heavy things?"}, {"body": "i've recently implemented a kalman filter to estimate altitude for a small robot with an imu+baro sensor mounted on it.\n\nmy objective is to get max precision i can have, using this two sensor, with small computing power that a mcu can provide me. i've tuned my filter and it seems to work pretty well.\n\ncan i obtain a significant improvement using an extended kalman filter instead of a normal kalman filter and if it worth time to implement it?\n\nmore in detail, since this request is too specific for each application, if a model function that use baro and accel as states should be linearized and used in a ekf and if this can improve data reliability compared to a simply kf?\n", "tags": "kalman-filter accelerometer ekf", "id": "10221", "title": "should i use or not ekf for baro-acc altitude estimation?"}, {"body": "hi the landmark are very used in slam , what are the algorithme those be used to extract them , and how robot can diferentiate the landmark , if they detecte one in point a at xt and another in xt+1 how the robot can know if its the same or not ?\nsorry for my bad english :/\n", "tags": "slam ekf lidar ransac", "id": "10224", "title": "landmark extraction algorithm"}, {"body": "are there any better/ advanced ways of steering a line following robot other than pid controller? if so what are them? \n", "tags": "pid line-following steering", "id": "10227", "title": "are there any others alternatives for pid controllers for line following robots?"}, {"body": "what are the criteria to consider when ordering dc motors for a line following robot?\n\nis there a way to calculate the torque required?\n", "tags": "motor line-following", "id": "10229", "title": "how to select dc motors for a line following robot?"}, {"body": "i would like to create a simulation model (basically a signal generator) which will allow me to generate the 3 output signals of an accelerometer based on 3 location input signals (x,y and z). i would like a more realistic model of the data produced by an accelerometer (with some noise and bias offsets).\n\nhow can i convert the series of points into a simulated accelerometer output?\n\nspecifically:\n\ni have a series of positions which describe a trajectory in 3d space...if an accelerometer was moving along the trajectory described by the series of positions, i am interested in knowing (simulating!) the data that the accelerometer would produce as the result of moving along the described trajectory. \n\ni could just calculate the 2nd derivative of the trajectory, but that would probably be too ideal. i am looking for a model which is more realistic. \n", "tags": "accelerometer simulation", "id": "10233", "title": "generate synthetic accelerometer data based on (x,y,z) coordinate"}, {"body": "my arduino + raspberry pi robot was working fine in the morning. i tested it, it ran perfectly, and then i switched it off. \n\nnow in the evening when i'm trying to run it again, with the same batteries and everything, it just doesn't move!\n\ni stripped it down to the motor compartment and found that when i try to run my main motor, i can see sparks through the translucent plastic on the back.\n\ndoes that mean my motor is gone?\n", "tags": "arduino motor raspberry-pi battery", "id": "10237", "title": "what does internal sparking in a motor mean?"}, {"body": "i'm using a hc-sr04 sensor to detect obstacles. what are the pitfalls with an ultrasonic sensor?\n\nhere are a couple i've found during my testing:\n\n\nthe signal can bounce off of one wall to another and then get picked up, distorting latency\nabsorbent materials sometimes don't bounce the signal back\ncheck the datasheet for supported range (min/max)\n\n", "tags": "ultrasonic-sensors", "id": "10239", "title": "what are some pitfalls of an ultrasonic sensor?"}, {"body": "my goal is to control drone by raspberry pi. the raspberry pi uses camera and opencv, sends control commands to avr microcontroller which will generate the pwm control signal. meaning that it will simulate pilot with transmitter-receiver setup.\n\nin other words (to make it more clear). raspberry tells the atmega8 that the drone needs to go more forward. atmega8 generates custom pwm signals on 8 pins. those signals are sent directly to cc3d pins responsible for roll, pitch etc. atmega8 replaces controller receiver in this setup. it generates signal not based on user input but on what raspberry tells it.\n\nin order to do that i need the parameters (period, voltage etc.) of the pwm signal that cc3d accepts to properly simulate it. i have found this topic:\n\ncc3d - replacing rc emitter with an rpi\n\nhe has the same problem as i do and he found the solution. unfortunately i can't send pm and i can't comment because i'm new to the site... so basically there is no way for me to contact him.\n\nso any help would be appreciated.\n", "tags": "quadcopter arduino raspberry-pi uav avr", "id": "10241", "title": "cc3d pwm control signal characteristic (to be simulated by raspberry pi)"}, {"body": "i want to add an robot to my machine the doosan lynx 220 lsy. for that i need this inputs and output:\n\ninputs:\ncycle start,\nchuck1 open,\nchuck1 close,\nchuck2 open,\nchuck3 close\n\noutputs:\ncycle finished,\ncheck chuck1 opend,\ncheck chuck1 closed,\ncheck chuck2 opend,\ncheck chuck2 closed\n\ni already found a book where i found those inputs and outputs but it justs says for example cycle start (sb373). i cant find these number on the i/o board or anywhere else.\ncan someone help me to find my listed outputs?\n", "tags": "cnc", "id": "10247", "title": "doosan lynx 220 where to find inputs and outputs"}, {"body": "what's an appropriate compass sensor to use on a robot?\n\nthere are a ton of cheap digital compass sensors, and i was thinking of using an mpu9250 combined accel/gyro/magnetometer as a compass, but i'm finding these are terribly unreliable and need constant calibration via the \"wave in a figure 8 pattern\" method whenever it gets near other electronics or small magnets, which a robot obviously won't be able to do. is there a digital compass technology that mimics traditional compasses that requires little to no calibration, appropriate for installation on a robot?\n", "tags": "magnetometer compass", "id": "10248", "title": "compass sensor for robot"}, {"body": "i'm trying to find known techniques for keeping a manually controlled robot within a known polygon fence. more specifically, a pilot controls a robot by issuing desired velocity vectors, and the autopilot adjusts the velocity so that the distance to any boundary is always at least the stopping distance of the robot.\n\nmy goal is to implement a system that:\n\n\ntries to follow the pilot's desired velocity as closely as possible.\nis robust to changes in position and desired velocity. at a minimum, i want the velocity to change continuously with respect to the position of the robot and desired velocity of the pilot. informally, this means that sufficiently small changes in the position or desired velocity of the pilot induce arbitrarily small changes in the velocity.\n\n\nthe second point is particularly important. suppose that the policy were to find the intersection with the boundary in the direction of the desired velocity and slow down smoothly to that point. the below figure depicts a couple of scenarios in which this would not be continuous. in this figure, the black lines represent the fence boundary, the red dot is the position of the robot, and the blue line is the desired velocity of the pilot. in figure (a), a small perturbation of the position to the left will cause a large increase in allowed velocity because the desired velocity will intersect the far edge instead of the near edge. in figure (b), a small clockwise rotation of the velocity vector will result in a large decrease in allowed velocity because the desired velocity will intersect the near edge instead of the far edge.\n\n\n\ni have searched for relevant papers, but most of the papers i've seen have dealt with fully autonomous obstacle avoidance. moreover, i haven't seen any papers address the robustness/continuity of the system.\n\n:edit:\n\nthe robot knows its own location and the location of the boundary at all times. i also have some equations for maximum velocity that allow a smooth ramp-down to a single line boundary (though i'd be interested in seeing a better one). i would like the velocity limits to be continuous in the position and desired velocity of the pilot.\n\ni want to continuously throttle the user's input such that a minimum safe distance between the robot and the boundary is maintained, but see the figure that i added to the question. the hard part (i think) is to make sure that small changes in position (e.g. due to sensor noise) or small changes in desired velocity (e.g. due to pilot noise) don't cause huge changes in what the autopilot allows.\n\ni want continuity because i think it will provide a much nicer experience for the pilot while still enforcing the fence boundary. there is a trade-off with optimally but i think this is worth it. even though the physical world smoothes any discontinuities in velocity, big changes could still cause large jerk which will be somewhat disturbing to the pilot. the goal is to not have the autopilot introduce large oscillations not intended by the pilot.\n\nthis will be implemented on a physical system that has sensors that provide an estimation of position, and the boundary shape is known and is unchanging. the actual system that i'm targeting is a quadcopter.\n", "tags": "control geometry reference-request", "id": "10249", "title": "fence avoidance for manually controlled robot"}, {"body": "i need some ideas for strategies or algorithms to apply on these strategies to perform obstacle avoidance while navigating.\n\nat the moment i'm doing offline path planning and obstacle avoidance of known obstacles with an occupancy grid. and running the a* algorithm over the created matrix. after that my robot follows along the resulting trajectory. this is done by splitting the whole trajectory into sub-path. the robot adjust it's heading to the new target and follows the straight line. the robot is controlled by a fuzzy logic controller to correct deviations from the ideal line (steering) and adjusting the velocity according to the steering action and distance to the target. so far so good. and it's working very well.\n\nas sensor system, i solely use the google project tango (motion tracking and area learning for proper path following). now i want to use the depth perception capability of the device. getting the appropriate depth information and extracting a possible obstacle is done with a quite simple strategy. the robot analyses the depth information in front of the robot and if any object is in between the robot and the target point of the sub-path, an obstacle must be there.\n\nnow i'm wondering how to bypass this obstacle most efficiently. the robot is only aware of the height and width of the obstacle, but has no clue about the depth (only the front of the obstacle is scanned). feeding the occupancy grid with this new obstacle and running again the a* algorithm is not effective, because of the missing depth. one possible strategy i could imagine is estimating a depth of the length of the grid cell, re-plan and continue the navigation. if the robot faces the same obstacle again, the depth is increased by the size of one additional grid cell length. but i think this is extremely ineffective. \n\nthe requirement is to only use the google project tango and no additional sensors, such as ultrasonic to sense the sides.\n\nupdate 1\n\nthe first picture illustrates the given trajectory from the path planning (orange). the gray and blue data points are the sensed obstacles in front of the robot. the notch behind the blue obstacle is actually the wall, but is shadowed by the blue obstacle. image 2 shows the same scene just from a different perspective.\n\nthe issue i have to treat is, how to optimally bypass the blue obstacle even i don't know how deep it is. driving to the left and to the right only to capture better data points (to generate a 3d model) is not possible. \n\n\n\n\nupdate 2\nyes, i'm using a depth sensor, the one integrated in google project tango. it's a visual measurement. a infra-red laser beams a grid onto the objects and a rgb-ir camera capture these information and evaluates the appropriate depth information.\n", "tags": "control motion-planning algorithm", "id": "10252", "title": "obstacle avoidance while navigating"}, {"body": "i am trying to assess the pros and cons of steering a robot car using different speeds of 2 or more dc motors versus using a servo and a steering mechanism? from your experience which is better in terms of:\n\n\nsteering accuracy (e.g. prompt responsiveness or skidding while on higher speeds)\nefficiency in electrical power consumption\ndurability and maintenance\ncontrol complexity (coding and electronics)\n\n\ni researched and understood how both approaches work, but i need some practical insight to select the most suitable approach. any hint or research direction is appreciated.\n", "tags": "motor servos steering", "id": "10253", "title": "steering using different speeds in dc motors or using a servo?"}, {"body": "i am working on a 6 dof robotic arm(industrial manipulator). i have the basic structural specs (dimensions, weights etc) for the links and joints with me. \n\nbasically, i want to simulate both static torque(due to the weight of the arm) and dynamic torque(due to the accelerating joint's motion) torque that the joints will need to bear for a given set of motions. \n\ni have looked on the web and found tools like the ros-moveit visualiser, gazebo, v-rep which let me visually see a robotic arm and simulate the position logic and external factors like collisions etc. but i have been unable to simulate/calculate dynamic torque values from these tools.\n\nideally, i'd want to define a fixed motion of the end effector(i.e. move the robot between 2 positions) and measure the torque(both static and dynamic) during that particular move.\n\nthese torque values are essential for selecting the optimum motors and gearboxes for my design and payload.\n", "tags": "robotic-arm dynamics torque simulation manipulator", "id": "10256", "title": "dynamic torque simulation for a 6 dof robotic arm"}, {"body": "i'm working on a extremely simple robot (very first project) that attempts to find the source of a bluetooth signal. there are two motors that drive the platform and each has an encoder. we've already used a kalman filter to calculate the approximate distance to the bluetooth beacon within reasonable error.\n\ni worked out a manual solution using some trig that solves the problem in theory, but it fails if there is any error (for example, it attempts to turn 73 degrees, but turns 60).\n\nmy question is how can i reasonably drive the motors based on the encoder data to continuously minimize the distance to the signal? furthermore, is there a generic solution to problems like these? (i guess you might call it a stochastic \"hotter/colder\" problem)\n\nthanks in advance.\n", "tags": "raspberry-pi wheeled-robot", "id": "10259", "title": "find object using only distance"}, {"body": "we know that a quadcopter needs to be tuned to its perfect pid values to minimise the pitch, roll , yaw errors and etc., before releasing to the market will they tune every unit and ship it ? or a any different algorithm is used which doesn\u2019t require any tuning ? because every motor/esc or a chassis will not be exactly same, which will add to the noise. \n", "tags": "quadcopter", "id": "10265", "title": "how does a quadcopter startup work ? will they tune every copter before releasing to market?"}, {"body": "i am working on path planning for a 2 arm 4dof (2 dof for each arm) robot. i am currently using a centralised planning methodology (considering the multi robot system as a single one with higher dof, 4 in this case) and a* algorithm to find the shortest path. the problem with this algorithm is its high computation time.is there any way to reduce the computation time while still obtaining the shortest route ?\n\nnote:decentralised path planning is not good enough for my case.\n", "tags": "robotic-arm motion-planning path-planning", "id": "10267", "title": "path planning of 2 arm 4dof robot"}, {"body": "good day,\n\ni am currently working on an obstacle avoiding uav using stereo vision to obtain depth maps. i noticed that the quadcopter would sometimes not steer to the correct direction.\n\ni am using the raspberry pi compute module io board which comes with two csi ports used with two v1 pi cameras.\n\nissue\n\ni soon found out that due to the latency between the cameras, the left and the right images are not in sync thus the errors in the depth map result.\n\nsteps taken:\n\ni noticed the image blur when moving the cameras around so i adjusted the shutter speed by setting the uv4l/raspicam driver. with the shutter speed, i also tried to increase the framerate as i've read, it improves the latency issue. in my code which uses the opencv library, i used the grab() and retrieve() commands to replace the read() command so that the frames from both cameras is grabbed at the nearest time possible however it didn't help much.\n\ndoes anyone know any possible solutions?\n", "tags": "computer-vision stereo-vision c++ opencv", "id": "10272", "title": "stereo vision using compute module: pi camera synchronization"}, {"body": "in the \u201cirobot_roomba_600_open_interface_spec.pdf\u201d provided for the irobot create 2, there is a section titled \u201croomba internal screw boss locations\u201d.  it states that \u201cscrews may be replaced with threaded standoffs.\u201d\n\ndoes anyone know what screw/thread size of standoffs should be used to match the screw threads?\n\n(i saw another similar thread but the only solution listed was to re-thread the holes, which i would like to avoid if at all possible.)\n\nthanks!\n", "tags": "irobot-create", "id": "10277", "title": "what is the thread/screw size for the irobot create 2 internal screw bosses described in the open interface spec doc"}, {"body": "i'm trying to build my own motorised camera gimbal using a bldc like this, where the shaft is hollow. does anyone know how the camera platform should be mounted? should a shaft be somehow pressed into the hole?\n\nany thought appreciated.\n", "tags": "brushless-motor", "id": "10284", "title": "mounting a gimbal bldc motor"}, {"body": "i am using a l298n ic and (not a driver shield) and an arduino.\ni would like to know how to use the ic with the arduino to run a six wire stepper motor.\n\napparently i am new to electronics.can i have a detailed explaination for wiring the ic connections on the breadboard and the arduino.\n\nthanks\n", "tags": "stepper-motor", "id": "10285", "title": "using a six wire stepper motor with l298n"}, {"body": "first off, just to be transparent, i'm a total newbie when it comes to dc motors (and pretty much anything robotic). \n\ni've got a couch that's right up to a window with the lever type openings (anderson windows). with the couch, i have no clearance to turn the lever to open it. given i've replaced most of my house switches/outlets with home automatable ones, i figured i'd see if i can build myself a small motor that i can automate to open these also. to be absolutely honest, i've got no clue where to start. i have no problem with coding the automation part, but i don't even know what kind of motors to look for that would be able to turn my knob (or rather how to actuate the thing my knob connects to)...\n\nhelp!\n\nthanks :)\n", "tags": "motor", "id": "10286", "title": "question about what motor to use for opening window"}, {"body": "consider multiple mobile bases driving around in some area. in order to get meaningful data from the lidar of each base, the sensors should be mounted as horizontal as possible. due to safety regulations, the lidars should also be mounted at a height of 15 cm from the floor. when i checked the data sheet of sick lidars, it shows that all models use the wavelength 904 nm. does that mean that mobile bases equipped with lidars with a coplanar scan lines will end up mutually blinding each other? \n\nif it is the case, how is this problem solved? (i don't consider tilting the lidars a solution as it defeats the purpose of having \"2d\" lidars where even if the tilting angle is known, what the lidar observes becomes dependent on the robot's pose and distance from eventual obstacles)\n", "tags": "sensors lidar rangefinder", "id": "10294", "title": "lidar problems in a multi-robot setup"}, {"body": "update\n\nhey i have the following subscriber on nvidia tx1 board running on an agricultural robot. we have the following issue with subscribing to sensor_msgs::compressed:\n\n\n\nand the callback function\n\n\n\nwhen i compile this i get an error:\n\n\n\nthe red error statement was:\n\n\n\ni am not using boost, and searching around hasn't helped me solve it\n", "tags": "ros c++ opencv", "id": "10295", "title": "compressedimage to an image in a node"}, {"body": "i am working on a differential drive robot with two motor wheels with encoders and caster wheels. the robot also has a intel realsense depth camera.\n\nwhen i launch rviz : thee global option > fixed frame is set to base_link and shows all the transforms for the differential driver nodes. but an error appears for the depth camera nodes with message saying :\n\nno transform from camera_depth_frame to baselink\n\nno transform from camera_depth_optical_frame to baselink\n\nno transform from camera_link to baselink\n\nno transform from camera_rgb_frame to baselink\n\nif i change the global option > fixed frame to camera_link i can see all the transforms for the depth camera but now the differential drive transforms are now not available\n\nhope you can help.\n", "tags": "ros", "id": "10296", "title": "rviz transform error base_link and camera_link"}, {"body": "i accidentally ended up supplying 12 v to the arduino 5v output pin instead of the vin pin. does that mean that i can't use the 5v output pin anymore i.e. its fried?\n", "tags": "arduino", "id": "10297", "title": "12 volt input to 5 volt ouput of arduino"}, {"body": "i am studying bacholar of dental surgery but have intrest in learning this subjet so tell me about a good book to read.\n", "tags": "mobile-robot", "id": "10301", "title": "what books do you suggest for a beginner like me ?"}, {"body": "i am currently busy with a final year project which requires me to track people walking through a doorway.\ni initially thought this may be possible using a normal camera and using some motion detection functions given in opencv, i have however come to the conclusion that the the camera is mounted too low for this to work effectively.(height shown in the image below)\n\n\n\ni have now been looking into using a 3d camera or a stereo camera to try and get around this problem.\n\ni have seen similar examples where a kinect(from xbox 360) has been used to generate a depth map which is then processed and used to do the tracking, this was however done from a higher vantage point, and i found that the minimum operating range of the kinect is 0.5m.\n\nfrom what i have found, the kinect uses an ir projector and receiver to generate its depth map, and have been looking at the orbbec astra s which uses a similar system and has a minimum working distance of 0.3m.\n\nmy question now:\n\nwhat exactly would the difference be between the depth maps produced by a 3d camera that uses an ir projector and receiver, and a stereo camera such as the duo/zed type options?\n\ni am just looking for some insight from people that may have used these types of cameras before\n\non a side note, am i going about this the right way? or should i be looking into time of flight cameras instead? \n\n----edit----:\n\nmy goal is to count the people moving into and out of the train doorway. i began this using opencv, initially with a background subtraction and blob detection method. this only worked for one person at a time and with a test video filmed at a higher vantage point as a \"blob-merging\" problem was encountered as shown in the left image below.\n\nso the next method tested involved an optical flow method using motion vectors obtained from opencv's dense optical flow algorithm.\nfrom which i was able to obtain motion vectors from the higher test videos and track them as shown in the middle image below, because of the densely packed and easily detected motion vectors it was simple to cluster them.\n\nbut when this same system was attempted with footage taken from inside a train at a lower height, it was unable to give a consistant output. my thoughts of the reasons for this was because of the low height of the camera, single camera tracking is able to function when there is sufficient space between the camera and the top of the person. but as the distance is minimized, the area of the frame that the moving person takes up becomes larger and larger, and the space to which the person can be compared is reduced (or atleast that is how i understand it). below on the right you can see how in the image the color of the persons clothing is almost uniform, optical flow is therefore unable to detect it as motion in both cases.\n\n\n\ni only started working with computer vision a few months ago so please forgive me if i have missed some crucial aspects.\n\nfrom what i have seen from research, most commercial systems make used of a 3d cameras, stereo cameras or time-of-flight cameras, but i am unsure as to how the specifics of each of these would be best suited for my application.\n", "tags": "computer-vision cameras stereo-vision", "id": "10312", "title": "difference between 3d camera(using ir projection) and stereo camera?"}, {"body": "i'm using openrave to simulate a quadruped, in order to get an idea of torque requirements. \n\nto get started i made a single dof, single link pendulum to test controllers etc out on.\ni've whipped up an inverse dynamics based pd controller using computeinversedynamics(), which i set the outputs using setdoftorques(). i then set a desired position, with the desired velocity being zero. this all appears to work well and i can start the simulation, with the pendulum driving up to the desired position and settling. \nmy concern is the value of the output torques. my pendulum is modeled as a simple box of length 1, mass manually set to 1, with a com of 0.5.\nwhen i run my simulation, i output the gravity component from computeinversedynamics(). this gives 4.9nm, which matches up with hand calculated torques i expect from the pendulum (eg the static case) when it is driven to the desired position (from down to horizontal).\nbut the output torques to setdoftorques() are much higher and vary depending what i set the simulation timestep to.\nif i maintain a controller update rate of 0.001 seconds, then for a simulation update of 0.0001 seconds, my output torque is approximately 87nm. if i alter the simulation timestep to 0.0005 seconds, keeping the controller rate the same the output torques drop down to about 18nm.\n\nas an experiment i removed the inverse dynamics controller and replaced it with a plain pd controller, but i still see large output torques.\n\ncan anyone shed some light on this? it's very possible i'm missing something here!\n\nthanks very much\n\nedits:\ni'm adding the main section of my code. there is no trajectory generation, really. i'm just trying to get to a fixed static position.\nin the code, if i keep dt fixed, and alter env.startsimulation(timestep=0.0001), i get the issues popping up.\n\n\n\nhere is some data for dt = 0.001 and env.startsimulation(timestep=0.0001)\n\nin this data,\n\n\ntaus is the torque command to the simulation, \ntorquegravity+torquecoriolis is returned from the inverse dynamics \na_cmd is the controller command and\nm*a_cmd is the command after being multiplied by the mass matrix\n\n\nthe gravity and coriolis parts appear to be correct for steady state, where it should be about 4.9nm\n\n\n\nand here is some data for dt = 0.001 and env.startsimulation(timestep=0.0005)\n\n\n\ndespite the differences in torque command (a_cmd) i still get similar performance, in that the arm drives to the right position fairly quickly.\nas another experiment i set the initial position to pi/2 and just fed back the gravity term to the torque output. my understanding of this is that the arm should float, ala a gravity compensation sort of thing. but it just drops as if a small torque is applied.\nthanks again! \n", "tags": "robotic-arm torque", "id": "10314", "title": "openrave output torques and simulation timestep"}, {"body": "i would like to know if there is any way to get all the possible solutions of inverse kinematics of a 6 dof robotic arm?\ni have found some good matlab codes but gives only one solution like in peter corke's book .\nthank you in advance. \n", "tags": "inverse-kinematics", "id": "10322", "title": "is it possible to get all possible solutions of inverse kinematics of a 6 dof arm?"}, {"body": "scott adams, creator of dilbert, recently shared an article about a robot the police used to kill a suspect by detonating a bomb in close range.\n\nthis made me wonder -- when was the first time a robot took a human life?\n\ngood comments were made on this which leads me to clarify that i mean a pureposeful taking of life. i shy away from the term \"murder\" because that involves legal concepts, but i mean an intentional killing.\n\nan interesting subdivision would be between robots under active human direction (\"remote control\") and those with a degree of autonomy.\n\n\n", "tags": "mobile-robot", "id": "10324", "title": "when was the first time a robot killed a human?"}, {"body": "is torque related to size or power at all in electric motors? and what about gas motors too?\n\ni have a go kart that is 2.5hp and it's 50cc and its about 1ft x 2ft x 1ft in size. i also see online there are .21 cubic inch gas motors for r/c cars that are also 2.5hp, the difference being that the r/c motor spins at 32k rpm while the go-kart motor spins at 12k rpm. if i were to put a gear reduction on the r/c motor, would it preform more or less the same as the go kart motor? why is there a size difference?\n\nsame for electric motors. i can buy an rc car electric motor that's 10hp and the size of a pop can. the cnc machine at work has a 10hp motor the size of a 5 gal bucket. again, the only difference is the rpm.\n\nif i were to reduce both setups so they spun at the same rpm, would they preform the same?\n\nthe only reasons i could think of is 1. cooling and 2. rpm control (for pid loops and sensors)\n", "tags": "motor power torque engine", "id": "10325", "title": "is horsepower related to torque in electric motors?"}, {"body": "i have an irobot create 2 and have been working with it and have gotten to the point where i can control it via bluetooth. this is great but i also want it to be able to be autonomous and navigate itself room to room for example. are there any slam irobot tutorials or any other materials you'd recommend for autonomous navigation?\n", "tags": "mobile-robot slam irobot-create", "id": "10326", "title": "slam with irobot create 2"}, {"body": "i am a third-year electrical engineering student and am working on an intelligent autonomous robot in my summer vacations.\n\nthe robot i am trying to make is supposed to be used in rescue operations. the information i would know is the position of the person (the coordinates of the person in a json file) to be rescued from a building on fire. i would also know the rooms of the building from a map, but i don't know where the robot may be placed inside the building to start the rescue operation.\n\nthat means i have to localise the robot placed at an unknown position in a known environment, and then the robot can plan its path to the person who has to be rescued. but, since this is not my domain i would like you to guide me on what is the best method for localising given that i can use an imu ( or gyro, accelerometer, magnetometer) and ultrasonic sensors to do the localising job. i cannot use a gps module or a camera for this purpose. \n\ni, however, do know how to do path planning.\n\nas far as my research on the internet is concerned i have found a method called \"kalman filtering\" that maybe can do the localising job. but there are i think some other filtering methods as well. which one should i use? or is there any other simpler/better method out there of which i don't know yet?\n\ni am also attaching the map of the building which is known to me.\n\n\n\nedit:\n\nthe terrain is flat, and i would like to know where the robot is on the map like at coordinate 0,4 etc.\n", "tags": "mobile-robot localization imu accelerometer gyroscope", "id": "10330", "title": "localising a robot placed at an unknown position in a known environment"}, {"body": "i'm doing a project with the irobot create 2. i want it to be able to map out a room and navigate to a point for example. my problem is that the robot doesn't have any distance sensors. what it can do is detect if there is an obstacle ahead of it or not (0 or 1) and it can measure how far it has traveled in millimeters. any good techniques out there or best to buy an ir sensor?\n", "tags": "mobile-robot irobot-create", "id": "10334", "title": "autonomous navigation without distance sensors"}, {"body": "i am building an application that executes graphslam using datasets recorded in a simulated environment. the dataset has been produced in mrpt using the gridmapnavsimul application. to simulate the laserscans one can issue the bearing and range error standard deviation of the range finder. \n\ncurrently i am using a dataset recorded with range_noise = 0.30m, bearing_noise = 0.15deg. am i exaggerating with these values? could somebody provide me with typical values for these quantities? do laser scanner manufacturers provide these values?\n\nthanks in advance,\n", "tags": "slam laser rangefinder", "id": "10335", "title": "typical laser scanner noise values"}, {"body": "i am trying to implement quaternions and i am using cc2650 sensortag board from ti. this board has mpu9250 from invensense which has digital motion processor (dmp) in it. this dmp gives quaternion, but for my understanding i implemented my own quaternion. i used gyroscope and acceleorometer values coming out of dmp (which are calibrated ) to calculate angle of rotation. i feed this angle, in 3 directions (x,y,z), to my quaternion. i am not able to match my quaternion values with dmp quaternion values. in fact it's way off, so wondering what i have done wrong.\n\nfollowing are detailed steps that i did :\n\n1)  tapped gyro sensor values from function \u201cread_from_mpl\u201d.\n\n2)  converted gyro values in to float by diving by 2^16. as gyro values are in q16 format.\n\n3)  now used gyro values of 3 axis and found out resultant using formula : \ngr = sqrt(gx^2+gy^2+gz^2)\n     where gx,gy and gz are gyro values along x-axis,y-axis and z-axis respectively.\n\n4)  now angle is derived using above found resultant gr by : \n*angle = gr*1/sample_rate*\n       where sample_rate is found using api call ,mpu_get_sample_rate(&amp;sample_rate)\n\n5)  this angle is fed to angle_to_quater function which basically converts angle to axis and then quaternion multiplication.\n\n\n\n6)  i also added  doing angle calculations from accelerometer as follows : here also accelerometer is converted to float by dividing by 2^16, as acceleorometer values also in q16 format.\n\n\n\n*find resultant angle of this also as :\n\n\n\n7)  then complimentary filter is :\n*finalangle = 0.96*angle + 0.04*inst_acc_angle;\nthis final angle is fed to step 5 to get quaternion.*\n\nquaternion multiplication is done as below and then normailized to get new quaternion (q). \n\nquater_mul :\n\n\n\nquat_normalize:\n\n\n\nwhen i check my quaternion values with dmp, they are way off. can you please provide some insights in to what could be wrong here. \n\nsource code :\n\n\n\n\n\nthis variation is coming when i am keeping my device stationary.\n\ny-axis : resultant of all 3 gyro axis.\n\nx-axis : the number of samples. (have not converted them to time)\n\nsample_rate is 3hz.\n", "tags": "sensor-fusion", "id": "10348", "title": "quaternion implementation"}, {"body": "i'm trying to build a line follower robot and i'm interested in predicting the curves on the track.\ni have 8 binary sensor array(qre1113).\nmy goal is to make a system that it can generalize what it learned about the curves and give me predictions about where should be at the line to pass it as fast as possible.\n\nhow can i integrate a system like q learning and how can i train it?\nand also how can i combine this system with a type c pid controlller ?\n\nthere is a paper about it ot you are willing to explain\n\nthis is a important project for me and i am kinda running on clock so quick help would be appreciated\n", "tags": "differential-drive", "id": "10350", "title": "q learning and kohonen maps for line follower robot"}, {"body": "good day,\n\ni am working on an autonomous quadcopter. may i ask if there is a significant difference if my control loop dropped from 500hz to 460hz due to added lines of code that would require retuning of the pid gains? and if retuning is required, is it correct to assume that only the i and d gains should be retweaked since they are the only constants which are time dependent? thank you :)\n", "tags": "quadcopter mobile-robot control pid stability", "id": "10357", "title": "pid gains: drop in control loop rate, need to retune?"}, {"body": "could you please see the attached battery images and tell me if it is safe to continue using this battery or should i discard it?\n\n\n", "tags": "battery lithium-polymer", "id": "10367", "title": "battery damaged?"}, {"body": "what are the specifications of the digital compass used in the iphone 6s?\ni am trying to measure yaw angle using the magnetometer.  i observed the magnetometer/digital compass in the iphone is really very stable. the north direction is always the same, while the magnetometer i am using (or the magnetometer used in nexus) needs to be calibrated again and again to function properly.\n\ni found that the digital compass ak8963c is used in the iphone 6, but it needs calibration.  so i am not sure what is inside iphone 6s because it works without a calibration procedure.\n", "tags": "imu sensor-fusion magnetometer", "id": "10369", "title": "what are the specifications of the digital compass used in iphone 6s"}, {"body": "i was looking for a python implementation of slam and stumbled upon breezyslam which implements tinyslam aka coreslam. \n\nmy robot is equipped with the hokuyo urg-04lx-ug01. \n\ni have odometry hence passing it to the updater: \n\n\n\nas i start moving the robot starts discovering room a and then room b &amp; c already the map seems to have rotated. i come back to room a and return the initial pose end=start using the same path. now i noticed room a has significantly rotated in relation to the other room. consequently the map isn't correct at all, neither is the path travelled by the robot. \n\n\nwasn't the slam supposed to store and keep the boundaries for the first room it discovered?\nwhy this rotation may be happening?\nhow could i try to troubleshoot this issue with the data i have collected (odometry, calculated position, lidar scans)?\ncan i tune slam to do a better job for my robot?\n\n\nslam is pretty new to me, so please bear with me, any pointers on literature that may clarify and moderate my expectations of what slam can do.\n\n\n\nextra\n\n... and here the best video i found to understand particle filter\n", "tags": "localization slam mapping", "id": "10371", "title": "understanding and correct drift when using breezyslam (aka tinyslam / coreslam)"}, {"body": "i am doing project on odometry using raspberry pi. i know that encoder motor will tell me how much distance my robot has covered, but i have no idea ho to implement completely. i just need guideline about which steps to follow. till now i have interfaced motor with raspberry pi and counted the number of rotation. i have questions as follow?\n\nhow to plot map of odometry using which language and library?\n\nif you know anything, just give me guideline about steps to follow.\n", "tags": "motor raspberry-pi odometry", "id": "10377", "title": "need help regarding odometry using encoder motor and raspberry pi"}, {"body": "i want to use a mpu9150 to give me the position (xy) and heading (angle) of a wheeled robot. this mpu9150 from invensense has a digital motion processor in it which can give me a quaternion. \nbut how do i convert this quaternion data to an xy-coordinate and an angle so i can plot the position of my vehicle?\n", "tags": "wheeled-robot imu sensor-fusion", "id": "10378", "title": "robot positioning using imu quaternion data?"}, {"body": "i recently bought a dw558 quadrocopter (http://www.gearbest.com/rc-quadcopters/pp_110531.html).\n\nafter few minutes, the battery is dead. which is understandable since the battery is so tiny. it is a 3.7v 250mah battery, included with the quadrocopter. i was thinking about buying spare batteries for it, and i have few questions about this:\n\n\n1: can i buy any kind of battery 3.7v 250mah of the same size or is there any other property i have to pay attention?\n2: can i buy batteries of 3.7v and 350mah (100 more than the included battery) and expect my quadrocopter to be more \"energic\"? is it bad to buy batteries with more mah ?\n2b: if i buy few 3.7v 350mah batteries, will i be able to charge them with the same charger i got with my 3.7v 250 mah batteries or do i have to buy a specific charger for these too?\n\n\n(these are the batteries i want to buy, any comment is greatly appreciated: 350mah batteries x5 http://www.gearbest.com/rc-quadcopter-parts/pp_196991.html and/or 4x 250mah batteries + charger http://www.gearbest.com/rc-quadcopter-parts/pp_331372.html)\n\nthank you very much for your input. i think i just discovered my new hobby and i can't wait to have my spare batteries!\n", "tags": "quadcopter battery", "id": "10379", "title": "battery question for dw558 explorer (small quadrocopter)"}, {"body": "i'm trying to develop an extended kalman filter (ekf) for the positioning of a wheeled vehicle. i have a 'baron' robot frame with 4 static wheels, all driven by a motor. on the 2 rear wheels i have an encoder. i want to fuse this odometry data with data from an 'mpu9150' 9 dof imu. \n\nthis is my mathlab code for the what i call 'medium-size' ekf. this uses data from encoders, accelerometer in x and y axis and gyroscope z-axis.\n\nmedium-size ekf\n\n\n  inputs:   x: \"a priori\" state estimate vector (8x1)\n           t: sampling time [s]\n           p: \"a priori\" estimated state covariance vector (8x8)\n           z: current measurement vector (5x1) (encoder left; encoder right; x-acceleration, y-acceleration, z-axis gyroscope)\n  output:   x: \"a posteriori\" state estimate vector (8x1)\n           p: \"a posteriori\" state covariance vector (8x8)\n  \n  state vector x: a 8x1 vector $\\begin{bmatrix} x \\rightarrow x-position in global frame \\\\ \\dot x \\rightarrow speed in x-direction global frame \\\\ \\ddot x \\rightarrow acceleration in x-direction global frame \\\\ y \\rightarrow y-position in global frame \\\\ \\dot y \\rightarrow speed in y-direction global frame \\\\ \\ddot y \\rightarrow acceleration in y-direction global frame \\\\ \\theta \\rightarrow vehicle angle in global frame \\\\ \\dot \\theta \\rightarrow angular speed of the vehicle \\end {bmatrix}$\n  \n  measurement vector z: \n  a 5x1 vector $\\begin{bmatrix} \\eta_{left} \\rightarrow wheelspeed pulses on left wheel \\\\ \\eta_{right} \\rightarrow wheelspeed pulses on right wheel \\\\ \\dot \\theta_z \\rightarrow gyroscopemeasurementinz-axisvehicleframe \\\\ a_x \\rightarrow accelerometermeasurementx-axisvehicleframe \\\\ a_y \\rightarrow accelerometermeasurementy-axisvehicleframe \\end {bmatrix}$\n\n\n\n\nend\n\nthis is the filter in schematic :\n\n\nthese are the state transition equations i use : \n$$\\ x_{t+1} = x_{t} + t \\cdot \\dot x_{t} + \\frac{t^{2}}{2} \\cdot \\ddot x_{t}$$\n$$\\ \\dot x_{t+1} = \\dot x_{t} + t \\cdot \\ddot x_{t} $$\n$$\\ \\ddot x_{t+1} = \\ddot x_{t} + u_{1} $$\n$$\\ y_{t+1} = y_{t} + t \\cdot \\dot y_{t} + \\frac{t^{2}}{2} \\cdot \\ddot y_{t}$$\n$$\\ \\dot y_{t+1} = \\dot y_{t} + t \\cdot \\ddot y_{t} $$\n$$\\ \\ddot y_{t+1} = \\ddot y_{t} + u_{2} $$\n$$\\ \\dot \\theta_{t+1} = \\dot \\theta_{t} + t \\cdot \\ddot \\theta_{t} $$\n$$\\ \\ddot \\theta_{t+1} = \\ddot \\theta_{t} + u_{3} $$\n\nthese are the observation equations i use :\n\n$$\\ \\eta_{left} = \\frac{t \\cdot n_{0}}{2 \\cdot \\pi \\cdot r} \\cdot \\sqrt{\\dot x^{2} + \\dot y^{2}} + \\frac{t \\cdot n_{0} \\cdot b}{2 \\cdot \\pi \\cdot r} \\cdot \\dot \\theta + n_{1}$$\n$$\\ \\eta_{right} = \\frac{t \\cdot n_{0}}{2 \\cdot \\pi \\cdot r} \\cdot \\sqrt{\\dot x^{2} + \\dot y^{2}} - \\frac{t \\cdot n_{0} \\cdot b}{2 \\cdot \\pi \\cdot r} \\cdot \\dot \\theta + n_{2}$$\n$$\\ \\dot \\theta_{z} = \\dot \\theta + n_{3}$$\n$$\\ a_{x} = \\ddot x \\sin \\theta - \\ddot y \\cos \\theta + n_{4}$$\n$$\\ a_{y} = \\ddot x \\cos \\theta + \\ddot y \\sin \\theta + n_{5}$$\n\nsmall-size ekf\n\ni wanted to test my filter, therefore i started with a smaller one, in which i only give the odometry measurements as input. this because i know that if i always receive the same amount of pulses on the left and right encoder, than my vehicle should be driving a straight line. \n\n\n  inputs:   x: \"a priori\" state estimate vector (6x1)\n           t: sampling time [s]\n           p: \"a priori\" estimated state covariance vector (6x6)\n           z: current measurement vector (2x1) (encoder left; encoder right)\n  output:   x: \"a posteriori\" state estimate vector (6x1)\n           p: \"a posteriori\" state covariance vector (6x6)\n  \n  state vector x: a 6x1 vector $\\begin{bmatrix} x \\rightarrow x-position in global frame \\\\ \\dot x \\rightarrow speed in x-direction global frame  \\\\ y \\rightarrow y-position in global frame \\\\ \\dot y \\rightarrow speed in y-direction global frame \\\\  \\theta \\rightarrow vehicle angle in global frame \\\\ \\dot \\theta \\rightarrow angular speed of the vehicle \\end {bmatrix}$\n  \n  measurement vector z:\n  a 2x1 vector $\\begin{bmatrix} \\eta_{left} \\rightarrow wheelspeed pulses on left wheel \\\\ \\eta_{right} \\rightarrow wheelspeed pulses on right wheel \\end {bmatrix}$\n\n\n\n\nend\n\nodometry observation equations\n\nif you would wonder how i come to the observation equations for the odometry data: \n\n$\\ v_{vl} = v{c} + \\dot \\theta \\cdot b \\rightarrow v_{vl} = \\sqrt{ \\dot x^{2} + \\dot y^{2}} + \\dot \\theta \\cdot b$\n\nproblem\n\nif i try the small-size ekf, using a matlab user interface, it does seem to drive a straight line, but not under a heading of 0\u00b0 like i would expect. eventhough i start with a state vector of $\\ x= \\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\0\\\\0\\end{bmatrix}$ meaning starting at position [0,0] in the global coordinate frame, with speed and acceleration of zero and under an angle of 0\u00b0.\n\n\nin the top right corner you can see the measurement data which i give as input, which is 5 wheelspeed counts on every wheel, every sampling period. (simulating straight driving vehicle)\nin the top left corner you see a plot of the x and y coordinate (from state vector) at the end of one predict+update cycle of the filter, labeled with the timecycle.\nbottom left corner is a plot of the angle in the state vector. you see that after 12 cycles the angle is still almost 0\u00b0 like i would expect.\n\ncould anyone please provide some insights in to what could be wrong here? \n\nsolutions i've been thinking on\n\n\ni could use the 'odometry motion model' like explained in this question. the difference is that the odometry data is inserted in the predict step of the filter. but if i would do this, i see 2 problems: 1) i don't see how to make a small-size version of this for testing purposes, because i don't know which measurements to add in the update-step and 2) for the medium-size version i don't know how to make the observation equations as the state vector doesn't imply velocity and acceleration.\ni could use the 'odometry motion model' and in the update step use the euler-angle, which can be linked to $\\ \\theta $. this euler-angle i can obtain from the digital motion processor (dmp), implemented in the imu. then it is no problem that angular velocity is not in the state matrix. but than i still have a problem with the acceleration observation equations. \n\n", "tags": "kalman-filter imu sensor-fusion odometry", "id": "10382", "title": "need help regarding development of extended kalman filter for sensor-data fusion of odometry and imu data"}, {"body": "currently, i'm using microsoft robotics dev studio, and the visual studio c# programming language to write some code that is able to drive the irobot create 2 on a particular path. moreover, when i run the code in simulation, it works fine, but if i connect to the actual irobot create 2, the code only executes the drivedistance part, and then stops. the problem is that, how come the simulation works, and real robot does not work? \n\nthe following is the code (i edited on \"roboticstutorial4.cs\" file. so, if anyone need additional code, you can just go to mrds sample 4 to see the entire file):\n\n\n", "tags": "mobile-robot irobot-create mrds", "id": "10383", "title": "issue with drivedistance and rotatedegree"}, {"body": "i recently bought a drone(quadcopter). i like how the drone works but now i would like to create an application that will allow me to control the drone remotely from my pc or phone.\n\nhow does a computer or phone interface to an aerial vehicle?\n\nsome of my initial thoughts were \n\n\nget a rf detector and detect what signals are being sent to the drone and replicate it using a new transmitter.\nreplace the control circuit in the drone with an arduino and send corresponding signals to it to fly the drone\n\n\nboth these methods seem to be kind of far fetched and hard, but i'm not sure how this could otherwise be done.\n", "tags": "quadcopter software radio-control wireless", "id": "10387", "title": "how do i interface with a drone?"}, {"body": "i'm trying to take a simple event in which the atlas steps on the ground plane. i want to see which functions ode calls and the functions ode uses to determine the constraint forces. i'd like to see this happen while the simulation is running. is there a way i could do that? i'd like to know what constraint equations and constraint forces ode is using for that particular case. thanks.\n", "tags": "ros dynamics gazebo", "id": "10388", "title": "how to check which gazebo/ode functions are being called?"}, {"body": "i need a distance sensor (ir or optical or any other) with 90 degree view angle to sense a rectangle surface.\nin this case sensor must putting at the same level of surface area. please help me to solve this.\n\n\n", "tags": "mobile-robot", "id": "10392", "title": "view angle of distance sensor"}, {"body": "i have 2 wheels for my robot, and they're both powered by the same battery. yet my robot has a differential motion(as both wheels are running at differnt speeds) why is this?\n", "tags": "mobile-robot motor", "id": "10394", "title": "why my 2 dc motors of same model run at different speeds?"}, {"body": "i have implemented a particle filter algorithm for the state estimation of a mobile robot.\n\nthere are several external range sensors(transmitters) in the environment which gives information on the distance (radius) of the robot based on the time taken for the receiver on the robot to send back its acknowledgement. so, using three or more such transmitters it will be possible to triangulate the position of the robot.\n\nthe particle filter is initialized with 15000 particles and the sensor noise is relatively low (0.02m).\n\nupdate phase: at each iteration a range information from an external sensor is received. this assigns higher weights to the particles along the radius of the external sensor. not all the particles are equally weighted since the process noise is low. hence in most of the cases, the particle relatively closer to the robot gets lower weight than an incorrect one that happens to be along the radius. the weight is a pdf.\n\nresampling phase: at this stage, the lower weighted particle(the correct one) that has negligible weight gets lost because the higher weighted particle gets picked up.\n\nall this happens at the first iteration and so when the range information from another sensor arrives, the robot is already kidnapped.\n\ngoogling around, said that this problem is called as sample impoverishment and the most common approach is to resample only when the particle variance is low. (effective sample size &lt; number of particles / 2)\n\nbut, when the particles are assigned negligible weights and there are relatively very few particles with higher weights, the diversity of the particles are lost at resampling phase. so, when the variance is higher resampling is done which removes the lower weighted particle and hence the diversity of the particles is lost. isnt this completely the opposite of the above idea of ess?\n\nis my understanding of sample impoverishment correct? is there a way this issue can be fixed?\n\nany pointers or help would be highly appreciated.\n", "tags": "mobile-robot localization particle-filter probability", "id": "10404", "title": "addressing the sample impoverishment in particle filter"}, {"body": "i implemented particle filters few years back. i was experimenting few things with particle filters.\ni learned that we can resolve robot kidnapping problem by introducing new particles.\nwhat if we left some particles unsampled e.g. 1% of the population\n\n\nwhat is the interpretation of unsampled particles in this context?\nhow they can effect our localization output?\n\n\n\n", "tags": "localization algorithm particle-filter", "id": "10407", "title": "what is the interpretation of unsampled particles in particle filters?"}, {"body": "i am a third-year electrical engineering student and am working on an intelligent autonomous robot in my summer vacations.\n\nthe robot i am trying to make is supposed to be used in rescue operations. the information i would know is the position of the person (the coordinates of the person in a json file that can be changed anytime except during the challenge) to be rescued from a building on fire. i would also know the rooms of the building from a map, but i don't know where the robot may be placed inside the building to start the rescue operation.\n\nthat means i have to localise the robot placed at an unknown position in a known environment, and then the robot can plan its path to the person who has to be rescued. i can use gyroscope, accelerometer, magnetometer and ultrasonic sensors to do the localising job. i cannot use a gps module or a camera for this purpose.\n\nthe object to be rescued (whose location is known in terms of coordinates &amp; can be changed anytime) is surrounded by walls from 3 sides. hence, adding more walls in this map.\n\naccording to my research particle filter is the best method used for localization of robot. but how can i deal with the landmarks (walls) that are fixed as shown in the map image and that are variable depending on the location of the object to be rescued being provided in the json file?\n\ni can do the path planning from a known position to the target position, but i'm not sure how to determine the starting position.\n\nmore about json file:\n(1) json file containing the coordinates of the object to be rescued can change. (2) it won't change during the challenge. (3) json file will be provided to me in an sd card that my robot has to read. i have successfully written the code that will allow the robot to read the json file and hence the coordinates of the object to be rescued.\n\nhere is the map of the building which is known to me.\n\n\n", "tags": "localization particle-filter", "id": "10408", "title": "localization of a robot to find it coordinates according to the known map"}, {"body": "i have a 6 dof robot arm, and i want to do some object grasping experiments. here, the robot is rigidly mounted to a table, and the object is placed on a different adjacent table. the robot must pick up the object with its gripper parallel to the normal of the object's table, such that it is pointing directly downwards at the point of grasping.\n\nnow, the two tables have adjustable heights. for any given height difference between them. there will be a fixed range of positions over which the robot arm can achieve this perpendicular pose. what i am trying to figure out, is what the optimum relative distance of the tables is such that this range of positions is maximum.\n\nis there a way to compute this analytically given the robot arm kinematics? or is there a solution which applies to all robot arms (e.g. it is optimum when the tables are at the same height)?\n\nif it is important, the arm is the kinova mico: https://www.youtube.com/watch?v=gurjtumivko.\n\nthanks!\n", "tags": "robotic-arm kinematics", "id": "10415", "title": "determining the graspable range of a robot arm"}, {"body": "i'm currently doing some research on collaborative robotics. one area of interest is the type of sensor(s) used in these kind of machines. \n\ni had a look at some robots by fanuc and universal robots and i've noticed that they do not come equipped with sensors; they are sold as an add-on.\n\nis this inherent with collaborative robots? do customers need to buy sensors as an add-on - which has advantages and disadvantages. \n\nthanks for your help.\n", "tags": "sensors industrial-robot", "id": "10416", "title": "sensors in collaborative robots"}, {"body": "i am trying to make a ir distance sensor. i have seen this online. my goal however is to see the distance between a ir transmitter and my ir sensor. in the example above he uses the ir led's ambient light and timing to track the distance. is there a way to find the distance between lets say a ir remote and a sensor? it would only have to be accurate to about 1 meter. i am also open to other ideas of accurately tracking distance between two objects weither that be bluetooth/ir/ultrasonic \n", "tags": "sensors", "id": "10425", "title": "ir distance sensor"}, {"body": "i'm programming a flight controller on an arduino. i've researched how other people have written theirs but without notes it's often so obfuscated that i've decided it will be easier and better to write my own.\n\nthis is my pseudocode thus far, will this work?\n\nall of this will happen inside the constant arduino loop\n\n\nread rx signal\ncalculate desired pitch, roll, and yaw angles from rx input\nsignal escs using pwm in order to match desired pitch, roll, and yaw from rx input\ngather imu values (using kalman filter to reduce noise)\ncompare filtered imu values vs. rx input to find errors in desired outcome vs. actual outcome\nuse pid algo to settle errors between imu vs. rx\nrinse and repeat\n\n\nsuggestions are greatly appreciated\n", "tags": "arduino microcontroller uav", "id": "10426", "title": "will this pseudocode work as a basis for a flight controller?"}, {"body": "my team has been working on a wearable glove to capture data about hand movements, and use it as a human-computer interface for a variety of applications. one of the major applications is the translation of sign language, shown here: https://www.youtube.com/watch?v=7kxrztdo39k\n\nright now we can only translate letters and numbers, because the signs for them require the person to hold their hand still in one position ('stationary' signs). i want to be able to translate words as well, which are non-stationary signs. also the position of the hands really matters when signing words, for example it matters whether the hand is in front of the forehead, eyes, mouth, chest, cheeks, etc.\n\nfor this we need a portable and highly accurate position sensor. we have tried getting position from a 9-dof imu (accelerometer, gyroscope, magnetometer) but as you might guess, there were many problems with double integration of the noise and accelerometer bias.\n\nso is there a device that can provide accurate position information? it should be portable and wearable (for example worn in the chest pocket, headband/cap, etc...be creative!).\n\nedit (more details):\n\ni'm going to emphasize certain aspects of this design that weren't clear before, based on people's comments:\n\n\nmy current problem of position detection is due to errors in double integration of the accelerometer data. so hopefully the solution has some incredibly powerful kalman filter (i think this is unlikely) or  uses some other portable device instead of an accelerometer.\ni do not need absolute position of the hand in space/on earth. i only need the hand position relative to some stable point on the body, such as the chest or belly. so maybe there can be a device on the hand that can measure position relative to a wearable device on the body. i don't know if such technology exists; i guess it'd use either magnets, ultrasound, bend sensors, or em waves of some sort. be creative :)\n\n", "tags": "sensors sensor-error precise-positioning", "id": "10427", "title": "is there a portable and accurate sensor to measure the position of the hand relative to the body?"}, {"body": "i'm reading this paper: http://arxiv.org/abs/1310.2053 (the role of rgb-d benchmark datasets: an overview) and see the following words:\n\n\n  thanks to accurate depth data, currently published papers could\n  present a broad range of rgb-d setups addressing well-known problems\n  in computer vision in which the microsoft kinect ranging from slam\n  [10, 12, 19, 17, 35, 11] over 3d reconstruction [2, 33, 38, 32, 1]\n  over realtime face [18] and hand [30] tracking to motion capturing and\n  gait analysis [34, 41, 8, 7, 4]\n\n\ni thought of the term slam and 3d reconstruction being the same thing, while the paper says the opposite with a bunch of citations (which still haven't tell the two apart).\n\nin my opinion, mapping in slam is the same term as 3d reconstruction, while localization is the essential part for mapping. so i don't find a difference between slam and 3d reconstruction, am i wrong (or is the author misclassfying)?\n", "tags": "slam 3d-reconstruction", "id": "10429", "title": "difference between slam and \"3d reconstruction\"?"}, {"body": "i went to go install ros for my rassberry pi and found that there are 5 different variants. what is the difference between them and where can i go to learn about these differences for future updates?\n\nlink to the rosberrypi downloads i'm talking about:\nhttp://wiki.ros.org/rosberrypi/setting%20up%20ros%20on%20raspberrypi\n", "tags": "ros raspberry-pi", "id": "10436", "title": "what is the difference between rosberry pi builds?"}, {"body": "am i correct in assuming that the i-robot create 2 does not have the 25-pin connector like the original version of i-robot create has? thanks much...rick\n", "tags": "irobot-create", "id": "10438", "title": "i-robot create 2 connectors"}, {"body": "i'm trying to detect obstacles for a distance of up to 10 meters in an outdoor environment. up to meaning that i also want to be able to detect obstacles that are close to the robot. i am thinking of doing this using stereo vision, but i am unsure if this is in fact even possible (before i buy expensive hardware). so is it possible? has anyone had any success?\n\nif this isn't possible, then what kind of sensors could give me a decent point cloud for such a range (outdoors)? i need a sensor that will fit a medium size robot. also it needs to be not overly expensive since i have a limited budget.\n\nthanks\n", "tags": "sensors stereo-vision", "id": "10443", "title": "stereo vision for outdoor obstacle detection"}, {"body": "i reference the following article.\nhttp://www.egr.msu.edu/classes/ece480/capstone/spring15/group02/assets/docs/nsappnote.pdf\n\ni have followed article's code, \nbut it appears:\n\n\n\nhow can i solve this problem?\n", "tags": "irobot-create", "id": "10445", "title": "nameerror: name 'tk' is not defined"}, {"body": "i am trying to install ros kinetic kame in ubuntu 16.04 , but after trying the first step setup your sources. list. \ni am getting  cannot create /etc/apt/sources.list.d/ros-latest.list: permission denied what to do now\n", "tags": "ros irobot-create", "id": "10447", "title": "unable to install ros kinetic in ubuntu 16.04"}, {"body": "i would like to use gps data as measurement input for an extended kalman filter. therefore i need to convert from gps longitude and lattitude to x and y coordinate. i found information about the  equirectangular projection given these formulas: \n$$\\ x = r_{earth} \\cdot \\lambda \\cdot cos(\\phi_0) $$\n$$\\ y = r_{earth} \\cdot \\phi $$\n\nhowever i think these formulas are only for use when the axis x- and y-axis of my local frame are parallel to north and south axis of the earth.\n\nbut my vehicle is starting in my local reference frame in the origin and heading straight in y-direction. in whatever compas angle i put my vehicle, this should always be the starting position. \ni can measure the angle $ \\alpha $ to north with a compass on my vehicle.\n\nnow what is the relationship between (longitude,lattitude) an (x,y) of my local frame?\n", "tags": "kalman-filter gps", "id": "10450", "title": "conversion gps (longitude,latitude) to (x,y) of local reference frame?"}, {"body": "i read a paper from 2015, \"structural bootstrapping - a novel, generative\nmechanism for faster and more efficient acquisition of action-knowledge\n\", which introduces a concept called, \"structural bootstrapping with semantic event chains and dynamic movement primitives,\" which confused me a little bit. \n\naccording to my knowledge a robotarm is controlled by a pddl-like planner. the pddl file is a \"qualitative physics engine\" which can predict future events. the paper says the \"qualitative physics engine\" consists of dynamic movement primitive (dmp) which are learned from motion capture data. \n\nmy question is: how can a dmp be used for simulating physics?\n", "tags": "motion-planning algorithm machine-learning", "id": "10454", "title": "how can a dmp be used for simulating physics?"}, {"body": "is it possible to replace just the wheels on the create2 robot? is it a standard shaft/coupling?\n", "tags": "irobot-create roomba", "id": "10455", "title": "replacing just the wheels on create2"}, {"body": "is it possible to measure the voltage of 2 different batteries on arduino? currently i am able to use a resistor divider / voltage divider of 2x 10k resistors to an analog pin to read the voltage of the battery supplying the arduino.\n\ncurrently the system looks like 6v battery -> 5v power regulator to arduino -> resistor divider attached to 6v (unregulated) battery. gnd is common throughout.\n\nhow could i measure the voltage of another battery given that it will be on a different circuit? e.g. different ground loop.\n", "tags": "arduino power", "id": "10461", "title": "measure 2 diffrent battery voltages on arduino"}, {"body": "i am trying to find the name (nomenclature) of the linkage (or carriage) that is being driven by the dual linear servo (actuator) arrangement in the following youtube videos:\n\n\nservo basic concepts\nyoutube - 4 x linear servo application\n\n\nthe linkage (carriage) appears to be able to rotate about a 180 degree arc.\n\nwhat is this metal linkage (or carriage) system called? \n\n", "tags": "mechanism arm", "id": "10462", "title": "name of the linkage (or carriage) in video"}, {"body": "i have 2 wheeled differential drive robot which i use pid for low level control to follow line. i implemented q learning which uses samples for 16 iterations then uses them to decide the best position to be on the line so car takes the turn from there. this allows pid to setup and smooth fast following. my question is how can i setup a reward function that improves the performance i.e. lets the q learning to find the best\n\nedit\nwhat it tries to learn is this, it has 16 inputs which contains the line positions for the last 15 iterations and this iteration. line position is between -1 and 1 which -1 means only left most sensor sees the line and 0 means the line is in the center. i want it to learn a line position that when it faces this input again it will set that line position like its the center and take the curve according to that line position. for example error is required position - line position so let say i had 16 0 as input then i calculated the required as 0.4. so after that the car will center itself at 0.4 i hope this helps :)\n\nyou asked for my source code i post it below\n\n\n\n}\n\nmy sensor reading returns a value between -1.0f and 1.0f. 1.0f means outer sensor on the right is only the line. i have 8 sensors.\n\n\n\n}\n\nhere is the logic for the value for qpredictor(i call the learning part as this).and finally qpredictor\n\n\n\ni now only have average error and currently not using learning because it's not complete without reward function. how can i adjust it according to the dimensions of my robot ?\n", "tags": "machine-learning", "id": "10463", "title": "reward function for q learning on a robot"}, {"body": "when i execute  following code,\n\n\n\nbut errors,\n\n\n\n1532 line program:\n\n\nhow can i solve?\n", "tags": "irobot-create python", "id": "10467", "title": "nameerror: global name 'sendcommandascii' is not defined"}, {"body": "given a desired transform matrix of the end effector relevant to the base frame of the p560:\n\n\njohn j. craig, in his book, introduction to robotics\nmechanics and control, computes the inverse kinematic solutions of a puma 560, with (correct me if wrong) modified dh parameters and gets the following set of equations for theta angles:\n\n\nand i noticed that there are no alpha angles in these calculations.\n\nso my question is why aren't the alpha angle values not used in the calculation for the desired pose with the given end effector transform. why is it independent of the axes twist angles of the robot?\n", "tags": "inverse-kinematics", "id": "10468", "title": "inverse kinematics computation -- why are alpha angle values not included"}, {"body": "i am working in the field of automated vehicles mainly in the domain of passenger and commercial vehicles. i have been studying whatever i can get regarding the measurement the state (relative position, relative velocity, relative heading and roation, a.k.a. yaw rate) of surrounding objects especially other vehicles using sensors.\n\nwhile everything else is possible to measure precisely using on-board sensors, i have found out that not much literature is available for measuring the vehicle heading and yaw rate of other vehicles which is baffling to me given the extreme precision of laser based sensing (albeit using time stamps).\n\ni am looking for:\n\n\nreference to literature with experiments for estimation of yaw rate and vehicle heading.\nas i can see from the literature available (or the lack thereof), no direct way of measuring yaw rate is available but by using lidar or camera with consecutive time stamps or scans of data. however, this inherently requires the data to be correct. hence, i would think that due to the inaccuracies involved, this method is not used! is this correct?\nare there any commercially available sensors that give accurate heading and yaw rate information of other vehicles?\n\n\nsources and research papers would be most welcome!\n\nedit: by this inherently requires the data to be correct i mean, given the high sensitivity to error in heading or yaw rate at high vehicle speeds, the values computed using sensor information is not accurate enough to be put to use in practice!\n", "tags": "localization", "id": "10469", "title": "heading and yaw rate measurements"}, {"body": "i am not able to clearly differentiate between the two platforms:\n\n\nroboearth, and;\nknowrob.\n\n", "tags": "theory cloud", "id": "10471", "title": "what is difference between roboearth and knowrob?"}, {"body": "i am having issues with bringing the robot out of its sleep or off mode. seems it goes into sleep mode when there is no activity for about 4 minutes. i am using the i-robot create 2 serial cable. when it is in its sleep mode i try removing the cable end plugged into robot and connect jumper wire between pins 5 and 6 on the robot 7 pin connector for a brief time period. this effectively shorts the brc pin to gnd for a short period of time ( less than 1 second). then i reconnect the serial port cable into the robot 7 pin connector and try giving the robot a command but no go. i have also read that commands 173 and 173 173 can help with this issue but i may be mistaken. any help on this is very much appreciated !!!! rick\n", "tags": "irobot-create", "id": "10472", "title": "i-robot create 2 reset after sleep issue?"}, {"body": "i was planning on using the odometry model in the prediction stage of an extended kalman filter.\nstate transition equations:\n$$ f(x_t,a_t) = \\begin{bmatrix}\nx_{t+1} = x_t + \\frac{\\delta s_r + \\delta s_l}{2} \\cdot \\cos(\\theta_t) +u_1\n\\\\ y_{t+1} = y_t + \\frac{\\delta s_r + \\delta s_l}{2} \\cdot \\sin(\\theta_t) + u_2\n\\\\ \\theta_{t+1} = \\theta_t + \\frac{\\delta s_r + \\delta s_l}{b} \\cdot \\sin(\\theta_t)+u_3\n\\end{bmatrix} $$\nwith $\\delta s_r$ and $\\delta s_l = \\frac{n}{n_0} \\cdot 2 \\cdot \\pi \\cdot r$\n\n\n  $x_t = \\begin{bmatrix} x_t &amp; y_t &amp; \\theta_t\\end{bmatrix}^t$ state matrix containing xy-coordinate and heading $\\theta$ of vehicle in global reference frame\n\n\n\n\n\n  $\\delta s_r$ and $\\delta s_l$ distance travelled by respectively right and left wheel\n\n\n\n\n\n  $b$ distance from center of the vehicle to the wheel\n\n\n\n\n\n  $n$ encoder pulses count during sampling period t\n\n\n\n\n\n  $n_0$ total pulses count in 1 wheelturn\n\n\n\n\n\n  $r$ wheel radius\n\n\n\n\n\n  $u_1,u_2$ and $u_3$ random noise n(0,$\\sigma^2$)\n\n\n\n\nnow i doubt if this noise indeed does have a zero mean?\nwheelslip will always make the estimated distance travelled shorter than the measured distance isn't it?\n", "tags": "kalman-filter odometry noise", "id": "10474", "title": "odometry motion model for kalman filter, but is the error zero mean?"}, {"body": "in the design of an extended kalman filter for the position estimation of a vehicle, i am searching for the observation equations for inserting gps data (longitude, latitude) into the update step of the filter.\n\nthe state vector of my filter $x_t = \\begin{bmatrix} x_t &amp; y_t &amp; \\phi_t\\end{bmatrix}$ contains the x and y coordinate of the vehicle in the local reference frame and the angle under which the vehicle is standing relative to the x-axis.\n\n\n\nthe observation equations should look like this:\n$$h(x_t) = \\begin{bmatrix} longitude = f(x_t,y_t) \\\\ latitude = f(x_t,y_t)\\end{bmatrix} $$\n\ncan anybody fill them in?\n", "tags": "kalman-filter gps", "id": "10479", "title": "gps observation equations for kalman filter?"}, {"body": "i'm trying to get the equations of motion of a 3d pendulum system (spherical pendulum), however i don't want to describe the system using spherical coordinates (which there is lot of information about). \n\ninstead i want to describe the system using x,y,z coordinates of the mass as well as the euler angles phi, theta, psi (the roll, pitch and yaw of the mass). \nthat is, i want to assume that the mass has a position and an orientation in relation to the inertial frame. \n\nfurthermore this is a 3d pendulum system, where the mass, which is symmetric, is actuated (notice that this is a simplification of the system needed to actuate the mass, where only the resulting forces and torques are taken into account):\n\n\nthere is a force f acting in the x-axis direction of the mass reference frame\nthere is a torque t acting about the z-axis of the mass reference frame\nthere is also the gravitational force acting on the center of mass (in the negative direction of z-axis of the inertial reference frame\n\n\nin order to clear misconceptions about how this forces are generated, think of the mass as a differential drive robot using fans instead of wheels.\n\nthe cable connecting the mass to the anchor point is assumed rigid and works as a distance constraint modeled by:\n$\\|r_{anchor} - r_{mass}\\| = cable\\_length$\n\nwhere $r_{anchor}$ is the position of the anchor point to which the mass is connected through the cable and $r_{mass}$ is the position of the mass. this constraint makes it so that it is similar to having two spherical joints: one at the anchor point and another at the mass. futhermore the cable is assumed to have no mass. \nit's important to note that this rigid connection (calbe) is meant to be modeled by the distant constraint refered above.\n\n\n\ni'm looking for help solving this system to obtain its equations of motion.\nthanks in advance\n", "tags": "kinematics dynamics", "id": "10484", "title": "equations of motion of 3d pendulum-like system"}, {"body": "\n\ni am trying to control a dobot arm.  the arm moves with angles whereas i need to work with cartesian coordinates. from inverse kinematics equations and polar coordinates i have implemented x,y and z coordinates working very well on their own. but i can not combine the coordinates in order to work all together. when i add them up it is not going to the desired place. how can i combine these coordinates? i got some help from (https://github.com/maxosprojects/open-dobot) but could not manage to successfully move dobot. \n\nedit: i've written the codes in qt and also i've added the triangles used for angle calculations.\n\n\n\n\n", "tags": "robotic-arm inverse-kinematics c++", "id": "10486", "title": "combine individually working cartesian coordinates"}, {"body": "as the title briefly explains, my question is, what type of motor is powerful enough be on a cycle?\n\nmy plan is to convert a cycle into an electric bike by mounting a motor and controlling it through either a raspberry pi or an arduino board.\n", "tags": "motor electric", "id": "10487", "title": "what type of motor can be hooked up on a bike?"}, {"body": "i recently thought about building a lab bench power supply, it comes in cheaper and i love to build things...\n\nbut then i also have a lipo charger an imax b6ac, that i had bought for my quadcopter, then came the idea of whether i can use the charger as a lab bench power supply...\n\nmy questions is, could this work and how could i make it work?\n", "tags": "power", "id": "10490", "title": "is it possible to use a lipo charger as a lab bench power supply?"}, {"body": "zeno behaviour or zeno phenomenon can be informally stated as the behavior of a system making an infinite number of jumps in a finite amount of time.\n\nwhile this is an important control system problem in ideal systems, can zeno behavior exist in real systems? any examples?\nif so, why don't noise or external factors deviate a system from achieving zeno?\n", "tags": "mobile-robot control", "id": "10491", "title": "examples of zeno behaviour in the real world"}, {"body": "why don't cheap toy robotic arms like this move smoothly?  why can't it even move itself smoothly, (even without any load)?\n\nin other words - what do real industrial robotic arms have, that cheap toys don't?\n", "tags": "robotic-arm", "id": "10492", "title": "why don't cheap toy robotic arms move smoothly?"}, {"body": "i want to basically make a pin matrix controlled either by spring, electromagnets or small motors(spring being the most viable option), something like what\u2019s shown in the image. i'm pretty new to arduino and hardware in general so any input would be appreciated.\ni mostly know the arduino end but don't have clue about the hardware part. plus i don't have the technical expertise, as in i know electromagnets won't be a good option as i have to control individual pins and not clusters. plus springs have the disadvantage of pushing them back in but other than that a very option. and its not viable to have individual motors for so many pins.\n\n\n", "tags": "arduino motor electronics", "id": "10502", "title": "what are good, low cost, actuators for a braille tablet to be controlled by arduino?"}, {"body": "is it possible to convert a cycle into an electric bike by using brushless outrunner motors that usually are for rc planes, multicopter, helicopter, etc?\n\nif it is possible, what specs do my motors need to be to provide enough power to bring my cycle to speed?\n\nwill i need a gear system?\n", "tags": "motor power electric", "id": "10503", "title": "running a cycle on brushless outrunner motors?"}, {"body": "i'm using sparkfun razor imu 9dof sensor which incorporates accelerometer, gyroscope, and magnetometer, for giving the euler's angles (yaw, pitch, and roll). i'm using the firmware at this link. it has processing sketch for calibration of magnetometer, but it doesn't give the precise measurements. especially, the yaw is imprecise. i'm using this sensor for measuring azimuth and altitude of stellar objects. the altitude is mostly correct, but azimuth (yaw) isn't. \n\ni have several questions:\n\n\nis there a better way to calibrate magnetometer? is the calibration sufficient, without using the madgwick or kalman filter?\nis there some nonlinearity present in the sensor? since the yaw offset isn't constant, it changes (around -12 degrees up north to almost correct value at southwest). and if it is, how could i measure that nonlinearity and apply to the yaw measurements?\nif i have to use madgwick or kalman, do i have to apply them on quaternions? i believe that applying them at the final yaw measurements wouldn't do the job.\n\n", "tags": "kalman-filter imu calibration magnetometer precise-positioning", "id": "10504", "title": "how to properly calibrate a magnetometer in imu for precise yaw?"}, {"body": "i have a localization data estimated and gps_truth and generated [3x3] covariance matrix along with them. \n\nwhat i would like to do is to see if the covariance is correct or not? \n\ncan we check it by plotting the covariance? \n", "tags": "localization slam visualization", "id": "10505", "title": "covariance check?"}, {"body": "i have the actual dh parameters of a robot:\n\n\n\nall other di's and ai's are zero.\n\ncan i use these for an inverse kinematics analytic closed form computation or should i measure the virtual distances in the 3d environment?\n\ni am actually asking if the theta angles that will be yeld after the computations are dependent on the scale of those distances.\n\nedit: scale factor is unknown\n", "tags": "inverse-kinematics dh-parameters 3d-model", "id": "10507", "title": "do dh parameters change for a scaled robot 3d model?"}, {"body": "i am planning to build a scale version of an airbus a340 as the title suggests and i have a few questions with the build...\n\n\nare there any template/plans for this build? if so please reply with the links...\ncan i control the plane with a cc3d revolution and fly it with ground station? if there are any ground telemetry application that will help me, please send me the link...\nhow do i make the landing gears? i don't really fancy spending above \u00a350 (i live in london)... i want to make all my landing gears shock absorbing and i want to make the nose gear have a steering mechanism with servos...\nhow do i make the outer shell for the edf motors? \n\n\ni am ready to spend as much time and effort with this build, i want the build to not be very costly, this is going to be a hobby. and i have also built a quadcopter, so i hope i can apply that knowledge to this build...\n", "tags": "servos", "id": "10508", "title": "building an rc airbus a340"}, {"body": "as a hardware engineer, i have studied quite a lot on sensor spec such as accel, gyro and magnetometer including custom made fluxgate. i have studied matrix and quadarion (complex number) and so on.\ni moving into calibration arena, i seen lot of article on calibration but not sure which is best solution to fix offset and mis-alignment axis. can anyone point to best open source code.\ni'm not interested in output results related to flight such as quadchopter and gps, but more interested in directional math for drilling pipes, where toolface, inclination, azimuth and position is most important. what the best thesis or paper that cover this topic and open-source or example code (in c) for this application. do i need kalnman filter or such advance post data capture processing. any tip how to avoid getting too involved with maths\n", "tags": "control kalman-filter imu calibration precise-positioning", "id": "10511", "title": "accelerometer, gyro, and magnetometer sensor fusion for material resource survey"}, {"body": "i have some measurements of a real life robot, and a 3d model of that robot (lets say in unity) and i want to know the scale factor, plus i dont want to find the 3d models measurements and then divide with the real world ones to find the scale factor, in order not to get more confused with more mathematics than i am already. so, is there a methodology or will i have to do it as i mentioned?\n", "tags": "3d-model", "id": "10514", "title": "scale factor of a 3d robot model relative to the real measurements of a robot"}, {"body": "i want to know what all essential components i need to have on a multirotor inorder to have a 'follow me' mode (with,me carrying a device/reference piece to track).? \n", "tags": "localization", "id": "10515", "title": "basic components for having a 'follow me' mode on quad?"}, {"body": "i can't find the package, in the ros site , i saw it not maintainable.\n\nhow can i migrate the package from (jade/indigo) to became a package in kinetic?\n\nthat important because it's dependency for my quad-gazebo pacakge\n", "tags": "ros", "id": "10516", "title": "can't find ros package in kinetic driver-base"}, {"body": "i am building a robotic arm with these specifications:\n\n\n1 meter long\napprox. 1kg weight\nit is made of 4 motors. (2 at the base. one for rotating the whole arm left and right and another one for rotating up down. 1 motor for rotating the second half of the arm, only up and down. 1 for the claw used for grabbing)\nit must be able to lift at least 4kg + 1kg (it's own weight), and have a speed of 180 degrees in 2 seconds = 360 degrees in 1 second resulting 60rpm.\n\n\nwhich kind of motor would be best for the project (servo or stepper) and how much torque will it need? (please also give me an explanation of how i can calculate the torque needed). could you also give me an example or two of the motors i would need and/or a gearbox for them (models or links).\n\n\n", "tags": "motor robotic-arm stepper-motor servos torque", "id": "10517", "title": "which kind of motors and how powerfull should they be for a robotic arm"}, {"body": "i'm researching dynamic gaits for bipedal robots. can anyone recommend a reference that reviews and explains the math behind modern approaches?\n", "tags": "walking-robot", "id": "10519", "title": "math for dynamic gait"}, {"body": "i am currently working on a slam-like application using a variable baseline stereo rig. assuming i'm trying to perform visual slam using a stereo camera, my initialization routine would involve producing a point cloud of all 'good' features i detect in the first pair of images.   \n\nonce this map is created and the cameras start moving, how do i keep 'track' of the original features that were responsible for this map, as i need to estimate my pose? along with feature matching between the stereo pair of cameras, do i also have to perform matching between the initial set of images and the second set to see how many features are still visible (and thereby get their coordinates)? or is there a more efficient way of doing this, through for instance, locating the overlap in the two points clouds?\n", "tags": "slam computer-vision stereo-vision", "id": "10520", "title": "once matching features are computed between a stereo pair, how can they be tracked?"}, {"body": "i have a robot with tracks. one of the tracks broke and i need to find a replacement, the tracks use the same plastic interconnects/pieces as this:\n\n\n\nthey were very popular years back. does anyone know the brand/name?\n", "tags": "mobile-robot tracks", "id": "10521", "title": "help finding robot tracks"}, {"body": "we need to determine the 2d position of my robot. to do so, we have a lidar at a known high, with an horizontal plane, which gives us the distance to the nearest point for each angular degree (so 360 points for one rotation). the environment is known, so i have a map of the position of every object that the lidar is susceptible to hit.\n\nmy question is, based on the scatter plot that the lidar is returning, how can we retrieve the position of my robot in the map ? we would need the ,  position in the map frame and also the  angle from the map frame to my robot frame.\n\nwe have tried to match objects on map with groups of points based on their distance between each other and by identifying those objects and the way the lidar \"sees\" them to retrieve the robot position. but it is still unsuccessful.\n\nto put it in a nutshell, we want to make  without the mapping part. how is it possible, and with what kind of algorithms ?\n\na first step could be to stop the robot while acquiring data if it seems easier to process.\n", "tags": "localization lidar precise-positioning", "id": "10523", "title": "determining position from a 2d map and lidar"}, {"body": "i experienced some drifting when coming near to magnetic fields with my imu, so i wondered if it is possible to completely shield the imu from external influences. would this be theoretically possible or does the imu rely on external fields like the earth magnetic field? are there maybe alternatives to imus that are less susceptible to magnetic interferences? i only need the rotational data of the sensor and don't use translational output.\n", "tags": "sensors imu rotation", "id": "10525", "title": "shield imu from magnetic interferences"}, {"body": "i'm currently designing an autonomous robotic system to manipulate clothes using computer vision and complex moving hardware. my current design incorporates quite a number of moving parts. my biggest worry is a frame (140 x 80 x 40 cm) rotates from 0 to 90 degrees every time it manipulates a piece of cloth. other than this the design involves various other moving parts to achieve successful manipulation of the cloth. it seems like the hardware is capable of achieving the task despite the high number of complex and moving parts. \n\nso the question is, what are the design considerations i should take in designing an automated system. should i think of a alternative design with less number of parts? or proceed with the current design if it does the job>? sorry i am in a position where i can't disclose much information about the project. \n\nthank you.   \n", "tags": "computer-vision automation", "id": "10527", "title": "is it a bad-design decision to implement high number of moving parts in an automation-robot?"}, {"body": "i want to make a simple arduino based programmable insect robot.\n\ni want it to walk on legs and legs will be made of hard aluminum wire. it needs to have 4 legs. i am planning to use arduino nano for that. i just had few questions like:\n\n\nhow to arrange servos and wire to have motion?\ni also want it to turn sideways?\nwhere can i read good theory on making insect like robots?\n\n", "tags": "arduino", "id": "10533", "title": "how to make a simple arduino insect robot?"}, {"body": "i recently came across this doubt... as the title suggests what's the difference between the two flight controller. they have a big price difference and size difference, that's all i know...\n\ndo they all function the same way, so does the two flight controller differ in size?\n\nanswers appreciated,\n\nsid\n", "tags": "quadcopter microcontroller", "id": "10536", "title": "what is the difference between cc3d revolution mini and cc3d revolution"}, {"body": "i am currently trying to implement a graphslam/sam algorithm for lidar. from papers i've read, you generate a directed graph from expected lidar measurements to landmarks similar to the image below (taken from the square root slam paper by dellaert).\n\n\n\nhowever in practice the point clouds you obtain from lidar are similar to this (taken from the kitti car collected dataset):\n\n\nit seems algorithms such as sift for 3d point clouds aren't as accurate yet. is there a commonly used technique to efficiently find features in consecutive point clouds to find landmarks for slam algorithms without using >30,000 points in a point cloud?\n", "tags": "localization slam lidar", "id": "10537", "title": "lidar points as landmarks"}, {"body": "ok, this may be a simple question, but here goes. i would like to build some type wheeled vehicle that can sustain a tripod mount but also be programmable to follow a programmed path. however, i would like to be able to change this path as well. \n\nthis started with trying to mount my ricoh theta s on an rc car to create 3d video of a room. i'm really not familiar with any type of robotics but i'm convinced it's possible to create something better... more efficient if you will... i've looked at drones, but for what i would like to do i don't think i can find something as precise as i would like (i could be completely wrong). \n\nany guidance on this is extremely appreciated.\n", "tags": "mobile-robot", "id": "10544", "title": "programmable wheeled vehicle"}, {"body": "i'd like to build a robot that can move to different places(like different rooms) in a floor based on the input that i'm giving to it dynamically. i've surfed about it and i don't need a line follower. except this, can you give me a way to implement this ??   thanks in advance :) \n", "tags": "wheeled-robot", "id": "10546", "title": "is it possible to implement a robot that moves to spcific locations based on dynamic inputs?"}, {"body": "i am building a robot and i want to be able to hear sounds from it's environment (ideally from my laptop). what is the best way to get live audio from my robot's microphone to my computer?\n\ni have looked into a few solutions for hosting live audio streams using packages such as  and . i'm just wondering about better solutions for robotic applications.\n\nadditional details:\n- i have access to hardware such as raspberry pi, arduino, etc.\n", "tags": "real-time digital-audio", "id": "10547", "title": "how to get live audio from robot?"}, {"body": "i am building a pi car with 4 gear motors (190-250mah each max). now i want to use my 10000mah usb power bank to power up raspberry pi along with the 4 gear motors.\n\ni can power up the raspberry pi directly but i want to use my power bank as the only source of power for the pi car. how can i connect both my rpi and motor controller l298n to my usb power bank?\n", "tags": "motor raspberry-pi power", "id": "10549", "title": "power solution for raspberry pi robot"}, {"body": "i want to create an amateur wire looping machine with arduino, that has similar functionality than this machine. i don't need the automatic wire feeding as for my purposes this part can be done manually. i just want to automate the wire loop creation process, assuming that i already have straight wires.\n\ni'm new to the world of motors, robotics, etc., so please be as descriptive as possible :)\n\nfrom the video, i can tell that there are two motors:\n\n\nmakes the initial wire bending\nspins to create the loop\n\n\nthe wire that i will be working with is galvanized steel of 11 gauge (2.0 - 2.5 mm diameter).\n\nso what type of motors would be recommended for this application, taking into account:\n\n\nthey need to be accurate in their positioning for repeat ability\nthey need to have enough torque (specially the one that creates the loop itself) to work with this type of material\nthey're as fast as (or close to) the ones in the video\nthis is not going to be an industrial grade machine that will be running all the time\nideally, they need to be not that expensive.. i don't want to be bankrupt by the end of this project :)\nif links can be included for recommended products, it would be great.\n\n\nthanks!\n", "tags": "motor", "id": "10554", "title": "what types of motor should i use for a particular application?"}, {"body": "i have \"tarot zyx-gs 3-axis gimbal stabilization system zyx13\" sensor that gives me the value of roll tilt and pan.the 3 axis accelerometer give me the value of x y and z.so can we use the gimbal stabiliztion system in place of accelerometer\n", "tags": "quadcopter sensors accelerometer", "id": "10555", "title": "3 axis gimbal stabilization system can replace with 3 axis accelerometer"}, {"body": "i'm starting out with robotics, got my first dc gear motor with quadrature encoder (https://www.pololu.com/product/2824):\n\n\n\ni ultimately plan to hook it up to a motor driver connected to a tiva launchpad. however, since i'm a noob, and curious, i am starting by just playing with it with my breadboard, oscilloscope, and voltage source. e.g., when i plug in the motor power lines into my (variable) voltage source the axis spins nicely as expected between 1 and 12 v.\n\nthe problems start when i try to check how the encoder works. to do this, first i plug a a 5v source into the encoder gnd/vcc, and then try to monitor the encoder output.\n\nwhile the motor is running, i check the yellow (encoder a output) cable (referencing it to the green (encoder gnd) cable).  i made a video that shows a representative output from one of the lines (no usb on my old oscilloscope so i took a video of it using my phone).\n\nas you would see at the video, the output doesn't look anything like the beautiful square waves you typically see in the documentation. instead, it is an extremely degraded noisy sin wave (at the correct frequency for the encoder). the amplitude of the sin is not constant, but changes drastically over time. strangely, sometimes it \"locks in\" and looks like the ideal square wave, for about a second or two, but then it gets all wonky again.\n\nboth of the lines (encoder a and b output) act this way, and they act this way at the same time (e.g., they will both lock in and square up at the same time, for those brief glorious moments of clarity). both of my motors are the same, so i don't think it's that i have a bad motor.\n\ni have also checked using vcc=12v, but it made no difference other than changing the amplitude of the output.\n\nnote i already posted this question at reddit:\nhttps://www.reddit.com/r/robotics/comments/502vjt/roboredditors_my_quadrature_encoder_output_is/\n", "tags": "motor quadrature-encoder", "id": "10556", "title": "quadrature encoder signal from dc motor is very noisy"}, {"body": "i have 3d gimbal system and i want to use this sensor in place of imu(3 axsis accelerometer) in quadcopter \n", "tags": "quadcopter accelerometer gyroscope", "id": "10561", "title": "can i use a 3d gimbal system as a simplistic quadcopter imu(3 axis accelerometer)?"}, {"body": "for this robot the gear attached to the motor is linked to the gear attached to the wheels by a bicycle chain (i am using a bicycle wheel and transmission set as the parts for the robots movements).\n\nhow does using a bicycle chain affect the power transmission efficiency, how does this impact the torque?\n", "tags": "mobile-robot torque gearing", "id": "10562", "title": "what is the torque transmission efficiency using a bycicle chain/setup for robot?"}, {"body": "i came across robotics library (rl), but quite unclear about its real purpose. is it a fk/ik solver library or simply an graphical simulator?. rl has poor documentation, so its not clear how to use it. im looking for some c++ library where there apis to solve fk/if analytically. thank you.\n", "tags": "inverse-kinematics c++ forward-kinematics", "id": "10567", "title": "is there any c++ library i could use to program a robotic manipulator involving forward and inverse kinematics?"}, {"body": "i have a particular example robot that interests me:\n\nhttp://www.scmp.com/tech/innovation/article/1829834/foxconns-foxbot-army-close-hitting-chinese-market-track-meet-30-cent\n\nsee first image, the bigger robot on the left, in particular the shoulder pitch joint that would support the most weight. i'm curious because i know it's really hard to balance strength and precision in these types of robots, and want to know how close a hobbyist could get.\n\nwould they be something similar to this: rotary tables w/ worm gears?\n\nhttp://www.velmex.com/products/rotary_tables/motorized-rotary-tables.html \n\nlooking for more actuation schemes to research, thanks!\n", "tags": "motor robotic-arm actuator torque", "id": "10568", "title": "what types of actuators do these industrial bots use?"}, {"body": "i have thought of a technique to increase the resolution of a pov (persistence of vision) display. in an usual pov display, the leds are arranged in a strip and spun in a circle. there are two limiting factors to increasing the radial resolution along the circumference of any one circular path that an led follows. one is, depending on the speed of the pov wheel, the minimum time required (decided by the microcontroller) to change the led's color in case of a rgb. the other factor is the led's width, that increases the 'bleeding' of color from one pixel to the neighboring pixel if the led changes color or brightness too fast. \n\nif one were to fix a slit in the front of an led, |*|  &lt;-- like so, would this help improve the resolution of the pov display; by doing this one would in effect be reducing the width of the 'pixel' along the circumference on which the led would be traversing.\n\nthus if one were to use a fast enough microcontroller and a narrow enough slit, one could probably obtain a very high resolution along one dimension at least.\n\nto be clear i've not yet implemented this, and am just looking for any experienced person who can tell if this will work or not.\n", "tags": "microcontroller electronics", "id": "10573", "title": "technique to increase pov resolution"}, {"body": "i am doing robotics project on raspberry pi and arduino. the arduino uno is connected to raspberry pi. i am using the raspberry pi in putty (ssh) now. \n\ni want to setup user interface for raspberry pi also most importantly i want to use arduino ide to work and load arduino sketch into system. how to do this?\n", "tags": "arduino raspberry-pi embedded-systems first-robotics linux", "id": "10580", "title": "how can i upload sketches to an arduino over a raspberry pi?"}, {"body": "how to understand the 2d laser scanner scanned data and use it in the implementation of ekf slam, if someone can provide an implementation of ekf slam in python with pseudo datasets.\n", "tags": "slam ekf first-robotics", "id": "10581", "title": "ekf slam 2d laser scanned datasets usage"}, {"body": "hey so i am trying to research into swarm robotics, and trying to find helpful information or even articles/papers to read on the process of setting up communication protocols between different robots. for instance using a lan connection, does each robot need to have a wireless adapter, and how would i begin setting up a network for say 5-10 smaller robots?\n\nmore generally could someone help me understand how devices connect and communicate across networks? i understand the basics of ip addressing, but i haven't researched into further complexities.\n\nany advice or direction is appreciated.\n", "tags": "wireless", "id": "10584", "title": "communication in swarm robotics"}]